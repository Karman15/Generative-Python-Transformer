{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "sequences_scores\n",
      "scores\n",
      "beam_indices\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "NEWLINECHAR = \"<N>\"\n",
    "\n",
    "def encode_newlines(inp):\n",
    "    return inp.replace(\"\\n\", NEWLINECHAR)\n",
    "\n",
    "def decode_newlines(inp):\n",
    "    return inp.replace(NEWLINECHAR, \"\\n\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('GPyT_1/GPyT_TOK_75GB')\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "})\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"GPyT_1/latest_model\").to(\"cpu\")\n",
    "\n",
    "inp = \"def __init__\"\n",
    "\n",
    "input_ids = tokenizer.encode(inp, return_tensors=\"pt\").to(\"cpu\")\n",
    "model_output = model.generate(\n",
    "    input_ids,\n",
    "    max_length = 100,\n",
    "    num_beams = 3,\n",
    "    temperature = 0.7,\n",
    "    no_repeat_ngram_size = 5,\n",
    "    num_return_sequence = 3,\n",
    "    return_dict_in_generate = True,\n",
    "    output_scores = True\n",
    ")\n",
    "\n",
    "for k in model_output:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def __init__ (self, pattern, flags=0):\n",
      "    \"\"\"The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.\"\"\"\n",
      "        super(Regex,self).__init__()\n",
      "\n",
      "        if isinstance(pattern, basestring):\n",
      "            if not pattern:\n",
      "                warnings.warn(\"null string\n"
     ]
    }
   ],
   "source": [
    "for seq in model_output['sequences']:\n",
    "    print(decode_newlines(tokenizer.decode(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(inp):\n",
    "    inp = encode_newlines(inp)\n",
    "    newline_count = inp.count(NEWLINECHAR)\n",
    "    input_ids = tokenizer.encode(inp, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    model_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length = 100,\n",
    "        num_beams = 5,\n",
    "        temperature = 0.7,\n",
    "        no_repeat_ngram_size = 5,\n",
    "        num_return_sequence = 3,\n",
    "        return_dict_in_generate = True,\n",
    "        output_scores = True\n",
    "    )\n",
    "\n",
    "    sequence =  model_output['sequences'][0] \n",
    "\n",
    "    print(20*\"-\")\n",
    "    decoded = decode_newlines(tokenizer.decode(sequence))\n",
    "    print(decoded)\n",
    "    print(20*\"-\")\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    auto_complete = \"\"\n",
    "    split = decoded.split('\\n')\n",
    "    #print(split)\n",
    "\n",
    "    for s in split[:newline_count+1]:\n",
    "        auto_complete += s + '\\n'\n",
    "    \n",
    "    return auto_complete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import torch\n",
      "from torch.nn import functional as F\n",
      "\n",
      "import torch.nn.functional as F\n",
      "import torchvision.transforms as transforms\n",
      "from torchvision.transforms import transforms\n",
      "\n",
      "\n",
      "def load_model(path):\n",
      "    model = torch.load(path)\n",
      "    model.load_state_dict(torch.load(\n",
      "--------------------\n",
      "\n",
      "\n",
      "Auto-completed:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ex_input = \"import matplotlib.pyplot\"\n",
    "\n",
    "ac = auto_complete(ex_input)\n",
    "print(\"Auto-completed:\\n\")\n",
    "print(ac)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad0acf4b485275a101ea0b01364b5d7c0c0e19d46b16b8bef581d038394fa94e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
