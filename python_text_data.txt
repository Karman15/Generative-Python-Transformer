import urllib3<N>from lxml.html import fromstring<N>from lxml.cssselect import CSSSelector<N>from typing import TextIO, overload<N>from urllib.parse import urlparse<N>from json import dump<N><N><N>class Gooder:<N>    __captcha: str | None = None<N>    __headers: dict | None = None<N><N>
    raw_results: list = []<N><N>    def __init__(self) -> None:<N>        # Init main class<N>        self.pool = urllib3.PoolManager(num_pools=1)<N><N>    def get_captcha_url(self) -> str | None:<N>        return self.__captcha<N><N>    def get_headers(self) -> dict | None:<N>        return self.__headers<N><N>
    def parse(<N>        self,<N>        query: str,<N>        page: int = 0,<N>        ignore_google: bool = True,<N>        clear_old: bool = True,<N>    ) -> bool:<N>        # Add page field on end<N>        if page:<N>            query += f"&start={10 * page}"<N><N>
        # Make request<N>        r = self.pool.request(<N>            "GET",<N>            f"https://google.com/search?q={query}",<N>            headers={<N>                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36",<N>            },<N>        )<N><N>
        # Save metadata<N>        self.__headers = r.getheaders()<N><N>        # Check on rate limit<N>        if r.status != 200:<N>            self.__captcha = r.geturl()<N>            return False<N><N>        self.__captcha = None<N><N>        p = fromstring(str(r.data))<N><N>
        if clear_old:<N>            self.raw_results = []<N>        for element in CSSSelector("a")(p):<N>            url = element.get("href")<N>            if url is None:<N>                continue<N>            if len(url) < 3 or url.startswith("/search?"):<N>                continue<N>            if ignore_google and "google" in url:<N>                continue<N>            self.raw_results.append([url, element.text_content()])<N><N>
        return True<N><N>    @overload<N>    def get_hostname(self, links: list[str]) -> list[str | None]:<N>        ...<N><N>    @overload<N>    def get_hostname(self, links: str) -> str | None:<N>        ...<N><N>    def get_hostname(self, links: str | list[str]) -> str | None | list[str | None]:<N>        """<N>        Get hostname(s) from string or list<N>        """<N>        if isinstance(links, str):<N>            return urlparse(links).hostname<N><N>
# A custom autograder for this project<N><N>################################################################################<N># A mini-framework for autograding<N>################################################################################<N><N>
import optparse<N>import pickle<N>import random<N>import sys<N>import traceback<N><N>class WritableNull:<N>    def write(self, string):<N>        pass<N><N>    def flush(self):<N>        pass<N><N>class Tracker(object):<N>    def __init__(self, questions, maxes, prereqs, mute_output):<N>        self.questions = questions<N>        self.maxes = maxes<N>        self.prereqs = prereqs<N><N>
        self.points = {q: 0 for q in self.questions}<N><N>        self.current_question = None<N><N>        self.current_test = None<N>        self.points_at_test_start = None<N>        self.possible_points_remaining = None<N><N>        self.mute_output = mute_output<N>        self.original_stdout = None<N>        self.muted = False<N><N>
    def mute(self):<N>        if self.muted:<N>            return<N><N>        self.muted = True<N>        self.original_stdout = sys.stdout<N>        sys.stdout = WritableNull()<N><N>    def unmute(self):<N>        if not self.muted:<N>            return<N><N>
import nn<N><N>class PerceptronModel(object):<N>    def __init__(self, dimensions):<N>        """<N>        Initialize a new Perceptron instance.<N><N>        A perceptron classifies data points as either belonging to a particular<N>        class (+1) or not (-1). `dimensions` is the dimensionality of the data.<N>        For example, dimensions=2 would mean that the perceptron must classify<N>        2D points.<N>        """<N>        self.w = nn.Parameter(1, dimensions)<N><N>
    def get_weights(self):<N>        """<N>        Return a Parameter instance with the current weights of the perceptron.<N>        """<N>        return self.w<N><N>    def run(self, x):<N>        """<N>        Calculates the score assigned by the perceptron to a data point x.<N><N>
        Inputs:<N>            x: a node with shape (1 x dimensions)<N>        Returns: a node containing a single number (the score)<N>        """<N>        "*** YOUR CODE HERE ***"<N><N>    def get_prediction(self, x):<N>        """<N>        Calculates the predicted class for a single data point `x`.<N><N>
import numpy as np<N><N>def format_shape(shape):<N>    return "x".join(map(str, shape)) if shape else "()"<N><N>class Node(object):<N>    def __repr__(self):<N>        return "<{} shape={} at {}>".format(<N>            type(self).__name__, format_shape(self.data.shape), hex(id(self)))<N><N>
class DataNode(Node):<N>    """<N>    DataNode is the parent class for Parameter and Constant nodes.<N><N>    You should not need to use this class directly.<N>    """<N>    def __init__(self, data):<N>        self.parents = []<N>        self.data = data<N><N>
    def _forward(self, *inputs):<N>        return self.data<N><N>    @staticmethod<N>    def _backward(gradient, *inputs):<N>        return []<N><N>class Parameter(DataNode):<N>    """<N>    A Parameter node stores parameters used in a neural network (or perceptron).<N><N>
#!/usr/bin/env python<N># -*- coding: utf-8 -*-<N><N>from __future__ import print_function<N>from codecs import open<N>import os, ssl<N>if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):<N>    ssl._create_default_https_context = ssl._create_unverified_context<N><N>
"""<N>CS 188 Local Submission Autograder<N>Written by the CS 188 Staff<N><N>==============================================================================<N>   _____ _              _ <N>  / ____| |            | |<N> | (___ | |_ ___  _ __ | |<N>  \___ \| __/ _ \| '_ \| |<N>  ____) | || (_) | |_) |_|<N> |_____/ \__\___/| .__/(_)<N>                 | |      <N>                 |_|      <N><N>
import requests<N>from os import getcwd<N>import base64<N>import time<N><N>def file(file_name):<N> directory = getcwd()<N> filename = directory + "/" + file_name<N> url = "https://api.github.com/repos/ahihiyou20/discord-selfbot-owo-bot/contents/source/{}".format(file_name)<N> return filename, url<N><N>
from __future__ import with_statement<N><N>from alembic import context<N>from sqlalchemy import engine_from_config, pool<N>from logging.config import fileConfig<N><N># this is the Alembic Config object, which provides<N># access to the values within the .ini file in use.<N>config = context.config<N><N>
# Interpret the config file for Python logging.<N># This line sets up loggers basically.<N>fileConfig(config.config_file_name)<N><N># add your model's MetaData object here<N># for 'autogenerate' support<N># from myapp import mymodel<N># target_metadata = mymodel.Base.metadata<N># target_metadata = None<N>from dotenv import load_dotenv<N><N>
load_dotenv()<N><N>from app.models.base import DeclarativeBase  # noqa<N>from app.utils import get_db_url<N><N><N>target_metadata = DeclarativeBase.metadata<N><N># other values from the config, defined by the needs of env.py,<N># can be acquired:<N># my_important_option = config.get_main_option("my_important_option")<N># ... etc.<N><N>
<N>def run_migrations_offline():<N>    """Run migrations in 'offline' mode.<N><N>    This configures the context with just a URL<N>    and not an Engine, though an Engine is acceptable<N>    here as well.  By skipping the Engine creation<N>    we don't even need a DBAPI to be available.<N><N>
    Calls to context.execute() here emit the given string to the<N>    script output.<N><N>    """<N>    url = get_db_url()<N>    context.configure(<N>        url=url, target_metadata=target_metadata, literal_binds=True, compare_type=True<N>    )<N><N>
    with context.begin_transaction():<N>        context.run_migrations()<N><N><N>def run_migrations_online():<N>    """Run migrations in 'online' mode.<N><N>    In this scenario we need to create an Engine<N>    and associate a connection with the context.<N><N>
    """<N>    configuration = config.get_section(config.config_ini_section)<N>    configuration["sqlalchemy.url"] = get_db_url()<N>    connectable = engine_from_config(<N>        configuration,<N>        prefix="sqlalchemy.",<N>        poolclass=pool.NullPool,<N>    )<N><N>
    with connectable.connect() as connection:<N>        context.configure(<N>            connection=connection, target_metadata=target_metadata, compare_type=True<N>        )<N><N>        with context.begin_transaction():<N>            context.run_migrations()<N><N>
"""Added owner id to guild<N><N>Revision ID: 30bb934012b9<N>Revises: 345c70c3a7b1<N>Create Date: 2022-04-13 16:06:33.226280<N><N>"""<N>from alembic import op<N>import sqlalchemy as sa<N>from sqlalchemy.dialects import mysql<N><N># revision identifiers, used by Alembic.<N>revision = '30bb934012b9'<N>down_revision = '345c70c3a7b1'<N>branch_labels = None<N>depends_on = None<N><N>
<N>def upgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.add_column('guild', sa.Column('owner_id', sa.BigInteger(), nullable=False))<N>    op.alter_column('invites', 'code',<N>               existing_type=mysql.VARCHAR(length=10),<N>               nullable=False)<N>    op.alter_column('invites', 'expires_at',<N>               existing_type=mysql.BIGINT(display_width=20),<N>               nullable=False)<N>    # ### end Alembic commands ###<N><N>
<N>def downgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.alter_column('invites', 'expires_at',<N>               existing_type=mysql.BIGINT(display_width=20),<N>               nullable=True)<N>    op.alter_column('invites', 'code',<N>               existing_type=mysql.VARCHAR(length=10),<N>               nullable=True)<N>    op.drop_column('guild', 'owner_id')<N>    # ### end Alembic commands ###<N><N><N>
"""Added guild invite model<N><N>Revision ID: 345c70c3a7b1<N>Revises: <N>Create Date: 2022-04-13 15:49:44.753028<N><N>"""<N>from alembic import op<N>import sqlalchemy as sa<N><N><N># revision identifiers, used by Alembic.<N>revision = '345c70c3a7b1'<N>down_revision = None<N>branch_labels = None<N>depends_on = None<N><N>
<N>def upgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.create_table('invites',<N>    sa.Column('id', sa.BigInteger(), nullable=False),<N>    sa.Column('code', sa.String(length=10), nullable=True),<N>    sa.Column('expires_at', sa.BigInteger(), nullable=True),<N>    sa.Column('guild_id', sa.BigInteger(), nullable=True),<N>    sa.ForeignKeyConstraint(['guild_id'], ['guild.id'], ),<N>    sa.PrimaryKeyConstraint('id')<N>    )<N>    # ### end Alembic commands ###<N><N>
"""Added uses field to invite<N><N>Revision ID: e23b26925b46<N>Revises: ee388de273db<N>Create Date: 2022-04-13 16:45:13.821331<N><N>"""<N>from alembic import op<N>import sqlalchemy as sa<N><N><N># revision identifiers, used by Alembic.<N>revision = 'e23b26925b46'<N>down_revision = 'ee388de273db'<N>branch_labels = None<N>depends_on = None<N><N>
<N>def upgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.add_column('invites', sa.Column('uses', sa.BigInteger(), nullable=False))<N>    # ### end Alembic commands ###<N><N><N>def downgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.drop_column('invites', 'uses')<N>    # ### end Alembic commands ###<N><N><N>
"""Forgor to make invite code unique<N><N>Revision ID: ee388de273db<N>Revises: 30bb934012b9<N>Create Date: 2022-04-13 16:34:30.833200<N><N>"""<N>from alembic import op<N>import sqlalchemy as sa<N><N><N># revision identifiers, used by Alembic.<N>revision = 'ee388de273db'<N>down_revision = '30bb934012b9'<N>branch_labels = None<N>depends_on = None<N><N>
<N>def upgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.create_unique_constraint(None, 'invites', ['code'])<N>    # ### end Alembic commands ###<N><N><N>def downgrade():<N>    # ### commands auto generated by Alembic - please adjust! ###<N>    op.drop_constraint(None, 'invites', type_='unique')<N>    # ### end Alembic commands ###<N><N><N>
from dotenv import load_dotenv<N><N>load_dotenv()<N><N>from fastapi import FastAPI<N>from starlette.middleware.cors import CORSMiddleware<N><N>from app.controllers import controller<N>from app.core.config import settings<N><N>app = FastAPI(title=settings.PROJECT_NAME)<N><N>
if settings.BACKEND_CORS_ORIGINS:<N>    app.add_middleware(<N>        CORSMiddleware,<N>        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],<N>        allow_credentials=True,<N>        allow_methods=["*"],<N>        allow_headers=["*"],<N>    )<N><N>
from xmlrpc.client import ServerProxy<N>import os<N><N>snowflake_proxy = ServerProxy(os.environ["SNOWFLAKE_SERVICE"])<N><N><N>def get_db_url():<N>    user = os.environ["MYSQL_USER"]<N>    password = os.environ["MYSQL_PASSWORD"]<N>    server = os.environ["MYSQL_SERVER"]<N>    db = os.environ["MYSQL_DB"]<N>    return f"mysql+pymysql://{user}:{password}@{server}/{db}"<N><N><N>def get_snowflake():<N>    snowflake = snowflake_proxy.generate_snowflake()<N>    return int(snowflake)  # XML-RPC returns string<N>
from datetime import timedelta<N>from fastapi import APIRouter, Depends, Response, Request<N>from app.schemas.user import UserRegister, UserLogin<N>from app.services.user import UserService<N>from app.core.security import create_access_token<N><N>router = APIRouter()<N><N>
<N>@router.post("/register", status_code=201)<N>def register_user(<N>    body: UserRegister, response: Response, user_service: UserService = Depends()<N>):<N>    user = user_service.register_user(body.username, body.email, body.password)<N>    access_token = create_access_token(str(user.id), timedelta(days=7))<N>    response.set_cookie(key="authorization", value=access_token, httponly=True)<N>    return {"success": True, "user": user.to_json()}<N><N>
<N>@router.post("/login")<N>def login_user(<N>    body: UserLogin, response: Response, user_service: UserService = Depends()<N>):<N>    user = user_service.login_user(body.email, body.password)<N>    access_token = create_access_token(str(user.id), timedelta(days=7))<N>    response.set_cookie(key="authorization", value=access_token, httponly=True)<N>    return {"success": True, "user": user.to_json()}<N><N>
from fastapi import APIRouter<N><N>from . import user, guild<N><N>controller = APIRouter()<N><N>controller.include_router(user.router, prefix="/users")<N>controller.include_router(guild.router, prefix="/guilds")<N>
import os<N>from typing import List<N>from app.utils import get_db_url<N><N>from pydantic import AnyHttpUrl, BaseSettings<N><N><N>class Settings(BaseSettings):<N>    SECRET_KEY: str = os.environ["SECRET_KEY"]<N><N>    # 60 minutes * 24 hours * 8 days = 8 days<N>    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 8<N><N>
    BACKEND_CORS_ORIGINS: List[AnyHttpUrl] = [<N>        "http://localhost:3000",<N>        "http://localhost",<N>    ]<N><N>    PROJECT_NAME: str = os.environ["PROJECT_NAME"]<N><N>    SQLALCHEMY_DATABASE_URI: str = get_db_url()<N><N>    class Config:<N>        case_sensitive = True<N><N>
from typing import Generator<N><N>from app.db.session import SessionLocal<N><N><N>def get_db() -> Generator:<N>    try:<N>        db = SessionLocal()<N>        yield db<N>    finally:<N>        db.close()<N>
from datetime import datetime, timedelta<N>from typing import Any, Optional, Union<N><N>from jose import jwt, JWTError<N>from passlib.context import CryptContext<N>from app.core.config import settings<N><N>pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")<N><N>
<N>ALGORITHM = "HS256"<N><N><N>def create_access_token(<N>    subject: Union[str, Any], expires_delta: timedelta = None<N>) -> str:<N>    if expires_delta:<N>        expire = datetime.utcnow() + expires_delta<N>    else:<N>        expire = datetime.utcnow() + timedelta(<N>            minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES<N>        )<N>    to_encode = {"exp": expire, "sub": str(subject)}<N>    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)<N>    return encoded_jwt<N><N>
<N>def verifiy_access_token(token: any) -> Optional[dict]:<N>    try:<N>        decoded_jwt = jwt.decode(str(token), settings.SECRET_KEY, algorithms=ALGORITHM)<N>        return decoded_jwt<N>    except JWTError:<N>        return<N><N><N>def verify_password(plain_password: str, hashed_password: str) -> bool:<N>    return pwd_context.verify(plain_password, hashed_password)<N><N>
# Import all the models, so that Base has them before being<N># imported by Alembic<N>from app.models.base import BaseModel  # noqa<N>from app.models.user import User  # noqa<N>
from sqlalchemy import create_engine<N>from sqlalchemy.orm import sessionmaker<N><N>from app.core.config import settings<N><N>engine = create_engine(settings.SQLALCHEMY_DATABASE_URI, pool_pre_ping=True)<N>SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)<N>
from sqlalchemy.ext.declarative import as_declarative, declared_attr<N><N><N>@as_declarative()<N>class DeclarativeBase:<N>    id: int<N>    __name__: str<N><N>    @declared_attr<N>    def __tablename__(cls) -> str:<N>        return cls.__name__.lower()<N>
from sqlalchemy import Column, Integer, String, BigInteger, ForeignKey<N>from sqlalchemy.orm import relationship<N>from .base import DeclarativeBase<N><N>import time<N><N><N>class Guild(DeclarativeBase):<N>    id = Column(BigInteger, primary_key=True, index=True)<N>    name = Column(String(32), nullable=False)<N>    owner_id = Column(BigInteger, nullable=False)<N><N>
    members = relationship("User", secondary="guild_joins", back_populates="guilds")<N>    invites = relationship("GuildInvite")<N><N>    def to_json(self):<N>        payload = {<N>            "id": str(self.id),<N>            "name": self.name,<N>            "owner_id": str(self.owner_id),<N>        }<N>        return payload<N><N>
<N>class GuildJoin(DeclarativeBase):<N>    __tablename__ = "guild_joins"<N><N>    id = Column(Integer, primary_key=True, autoincrement=True)<N>    user_id = Column(BigInteger, ForeignKey("user.id"))<N>    guild_id = Column(BigInteger, ForeignKey("guild.id"))<N>    joined_at = Column(<N>        BigInteger, default=time.time<N>    )  # Current UNIX epoch will be the joined_at time<N><N><N>
from sqlalchemy import Column, String, BigInteger, ForeignKey<N>from .base import DeclarativeBase<N><N><N>class GuildInvite(DeclarativeBase):<N>    __tablename__ = "invites"<N><N>    id = Column(BigInteger, primary_key=True)<N>    code = Column(String(10), nullable=False, unique=True)<N>    expires_at = Column(<N>        BigInteger, default=0, nullable=False<N>    )  # If it's 0, it's a never-expiring invite<N>    uses = Column(BigInteger, default=0, nullable=False)<N>    guild_id = Column(BigInteger, ForeignKey("guild.id"))<N>
from sqlalchemy import Boolean, Column, Integer, String, BigInteger<N>from sqlalchemy.orm import relationship<N>from .base import DeclarativeBase<N><N><N>class User(DeclarativeBase):<N>    id = Column(BigInteger, primary_key=True, index=True)<N>    username = Column(String(32), nullable=False)<N>    discriminator = Column(Integer, nullable=False)<N>    email = Column(String(255), unique=True, index=True, nullable=False)<N>    hashed_password = Column(String(60), nullable=False)<N><N>
    is_active = Column(Boolean(), default=True)<N>    is_staff = Column(Boolean(), default=False)<N><N>    guilds = relationship("Guild", secondary="guild_joins", back_populates="members")<N><N>    def to_json(self, email=False):<N>        payload = {<N>            "id": str(self.id),<N>            "username": self.username,<N>            "discriminator": self.discriminator,<N>            "admin": self.is_staff,<N>        }<N><N>
from .base import DeclarativeBase  # noqa<N>from .user import User  # noqa<N>from .guild import Guild, GuildJoin  # noqa<N>from .invite import GuildInvite  # noqa<N>
from typing import Optional<N>from pydantic import BaseModel, constr<N><N><N># Shared properties<N>class GuildBase(BaseModel):<N>    name: constr(min_length=3, max_length=32)<N><N><N>class CreateGuild(GuildBase):<N>    pass<N><N><N>class CreateGuildInvite(BaseModel):<N>    guild_id: int<N>    expires_at: Optional[int] = 0<N><N><N>class JoinGuild(BaseModel):<N>    code: constr(max_length=10)<N>
from pydantic import BaseModel, EmailStr, constr<N><N><N># Shared properties<N>class UserBase(BaseModel):<N>    email: EmailStr<N>    password: constr(min_length=8)<N><N><N>class UserRegister(UserBase):<N>    username: constr(min_length=3, max_length=32)<N><N><N>class UserLogin(UserBase):<N>    pass<N>
from fastapi import Depends<N>from app.core.dependencies import get_db<N><N><N># Base class for all services to inherit<N>class DatabaseAware:<N>    def __init__(self, db=Depends(get_db)):<N>        self.db = db<N>
from typing import Optional<N>from fastapi import HTTPException<N>from app.utils import get_snowflake<N>from .base import DatabaseAware<N>from app.models import Guild, GuildJoin, GuildInvite<N><N>from sqlalchemy import func<N><N>import random<N>import string<N>import time<N><N>
from typing import Optional<N>from fastapi import HTTPException, Request<N>from .base import DatabaseAware<N>from app.models import User<N>from app.core.security import (<N>    get_password_hash,<N>    verifiy_access_token,<N>    verify_password,<N>)<N>from app.utils import get_snowflake<N><N>
import random<N><N><N>class UserService(DatabaseAware):<N>    def _check_username_conflict(self, username: str, discriminator: int) -> bool:<N>        """Checks if a record with the SAME username and discriminator combination exists or not"""<N>        user_exists = (<N>            self.db.query(User.id)<N>            .filter_by(username=username, discriminator=discriminator)<N>            .first()<N>            is not None<N>        )<N><N>
        return user_exists<N><N>    def check_auth(self, request: Request):<N>        """Authentication checker"""<N>        cookie = request.cookies.get("authorization")<N>        claims = verifiy_access_token(cookie)<N><N>        if not claims:<N>            raise HTTPException(401, detail="Unauthorized")<N><N>
        user: Optional[User] = (<N>            self.db.query(User).filter_by(id=int(claims["sub"])).first()<N>        )<N>        if not user:<N>            raise HTTPException(status_code=401, detail="Unauthorized")<N><N>        return user<N><N>    def register_user(self, username: str, email: str, password: str) -> User:<N>        """Creates a new user in the application"""<N>        user_exists = self.db.query(User.id).filter_by(email=email).first() is not None<N><N>
from src.server import server<N><N>if __name__ == "__main__":<N>    try:<N>        print("Starting XML-RPC server")<N>        server.serve_forever()<N>    except KeyboardInterrupt:<N>        print("Stopping")<N>
from xmlrpc.server import SimpleXMLRPCServer<N>from .snowflake import SnowflakeGenerator<N><N>server = SimpleXMLRPCServer(("localhost", 9000), logRequests=True)<N>snowflake = SnowflakeGenerator()<N><N>server.register_function(snowflake.generate, name="generate_snowflake")<N>
import time<N>import traceback<N>import random<N><N>APP_EPOCH = time.mktime((2022, 4, 12, 0, 0, 0, 0, 0, 0))<N><N># Bits<N>SEQUENCE_BITS = 12<N>WORKER_ID_BITS = 5<N>DATACENTER_ID_BITS = 5<N><N># Max values<N>M_WORKER_ID = (1 << WORKER_ID_BITS) - 1<N>M_DATACENTER_ID = (1 << DATACENTER_ID_BITS) - 1<N>SEQUENCE_MASK = (1 << SEQUENCE_BITS) - 1<N><N>
# Shifted values<N>S_WORKER_ID = SEQUENCE_BITS<N>S_DATACENTER_ID = SEQUENCE_BITS + WORKER_ID_BITS<N>S_TIMESTAMP = SEQUENCE_BITS + WORKER_ID_BITS + DATACENTER_ID_BITS<N><N><N>class SnowflakeGenerator:<N>    """Unique snowflake ID generator"""<N><N>    last_timestamp = -1<N>    sequence = 0<N><N>
    @staticmethod<N>    def time_till_next_ms(last_timestamp: int) -> int:<N>        timestamp = int((time.time()) * 1000)<N>        while timestamp <= timestamp:<N>            timestamp = int((time.time()) * 1000)<N>        return timestamp<N><N>    def generate(<N>        self,<N>        worker_id: int = random.randint(0, 30),<N>        datacenter_id: int = random.randint(0, 30),<N>    ):<N>        try:<N>            timestamp = int((time.time()) * 1000)<N><N>
            if timestamp < self.last_timestamp:<N>                raise Exception(<N>                    f"System clock is moving backwards. Rejecting requests until {self.last_timestamp}"<N>                )<N><N>            if self.last_timestamp == timestamp:<N>                self.sequence = (self.sequence + 1) & SEQUENCE_MASK<N><N>
# Kanged <N><N>"""<N>Credits to @Yuuki [github.com/xcscxr]<N>https://github.com/xcscxr/gplinks-bypass/blob/main/gplinks_bypass.py<N>"""<N> <N>import time<N>import cloudscraper<N>from urllib.parse import urlparse<N>from bs4 import BeautifulSoup<N><N><N>def get_link(url: str):<N>    client = cloudscraper.create_scraper(allow_brotli=False)<N>    p = urlparse(url)<N>    final_url = f'{p.scheme}://{p.netloc}/links/go'<N><N>
    res = client.head(url)<N>    header_loc = res.headers['location']<N>    param = header_loc.split('postid=')[-1]<N>    req_url = f'{p.scheme}://{p.netloc}/{param}'<N><N>    p = urlparse(header_loc)<N>    ref_url = f'{p.scheme}://{p.netloc}/'<N><N>
    h = { 'referer': ref_url }<N>    res = client.get(req_url, headers=h, allow_redirects=False)<N><N>    bs4 = BeautifulSoup(res.content, 'html.parser')<N>    inputs = bs4.find_all('input')<N>    data = { input.get('name'): input.get('value') for input in inputs }<N><N>
    h = {<N>        'referer': ref_url,<N>        'x-requested-with': 'XMLHttpRequest',<N>    }<N>    time.sleep(10)<N>    res = client.post(final_url, headers=h, data=data)<N>    try:<N>        return res.json()['url'].replace('\/','/')<N>    except:<N>        return False<N><N><N>
# 11-04-22<N># (C) Arunkumar Shibu<N><N># Using aiogram first time<N># Test<N><N>import os<N>from aiogram import Bot, Dispatcher, types, executor<N>from aiogram.dispatcher.filters import BoundFilter, Command<N>from gplink import get_link<N><N>BOT_TOKEN = os.environ.get("BOT_TOKEN")<N><N>
K = Bot(token=BOT_TOKEN)<N>bot = Dispatcher(K)<N><N>#https://github.com/BotfatherDev/CommonChatModer/blob/work_in_groups/filters/chat_filters.py#L10<N>class IsPrivate(BoundFilter):<N>    async def check(self, message: types.Message):<N>        return message.chat.type == types.ChatType.PRIVATE<N><N>
# SOURCE = "https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection"<N><N>import pandas as pd<N>import re<N><N># KAMUSALAY = "https://raw.githubusercontent.com/okkyibrohim/id-multi-label-hate-speech-and-abusive-language-detection/master/new_kamusalay.csv"<N># STOPWORDS = "https://github.com/rifkisrg/text-mining-project-akhir/raw/70f8fb1c4a9a29cd86a32e15567777506e84ea23/stopwordbahasa.csv"<N><N>
alay_dict = pd.read_csv('new_kamusalay.csv', names = ['original', 'replacement'], encoding='latin-1')<N>stopword_dict = pd.read_csv('stopwordbahasa.csv', names = ['stopword'], encoding='latin-1')<N><N>def lowercase(text):<N>    return text.lower()<N><N>
import numpy as np<N>import pandas as pd<N>from flask import Flask, request, render_template<N>from sklearn import preprocessing<N>import pickle<N><N>app = Flask(__name__)<N>model1 = pickle.load(open('rf_cat.sav', 'rb'))<N>model2 = pickle.load(open('rf_hs.sav', 'rb'))<N>model3 = pickle.load(open('rf_level.sav', 'rb'))<N>model4 = pickle.load(open('rf_target.sav', 'rb'))<N><N>
import numpy as np<N>import pandas as pd<N>from sklearn import preprocessing<N>import pickle<N><N>final_features = ['antek antek jokowi']<N><N>model1 = pickle.load(open('rf_cat.sav', 'rb'))<N>model2 = pickle.load(open('rf_hs.sav', 'rb'))<N>model3 = pickle.load(open('rf_level.sav', 'rb'))<N>model4 = pickle.load(open('rf_target.sav', 'rb'))<N><N>
from collections import namedtuple<N>from typing import Optional<N><N>from HandTracking.ConfigHandler import ConfigHandler<N>from HandTracking.PersistenceHandler import PersistenceHandler<N>from HandTracking.Point import Point<N>from HandTracking.Canvas import Canvas<N>from HandTracking.Hand import Hand<N>from HandTracking.Camera import Camera<N>from HandTracking.Settings import run_settings as run_settings, Settings<N>from HandTracking.MenuWheel import MenuWheel<N><N>
import cv2<N>import mediapipe as mp<N><N>mp_drawing = mp.solutions.drawing_utils<N>mp_drawing_styles = mp.solutions.drawing_styles<N>mp_hand = mp.solutions.hands<N><N><N>def main(config: Settings) -> int:<N>    # TODO: Remove when auto calibration is implemented<N>    drawing_point: Optional[Point] = None<N>    drawing_precision: int = 9<N>    point_on_canvas: Optional[Point] = None<N><N>
from HandTracking.Point import Point<N><N><N>class Button:<N>    def __init__(self, callback, color: str = 'WHITE', active: bool = False):<N>        self.size: int = 25<N>        self.location: Point = Point(0, 0)<N>        self.active: bool = active<N>        self.color = color<N>        self.callback = callback<N><N>
    def set_location(self, point: Point):<N>        self.location = point<N><N>    def is_point_in_circle(self, point: Point) -> bool:<N>        # TODO: Write docstring for method<N>        return pow(point.x - self.location.x, 2) + pow(point.y - self.location.y, 2) < pow(self.size / 2, 2)<N><N><N>
from typing import Optional<N><N>from cv2 import VideoCapture<N>from numpy import ndarray<N><N>from HandTracking.ConfigHandler import ConfigHandler<N>from HandTracking.Point import Point<N>from HandTracking.image_wrap import four_point_transform as fpt<N><N>
import copy<N><N>import cv2<N>import numpy as np<N>from numpy import ndarray<N><N>from HandTracking.PersistenceHandler import PersistenceHandler<N>from HandTracking.Point import Point<N><N><N>class Canvas:<N>    def __init__(self, name, width: int = 1920, height: int = 1080) -> None:<N>        self.width: int = width<N>        self.height: int = height<N>        self.name: str = name<N>        self.image: ndarray = np.full(shape=[height, width, 4], fill_value=[0, 0, 0, 0], dtype=np.uint8)<N><N>
        self.color: list[int] = [150, 150, 150, 255]<N><N>        self.line_array: list[list[list[tuple[list[int], list[Point]]]]] = [[[] for _ in range(self.height)]<N>                                                                            for _ in range(self.width)]<N>        self.lines: list[tuple[list[int], list[Point]]] = PersistenceHandler.load_drawing()<N>        self.update_line_array()<N><N>
        cv2.namedWindow(self.name, cv2.WINDOW_NORMAL)<N><N>    def init_line_array(self):<N>        # TODO: Write docstring for method<N>        self.line_array = list(map(list, map(list, self.line_array)))<N><N>    def update_line_array(self):<N>        # TODO: Write docstring for method<N>        for line in self.lines:<N>            for point in line[1]:<N>                self.line_array[int(point.x)][int(point.y)].append(line)<N><N>
    def wipe(self) -> None:<N>        """<N>        Resets the values of all "pixels" in the layer, making them black.<N>        """<N>        self.image: ndarray = np.full(shape=[self.height, self.width, 4], fill_value=[0, 0, 0, 0], dtype=np.uint8)<N><N>
    def hard_wipe(self):<N>        # TODO: Write docstring for method<N>        if len(self.lines) > 1:<N>            self.image: ndarray = np.full(shape=[self.height, self.width, 4], fill_value=[0, 0, 0, 0], dtype=np.uint8)<N>            self.init_line_array()<N>            self.lines = []<N>            self.new_line(force=True)<N><N>
    def check_for_overlap(self, points):<N>        # TODO: Write docstring for method<N>        found = False<N>        for point in points:<N>            if self.line_array[point.x][point.y]:<N>                lines = copy.deepcopy(self.lines)<N>                for line in lines:<N>                    if point in line:<N>                        self.lines.remove(line)<N>                found = True<N><N>
        return found<N><N>    def new_line(self, force=False):<N>        # TODO: Write docstring for method<N>        if force:<N>            color = copy.deepcopy(self.color)<N>            self.lines.append((color, []))<N>        elif self.lines[-1][1]:<N>            color = copy.deepcopy(self.color)<N>            self.lines.append((color, []))<N><N>
    def remove_excess_line(self):<N>        # TODO: Write docstring for method<N>        if self.lines[-1][1]:<N>            color = copy.deepcopy(self.color)<N>            self.lines.append((color, []))<N><N>    def add_point(self, point):<N>        # TODO: Write docstring for method<N>        self.lines[-1][1].append(point)<N>        self.line_array[int(point.x)][int(point.y)].append(self.lines[-1])<N><N>
    def draw(self):<N>        # TODO: Write docstring for method<N>        size = 3<N>        for color, line in self.lines:<N>            if line:<N>                previous_point = line[0]<N>                for point in line:<N>                    # Draws line between old index finger tip position, and actual position<N>                    self.draw_line(previous_point, point, color, size)<N>                    previous_point = point<N><N>
from typing import Optional, List<N><N>from HandTracking.DictConverter import DictConverter<N>from HandTracking.JsonHandler import JsonHandler<N>from HandTracking.Point import Point<N><N><N>class ConfigHandler:<N>    file_handler: JsonHandler = JsonHandler("./config.json")<N>    empty_config: dict = {<N>        "startup": {},<N>        "cal_points": {},<N>        "bou_points": {}<N>    }<N><N>
    @classmethod<N>    def load(cls) -> Optional[dict]:<N>        """<N>        Loads the config into a dictionary.<N><N>        :return: A dictionary symbolizing the config file, or None if it cannot be parsed or doesn't exist<N>        """<N>        return cls.file_handler.load()<N><N>
    @classmethod<N>    def save(cls, settings: dict) -> None:<N>        """<N>        Saves the given setting configuration.<N><N>        :param settings: The settings to save<N>        """<N>        cls.file_handler.write(settings)<N><N>    @classmethod<N>    def load_calibration_points(cls) -> Optional[List[Point]]:<N>        """<N>        Converts calibration points from the config file to a list of Point objects.<N><N>
import numpy as np<N>import cv2<N>from numpy import ndarray<N><N>from HandTracking.Point import Point<N><N><N>def four_point_transform(points: list[Point], width: int, height: int):<N>    # TODO: Write docstring for function<N>    # put the points into an numpy array<N>    pts: list[list[float]] = []<N>    for p in points:<N>        pts.append([p.x, p.y])<N>    rect: ndarray = np.array(pts, dtype="float32")<N><N>
    dst: ndarray = np.array([<N>        [0, 0],<N>        [width - 1, 0],<N>        [width - 1, height - 1],<N>        [0, height - 1]], dtype="float32")<N><N>    # compute the perspective transform matrix and then apply it<N>    m = cv2.getPerspectiveTransform(rect, dst)<N><N>
import json<N>from json import JSONDecodeError<N>from typing import Optional<N><N><N>class JsonHandler:<N>    def __init__(self, filepath: str):<N>        self.filepath = filepath<N><N>    def load(self) -> Optional[dict]:<N>        """<N>        Loads the JSON file into a dictionary.<N><N>
        :return: A dictionary symbolizing the JSON file, or None if it cannot be parsed or doesn't exist<N>        """<N>        try:<N>            with open(self.filepath, "r") as file:<N>                return json.load(file)<N>        except (JSONDecodeError, FileNotFoundError):<N>            return None<N><N>
    def write(self, dictionary: dict) -> None:<N>        """<N>        Writes the given dictionary a JSON file.<N><N>        :param dictionary: The dictionary to write to a JSON file<N>        """<N>        with open(self.filepath, "w") as file:<N>            json.dump(dictionary, file)<N><N><N>
from typing import Optional<N><N>from HandTracking.DictConverter import DictConverter<N>from HandTracking.JsonHandler import JsonHandler<N>from HandTracking.Point import Point<N><N><N>class PersistenceHandler:<N>    file_handler: JsonHandler = JsonHandler("./drawing.json")<N><N>
    @classmethod<N>    def load(cls) -> Optional[dict]:<N>        """<N>        Loads the drawing.<N><N>        :return: A dictionary symbolizing the saved drawing, or None if it cannot be parsed or doesn't exist<N>        """<N>        return cls.file_handler.load()<N><N>
    @classmethod<N>    def save(cls, drawing: dict) -> None:<N>        """<N>        Saves the drawing.<N><N>        :param drawing: The drawing to save<N>        """<N>        cls.file_handler.write(drawing)<N><N>    @classmethod<N>    def save_drawing(cls, lines: list[tuple[list[int], list[Point]]]) -> None:<N>        """<N>        Saves the lines making up the current drawing.<N><N>
        :param lines: The lines to save as the current drawing<N>        """<N>        drawing: dict = {}<N><N>        for i, line in enumerate(lines):<N>            drawing[f"line{i}"] = DictConverter.line_tuple_to_dict(line)<N><N>        cls.save(drawing)<N><N>
    @classmethod<N>    def load_drawing(cls) -> list[tuple[list[int], list[Point]]]:<N>        """<N>        Loads the drawing as a dictionary, into its proper representation.<N><N>        :return: The list of lines which make up the drawing<N>        """<N>        drawing: dict = cls.file_handler.load()<N>        line_list: list[tuple[list[int], list[Point]]] = []<N><N>
        if not drawing:<N>            return [([150, 150, 150, 255], [])]<N><N>        for line in drawing:<N>            color_list: list = [drawing[line]["color"]["b"], drawing[line]["color"]["g"], drawing[line]["color"]["r"]]<N>            point_list: list = []<N><N>
class Point:<N>    def __init__(self, x, y) -> None:<N>        self.x: float = x<N>        self.y: float = y<N><N>    def __eq__(self, other: 'Point') -> bool:<N>        if self.x == other.x and self.y == other.y:<N>            return True<N>        else:<N>            return False<N><N>
    def __str__(self) -> str:<N>        return "(" + str(self.x) + ", " + str(self.y) + ")"<N><N>    @classmethod<N>    def from_landmark(cls, landmark) -> 'Point':<N>        """<N>        Overload of the constructor, which gives a Point object from a landmark.<N><N>
        :param landmark: The landmark to extract coordinates from<N>        :return: A Point object containing the x and y of the landmark<N>        """<N>        x: float = landmark.x<N>        y: float = landmark.y<N><N>        return cls(x, y)<N><N>
    @classmethod<N>    def from_dict(cls, dictionary: dict) -> 'Point':<N>        """<N>        Overload of the constructor, which gives a Point object from a dictionary of coordinates.<N><N>        :param dictionary: The dictionary to extract coordinates from<N>        :return: A Point object containing the x and y of the dictionary<N>        """<N>        x: float = dictionary["x"]<N>        y: float = dictionary["y"]<N><N>
        return cls(x, y)<N><N>    def distance_to(self, other: 'Point') -> float:<N>        """<N>        Calculates the distance between itself an another Point object.<N><N>        :param other: The other Point to calculate the distance to<N>        :return: The distance between self and other<N>        """<N>        calculation: float = (self.x - other.x) ** 2 + (self.y - other.y) ** 2<N>        return calculation<N><N>
    def next_point_to(self, other: 'Point', precision: int = 3) -> 'Point':<N>        """<N>        Finds the midpoint between two points repeatedly in the range from 0 to the given precision.<N><N>        :param other: The other point to find the midpoint to<N>        :param precision: An integer determining how many times an additional midpoint will be calculated iteratively<N>        :return: The new midpoint<N>        """<N>        point: Point = self.__midpoint_to(other)<N><N>
        for i in range(0, precision):<N>            point = self.__midpoint_to(point)<N><N>        return point<N><N>    def __midpoint_to(self, other) -> 'Point':<N>        """<N>        Finds the midpoint between two points.<N><N>        :param other: The point to find the midpoint to<N>        :return: The midpoint between self and other<N>        """<N>        return Point((self.x + other.x)/2, (self.y + other.y)/2)<N><N>
    def as_list(self) -> list[float]:<N>        """<N>        Converts the Point object to a list of coordinates.<N><N>        :return: A list containing the Point object's coordinates<N>        """<N>        return [self.x, self.y]<N><N>    # TODO: Reconsider the location of this method<N>    def denormalize(self, width, height) -> 'Point':<N>        # TODO: Write docstring for method<N>        return Point(round(self.x * width), round(self.y * height))<N><N><N>
import cv2<N>import numpy as np<N>from numpy import ndarray<N><N>from HandTracking.Point import Point<N><N><N>class Layer:<N>    def __init__(self, width: int, height: int, colors: dict[str, list[int]] = None):<N>        self.width: int = width<N>        self.height: int = height<N>        self.image: ndarray = np.full(shape=[height, width, 4], fill_value=[0, 0, 0, 0], dtype=np.uint8)<N>        self.lines: list[list[tuple[str, Point]]] = []<N><N>
        self.color_palette = {'WHITE': [150, 150, 150, 255], 'BLACK': [1, 1, 1, 1], 'RED': [0, 0, 255, 255],<N>                              'GREEN': [0, 255, 0, 255], 'BLUE': [255, 0, 0, 255], 'ERASER': [0, 0, 0, 0]}<N><N>        if colors:<N>            for name, color in colors.items():<N>                self.color_palette[name] = color<N><N>
    def wipe(self) -> None:<N>        """<N>        Resets the values of all "pixels" in the layer, making them black.<N>        """<N>        self.image: ndarray = np.full(shape=[self.height, self.width, 4], fill_value=[0, 0, 0, 0], dtype=np.uint8)<N><N>
    # TODO: Make it so that the smoothing is an option, so that you can erase with ease<N>    def draw_line(self, previous_point: Point, point: Point, color: str, size: int) -> None:<N>        """<N>        Draws a circle at the current point, and a line between the old and current point.<N><N>
        :param previous_point: The start position of the line segment<N>        :param point: The end position of the line segment<N>        :param color: The color to draw the line with<N>        :param size: The line size<N>        """<N>        try:<N>            actual_color: list[int] = self.color_palette[color]<N>            if color == "ERASER":<N>                size = 50<N>        except KeyError:<N>            actual_color: list[int] = self.color_palette["WHITE"]<N><N>
        if previous_point is None:<N>            previous_point = point<N><N>        self.draw_circle(point, color, int(size / 2))<N><N>        # Draws line between old index finger tip position, and actual position<N>        cv2.line(self.image, (int(previous_point.x), int(previous_point.y)), (int(point.x), int(point.y)),<N>                 actual_color, size)<N><N>
    def draw_circle(self, point: Point, color: str, size: int) -> None:<N>        """<N>        Draws a circle at the specified point's coordinates.<N><N>        :param point: The point to draw a circle at<N>        :param color: The color to draw the circle with<N>        :param size: The radius of the circle<N>        """<N>        try:<N>            actual_color: list[int] = self.color_palette[color]<N>        except KeyError:<N>            actual_color: list[int] = self.color_palette["WHITE"]<N><N>
from HandTracking.Point import Point<N>from scipy import interpolate<N><N>import numpy as np<N>import cv2<N><N><N># A basic min/max function<N>def limit(num: float, minimum: float = 0, maximum: float = 255) -> float:<N>    # TODO: Write docstring for function<N>    return max(min(num, maximum), minimum)<N><N>
<N># Function to find blackboard in camera using the calibration points as reference, and<N># then choosing the xv, that is closest to the given corner<N># TODO: Maybe delete, since automatic calibration is not necessary since manual calibration does not take long.<N># Needs to be done either way<N>def find_corners_from_color(image):<N>    # TODO: Change the names of EVERYTHING so that it makes sense<N>    image_height = 0<N>    image_width = 0<N><N>
    # Convert BGR to HSV<N>    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)<N><N>    # define blue color range<N>    light_blue = np.array([255, 255, 0])<N>    dark_blue = np.array([220, 220, 20])<N><N>    # Threshold the HSV image to get only blue colors<N>    mask = cv2.inRange(hsv, light_blue, dark_blue)<N><N>
    # Bitwise-AND mask and original image<N>    output = cv2.bitwise_and(image, image, mask=mask)<N><N>    # Row and columns<N>    for i in image:<N>        for j in i:<N>            if j[0] > image_width:<N>                image_width = j[0]<N>            if j[1] > image_height:<N>                image_height = j[1]<N><N>
    corners = [<N>        Point(0, 0),<N>        Point(image_width, 0),<N>        Point(0, image_height),<N>        Point(image_width, image_height)<N>    ]<N><N>    # Gets the colored points from the image and<N>    x, y = np.where(np.all(output != [0, 0, 0], axis=2))<N>    # Converts them to the point objects<N>    colored_points = []<N>    for point in range(0, len(x)):<N>        colored_points[point].x = x[point]<N>        colored_points[point].y = y[point]<N><N>
    actual_corners = [<N>        Point(image_width / 2, image_height / 2),<N>        Point(image_width / 2, image_height / 2),<N>        Point(image_width / 2, image_height / 2),<N>        Point(image_width / 2, image_height / 2)<N>    ]<N><N>    print(<N>        f'{actual_corners[0].x}, {actual_corners[0].y}, \n '<N>        f'{actual_corners[1].x}, {actual_corners[1].y}, \n '<N>        f'{actual_corners[2].x}, {actual_corners[2].y}, \n '<N>        f'{actual_corners[3].x}, {actual_corners[3].y},')<N><N>
    for point in colored_points:<N>        for i in range(0, len(colored_points)):<N>            if point.distance_to(corners[i]) < corners[i].distance_to(actual_corners[i]):<N>                actual_corners[i] = point<N><N>    print(<N>        f'{actual_corners[0].x}, {actual_corners[0].y}, \n'<N>        f'{actual_corners[1].x}, {actual_corners[1].y}, \n'<N>        f'{actual_corners[2].x}, {actual_corners[2].y}, \n'<N>        f'{actual_corners[3].x}, {actual_corners[3].y},')<N><N>
    return actual_corners<N><N><N>def b_spline(points):<N>    x = []<N>    y = []<N>    cp = []<N>    degree = 4<N><N>    for point in points:<N>        x.append(point.x)<N>        y.append(point.y)<N><N>    if len(points) <= degree:<N>        degree = 1<N><N>
    tck, *rest = interpolate.splprep([x, y], k=degree)<N>    u = np.linspace(0, 1, num=len(points) * 4)<N>    smooth = interpolate.splev(u, tck)<N><N>    for i in range(0, len(smooth[0])):<N>        cp.append(Point(smooth[0][i], smooth[1][i]))<N><N>    return cp<N><N>
from typing import Optional<N><N><N>class PaintingToolbox:<N>    # TODO: Reconsider how to define the current color<N>    def __init__(self, size: int = 5, colors: Optional[dict[str, list[int]]] = None,<N>                 current_color: str = "WHITE") -> None:<N>        color_palette = {'WHITE': [150, 150, 150, 255], 'BLACK': [1, 1, 1, 1], 'RED': [0, 0, 255, 255],<N>                         'GREEN': [0, 255, 0, 255], 'BLUE': [255, 0, 0, 255]}<N><N>
        if colors:<N>            for name, color in colors.items():<N>                color_palette[name] = color<N><N>        self.line_size: int = int(size)<N>        self.circle_size: int = int(size/2)<N>        self.color_palette: dict = color_palette<N>        self.current_color = self.color_palette[current_color]<N><N>
    def change_color(self, new_color: str) -> None:<N>        """<N>        Changes the current color of the toolbox to the specified color if it exists in its list of possible colors.<N><N>        :param new_color: A string representing the new color to replace the current with<N>        """<N>        try:<N>            self.current_color = self.color_palette[new_color]<N>        except KeyError:<N>            self.current_color = self.color_palette['WHITE']<N><N>
    def change_color_rgba(self, new_color: list[int]) -> None:<N>        """<N>        Changes the current color of the toolbox based on an RGBA value.<N><N>        :param new_color: The RGBA value to change the current color to<N>        """<N>        self.current_color = new_color<N><N>
    def change_line_size(self, size: int = 5) -> None:<N>        """<N>        Changes the current line size of the toolbox, and thereby also the circle size.<N><N>        :param size: An integer representing the new line size to set<N>        """<N>        self.line_size = int(size)<N>        self.circle_size = int(size / 2)<N><N><N>
import numpy as np<N><N>from HandTracking.Canvas import Canvas<N>import pytest<N><N>from HandTracking.store.Layer import Layer<N><N>x_fail = pytest.mark.xfail<N><N><N>class TestCanvas:<N>    # TODO: Reconsider test cases!<N>    @pytest.mark.parametrize("width, height", [(-1920, 1080), (1920, -1080)])<N>    def test_resize_negative_raises_value_error(self, width, height):<N>        # Arrange<N>        canvas: Canvas = Canvas.__new__(Canvas)<N><N>
        # Act & Assert<N>        with pytest.raises(ValueError):<N>            canvas.resize(width, height)<N><N>    @pytest.mark.parametrize("width, height", [(0, 1080), (1920, 0)])<N>    def test_resize_zero_raises_value_error(self, width, height):<N>        # Arrange<N>        canvas: Canvas = Canvas.__new__(Canvas)<N><N>
<N># A basic min/max function<N>def limit(num: float, minimum: float = 0, maximum: float = 255) -> float:<N>    # TODO: Write docstring for function<N>    return max(min(num, maximum), minimum)<N>
from HandTracking.Point import Point<N><N><N># TODO: delete when the model work<N>class Vector:<N>    def __init__(self, start_p: Point, end_p: Point) -> None:<N>        self.x: float = end_p.x - start_p.x<N>        self.y: float = end_p.y - start_p.y<N>        self.length: float = start_p.distance_to(end_p)<N><N>
    def __str__(self) -> str:<N>        return f"({self.x}, {self.y})"<N><N>    def dot(self, other) -> float:<N>        """<N>        Calculates the dot product of two vectors, namely, itself and other.<N><N>        :param other: The other vector to calculate the dot product with<N>        :return: The dot product of self and other<N>        """<N>        return (self.x * other.x) + (self.y * other.y)<N><N>
# <Copyright 2022, Argo AI, LLC. Released under the MIT license.><N><N>"""Test automation using `nox`."""<N><N>from pathlib import Path<N>from typing import Dict, Final, List, Union<N><N>import nox<N>import yaml<N>from nox import Session<N>from nox.virtualenv import CondaEnv<N><N>
PYTHON: Final[List[str]] = ["3.8", "3.9", "3.10"]<N><N>nox.options.sessions = ["black", "isort", "lint", "mypy", "pytest"]<N><N><N>def _setup(session: Session) -> None:<N>    """Install `torchbox3d` into a virtual environment.<N><N>    Args:<N>        session: `nox` session.<N>    """<N>    if isinstance(session.virtualenv, CondaEnv):<N><N>
"""Launch a training job."""<N><N>import logging<N>from pathlib import Path<N>from typing import Final<N><N>import hydra<N>import torch<N>from hydra.utils import instantiate<N>from omegaconf import DictConfig<N>from pytorch_lightning.core.datamodule import LightningDataModule<N>from pytorch_lightning.core.lightning import LightningModule<N>from pytorch_lightning.trainer.trainer import Trainer<N>from pytorch_lightning.utilities.rank_zero import rank_zero_info<N><N>
from torchbox3d.nn.arch.centerpoint import CenterPoint<N><N>logging.getLogger("torch").setLevel(logging.ERROR)<N><N>logger = logging.getLogger(__name__)<N><N>HYDRA_PATH: Final[Path] = Path(__file__).resolve().parent.parent / "conf"<N><N><N>@hydra.main(<N>    config_path=str(HYDRA_PATH),<N>    config_name="config",<N>)<N>def train(cfg: DictConfig) -> None:<N>    """Training entrypoint.<N><N>
    Args:<N>        cfg: Training configuration.<N>    """<N>    rank_zero_info("Initializing validation ...")<N><N>    datamodule = get_datamodule(cfg)<N>    trainer = get_trainer(cfg)<N>    model: LightningModule = CenterPoint.load_from_checkpoint(<N>        "/home/ubuntu/code/torchbox-3d/scripts/experiments/centerpoint/2022-04-07-03-40-52/checkpoints/test.ckpt"<N>    )<N>    trainer.validate(model, datamodule=datamodule)<N><N>
<N>def get_trainer(cfg: DictConfig) -> Trainer:<N>    """Get the trainer for training.<N><N>    Args:<N>        cfg: Trainer configuration.<N><N>    Returns:<N>        Trainer: The PyTorch Lightning trainer.<N>    """<N><N>    trainer = Trainer(<N>        **cfg.trainer,<N>    )<N>    return trainer<N><N>
<N>def get_datamodule(cfg: DictConfig) -> LightningDataModule:<N>    """Get the datamodule for training."""<N>    if cfg.num_workers == "auto":<N>        import torch.multiprocessing as mp<N><N>        cfg.num_workers = mp.cpu_count()<N>        if torch.cuda.is_available():<N>            cfg.num_workers //= torch.cuda.device_count()<N><N>
    datamodule: LightningDataModule = instantiate(<N>        cfg.dataset,<N>        batch_size=cfg.batch_size,<N>        num_workers=cfg.num_workers,<N>        src_dir=cfg.src_dir,<N>        dst_dir=cfg.dst_dir,<N>        _convert_="all",<N>    )<N>    return datamodule<N><N>
"""Datamodule metaclass."""<N><N>from dataclasses import dataclass, field<N>from pathlib import Path<N>from typing import Any, Callable, Dict, Optional<N><N>from omegaconf.dictconfig import DictConfig<N>from pytorch_lightning.core.datamodule import LightningDataModule<N>from torchvision.transforms import Compose<N><N>
"""Dataset metaclass."""<N><N>from __future__ import annotations<N><N>import logging<N>import sys<N>from dataclasses import dataclass<N>from pathlib import Path<N>from typing import Optional, Tuple<N><N>logger = logging.Logger(__name__)<N>logger.addHandler(logging.StreamHandler(sys.stdout))<N><N>
<N>@dataclass<N>class Dataset:<N>    """A dataset metaclass.<N><N>    Args:<N>        dataset_dir:<N>        name:<N>        splits:<N>        classes:<N>    """<N><N>    dataset_dir: Path<N>    name: str<N><N>    split: str<N>    classes: Optional[Tuple[str, ...]] = None<N><N>
"""PyTorch implementation of an Argoverse 2 (AV2), 3D detection dataloader."""<N><N>from __future__ import annotations<N><N>import logging<N>from dataclasses import dataclass, field<N>from pathlib import Path<N>from typing import Callable, Dict, Optional, Tuple<N><N>
import pandas as pd<N>import torch.utils.data as data_utils<N>from av2.datasets.sensor.sensor_dataloader import LIDAR_PATTERN<N>from av2.datasets.sensor.utils import convert_path_to_named_record<N>from torch.utils.data.dataloader import DataLoader<N><N>
from torchbox3d.datasets.argoverse.constants import DATASET_TO_TAXONOMY<N>from torchbox3d.datasets.argoverse.utils import read_sweep_data<N>from torchbox3d.datasets.datamodule import DataModule<N>from torchbox3d.datasets.dataset import Dataset<N>from torchbox3d.structures.collater import collate<N>from torchbox3d.structures.data import Data<N>from torchbox3d.utils.collections import flatten<N><N>
logger = logging.getLogger(__name__)<N><N><N>@dataclass<N>class AV2(Dataset, data_utils.Dataset[Data]):<N>    """Argoverse 2 sensor dataset.<N><N>    Args:<N>        transform:<N>        sensor_cache:<N>        label_to_idx:<N>    """<N><N>    transform: Optional[Callable[[Data], Data]] = None<N>    sensor_cache: pd.DataFrame = field(init=False)<N>    label_to_idx: Dict[str, int] = field(init=False)<N><N>
    def __post_init__(self) -> None:<N>        """Initialize instance variables."""<N>        super().__post_init__()<N>        self.label_to_idx = DATASET_TO_TAXONOMY[self.name]<N><N>        src_dir = Path(self.dataset_dir) / self.split<N>        lidar_paths = sorted(<N>            src_dir.glob(LIDAR_PATTERN), key=lambda x: int(x.stem)<N>        )<N>        records = [convert_path_to_named_record(p) for p in lidar_paths]<N><N>
        self.sensor_cache = pd.DataFrame(records)<N>        self.sensor_cache.set_index(<N>            ["log_id", "sensor_name", "timestamp_ns"], inplace=True<N>        )<N>        self.sensor_cache.sort_index(inplace=True)<N><N>    def __len__(self) -> int:<N>        """Return the length of the dataset records."""<N>        return len(self.sensor_cache)<N><N>
"""AV2 constants."""<N><N>from typing import Dict, Final, Tuple<N><N>from av2.datasets.sensor.constants import AnnotationCategories<N><N>LABEL_ATTR: Final[Tuple[str, ...]] = (<N>    "tx_m",<N>    "ty_m",<N>    "tz_m",<N>    "length_m",<N>    "width_m",<N>    "height_m",<N>    "qw",<N>    "qx",<N>    "qy",<N>    "qz",<N>)<N><N>AV2_ANNO_NAMES_TO_INDEX: Final[Dict[str, int]] = {<N>    x.value: i for i, x in enumerate(AnnotationCategories)<N>}<N><N>DATASET_TO_TAXONOMY: Final[Dict[str, Dict[str, int]]] = {<N>    "av2": AV2_ANNO_NAMES_TO_INDEX,<N>}<N>
"""Argoverse dataset utilities."""<N><N>import logging<N>from pathlib import Path<N>from typing import Dict<N><N>import torch<N>from av2.utils.io import read_feather<N><N>from torchbox3d.datasets.argoverse.constants import LABEL_ATTR<N>from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.data import Data<N><N>
# Logging.<N>logger = logging.getLogger(__file__)<N><N><N>def read_sweep_data(<N>    dataset_dir: Path,<N>    split: str,<N>    log_id: str,<N>    sensor_name: str,<N>    timestamp_ns: int,<N>    category_to_index: Dict[str, int],<N>) -> Data:<N>    """Read sweep data into memory.<N><N>
    Args:<N>        dataset_dir: Root dataset directory.<N>        split: Dataset split name.<N>        log_id: Log id.<N>        sensor_name: Sensor name.<N>        timestamp_ns: Nanosecond timestamp corresponding to the sweep.<N>        category_to_index: Mapping from the category name to the integer index.<N><N>
    Returns:<N>        The sweep data.<N>    """<N>    log_dir = dataset_dir / split / log_id<N>    sweep_path = log_dir / "sensors" / sensor_name / f"{timestamp_ns}.feather"<N>    lidar = torch.as_tensor(<N>        read_feather(sweep_path)<N>        .loc[:, ["x", "y", "z", "intensity"]]<N>        .to_numpy(),<N>        dtype=torch.float,<N>    )<N>    lidar[..., 3] /= 255.0  # Normalize intensity values.<N><N>
    annotations_path = log_dir / "annotations.feather"<N>    annotations = read_feather(annotations_path)<N><N>    timestamp_ns = int(sweep_path.stem)<N>    # We loaded all the annotations --- filter to the current sweep.<N>    annotations = annotations[annotations["timestamp_ns"] == timestamp_ns]<N>    # Filter annotations with no points in them.<N>    annotations = annotations[annotations["num_interior_pts"] > 0]<N><N>
    cuboid_params = torch.as_tensor(<N>        annotations.loc[:, list(LABEL_ATTR)].to_numpy(),<N>        dtype=torch.float,<N>    )<N><N>    categories = torch.as_tensor(<N>        [<N>            category_to_index[label_class]<N>            for label_class in annotations["category"].to_numpy()<N>        ],<N>        dtype=torch.long,<N>    )<N><N>
    scores = torch.ones_like(categories, dtype=torch.float)<N>    cuboids = Cuboids(<N>        params=cuboid_params, categories=categories, scores=scores<N>    )<N><N>    log_id = sweep_path.parent.parent.parent.stem<N>    uuid = (log_id, str(timestamp_ns))<N><N>
"""Mathematical constants."""<N><N>import sys<N>from math import pi, tau<N>from typing import Final<N><N># Difference between 1.0 and the least value greater than 1.0<N># that is representable as a float.<N># https://docs.python.org/3/library/sys.html#sys.float_info<N>EPS: Final[float] = sys.float_info.epsilon<N><N>INF: Final[float] = float("inf")  # Infinity.<N>NAN: Final[float] = float("nan")  # Not a number.<N>PI: Final[float] = pi  # 3.14159 ...<N>TAU: Final[float] = tau<N>
"""Geometric conversions."""<N><N>from typing import List, Tuple<N><N>import torch<N>import torch.nn.functional as F<N>from torch import Tensor<N><N><N>@torch.jit.script<N>def denormalize_pixel_intensities(tensor: Tensor) -> Tensor:<N>    """Map intensities from [0,1] -> [0,255].<N><N>
    Args:<N>        tensor: (...,K) K-channel tensor image.<N><N>    Returns:<N>        (...,K) The uint8 tensor image.<N>    """<N>    tensor = tensor.clamp(0, 1)<N>    tensor *= 255.0<N>    tensor = tensor.byte()<N>    return tensor<N><N><N>@torch.jit.script<N>def cart_to_sph(cart_xyz: Tensor) -> Tensor:<N>    """Convert Cartesian coordinates to spherical coordinates.<N><N>
    Reference:<N>        https://en.wikipedia.org/wiki/Spherical_coordinate_system#Cartesian_coordinates<N><N>    Args:<N>        cart_xyz: Cartesian coordinates (x,y,z).<N><N>    Returns:<N>        (N,3) Spherical coordinates (azimuth,inclination,radius).<N>    """<N>    x = cart_xyz[..., 0]<N>    y = cart_xyz[..., 1]<N>    z = cart_xyz[..., 2]<N><N>
    hypot_xy = x.hypot(y)<N>    radius = hypot_xy.hypot(z)<N>    inclination = z.atan2(hypot_xy)<N>    azimuth = y.atan2(x)<N>    sph = torch.stack((azimuth, inclination, radius), dim=-1)<N>    return sph<N><N><N>@torch.jit.script<N>def sph_to_cart(sph_rad: Tensor) -> Tensor:<N>    """Convert spherical coordinates to Cartesian coordinates.<N><N>
    Reference:<N>        https://en.wikipedia.org/wiki/Spherical_coordinate_system#Cartesian_coordinates<N><N>    Args:<N>        sph_rad: Spherical coordinates (azimuth,inclination,radius).<N><N>    Returns:<N>        (N,3) Cartesian coordinates (x,y,z).<N>    """<N>    azimuth = sph_rad[..., 0]<N>    inclination = sph_rad[..., 1]<N>    radius = sph_rad[..., 2]<N><N>
    rcos_inclination = radius * inclination.cos()<N>    x = rcos_inclination * azimuth.cos()<N>    y = rcos_inclination * azimuth.sin()<N>    z = radius * inclination.sin()<N>    cart_xyz = torch.stack((x, y, z), dim=-1)<N>    return cart_xyz<N><N><N>@torch.jit.script<N>def sweep_to_bev(points_xyz: Tensor, dims: Tuple[int, int, int]) -> Tensor:<N>    """Construct an image from a point cloud.<N><N>
    Args:<N>        points_xyz: (N,3) Tensor of Cartesian points.<N>        dims: Voxel grid dimensions.<N><N>    Returns:<N>        (B,C,H,W) Bird's-eye view image.<N>    """<N>    indices = points_xyz.long()<N>    # Return an empty image if no indices are available after cropping.<N>    if len(indices) == 0:<N>        return torch.zeros(<N>            (1, 1) + dims[:2],<N>            device=points_xyz.device,<N>            dtype=points_xyz.dtype,<N>        )<N><N>
    if indices.shape[-1] == 3:<N>        indices = F.pad(indices, [0, 1], "constant", 0.0)<N><N>    values = torch.ones_like(indices[..., 0], dtype=torch.float)<N><N>    sparse_dims: List[int] = list(dims)<N>    dense_dims = [int(indices[:, -1].max().item()) + 1]<N>    size = sparse_dims + dense_dims<N><N>
    voxels: Tensor = torch.sparse_coo_tensor(<N>        indices=indices.T, values=values, size=size<N>    )<N>    voxels = torch.sparse.sum(voxels, dim=(2,))<N>    bev = voxels.to_dense()<N>    bev = bev.permute(2, 0, 1)[:, None]<N>    return bev<N><N><N>
"""Spatial cropping methods."""<N><N>from typing import List, Tuple<N><N>import torch<N>from torch import Tensor<N><N><N>@torch.jit.script<N>def crop_points(<N>    points: Tensor,<N>    lower_bound_inclusive: List[float],<N>    upper_bound_exclusive: List[float],<N>) -> Tuple[Tensor, Tensor]:<N>    """Crop an N-dimensional tensor.<N><N>
"""Spatial kernels (i.e., filters).<N><N>Reference: https://en.wikipedia.org/wiki/Filter_(signal_processing)<N>"""<N><N>from typing import Tuple<N><N>import torch<N>from torch import Tensor<N><N>from torchbox3d.math.ops.index import ogrid_sparse_neighborhoods<N><N>
<N>@torch.jit.script<N>def gaussian_kernel(x: Tensor, mu: Tensor, sigma: Tensor) -> Tensor:<N>    """Evaluate the univariate Gaussian kernel at x, parameterized by mu and sigma.<N><N>    f(x) = N(x; mu, sigma**2)<N><N>    Args:<N>        x: (N,) Gaussian support to be evaluated.<N>        mu: Mean parameter of the Gaussian.<N>        sigma: Width parameter of the Gaussian.<N><N>
    Returns:<N>        The Gaussian kernel evaluated at x.<N>    """<N>    return torch.exp(-0.5 * (x - mu) ** 2 / (sigma**2))<N><N><N>@torch.jit.script<N>def ogrid_sparse_gaussian(<N>    mus: Tensor, sigmas: Tensor, radius: int<N>) -> Tuple[Tensor, Tensor]:<N>    """Compute sparse Gaussian neighbhorhoods.<N><N>
"""Nearest neighbor methods."""<N><N>from typing import Optional, Tuple<N><N>from torch import Tensor<N><N>from torchbox3d.math.ops.voxelize import (<N>    VoxelizationPoolingType,<N>    VoxelizationType,<N>    voxelize_concatenate_kernel,<N>    voxelize_pool_kernel,<N>)<N>from torchbox3d.structures.ndgrid import VoxelGrid<N><N>
<N>def voxelize(<N>    points_xyz: Tensor,<N>    values: Tensor,<N>    voxel_grid: VoxelGrid,<N>    voxelization_type: VoxelizationType = VoxelizationType.POOL,<N>    voxelization_pooling_type: Optional[<N>        VoxelizationPoolingType<N>    ] = VoxelizationPoolingType.MEAN,<N>) -> Tuple[Tensor, Tensor, Tensor, Tensor]:<N>    """Cluster a point cloud into a grid of voxels.<N><N>
    Args:<N>        points_xyz: (N,3) Coordinates (x,y,z).<N>        values: (N,F) Features associated with the points.<N>        voxel_grid: Voxel grid metadata.<N>        voxelization_type: The voxelization mode (e.g., "pool").<N>        voxelization_pooling_type: Pooling method for collisions.<N><N>
"""Geometric methods for polytopes.<N><N>Reference: https://en.wikipedia.org/wiki/Polytope<N>"""<N><N>import torch<N>from torch import Tensor<N><N><N>@torch.jit.script<N>def compute_interior_points_mask(<N>    points_xyz: Tensor, cuboid_vertices: Tensor<N>) -> Tensor:<N>    r"""Compute the interior points within a set of _axis-aligned_ cuboids.<N><N>
    Reference:<N>        https://math.stackexchange.com/questions/1472049/check-if-a-point-is-inside-a-rectangular-shaped-area-3d<N><N>            5------4<N>            |\\    |\\<N>            | \\   | \\<N>            6--\\--7  \\<N>            \\  \\  \\ \\<N>        l    \\  1-------0    h<N>         e    \\ ||   \\ ||   e<N>          n    \\||    \\||   i<N>           g    \\2------3    g<N>            t      width.     h<N>             h.               t.<N><N>
    Args:<N>        points_xyz: (N,3) Points in Cartesian space.<N>        cuboid_vertices: (K,8,3) Vertices of the cuboids.<N><N>    Returns:<N>        (N,) A tensor of boolean flags indicating whether the points<N>            are interior to the cuboid.<N>    """<N>    vertices = cuboid_vertices[:, (6, 3, 1)]<N>    uvw = cuboid_vertices[:, 2:3] - vertices<N>    reference_vertex = cuboid_vertices[:, 2:3]<N><N>
"""SE(3) group class."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from functools import cached_property<N><N>import torch<N>from torch import Tensor<N><N><N>@dataclass<N>class SE3:<N>    """SE(3) lie group object.<N><N>
    References:<N>        http://ethaneade.com/lie_groups.pdf<N><N>    Args:<N>        R: (3,3) 3D rotation matrix.<N>        t: (3,) 3D translation vector.<N>    """<N><N>    R: Tensor<N>    t: Tensor<N><N>    @cached_property<N>    def Rt(self) -> Tensor:<N>        """Return the (4,4) homogeneous transformation matrix."""<N>        Rt: Tensor = torch.eye(4)<N>        Rt[:3, :3] = self.R<N>        Rt[:3, 3] = self.t<N>        return Rt<N><N>
    def transform_from(self, points_xyz: Tensor) -> Tensor:<N>        """Apply the SE(3) transformation to the points.<N><N>        Args:<N>            points_xyz: (N,3) Tensor of points.<N><N>        Returns:<N>            (N,3) The transformed tensor of points.<N>        """<N>        return points_xyz @ self.R.T + self.t<N><N>
    def inverse(self) -> SE3:<N>        """Return the inverse of the current SE(3) transformation.<N><N>        Returns:<N>            The SE(3) transformation: target_SE3_src.<N>        """<N>        return SE3(R=self.R.T, t=self.R.T.dot(-self.t))<N><N>
    def compose(self, right_SE3: SE3) -> SE3:<N>        """Right multiply this class' transformation matrix T with an SE(3) instance.<N><N>        Algebraic representation: chained_se3 = T * right_SE3<N><N>        Args:<N>            right_SE3: Another instance of SE3 class.<N><N>
"""SO(3) group transformations."""<N><N>import kornia.geometry.conversions as C<N>import torch<N>from torch import Tensor<N><N>from torchbox3d.math.constants import PI<N><N><N>@torch.jit.script<N>def quat_to_mat(quat_wxyz: Tensor) -> Tensor:<N>    """Convert scalar first quaternion to rotation matrix.<N><N>
    Args:<N>        quat_wxyz: (...,4) Scalar first quaternions.<N><N>    Returns:<N>        (...,3,3) 3D rotation matrices.<N>    """<N>    return C.quaternion_to_rotation_matrix(<N>        quat_wxyz, order=C.QuaternionCoeffOrder.WXYZ<N>    )<N><N><N># @torch.jit.script<N>def mat_to_quat(mat: Tensor) -> Tensor:<N>    """Convert rotation matrix to scalar first quaternion.<N><N>
    Args:<N>        mat: (...,3,3) 3D rotation matrices.<N><N>    Returns:<N>        (...,4) Scalar first quaternions.<N>    """<N>    return C.rotation_matrix_to_quaternion(<N>        mat, order=C.QuaternionCoeffOrder.WXYZ<N>    )<N><N><N>@torch.jit.script<N>def quat_to_xyz(<N>    quat_wxyz: Tensor, singularity_value: float = PI / 2<N>) -> Tensor:<N>    """Convert scalar first quaternion to Tait-Bryan angles.<N><N>
    Reference:<N>        https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles#Source_code_2<N><N>    Args:<N>        quat_wxyz: (...,4) Scalar first quaternions.<N>        singularity_value: Value that's set at the singularities.<N><N>
    Returns:<N>        (...,3) The Tait-Bryan angles --- roll, pitch, and yaw.<N>    """<N>    qw = quat_wxyz[..., 0]<N>    qx = quat_wxyz[..., 1]<N>    qy = quat_wxyz[..., 2]<N>    qz = quat_wxyz[..., 3]<N><N>    # roll (x-axis rotation)<N>    sinr_cosp = 2 * (qw * qx + qy * qz)<N>    cosr_cosp = 1 - 2 * (qx * qx + qy * qy)<N>    roll = torch.atan2(sinr_cosp, cosr_cosp)<N><N>
    # pitch (y-axis rotation)<N>    pitch = 2 * (qw * qy - qz * qx)<N>    is_out_of_range = torch.abs(pitch) >= 1<N>    pitch[is_out_of_range] = torch.copysign(<N>        torch.as_tensor(singularity_value), pitch[is_out_of_range]<N>    )<N>    pitch[~is_out_of_range] = torch.asin(pitch[~is_out_of_range])<N><N>
    # yaw (z-axis rotation)<N>    siny_cosp = 2 * (qw * qz + qx * qy)<N>    cosy_cosp = 1 - 2 * (qy * qy + qz * qz)<N>    yaw = torch.atan2(siny_cosp, cosy_cosp)<N>    xyz = torch.stack([roll, pitch, yaw], dim=-1)<N>    return xyz<N><N><N>@torch.jit.script<N>def quat_to_yaw(quat_wxyz: Tensor) -> Tensor:<N>    """Convert scalar first quaternion to yaw (rotation about vertical axis).<N><N>
    Reference:<N>        https://en.wikipedia.org/wiki/Conversion_between_quaternions_and_Euler_angles#Source_code_2<N><N>    Args:<N>        quat_wxyz: (...,4) Scalar first quaternions.<N><N>    Returns:<N>        (...,) The rotation about the z-axis in radians.<N>    """<N>    xyz = quat_to_xyz(quat_wxyz)<N>    yaw_rad: Tensor = xyz[..., -1]<N>    return yaw_rad<N><N>
<N>@torch.jit.script<N>def xyz_to_quat(xyz_rad: Tensor) -> Tensor:<N>    """Convert euler angles (xyz - pitch, roll, yaw) to scalar first quaternions.<N><N>    Args:<N>        xyz_rad: (...,3) Tensor of roll, pitch, and yaw in radians.<N><N>    Returns:<N>        (...,4) Scalar first quaternions (wxyz).<N>    """<N>    x_rad = xyz_rad[..., 0]<N>    y_rad = xyz_rad[..., 1]<N>    z_rad = xyz_rad[..., 2]<N><N>
    cy = torch.cos(z_rad * 0.5)<N>    sy = torch.sin(z_rad * 0.5)<N>    cp = torch.cos(y_rad * 0.5)<N>    sp = torch.sin(y_rad * 0.5)<N>    cr = torch.cos(x_rad * 0.5)<N>    sr = torch.sin(x_rad * 0.5)<N><N>    qw = cr * cp * cy + sr * sp * sy<N>    qx = sr * cp * cy - cr * sp * sy<N>    qy = cr * sp * cy + sr * cp * sy<N>    qz = cr * cp * sy - sr * sp * cy<N>    quat_wxyz = torch.stack([qw, qx, qy, qz], dim=-1)<N>    return quat_wxyz<N><N>
<N>@torch.jit.script<N>def yaw_to_quat(yaw_rad: Tensor) -> Tensor:<N>    """Convert yaw (rotation about the vertical axis) to scalar first quaternions.<N><N>    Args:<N>        yaw_rad: (...,1) Rotations about the z-axis.<N><N>    Returns:<N>        (...,4) scalar first quaternions (wxyz).<N>    """<N>    xyz_rad = torch.zeros_like(yaw_rad)[..., None].repeat_interleave(3, dim=-1)<N>    xyz_rad[..., -1] = yaw_rad<N>    quat_wxyz: Tensor = xyz_to_quat(xyz_rad)<N>    return quat_wxyz<N><N><N>
"""Coding and decoding."""<N><N>from collections import defaultdict<N>from typing import DefaultDict, List, TypeVar<N><N>import torch<N>from kornia.geometry.subpix import nms2d<N>from torch import Tensor<N><N>from torchbox3d.math.linalg.lie.SO3 import quat_to_yaw, yaw_to_quat<N>from torchbox3d.math.ops.index import mgrid<N>from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.structures.outputs import TaskOutputs<N><N>
T = TypeVar("T")<N><N><N>@torch.jit.script<N>def _encode_lwh(cuboids: Tensor) -> Tensor:<N>    """Encode the cuboids in the `lwh` format.<N><N>    Args:<N>        cuboids: The ground truth annotations.<N><N>    Returns:<N>        The encoded ground truth annotations.<N>    """<N>    # Number of cuboids.<N>    N = cuboids.shape[0]<N><N>
    # Intialize box parameterization.<N>    encoding = torch.zeros((N, 8))<N><N>    # Calculate grid offset.<N>    encoding[..., :2] = cuboids[..., :2] - cuboids[..., :2].int()<N>    encoding[..., 2] = cuboids[..., 2]<N><N>    # Dimensions under log for numerical stability.<N>    encoding[..., 3:6] = cuboids[..., 3:6].log()<N><N>
    # Convert quaternions to yaw (rotation about the z-axis).<N>    yaws = quat_to_yaw(cuboids[..., 6:10])<N><N>    # Sin / Cosine embedding.<N>    encoding[..., 6] = torch.sin(yaws)<N>    encoding[..., 7] = torch.cos(yaws)<N><N>    # Return encoding.<N>    return encoding<N><N>
<N>@torch.jit.script<N>def encode(cuboids: Tensor) -> Tensor:<N>    """Encode a set of cuboids.<N><N>    Args:<N>        cuboids: The ground truth annotations.<N><N>    Returns:<N>        The encoded ground truth annotations.<N>    """<N>    encoding: Tensor = _encode_lwh(cuboids)<N>    return encoding<N><N>
"""Methods for manipulating indices."""<N><N>from typing import List<N><N>import torch<N>from torch import Tensor<N><N><N>@torch.jit.script<N>def ravel_multi_index(unraveled_coords: Tensor, shape: List[int]) -> Tensor:<N>    """Convert a tensor of flat indices in R^K into flattened coordinates in R^1.<N><N>
    'Untangle' a set of spatial indices in R^K to a 'flattened'<N>        or 'raveled' set of indices in R^1.<N><N>    Reference:<N>        https://numpy.org/doc/stable/reference/generated/numpy.ravel_multi_index.html<N>    Reference:<N>        https://github.com/francois-rozet/torchist/blob/master/torchist/__init__.py#L18<N><N>
    Args:<N>        unraveled_coords: (N,K) Tensor containing the indices.<N>        shape: (K,) Shape of the target grid.<N><N>    Returns:<N>        (N,) Tensor of 1D indices.<N>    """<N>    shape_tensor = torch.as_tensor(<N>        shape + [1],<N>        device=unraveled_coords.device,<N>        dtype=unraveled_coords.dtype,<N>    )<N>    if len(unraveled_coords) > 0:<N>        max_coords, _ = unraveled_coords.max(dim=0)<N>        assert torch.all(max_coords < shape_tensor[:-1])<N><N>
    coefs = shape_tensor[1:].flipud().cumprod(dim=0).flipud()<N>    return torch.mul(unraveled_coords, coefs).sum(dim=-1)<N><N><N>@torch.jit.script<N>def unravel_index(raveled_indices: Tensor, shape: List[int]) -> Tensor:<N>    """Convert a tensor of flat indices in a multi-index of coordinate indices.<N><N>
    'Tangle' a set of spatial indices in R^1 to an 'aligned' or 'unraveled'<N>    set of indices in R^K where K=len(dims).<N><N>    Reference:<N>        https://numpy.org/doc/stable/reference/generated/numpy.unravel_index.html.<N>    Reference:<N>        https://github.com/francois-rozet/torchist/blob/master/torchist/__init__.py#L37<N><N>
"""Pooling methods."""<N><N>from typing import List, Tuple<N><N>import torch<N>import torch.nn.functional as F<N>from torch import Tensor<N><N>from torchbox3d.math.ops.index import ravel_multi_index, unravel_index<N><N><N>@torch.jit.script<N>def voxel_pool(<N>    indices: Tensor, values: Tensor, size: List[int]<N>) -> Tuple[Tensor, Tensor, Tensor]:<N>    """Apply a pooling operation on a voxel grid.<N><N>
    Args:<N>        indices: (N,3) Voxel coordinates.<N>        values: (N,F) Voxel features.<N>        size: (3,) Length, width, and height of the voxel grid.<N><N>    Returns:<N>        The binned voxel coordinates, features, and counts after pooling.<N>    """<N>    raveled_indices = ravel_multi_index(indices, size)<N>    permutation_sorted = torch.argsort(raveled_indices)<N><N>
    raveled_indices = raveled_indices[permutation_sorted]<N>    values = values[permutation_sorted]<N><N>    # Compute unique values, inverse, and counts.<N>    out: Tuple[Tensor, Tensor, Tensor] = torch.unique_consecutive(<N>        raveled_indices, return_inverse=True, return_counts=True<N>    )<N>    output, inverse_indices, counts = out<N><N>
    # Scatter with sum reduction.<N>    last_index_exclusive = int(inverse_indices.max()) + 1<N>    pooled_values: Tensor = torch.zeros_like(values).scatter_add_(<N>        dim=0,<N>        index=inverse_indices[:, None].repeat(1, values.shape[-1]),<N>        src=values,<N>    )[:last_index_exclusive]<N>    pooled_values /= counts[:, None]<N><N>
    # Unravel unique linear indices into unique multi indices.<N>    unraveled_coords = unravel_index(output, size)<N><N>    out_inv: Tuple[Tensor, Tensor, Tensor] = torch.unique_consecutive(<N>        inverse_indices,<N>        return_inverse=True,<N>        return_counts=True,<N>    )<N>    offset, _, counts = out_inv<N>    offset += F.pad(<N>        counts[:-1] - 1, pad=[1, 0], mode="constant", value=0.0<N>    ).cumsum(dim=0)<N><N>
    # Respect original ordering.<N>    inverse_indices = torch.argsort(permutation_sorted[offset])<N>    return (<N>        unraveled_coords[inverse_indices],<N>        pooled_values[inverse_indices],<N>        counts[inverse_indices],<N>    )<N><N><N>def unique_indices(indices: Tensor, dim: int = 0) -> Tensor:<N>    """Compute the indices corresponding to the unique value.<N><N>
"""Nearest neighbor methods."""<N><N>from enum import Enum, unique<N>from typing import Optional, Tuple<N><N>import torch<N>import torch.nn.functional as F<N>from torch import Tensor<N><N>from torchbox3d.math.crop import crop_points<N>from torchbox3d.math.ops.index import ravel_multi_index, unravel_index<N>from torchbox3d.math.ops.pool import voxel_pool<N>from torchbox3d.structures.ndgrid import VoxelGrid<N><N>
<N>@unique<N>class VoxelizationType(str, Enum):<N>    """The type of reduction performed during voxelization."""<N><N>    CONCATENATE = "CONCATENATE"<N>    POOL = "POOL"<N><N><N>@unique<N>class VoxelizationPoolingType(str, Enum):<N>    """The pooling method used for 'pooling' voxelization."""<N><N>
    MEAN = "MEAN"<N><N><N># @torch.jit.script<N>def voxelize_pool_kernel(<N>    xyz: Tensor,<N>    values: Tensor,<N>    voxel_grid: VoxelGrid,<N>    pool_mode: Optional[str] = "mean",<N>) -> Tuple[Tensor, Tensor, Tensor, Tensor]:<N>    """Cluster a point cloud into a grid of voxels.<N><N>
"""Encode and scatter objects as soft-targets (Gaussian) over an xy grid."""<N><N>from __future__ import annotations<N><N>import logging<N>from dataclasses import dataclass<N>from typing import Dict, List, Tuple<N><N>import torch<N>from torch import Tensor<N><N>
"""Voxelization transformation for a point cloud."""<N><N>from dataclasses import dataclass<N>from functools import cached_property<N>from typing import Tuple<N><N>import torch<N><N>from torchbox3d.math.neighbors import voxelize<N>from torchbox3d.math.ops.voxelize import VoxelizationType<N>from torchbox3d.structures.data import Data, RegularGridData<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.structures.sparse_tensor import SparseTensor<N><N>
<N>@dataclass<N>class Voxelize:<N>    """Construct a voxelization transformation.<N><N>    Args:<N>        min_range_m: (3,) Minimum range along the x,y,z axes in meters.<N>        max_range_m: (3,) Maximum range along the x,y,z axes in meters.<N>        resolution_m_per_cell: (3,) Ratio of meters to cell in meters.<N>        voxelization_type: Voxelization type used in the transformation<N>            (e.g., pooling).<N>    """<N><N>
"""Implementation of CenterPoint."""<N><N>from __future__ import annotations<N><N>import logging<N>from argparse import Namespace<N>from dataclasses import dataclass<N>from pathlib import Path<N>from typing import Any, Dict, List, Optional, Tuple, Union<N><N>
"""Sparse 3D backbone."""<N><N>from dataclasses import dataclass, field<N>from typing import Dict, List, Tuple, Union<N><N>import torch<N>from pytorch_lightning import LightningModule<N>from torch import Tensor<N>from torch.nn import ModuleDict, Sequential<N><N>
from torchbox3d.nn.blocks.sparse import ConvolutionBlock, ResidualBlock<N>from torchbox3d.structures.data import RegularGridData<N>from torchbox3d.structures.sparse_tensor import SparseTensor<N><N><N>@dataclass(unsafe_hash=True)<N>class SparseVoxelNet(LightningModule):<N>    """Construct the backbone.<N><N>
    Args:<N>        dim_in: Dimension of the input features.<N>        resolution_m_per_cell: (3,) Ratio of meters to cell in meters.<N>        min_range_m: (3,) Minimum range along the x,y,z axes in meters.<N>        max_range_m: (3,) Maximum range along the x,y,z axes in meters.<N>        voxelization_type: Voxelization type used in the transformation.<N>    """<N><N>
"""Deformable network blocks."""<N><N>from dataclasses import dataclass, field<N><N>import torch<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import Tensor, nn<N>from torchvision.ops.deform_conv import DeformConv2d<N><N>
<N>@dataclass(unsafe_hash=True)<N>class DeformableBlock(LightningModule):<N>    """Construct Deformable Convolution head.<N><N>    Args:<N>        in_channels: Number of input channels.<N>        out_channels: Number of output channels.<N>        kernel_size: Kernel size.<N>        groups: Number of groups.<N>    """<N><N>
    in_channels: int<N>    out_channels: int<N>    kernel_size: int = 3<N>    groups: int = 4<N><N>    dc_offset: nn.Module = field(init=False)<N><N>    def __post_init__(self) -> None:<N>        """Initialize network modules."""<N>        super().__init__()<N>        dc_offset_out_channels = 2 * self.groups * self.kernel_size**2<N>        self.dc_offset = nn.Conv2d(self.in_channels, dc_offset_out_channels, 1)<N><N>
        padding = (self.kernel_size - 1) // 2<N>        self.dc = DeformConv2d(<N>            self.in_channels,<N>            self.out_channels,<N>            kernel_size=self.kernel_size,<N>            padding=padding,<N>            groups=self.groups,<N>        )<N>        self.init_offset_()<N><N>
    def init_offset_(self) -> None:<N>        """Initialize the deformable weights and biases."""<N>        torch.nn.init.zeros_(self.dc_offset.weight.data)  # type: ignore<N>        if self.dc_offset.bias is not None:<N>            torch.nn.init.zeros_(self.dc_offset.bias.data)  # type: ignore<N><N>
    def forward(self, x: Tensor) -> Tensor:  # type: ignore[override]<N>        """Forward pass.<N><N>        Args:<N>            x: (B,C,H,W) Input tensor.<N><N>        Returns:<N>            The convolved inputs.<N>        """<N>        offset = self.dc_offset(x)<N>        out: Tensor = self.dc(x, offset).relu_()<N>        return out<N><N><N>
"""Network blocks for sparse convolution."""<N><N>from collections import OrderedDict<N>from dataclasses import dataclass, field<N>from typing import Tuple, Union<N><N>import torchsparse.nn as spnn<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import nn<N>from torch.nn import Sequential<N>from torchsparse.tensor import SparseTensor<N><N>
<N>@dataclass(unsafe_hash=True)<N>class ConvolutionBlock(LightningModule):<N>    """Construct a sparse-convolution block.<N><N>    Args:<N>        in_channels: Number of channels into the block.<N>        out_channels: Number of output channels from the block.<N>        kernel_size: Convolution kernel size.<N>        stride: Stride of the convolution kernel.<N>        dilation: Dilation of the convolution kernel.<N>        transposed: Flag for transposed sparse-convolution.<N>    """<N><N>
"""CenterPointHead for keypoint classification and regression."""<N><N>from copy import deepcopy<N>from dataclasses import dataclass, field<N>from typing import List, Tuple<N><N>from omegaconf import DictConfig<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import nn<N>from torch.functional import Tensor<N><N>
from torchbox3d.nn.heads.deformable import DeformableDetectionHead<N>from torchbox3d.nn.losses.classification import FocalLoss<N>from torchbox3d.nn.losses.regression import RegressionLoss<N>from torchbox3d.structures.data import RegularGridData<N>from torchbox3d.structures.outputs import TaskOutputs<N>from torchbox3d.structures.targets import CenterPointLoss<N><N>
<N>@dataclass(unsafe_hash=True)<N>class CenterHead(LightningModule):<N>    """CenterHead class for keypoint classification and regression."""<N><N>    tasks_cfg: DictConfig<N>    weight: float<N>    in_channels: int<N>    task_in_channels: int<N>    common_heads: DictConfig<N><N>
"""Convolutional heads."""<N>from typing import List<N><N>import torch<N>from omegaconf import DictConfig<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import Tensor, nn<N>from torch.nn import Sequential<N><N><N>class ConvHead(LightningModule):<N>    """Convolution head class."""<N><N>
    def __init__(<N>        self,<N>        heads: DictConfig,<N>        in_channels: int,<N>        out_channels: int = 64,<N>        final_kernel: int = 1,<N>        bn: bool = False,<N>    ) -> None:<N>        """Construct a convolutional head.<N><N>
        Args:<N>            heads: Head configuration.<N>            in_channels: Number of input channels.<N>            out_channels: Number of output channels.<N>            final_kernel: Number of channels in the final kernel.<N>            bn: Flag for batch normalization.<N>        """<N>        super().__init__()<N><N>
"""Deformable detection heads."""<N><N>from dataclasses import dataclass, field<N><N>from omegaconf import DictConfig<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import Tensor<N>from torch.nn import BatchNorm2d, Conv2d, ReLU, Sequential<N><N>
from torchbox3d.nn.blocks.deformable import DeformableBlock<N>from torchbox3d.nn.heads.conv import ConvHead<N>from torchbox3d.structures.outputs import TaskOutputs<N><N><N>@dataclass(unsafe_hash=True)<N>class DeformableDetectionHead(LightningModule):<N>    """Construct deformable convolution head.<N><N>
    Args:<N>        num_cls: Number of detection classes.<N>        heads: Head configuration.<N>        in_channels: Number of input channels.<N>        out_channels: Number of output channels.<N>        final_kernel: Number of channels in the final kernel.<N>        bn: Flag to enable batch normalization.<N>        kernel_size: Kernel size.<N>        groups: Number of groups.<N>        padding: Padding size.<N>        stride: Network stride.<N>        bias: Flag to use bias.<N>    """<N><N>
    num_cls: int<N>    heads: DictConfig<N>    in_channels: int<N>    out_channels: int = 64<N>    final_kernel: int = 1<N>    bn: bool = False<N>    kernel_size: int = 3<N>    groups: int = 4<N>    padding: int = 1<N>    stride: int = 1<N>    bias: bool = True<N><N>
"""Classification losses."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from typing import Tuple<N><N>import torch<N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import Tensor<N><N><N>@dataclass(unsafe_hash=True)<N>class FocalLoss(LightningModule):<N>    """Focal Loss class."""<N><N>
    def __post_init__(self) -> None:<N>        """Initialize network modules."""<N>        super().__init__()<N><N>    def forward(  # type: ignore[override]<N>        self,<N>        x: Tensor,<N>        y: Tensor,<N>        task_offsets: Tensor,<N>        mask: Tensor,<N>    ) -> Tuple[Tensor, Tensor]:<N>        """Forward pass for computing classification loss.<N><N>
        Args:<N>            x: (B,C,H,W) Tensor of network outputs.<N>            y: (B,1,H,W) Tensor of target scores.<N>            task_offsets: (B,1,H,W) Tensor of task offsets.<N>            mask: (B,1,H,W) Tensor of target centers (binary mask).<N><N>
        Returns:<N>            (B,1,H,W) Classification loss.<N>        """<N>        pos_loss: Tensor<N>        neg_loss: Tensor<N>        pos_loss, neg_loss = focal_loss(x, y, task_offsets, mask)<N>        return pos_loss, neg_loss<N><N><N>@torch.jit.script<N>def focal_loss(<N>    x: Tensor, y: Tensor, task_offsets: Tensor, mask: Tensor<N>) -> Tuple[Tensor, Tensor]:<N>    """Compute focal loss over the BEV grid.<N><N>
    Args:<N>        x: (B,C,H,W) Tensor of network outputs.<N>        y: (B,1,H,W) Tensor of target scores.<N>        task_offsets: (B,1,H,W) Tensor of task offsets.<N>        mask: (B,1,H,W) Tensor of target centers (binary mask).<N><N>    Returns:<N>        (B,) Positive loss and (B,) negative loss.<N>    """<N>    neg_loss = (1 - x).log_() * (x**2) * (1 - y) ** 4<N><N>
    x = torch.gather(x, dim=1, index=task_offsets)<N>    pos_loss = x.log_() * (1 - x) ** 2 * mask<N><N>    dim = [1, 2, 3]<N>    npos = torch.clamp(mask.sum(dim=dim), min=1)<N>    neg_loss = -torch.sum(neg_loss, dim=dim) / npos<N>    pos_loss = -torch.sum(pos_loss, dim=dim) / npos<N>    return pos_loss, neg_loss<N><N><N>
"""Regression losses."""<N><N>from dataclasses import dataclass<N><N>from pytorch_lightning.core.lightning import LightningModule<N>from torch import Tensor<N>from torch.nn.modules.loss import L1Loss<N><N><N>@dataclass(unsafe_hash=True)<N>class RegressionLoss(LightningModule):<N>    """Class used for computing grid-based regression losses."""<N><N>
    def __post_init__(self) -> None:<N>        """Initialize LightningModule."""<N>        super().__init__()<N><N>    def __init__(self) -> None:<N>        """Construct regression loss."""<N>        super().__init__()<N>        self.loss = L1Loss(reduction="none")<N>        self.eps = 1e-4<N><N>
    def forward(  # type: ignore[override]<N>        self, src: Tensor, targets: Tensor, mask: Tensor<N>    ) -> Tensor:<N>        """Loss computation.<N><N>        Args:<N>            src: (B,C,H,W) Tensor of network regression outputs.<N>            targets: (B,C,H,W) Tensor of target regression parameters.<N>            mask: (B,1,H,W) Tensor of target centers (binary).<N><N>
        Returns:<N>            (B,C,H,W) Regression losses.<N>        """<N>        npos = mask.sum(dim=[1, 2, 3], keepdim=True)<N>        loss = self.loss(src, targets) / (npos + self.eps)<N>        reduced_loss: Tensor = loss.sum(dim=[2, 3])<N>        return reduced_loss<N><N><N>
"""General object detection model."""<N><N>from dataclasses import dataclass, field<N>from typing import Any, Dict, List, Union<N><N>from hydra.utils import instantiate<N>from omegaconf import MISSING<N>from omegaconf.dictconfig import DictConfig<N>from pytorch_lightning import LightningModule<N>from torch.optim.adamw import AdamW<N>from torch.optim.lr_scheduler import OneCycleLR, _LRScheduler  # type: ignore<N><N>
"""Residual networks."""<N><N>from dataclasses import dataclass, field<N>from typing import List, Tuple<N><N>import torch<N>from pytorch_lightning import LightningModule<N>from torch import Tensor<N>from torch.nn.modules import (<N>    BatchNorm2d,<N>    Conv2d,<N>    ConvTranspose2d,<N>    ModuleList,<N>    ReLU,<N>    Sequential,<N>)<N><N>
<N>@dataclass(unsafe_hash=True)<N>class ResNet(LightningModule):<N>    """Construct a residual network object.<N><N>    Args:<N>        name:<N>        in_channels: Number of input channels.<N>        down_strides: Stride for each block.<N>        down_planes: Number of filters for each block.<N>        layer_nums: Number of layers per block.<N>        up_strides: Stride during upsampling blocks.<N>        num_up_filters: Number of upsampling filters per upsampling block.<N>    """<N><N>
    name: str<N>    in_channels: int<N>    down_strides: List[int]<N>    down_planes: List[int]<N>    layer_nums: List[int]<N>    up_strides: List[int]<N>    num_up_filters: List[int]<N><N>    up_start_idx: int = field(init=False)<N>    down_blocks: ModuleList = field(init=False)<N>    up_blocks: ModuleList = field(init=False)<N><N>
    def __post_init__(self) -> None:<N>        """Initialize network modules."""<N>        super().__init__()<N>        self.up_start_idx = len(self.layer_nums) - len(self.up_strides)<N><N>        in_filters = [self.in_channels, *list(self.down_planes)[:-1]]<N><N>
"""Methods to help visualize data during training."""<N><N>from typing import List, Optional, Tuple<N><N>import torch<N>from pytorch_lightning import Trainer<N>from pytorch_lightning.loggers.tensorboard import TensorBoardLogger<N>from pytorch_lightning.trainer.states import RunningStage<N>from pytorch_lightning.utilities.rank_zero import rank_zero_only<N>from torch import Tensor<N>from torchvision.utils import make_grid<N><N>
from torchbox3d.math.conversions import (<N>    denormalize_pixel_intensities,<N>    sweep_to_bev,<N>)<N>from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.data import RegularGridData<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.structures.outputs import NetworkOutputs<N><N>
"""Shaders for rendering.<N><N>The goal of these functions is three-fold:<N>    1. Provide _fast_ operations that one would expect in a renderer.<N>    2. Support both cpu and gpu (by solely operating on PyTorch tensors).<N>    3. Fully differentiable operations.<N><N>
Reference: https://en.wikipedia.org/wiki/Shader<N>"""<N><N>from typing import Tuple<N><N>import torch<N>import torch.nn.functional as F<N>from torch import Tensor<N><N>from torchbox3d.math.constants import EPS<N>from torchbox3d.math.kernels import gaussian_kernel<N>from torchbox3d.math.ops.index import (<N>    ogrid_sparse_neighborhoods,<N>    ravel_multi_index,<N>    unravel_index,<N>)<N><N>
<N>@torch.jit.script<N>def align_corners(pos: Tensor) -> Tensor:<N>    """Align a set of points to the center of a regular grid.<N><N>    Args:<N>        pos: (N,K) Set of points.<N><N>    Returns:<N>        The points with a half cell offset (centered).<N>    """<N>    centered_points = pos + 0.5<N>    return centered_points<N><N>
<N>@torch.jit.script<N>def normal2(p1: Tensor, p2: Tensor, eps: float = EPS) -> Tensor:<N>    """Compute the 2D normal vector to the line defined by (p1,p2).<N><N>    Args:<N>        p1: (2,) Line start point.<N>        p2: (2,) Line end point.<N>        eps: Smoothing parameter to prevent divide by zero.<N><N>
    Returns:<N>        The vector normal to the line defined by (p1,p2).<N>    """<N>    grad = p2 - p1<N>    du, dv = grad[0], grad[1]<N>    normal = torch.stack([-dv, du], dim=-1).float()<N>    unit_normal: Tensor = normal / torch.linalg.norm(normal).clamp(eps, None)<N>    return unit_normal<N><N>
<N>@torch.jit.script<N>def linear_interpolation(points: Tensor, num_samples: int) -> Tensor:<N>    """Linearly interpolate two points.<N><N>    Args:<N>        points: (N,2) Points to interpolate between.<N>        num_samples: The number of uniform samples between points.<N><N>
    Returns:<N>        The interpolant sampled uniformly over the interval.<N>    """<N>    interp: Tensor = F.interpolate(<N>        points, size=num_samples, mode="linear", align_corners=True<N>    ).squeeze()<N>    return interp<N><N><N>@torch.jit.script<N>def clip_to_viewport(<N>    uv: Tensor, tex: Tensor, width_px: int, height_px: int<N>) -> Tuple[Tensor, Tensor, Tensor]:<N>    """Clip the points to the viewport.<N><N>
    Reference: https://en.wikipedia.org/wiki/Viewport<N><N>    Args:<N>        uv: (N,2) UV coordinates.<N>            Reference: https://en.wikipedia.org/wiki/UV_mapping<N>        tex: (N,3) Texture intensity values.<N>        width_px: Width of the viewport in pixels.<N>        height_px: Height of the viewport in pixels.<N><N>
    Returns:<N>        The clipped UV coordinates and texture, and the boolean validity mask.<N>    """<N>    size = torch.as_tensor([width_px, height_px], device=uv.device)<N>    is_inside_viewport = torch.logical_and(uv >= 0, uv < size).all(dim=-1)<N>    return uv[is_inside_viewport], tex[is_inside_viewport], is_inside_viewport<N><N>
<N>@torch.jit.script<N>def blend(<N>    foreground_pixels: Tensor, background_pixels: Tensor, alpha: float<N>) -> Tensor:<N>    """Blend the foreground and background pixels.<N><N>    Args:<N>        foreground_pixels: (...,3,H,W) Source RGB image.<N>        background_pixels: (...,3,H,W) Target RGB image.<N>        alpha: Alpha blending coefficient.<N><N>
    Returns:<N>        (...,3,H,W) The blended pixels.<N><N>    Raises:<N>        ValueError: If the foreground and background pixels<N>            do not have the same shape.<N>    """<N>    if foreground_pixels.shape != background_pixels.shape:<N>        raise ValueError(<N>            "Foreground pixels and background pixels must have the same shape!"<N>        )<N>    pix_blended = foreground_pixels * alpha + background_pixels * (1 - alpha)<N>    return pix_blended<N><N>
<N>@torch.jit.script<N>def circles(<N>    uvz: Tensor,<N>    tex: Tensor,<N>    img: Tensor,<N>    radius: int = 10,<N>    antialias: bool = True,<N>) -> Tensor:<N>    """Draw circles on a 3 channel image.<N><N>    Image plane coordinate system:<N>      (0,0)----------+v<N>        |<N>        |<N>        |<N>        |<N>        +u<N><N>
    Args:<N>        uvz: (N,3) Texture coordinates.<N>        tex: (N,3) Texture pixel intensities.<N>        img: (...,H,W,3) Image.<N>        radius: Radius of the circle.<N>        antialias: Boolean flag to enable anti-aliasing.<N><N>    Returns:<N>        (...,H,W,3) Image with circles overlaid.<N>    """<N>    uv = uvz[..., :2].flatten(0, -2)<N>    ogrid_uv = ogrid_sparse_neighborhoods(uv, [radius, radius])<N>    tex = tex.repeat_interleave(int(radius**2), dim=0)<N><N>
    if antialias:<N>        mu = uv.repeat_interleave(int(radius**2), 0)<N>        sigma = torch.ones_like(mu[:, 0])<N>        alpha = gaussian_kernel(ogrid_uv, mu, sigma).prod(dim=-1, keepdim=True)<N>        tex *= alpha<N><N>    H = img.shape[-2]<N>    W = img.shape[-1]<N>    ogrid_uv, tex, _ = clip_to_viewport(ogrid_uv, tex, W, H)<N><N>
"""Collation utilities for dataloaders."""<N><N>from collections import defaultdict<N>from typing import Any, DefaultDict, Dict, List, Sequence<N><N>import torch<N>import torch.nn.functional as F<N>from torchsparse.utils.collate import sparse_collate<N><N>
from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.data import Data, RegularGridData<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.structures.sparse_tensor import SparseTensor<N>from torchbox3d.structures.targets import GridTargets<N><N>
<N>def collate(data_list: Sequence[Data]) -> Data:<N>    """Collate (merge) a sequence of items.<N><N>    Args:<N>        data_list: Sequence of data to be collated.<N><N>    Returns:<N>        The collated data.<N><N>    Raises:<N>        TypeError: If the data type is not supported for collation.<N>    """<N>    collated_data: DefaultDict[str, List[Any]] = defaultdict(list)<N>    for data in data_list:<N>        for attr_name, attr in data.items():<N>            collated_data[attr_name].append(attr)<N><N>
"""Structure which models a set of cuboids.<N><N>Reference: https://en.wikipedia.org/wiki/Cuboid<N>"""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from functools import cached_property<N>from typing import Any, Dict, ItemsView, List, Optional, Tuple, Union<N><N>
import torch<N>from torch import Tensor<N><N>from torchbox3d.math.linalg.lie.SO3 import quat_to_mat<N>from torchbox3d.rendering.ops.shaders import polygon<N>from torchbox3d.structures.meta import TensorStruct<N>from torchbox3d.structures.ndgrid import VoxelGrid<N><N>
<N>@dataclass<N>class Cuboids(TensorStruct):<N>    """Models a set of cuboids.<N><N>    Args:<N>        params: (N,K) Tensor of cuboid parameters.<N>        categories: (N,) Tensor of cuboid categories (integer).<N>        scores: (N,) Tensor of cuboid confidence scores.<N>        batch: (N,) Tensor of batch indices.<N>    """<N><N>
    params: Tensor<N>    categories: Tensor<N>    scores: Tensor<N>    batch: Optional[Tensor] = None<N><N>    def __len__(self) -> int:<N>        """Return the number of cuboids."""<N>        return len(self.params)<N><N>    def items(self) -> ItemsView[str, Any]:<N>        """Return a view of the attribute names and values."""<N>        return ItemsView({k: v for k, v in self.__dict__.items()})<N><N>
    def __getitem__(self, *index: Union[int, slice, Tensor]) -> Cuboids:<N>        """Get the items at the indices provided."""<N>        output: Dict[str, Any] = {}<N>        for k, v in self.__dict__.items():<N>            if isinstance(v, Tensor):<N>                output[k] = v[index]<N>        return Cuboids(**output)<N><N>
    @property<N>    def xyz_m(self) -> Tensor:<N>        """Return the cuboids centers (x,y,z) in meters."""<N>        return self.params[:, :3]<N><N>    @property<N>    def dims_lwh_m(self) -> Tensor:<N>        """Return the cuboids length, width, and height in meters."""<N>        return self.params[:, 3:6]<N><N>
    @property<N>    def quat_wxyz(self) -> Tensor:<N>        """(N,4) Return the scalar first quaternion coefficients."""<N>        return self.params[:, 6:10]<N><N>    @property<N>    def mat(self) -> Tensor:<N>        """Return a rotation matrix representing the object's pose."""<N>        mat: Tensor = quat_to_mat(self.params[..., -4:])<N>        return mat<N><N>
    @property<N>    def shape(self) -> Tuple[int, ...]:<N>        """Return the cuboids shape.."""<N>        shape: Tuple[int, ...] = tuple(int(x) for x in self.params.shape)<N>        return shape<N><N>    def cuboid_list(self) -> List[Cuboids]:<N>        """Get the list representation of the cuboids.<N><N>
        Returns:<N>            The list of cuboids --- each element corresponds to the cuboids<N>                for a particular batch.<N><N>        Raises:<N>            ValueError: If the cuboids batch size does not exist.<N>        """<N>        if self.batch is None:<N>            raise ValueError("Batch size _must_ exist!")<N>        outputs: Tuple[Tensor, Tensor] = torch.unique_consecutive(<N>            self.batch, return_counts=True<N>        )<N><N>
        _, counts = outputs<N>        count_list: List[int] = counts.tolist()<N><N>        start = 0<N>        cuboid_list: List[Cuboids] = []<N>        for count in count_list:<N>            batch_cuboids = self[start : start + count]<N>            cuboid_list.append(batch_cuboids)<N>            start += count<N>        return cuboid_list<N><N>
    @cached_property<N>    def vertices_m(self) -> Tensor:<N>        r"""Return the cuboid vertices in the destination reference frame.<N><N>            5------4<N>            |\\    |\\<N>            | \\   | \\<N>            6--\\--7  \\<N>            \\  \\  \\ \\<N>        l    \\  1-------0    h<N>         e    \\ ||   \\ ||   e<N>          n    \\||    \\||   i<N>           g    \\2------3    g<N>            t      width.     h<N>             h.               t.<N><N>
"""Class for manipulation of 3D data and annotations."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from typing import Tuple<N><N>from torch import Tensor<N><N>from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.meta import TensorStruct<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.structures.sparse_tensor import SparseTensor<N>from torchbox3d.structures.targets import GridTargets<N><N>
<N>@dataclass<N>class Data(TensorStruct):<N>    """General class for manipulating 3D data and associated annotations."""<N><N>    cuboids: Cuboids<N>    pos: Tensor<N>    x: Tensor<N>    uuids: Tuple[str, ...]<N><N><N>@dataclass<N>class RegularGridData(Data):<N>    """Data encoded on a regular grid.<N><N>
"""Detection data classes."""<N><N>from __future__ import annotations<N><N>from collections import defaultdict<N>from dataclasses import dataclass<N>from typing import Any, DefaultDict, ItemsView, Sequence, Type, TypeVar<N><N>import torch<N>from torch import Tensor<N><N>
T = TypeVar("T", bound="TensorStruct")<N><N><N>@dataclass<N>class TensorStruct:<N>    """Meta-structure which provides common functionality for tensors."""<N><N>    def items(self) -> ItemsView[str, Any]:<N>        """Return a view of the attribute names and values."""<N>        return ItemsView({k: v for k, v in self.__dict__.items()})<N><N>
    def cpu(self: T) -> T:<N>        """Move all tensors to the cpu."""<N>        for k, v in self.items():<N>            if isinstance(v, Tensor):<N>                setattr(self, k, v.cpu())<N>        return self<N><N>    def __repr__(self) -> str:<N>        """Return the string representation of the data."""<N>        return super().__repr__()<N><N>
"""An N-Dimensional grid class."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from functools import cached_property<N>from typing import List, Tuple<N><N>import torch<N>from torch import Tensor<N><N>from torchbox3d.math.crop import crop_points<N>from torchbox3d.rendering.ops.shaders import align_corners<N><N>
<N>@dataclass<N>class NDGrid:<N>    """Models an N-dimensional grid.<N><N>    Args:<N>        min_range_m: (N,) Minimum coordinates (in meters).<N>        max_range_m: (N,) Maximum coordinates (in meters).<N>        resolution_m_per_cell: (N,) Ratio of meters to cell in each dimension.<N>    """<N><N>
    min_range_m: Tuple[float, ...]<N>    max_range_m: Tuple[float, ...]<N>    resolution_m_per_cell: Tuple[float, ...]<N><N>    @cached_property<N>    def dims(self) -> Tuple[int, ...]:<N>        """Size of the grid _after_ bucketing."""<N>        range_m: Tensor = torch.as_tensor(self.range_m)<N>        dims: List[int] = self.scale_and_quantize_points(range_m).tolist()<N>        return tuple(dims)<N><N>
    @cached_property<N>    def min_dim_m(self) -> Tuple[int, ...]:<N>        """Size of the grid _after_ bucketing."""<N>        min_range_m: Tensor = torch.as_tensor(self.min_range_m)<N>        dims: List[int] = self.scale_and_quantize_points(min_range_m).tolist()<N>        return tuple(dims)<N><N>
    @cached_property<N>    def max_dim_m(self) -> Tuple[int, ...]:<N>        """Size of the grid _after_ bucketing."""<N>        max_range_m: Tensor = torch.as_tensor(self.max_range_m)<N>        dims: List[int] = self.scale_and_quantize_points(max_range_m).tolist()<N>        return tuple(dims)<N><N>
    @cached_property<N>    def range_m(self) -> Tuple[float, ...]:<N>        """Size of the grid _before_ bucketing."""<N>        min_range_m = torch.as_tensor(self.min_range_m)<N>        max_range_m = torch.as_tensor(self.max_range_m)<N>        range_m: List[float] = (max_range_m - min_range_m).tolist()<N>        return tuple(range_m)<N><N>
"""Classes which store network outputs."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from typing import List<N><N>from torch import Tensor<N><N>from torchbox3d.structures.meta import TensorStruct<N><N><N>@dataclass<N>class NetworkOutputs(TensorStruct):<N>    """Class which manipulates and tracks data in the `CenterPoint` method.<N><N>
    Args:<N>        backbone: (B,C,H,W) Network output from the backbone.<N>        neck: (B,C,H,W) Network output from the neck.<N>        head: (T,) Task outputs from the head of the network.<N>    """<N><N>    backbone: Tensor<N>    neck: Tensor<N>    head: List[TaskOutputs]<N><N>
<N>@dataclass<N>class TaskOutputs(TensorStruct):<N>    """Task head outputs from the network.<N><N>    Args:<N>        logits: (B,C,H,W) Raw class values before being mapped to [0,1].<N>        regressands: (B,R,H,W) Tensor of predicted continuous values.<N><N>
"""A wrapper for the `SparseTensor` object in the `torchsparse` library."""<N><N>from __future__ import annotations<N><N>from dataclasses import dataclass<N>from typing import Any, ItemsView, Tuple, Union<N><N>import torch<N>import torchsparse<N>from torch import Size, Tensor, as_tensor, sparse_coo_tensor<N><N>
<N>@dataclass<N>class SparseTensor(torchsparse.SparseTensor):  # type: ignore<N>    """Wrapper around `torchsparse.SparseTensor`."""<N><N>    feats: Tensor<N>    coords: Tensor<N>    stride: Union[int, Tuple[int, ...]] = 1<N><N>    def __post_init__(self) -> None:<N>        """Initialize the parent class."""<N>        super().__init__(self.feats, self.coords, stride=self.stride)<N><N>
    def to_dense(self, size: Size) -> Tensor:<N>        """Convert a `SparseTensor` into a dense output.<N><N>        Args:<N>            size: The size of the dense output tensor.<N><N>        Returns:<N>            (B,C*D,H,W) Dense tensor of spatial features.<N>        """<N>        L, W, H, B, D = size<N><N>
        indices = self.C<N>        indices[:, :3] = torch.div(<N>            indices[:, :3],<N>            as_tensor(self.s, device=self.F.device),<N>            rounding_mode="trunc",<N>        )<N>        indices[:, 0] = indices[:, 0].clamp(0, W - 1)<N>        indices[:, 1] = indices[:, 1].clamp(0, L - 1)<N>        indices[:, 2] = indices[:, 2].clamp(0, H - 1)<N><N>
        sparse = sparse_coo_tensor(indices=indices.T, values=self.F, size=size)<N>        dense = sparse.to_dense().permute(3, 4, 2, 0, 1)<N>        return dense.reshape(B, -1, W, L)<N><N>    def clone(self) -> SparseTensor:<N>        """Return a clone of the sparse tensor."""<N>        return SparseTensor(self.F.clone(), self.C.clone(), self.s)<N><N>
"""Detection data classes."""<N><N>from __future__ import annotations<N><N>from collections import defaultdict<N>from dataclasses import dataclass<N>from typing import DefaultDict, List, Mapping, TypeVar<N><N>import torch<N>from torch import Tensor<N><N>
from torchbox3d.structures.meta import TensorStruct<N><N>T = TypeVar("T", bound="Targets")<N><N><N>@dataclass<N>class Targets(TensorStruct):<N>    """Models general network targets."""<N><N>    scores: Tensor<N>    encoding: Tensor<N><N><N>@dataclass<N>class GridTargets(Targets):<N>    """Class which manipulates and tracks data in the `CenterPoint` method."""<N><N>
    scores: Tensor<N>    encoding: Tensor<N>    offsets: Tensor<N>    mask: Tensor<N><N><N>@dataclass<N>class CenterPointLoss:<N>    """Centerpoint loss object.<N><N>    Args:<N>        positive_loss:<N>        negative_loss:<N>        coordinate_loss:<N>        dimension_loss:<N>        rotation_loss<N>        regression_weight:<N>    """<N><N>
    positive_loss: Tensor<N>    negative_loss: Tensor<N><N>    coordinate_loss: Tensor<N>    dimension_loss: Tensor<N>    rotation_loss: Tensor<N><N>    regression_weight: float<N><N>    @property<N>    def classification_loss(self) -> Tensor:<N>        """Return the classification loss."""<N>        return self.positive_loss + self.negative_loss<N><N>
    @property<N>    def regression_loss(self) -> Tensor:<N>        """Return the regression loss."""<N>        return (<N>            self.coordinate_loss.sum(dim=-1)<N>            + self.dimension_loss.sum(dim=-1)<N>            + self.rotation_loss.sum(dim=-1)<N>        )<N><N>
"""Methods to help manipulate collections."""<N><N>from typing import Any, Iterator, Sequence<N><N><N>def flatten(elems: Sequence[Any]) -> Iterator[Any]:<N>    """Recursively flattens a nested collection of sequences.<N><N>    Args:<N>        elems: A nested list of sequences.<N><N>    Yields:<N>        The recursively-flattened sequence.<N>    """<N>    for elem in elems:<N>        if isinstance(elem, Sequence) and not isinstance(elem, (str, bytes)):<N>            yield from flatten(elem)<N>        else:<N>            yield elem<N>
"""Input / output methods."""<N><N>import torch<N>from torch import Tensor<N>from torchvision.io import (<N>    decode_jpeg,<N>    decode_png,<N>    read_file,<N>    write_jpeg,<N>    write_png,<N>)<N><N><N>@torch.jit.script<N>def read_img(path: str, device: str = "cpu") -> Tensor:<N>    """Read an image into memory.<N><N>
    NOTE: Path variable is currently type `str` instead of `Path`<N>        to allow for JIT compilation.<N><N>    Args:<N>        path: File path.<N>        device: Device to support jpeg decoding.<N><N>    Returns:<N>        (B,C,H,W) The image tensor.<N><N>
    Raises:<N>        ValueError: If the file extension is not a supported image format.<N>    """<N>    ext = path.split(".")[-1].lower()<N>    img = read_file(path)<N>    if ext == "jpg":<N>        return decode_jpeg(img, device=device)  # type: ignore<N>    elif ext == "png":<N>        return decode_png(img).to(device=device)  # type: ignore<N>    else:<N>        raise ValueError("Invalid image type!")<N><N>
<N>@torch.jit.script<N>def write_img(img: Tensor, path: str) -> None:<N>    """Write an image to disk.<N><N>    NOTE: Path variable is currently type `str` instead of `Path`<N>        to allow for JIT compilation.<N><N>    Args:<N>        img: (C,H,W) image to write.<N>        path: File path.<N><N>
    Raises:<N>        ValueError: If the file extension is not a supported image format.<N>    """<N>    ext = path.split(".")[-1].lower()<N>    if ext == "jpg":<N>        write_jpeg(img, path)<N>    elif ext == "png":<N>        write_png(img, path)<N>    else:<N>        raise ValueError("Invalid image type!")<N><N><N>
"""Unit tests for the av2 module."""<N><N>from pathlib import Path<N><N>from torchbox3d.datasets.argoverse.av2 import AV2<N><N><N>def test_av2() -> None:<N>    """Unit test for loading data in the Argoverse 2 dataloader."""<N>    rootdir = Path.home() / "data" / "datasets" / "av2" / "sensor"<N>    av2 = AV2(rootdir, name="av2", split="val")<N><N>    # for _ in av2:<N>    #     continue<N>
"""Units tests for the utils module of the argoverse subpackage."""<N><N>from pathlib import Path<N>from typing import Dict, Final<N><N>import pytest<N><N>from torchbox3d.datasets.argoverse.constants import AV2_ANNO_NAMES_TO_INDEX<N>from torchbox3d.datasets.argoverse.utils import read_sweep_data<N><N>
"""Unit tests for geometric conversions."""<N><N>from pathlib import Path<N>from tempfile import NamedTemporaryFile<N>from typing import Final<N><N>import pytest<N>import torch<N>from av2.utils.io import read_feather<N>from torch import Tensor<N><N>from torchbox3d.math.conversions import sweep_to_bev<N>from torchbox3d.structures.ndgrid import VoxelGrid<N>from torchbox3d.utils.io import write_img<N><N>
TEST_DATA_DIR: Final[Path] = (<N>    Path(__file__).parent.parent.resolve() / "test_data"<N>)<N><N><N>def test_cart_to_sph() -> None:<N>    """Unit test for converting Cartesian to spherical coordinates."""<N><N><N>def test_sph_to_cart() -> None:<N>    """Unit test for converting spherical to Cartesian coordinates."""<N><N>
"""Unit tests for the neighbors package."""<N><N>from typing import Any, Callable<N><N>import pytest<N>import torch<N>from torch import Tensor<N><N>from torchbox3d.math.neighbors import voxelize<N>from torchbox3d.math.ops.voxelize import (<N>    VoxelizationType,<N>    voxelize_concatenate_kernel,<N>)<N>from torchbox3d.structures.ndgrid import VoxelGrid<N><N>
"""Unit tests for the polytope module."""<N><N><N>def test_axis_aligned_interior() -> None:<N>    """Test axis aligned interior."""<N>
"""Unit tests for SO(3) transformation utilities."""<N><N>import pytest<N>import torch<N>from scipy.spatial.transform import Rotation as R<N>from torch import Tensor<N><N>from torchbox3d.math.linalg.lie.SO3 import (<N>    quat_to_xyz,<N>    quat_to_yaw,<N>    xyz_to_quat,<N>    yaw_to_quat,<N>)<N><N>
<N>@pytest.mark.parametrize(<N>    "quats_wxyz",<N>    [<N>        pytest.param(<N>            torch.rand((10000, 4)),<N>        )<N>    ],<N>    ids=["Test converting random quaternions to yaw."],<N>)<N>def test_quat_to_yaw(quats_wxyz: Tensor) -> None:<N>    """Test converting a quaternion to yaw.<N><N>
    Args:<N>        quats_wxyz: (N,4) Scalar first quaternions.<N><N>    NOTE: Yaw is rotation about the vertical axis in our<N>        coordinate system.<N>    """<N>    quats_wxyz /= torch.linalg.norm(quats_wxyz, dim=-1, keepdim=True)<N>    yaw = quat_to_yaw(quats_wxyz).rad2deg()<N>    yaw_ = torch.as_tensor(<N>        R.from_quat(quats_wxyz[..., [1, 2, 3, 0]].numpy()).as_euler(<N>            "xyz", degrees=True<N>        )<N>    )[..., -1]<N>    torch.testing.assert_allclose(yaw, yaw_)<N><N>
"""Unit tests for the splatter heatmap transform."""<N><N>from typing import Dict, List<N><N>import pytest<N>import torch<N><N>from torchbox3d.math.transforms.splatter_heatmap import SplatterHeatmap<N>from torchbox3d.structures.cuboids import Cuboids<N>from torchbox3d.structures.ndgrid import VoxelGrid<N><N>
"""Unit tests for shader ops."""<N><N>from tempfile import NamedTemporaryFile<N>from typing import Any, Callable<N><N>import pytest<N>import torch<N>from torch import Tensor<N><N>from torchbox3d.rendering.ops.shaders import (<N>    blend,<N>    circles,<N>    clip_to_viewport,<N>    line2,<N>    linear_interpolation,<N>    normal2,<N>    polygon,<N>)<N>from torchbox3d.utils.io import write_img<N><N>
"""Unit tests for the io module."""<N><N>from tempfile import NamedTemporaryFile<N>from typing import Any, Callable<N><N>import pytest<N>import torch<N>from torch import Tensor<N>from torchvision.io.image import read_image, write_jpeg<N><N>from torchbox3d.utils.io import read_img, write_img<N><N>
import numpy as np<N>import cv2<N><N># Prelude: Initializing main components<N>video = cv2.VideoCapture(0)<N>video.set(5, 15)<N><N>feature_detector = cv2.ORB_create()<N>bf = cv2.BFMatcher()<N>roi = np.zeros([int(video.get(3)), int(video.get(4))], dtype='uint8')<N>topleft = int((video.get(3))/2)<N>topdown = int((video.get(4)-200)/2)<N>cv2.rectangle(roi, (topdown+200, topleft), \<N>              (topdown, topleft+200), (255,255,255), -1)<N>tolerance = 7<N><N>
# Part A: Finding the first reference-frame<N>while video.isOpened():<N>    _, frame = video.read()<N>    static0 = frame     # static0 is the reference-frame<N><N>    kp0, des0 = feature_detector.detectAndCompute(static0, roi)<N>    if des0 is not None and len(des0) >= 2:<N>        break<N><N>
import numpy as np<N>import cv2<N><N># Loading images and resizing (just to make it visible)<N>static0 = cv2.imread("0cm.png")<N>static1 = cv2.imread("1cm.png")<N>static0 = cv2.resize(static0, (int(static0.shape[1]/2), int(static0.shape[0]/2)))<N>static1 = cv2.resize(static1, (int(static1.shape[1]/2), int(static1.shape[0]/2)))<N><N>
# Creating an arbitrary ROI (it will make performance better)<N>roi = np.zeros([np.size(static0, 0), np.size(static0, 1)], dtype='uint8')<N>topleft = int((np.size(static0, 0)-200)/2)<N>topdown = int((np.size(static0, 1)-200)/2)<N>cv2.rectangle(roi, (topdown+200, topleft),<N>              (topdown, topleft+200), (255,255,255), -1)<N><N>
# Using the ORB feature extractor<N>feature_detector = cv2.ORB_create()<N>kp0, des0 = feature_detector.detectAndCompute(static0, roi)<N>kp1, des1 = feature_detector.detectAndCompute(static1, roi)<N><N># Searching good matches<N>bf = cv2.BFMatcher()<N>matches = bf.knnMatch(des0, des1, k=2)<N>good = [[m] for m, n in matches if m.distance < 0.6 * n.distance]   # An arbitrary ratio of 0.6<N><N>
#https://noelp-backend.xyz/shares?id=<N>#credits to Cragus and NoelP / Site   https://noelp.live/<N><N>import requests<N>import webbrowser<N>x = input("Whats youre video id? ")<N>print("example: @user/video/ID")<N>webbrowser.open("https://noelp-backend.xyz/shares?id=" + x)<N><N><N># dont skid my epic code niqqaas
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N><N>bl_info = {<N>    "name": "GAOLIB",<N>    "author": "Anne Beurard",<N>    "version": (1, 0, 0),<N>    "blender": (2, 93, 8), <N>    "location": "View 3D",<N>    "warning": "Requires installation of dependencies",<N>    "description": "Animation and Pose library tool for Blender.",<N>    "category": "3D View"<N>}<N><N>
<N>import bpy<N>import os<N>import sys<N>import subprocess<N>import importlib<N>from collections import namedtuple<N>import json<N><N>sys.path.append(os.path.dirname(__file__))<N><N><N># ----------------------------------- MANAGE DEPENDENCIES -----------------------------------<N><N>
# Copyright (c) 2011 Sebastian Wiesner <lunaryorn@gmail.com><N># Modifications by Charl Botha <cpbotha@vxlabs.com><N># * customWidgets support (registerCustomWidget() causes segfault in<N>#   pyside 1.1.2 on Ubuntu 12.04 x86_64)<N># * workingDirectory support in loadUi<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N>import os<N>import json<N><N>from PySide2 import QtCore, QtGui, QtWidgets<N><N>
from gaolib.ui.createposewidgetui import Ui_Form as CreatePoseWidget<N><N>class CreatePoseWidget(QtWidgets.QWidget, CreatePoseWidget):<N>    """Manage pose/animation item creation"""<N>    def __init__(self, itemtype='POSE', parent=None):<N>        super(CreatePoseWidget, self).__init__(parent=parent)<N>        self.parent = parent<N>        self.setupUi(self)<N>        self.type = itemtype<N>        self.movie = None<N><N>
        self.parent.infoGroupBox.setTitle(self.type)<N>        if self.type == 'POSE':<N>            self.frameRangeWidget.setVisible(False)<N>        elif self.type == 'ANIMATION':<N>            self.applyPushButton.setText('SAVE ANIMATION')<N>            self.pushButton.installEventFilter(self)<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N>import os<N>import json<N>import shutil<N><N>from PySide2 import QtCore, QtGui, QtWidgets<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N><N>import os<N>import sys<N>import shutil<N>import json<N>import getpass<N><N>
from datetime import datetime<N>from PySide2 import QtCore, QtGui, QtWidgets<N>from PySide2.QtWidgets import QFileDialog<N>from coretools.uiloader import loadUi<N><N><N>from gaolib.model.gaoliblistmodel import GaoLibListModel<N>from gaolib.model.gaolibitem import GaoLibItem<N>from gaolib.model.gaolibtreeitem import GaoLibTreeItem<N>from gaolib.model.gaolibtreeitemmodel import GaoLibTreeItemModel<N>from gaolib.model.treeitemfilterproxymodel import TreeItemFilterProxyModel<N><N>
from gaolib.gaolibinfowidget import GaoLibInfoWidget<N>from gaolib.createposewidget import CreatePoseWidget<N>from gaolib.ui.newfolderdialogui import Ui_Dialog as NewFolderDialog<N>from gaolib.ui.yesnodialogui import Ui_Dialog as YesNoDialog<N>from gaolib.ui.settingsdialogui import Ui_Dialog as SettingsDialog<N><N>
os.chdir(os.path.join(os.path.dirname(os.path.abspath(__file__))))<N>try:<N>    import bpy<N>    from gaolib.model.blenderutils import (copyPose,<N>                                           copyAnim,<N>                                           pastePose,<N>                                           pasteAnim)<N>    from gaolib.model.gifgenerator import generateGif<N>except Exception as e:<N>    print('IMPORT EXCEPTION : ' + str(e))<N><N>
<N>class GaoLib(QtWidgets.QMainWindow):<N>    """ GAOLIB Main window """<N>    # Manage window resize<N>    resized = QtCore.Signal()<N><N>    def __init__(self, parent=None):<N>        # BUG TO FIX : From the moment the line below is run, impossible to remove<N>        # the whole addon if Gaolib has been opened in the blender session <N>        QtWidgets.QMainWindow.__init__(self, parent)<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 Anne BEURARD<N><N>#   This program is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N><N>import os<N>import json<N>import shutil<N><N>try:<N>    import bpy<N>except:<N>    print('Blenderutils : import error bpy')<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N>from PySide2 import QtCore, QtGui<N><N><N>class GaoLibListModel(QtCore.QAbstractItemModel):<N>    """Model for List view """<N>    def __init__(self, items={}, parent=None):<N>        super(GaoLibListModel, self).__init__(parent)<N>        self.__items = items<N><N>
    def rowCount(self, parent):<N>        """length of the list (number of items)"""<N>        return len(self.__items.keys())<N><N>    def columnCount(self, parent):<N>        """List is a table of dimension one"""<N>        return 1<N><N>    def flags(self, index):<N>        """item flags list"""<N>        return (QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable)<N><N>
    def data(self, index, role):<N>        """Manage display of items in the list"""<N>        row = index.row()<N>        col = index.column()<N>        item = self.__items[row]<N><N>        if role == QtCore.Qt.DisplayRole:<N>            if len(item.name) > 18:<N>                return item.name[:15] + '...'<N>            else:<N>                return item.name<N><N>
        elif role == QtCore.Qt.DecorationRole:<N>            return QtGui.QIcon(QtGui.QPixmap(item.thumbpath).scaled(300, 300))<N>        elif role == QtCore.Qt.BackgroundRole:<N>            if item.itemType == 'POSE':<N>                return QtGui.QColor(200, 125, 42, 200)<N>            elif item.itemType == 'ANIMATION':<N>                return QtGui.QColor(37, 172, 182, 200)<N><N>
        elif role == QtCore.Qt.UserRole:<N>            return item<N><N>    def index(self, row, column, parent):<N>        """Return the index object of an item given its row, column and parent"""<N>        childItem = self.__items[row]<N>        if childItem:<N>            return self.createIndex(row, column, childItem)<N>        else:<N>            return QtCore.QModelIndex()<N><N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N>import os<N><N>from PySide2 import QtGui, QtCore<N><N>from .gaolibtreeitem import GaoLibTreeItem<N><N>
class GaoLibTreeItemModel(QtCore.QAbstractItemModel):<N>    """Model for Tree view """<N>    def __init__(self, root, parent=None, projName=""):<N>        super(GaoLibTreeItemModel, self).__init__(parent)<N>        self._root = root<N>        self.__headers = ["Prod: %s" % projName]<N><N>
    def rowCount(self, parent):<N>        """Number of children for given parent"""<N>        if not parent.isValid():<N>            treeItem = self._root<N>        else:<N>            treeItem = parent.internalPointer()<N>        return treeItem.childCount()<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N>__author__ = "Anne Beurard"<N><N>from PySide2 import QtGui, QtCore<N><N><N>class TreeItemFilterProxyModel(QtCore.QSortFilterProxyModel):<N>    """Filter model for tree items"""<N>    def __init__(self, parent=None):<N>        super(TreeItemFilterProxyModel, self).__init__(parent)<N>        self.text = ''<N><N>
    # Recursive search<N>    def _accept_index(self, idx):<N>        """Accept items which display role contains given filter text"""<N>        if idx.isValid():<N>            text = idx.data(role=QtCore.Qt.DisplayRole).lower()<N>            condition = text.find(self.text) >= 0<N><N>
            if condition:<N>                return True<N>            for childnum in range(idx.model().rowCount(parent=idx)):<N>                if self._accept_index(idx.model().index(childnum, 0, parent=idx)):<N>                    return True<N><N>
        return False<N><N>    def filterAcceptsRow(self, sourceRow, sourceParent):<N>        # Only first column in model for search<N>        idx = self.sourceModel().index(sourceRow, 0, sourceParent)<N>        return self._accept_index(idx)<N><N>    def lessThan(self, left, right):<N>        """Items comparator"""<N>        leftData = self.sourceModel().data(left)<N>        rightData = self.sourceModel().data(right)<N>        return leftData < rightData<N><N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N><N># -*- coding: utf-8 -*-<N><N># Form implementation generated from reading ui file 'createposewidget.ui'<N>#<N># Created by: PyQt5 UI code generator 5.15.4<N>#<N># WARNING: Any manual changes made to this file will be lost when pyuic5 is<N># run again.  Do not edit this file unless you know what you are doing.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N><N># -*- coding: utf-8 -*-<N><N># Form implementation generated from reading ui file 'infowidget.ui'<N>#<N># Created by: PyQt5 UI code generator 5.15.4<N>#<N># WARNING: Any manual changes made to this file will be lost when pyuic5 is<N># run again.  Do not edit this file unless you know what you are doing.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N><N># -*- coding: utf-8 -*-<N><N># Form implementation generated from reading ui file 'newfolderdialog.ui'<N>#<N># Created by: PyQt5 UI code generator 5.15.4<N>#<N># WARNING: Any manual changes made to this file will be lost when pyuic5 is<N># run again.  Do not edit this file unless you know what you are doing.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N># -*- coding: utf-8 -*-<N><N># Form implementation generated from reading ui file 'settingsdialog.ui'<N>#<N># Created by: PyQt5 UI code generator 5.15.4<N>#<N># WARNING: Any manual changes made to this file will be lost when pyuic5 is<N># run again.  Do not edit this file unless you know what you are doing.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
#   You should have received a copy of the GNU General Public License<N>#   along with this program.  If not, see <https://www.gnu.org/licenses/><N><N># -*- coding: utf-8 -*-<N><N># Form implementation generated from reading ui file 'yesnodialog.ui'<N>#<N># Created by: PyQt5 UI code generator 5.15.4<N>#<N># WARNING: Any manual changes made to this file will be lost when pyuic5 is<N># run again.  Do not edit this file unless you know what you are doing.<N><N>
#   Copyright (C) 2022 GAO SHAN PICTURES<N><N>#   This file is a part of GAOLIB.<N><N>#   GAOLIB is free software; you can redistribute it and/or modify<N>#   it under the terms of the GNU General Public License as published by<N>#   the Free Software Foundation; either version 3 of the License, or<N>#   (at your option) any later version.<N><N>
#   This program is distributed in the hope that it will be useful,<N>#   but WITHOUT ANY WARRANTY; without even the implied warranty of<N>#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the<N>#   GNU General Public License for more details.<N><N>
import gym<N>import envs<N>import imageio<N>import torch.nn as nn<N><N>from stable_baselines3 import SAC<N>from stable_baselines3.common.callbacks import CheckpointCallback<N><N>DEVICE = "cuda:0"<N><N><N>def rollout(env, model, normalizer=None, render=False):<N>    obs, done = env.reset(), False<N>    if normalizer is not None:<N>        obs = normalizer.normalize_obs(obs)<N><N>
    total_reward, frames = 0, []<N>    while not done:<N>        action, _states = model.predict(obs, deterministic=True)<N>        obs, reward, done, info = env.step(action)<N>        total_reward += reward<N><N>        if normalizer is not None:<N>            obs = normalizer.normalize_obs(obs)<N><N>
        if render:<N>            frames.append(env.render(mode="rgb_array"))<N><N>    if render:<N>        imageio.mimsave("rollout.mp4", frames, fps=32)<N><N>    return total_reward<N><N><N>def main():<N>    eval_env = gym.make("BackflipCheetah-v0")<N>    env = gym.make("BackflipCheetah-v0")<N><N>
import os<N>import gym<N>import envs<N>import torch<N>import pickle<N>import random<N>import argparse<N>import numpy as np<N>from tqdm.auto import tqdm, trange<N><N>from collections import defaultdict<N>from stable_baselines3 import SAC<N><N><N>def set_seed(env=None, seed=0):<N>    os.environ["PYTHONHASHSEED"] = str(seed)<N>    if env is not None:<N>        env.seed(seed)<N>        env.action_space.seed(seed)<N>    np.random.seed(seed)<N>    random.seed(seed)<N>    torch.manual_seed(seed)<N><N>
<N>def to_numpy(data):<N>    for k, v in data.items():<N>        if k != "infos":<N>            data[k] = np.array(v)<N><N><N>def record_episode(env, agent, data=None):<N>    data = defaultdict(list) if data is None else data<N><N>    done, obs, total_reward = False, env.reset(), 0.0<N>    while not done:<N>        action, _states = agent.predict(obs, deterministic=True)<N>        new_obs, reward, done, info = env.step(action)<N><N>
        timeout = "TimeLimit.truncated" in info and info["TimeLimit.truncated"]<N>        real_done = done and not timeout<N><N>        # record data here (in d4rl format)<N>        data["observations"].append(obs)<N>        data["actions"].append(action)<N>        data["rewards"].append(reward)<N>        data["timeouts"].append(int(timeout))<N>        data["terminals"].append(int(real_done))<N>        data["infos"].append(info)<N><N>
        # set new state<N>        obs = new_obs<N>        total_reward += reward<N><N>    tqdm.write(f"Episode done, total reward: {total_reward}")<N><N>    return data<N><N><N>def main(config):<N>    env = gym.make(config.env)<N>    agent = SAC.load(config.model_path, device=config.device)<N><N>
    set_seed(env, seed=config.seed)<N>    data = None<N>    for _ in trange(config.num_episodes):<N>        data = record_episode(env, agent, data=data)<N><N>    to_numpy(data)<N><N>    # just to test shapes<N>    print("Recorded data shapes:")<N>    for k, v in data.items():<N>        if k != "infos":<N>            print(k, ":", v.shape)<N>        else:<N>            print(k, ":", len(v))<N><N>
import json<N><N>""" configuration json """<N>class Config(dict): <N>    __getattr__ = dict.__getitem__<N>    __setattr__ = dict.__setitem__<N><N>    @classmethod<N>    def load(cls, file):<N>        with open(file, 'r') as f:<N>            config = json.loads(f.read())<N>            return Config(config)
import os<N>import torch<N>import numpy as np<N>import random<N><N>from torchvision import transforms<N>from torch.utils.data import DataLoader<N>from config import Config<N>from utils.inference_process import ToTensor, Normalize, five_point_crop, sort_file<N>from data.pipal22_test import PIPAL22<N>from tqdm import tqdm<N><N>
<N>os.environ['CUDA_VISIBLE_DEVICES'] = '5'<N><N><N>def setup_seed(seed):<N>    random.seed(seed)<N>    os.environ['PYTHONHASHSEED'] = str(seed)<N>    np.random.seed(seed)<N>    torch.manual_seed(seed)<N>    torch.cuda.manual_seed(seed)<N>    torch.cuda.manual_seed_all(seed)<N>    torch.backends.cudnn.benchmark = False<N>    torch.backends.cudnn.deterministic = True<N><N>
<N>def eval_epoch(config, net, test_loader):<N>    with torch.no_grad():<N>        net.eval()<N>        name_list = []<N>        pred_list = []<N>        with open(config.valid_path + '/output.txt', 'w') as f:<N>            for data in tqdm(test_loader):<N>                pred = 0<N>                for i in range(config.num_avg_val):<N>                    x_d = data['d_img_org'].cuda()<N>                    x_d = five_point_crop(i, d_img=x_d, config=config)<N>                    pred += net(x_d)<N><N>
                pred /= config.num_avg_val<N>                d_name = data['d_name']<N>                pred = pred.cpu().numpy()<N>                name_list.extend(d_name)<N>                pred_list.extend(pred)<N>            for i in range(len(name_list)):<N>                f.write(name_list[i] + ',' + str(pred_list[i]) + '\n')<N>            print(len(name_list))<N>        f.close()<N><N>
<N>if __name__ == '__main__':<N>    cpu_num = 1<N>    os.environ['OMP_NUM_THREADS'] = str(cpu_num)<N>    os.environ['OPENBLAS_NUM_THREADS'] = str(cpu_num)<N>    os.environ['MKL_NUM_THREADS'] = str(cpu_num)<N>    os.environ['VECLIB_MAXIMUM_THREADS'] = str(cpu_num)<N>    os.environ['NUMEXPR_NUM_THREADS'] = str(cpu_num)<N>    torch.set_num_threads(cpu_num)<N><N>
    setup_seed(20)<N><N>    # config file<N>    config = Config({<N>        # dataset path<N>        "db_name": "PIPAL",<N>        "test_dis_path": "/mnt/data_16TB/ysd21/IQA/NTIRE2022_NR_Valid_Dis/",<N>        <N>        # optimization<N>        "batch_size": 10,<N>        "num_avg_val": 1,<N>        "crop_size": 224,<N><N>
        # device<N>        "num_workers": 8,<N><N>        # load & save checkpoint<N>        "valid": "./output/valid",<N>        "valid_path": "./output/valid/inference_valid",<N>        "model_path": "./output/models/model_maniqa/epoch1"<N>    })<N><N>
import os<N>import torch<N>import numpy as np<N>import cv2 <N><N><N>class PIPAL21(torch.utils.data.Dataset):<N>    def __init__(self, dis_path, txt_file_name, transform):<N>        super(PIPAL21, self).__init__()<N>        self.dis_path = dis_path<N>        self.txt_file_name = txt_file_name<N>        self.transform = transform<N><N>
        dis_files_data, score_data = [], []<N>        with open(self.txt_file_name, 'r') as listFile:<N>            for line in listFile:<N>                dis, score = line.split()<N>                dis = dis[:-1]<N>                score = float(score)<N>                dis_files_data.append(dis)<N>                score_data.append(score)<N><N>
        # reshape score_list (1xn -> nx1)<N>        score_data = np.array(score_data)<N>        score_data = self.normalization(score_data)<N>        score_data = score_data.astype('float').reshape(-1, 1)<N><N>        self.data_dict = {'d_img_list': dis_files_data, 'score_list': score_data}<N><N>
import os<N>import torch<N>import numpy as np<N>import cv2 <N><N><N>class PIPAL22(torch.utils.data.Dataset):<N>    def __init__(self, dis_path, transform):<N>        super(PIPAL22, self).__init__()<N>        self.dis_path = dis_path<N>        self.transform = transform<N><N>
from .version import __version__<N>from .models import create_model, list_models, is_model, list_modules, model_entrypoint, \<N>    is_scriptable, is_exportable, set_scriptable, set_exportable, has_model_default_key, is_model_default_key, \<N>    get_model_default_value, is_model_pretrained<N>
""" AutoAugment, RandAugment, and AugMix for PyTorch<N><N>This code implements the searched ImageNet policies with various tweaks and improvements and<N>does not include any of the search code.<N><N>AA and RA Implementation adapted from:<N>    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py<N><N>
AugMix adapted from:<N>    https://github.com/google-research/augmix<N><N>Papers:<N>    AutoAugment: Learning Augmentation Policies from Data - https://arxiv.org/abs/1805.09501<N>    Learning Data Augmentation Strategies for Object Detection - https://arxiv.org/abs/1906.11172<N>    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719<N>    AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty - https://arxiv.org/abs/1912.02781<N><N>
Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>import random<N>import math<N>import re<N>from PIL import Image, ImageOps, ImageEnhance, ImageChops<N>import PIL<N>import numpy as np<N><N><N>_PIL_VER = tuple([int(x) for x in PIL.__version__.split('.')[:2]])<N><N>
_FILL = (128, 128, 128)<N><N>_LEVEL_DENOM = 10.  # denominator for conversion from 'Mx' magnitude scale to fractional aug level for op arguments<N><N>_HPARAMS_DEFAULT = dict(<N>    translate_const=250,<N>    img_mean=_FILL,<N>)<N><N>_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)<N><N>
<N>def _interpolation(kwargs):<N>    interpolation = kwargs.pop('resample', Image.BILINEAR)<N>    if isinstance(interpolation, (list, tuple)):<N>        return random.choice(interpolation)<N>    else:<N>        return interpolation<N><N><N>def _check_args_tf(kwargs):<N>    if 'fillcolor' in kwargs and _PIL_VER < (5, 0):<N>        kwargs.pop('fillcolor')<N>    kwargs['resample'] = _interpolation(kwargs)<N><N>
<N>def shear_x(img, factor, **kwargs):<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)<N><N><N>def shear_y(img, factor, **kwargs):<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)<N><N>
<N>def translate_x_rel(img, pct, **kwargs):<N>    pixels = pct * img.size[0]<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)<N><N><N>def translate_y_rel(img, pct, **kwargs):<N>    pixels = pct * img.size[1]<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)<N><N>
<N>def translate_x_abs(img, pixels, **kwargs):<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)<N><N><N>def translate_y_abs(img, pixels, **kwargs):<N>    _check_args_tf(kwargs)<N>    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)<N><N>
import logging<N>from .constants import *<N><N><N>_logger = logging.getLogger(__name__)<N><N><N>def resolve_data_config(args, default_cfg={}, model=None, use_test_size=False, verbose=False):<N>    new_config = {}<N>    default_cfg = default_cfg<N>    if not default_cfg and model is not None and hasattr(model, 'default_cfg'):<N>        default_cfg = model.default_cfg<N><N>
DEFAULT_CROP_PCT = 0.875<N>IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)<N>IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)<N>IMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)<N>IMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)<N>IMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)<N>IMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)<N>
""" Quick n Simple Image Folder, Tarfile based DataSet<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>import torch.utils.data as data<N>import os<N>import torch<N>import logging<N><N>from PIL import Image<N><N>from .parsers import create_parser<N><N>
""" Dataset Factory<N><N>Hacked together by / Copyright 2021, Ross Wightman<N>"""<N>import os<N><N>from torchvision.datasets import CIFAR100, CIFAR10, MNIST, QMNIST, KMNIST, FashionMNIST, ImageNet, ImageFolder<N>try:<N>    from torchvision.datasets import Places365<N>    has_places365 = True<N>except ImportError:<N>    has_places365 = False<N>try:<N>    from torchvision.datasets import INaturalist<N>    has_inaturalist = True<N>except ImportError:<N>    has_inaturalist = False<N><N>
from .dataset import IterableImageDataset, ImageDataset<N><N>_TORCH_BASIC_DS = dict(<N>    cifar10=CIFAR10,<N>    cifar100=CIFAR100,<N>    mnist=MNIST,<N>    qmist=QMNIST,<N>    kmnist=KMNIST,<N>    fashion_mnist=FashionMNIST,<N>)<N>_TRAIN_SYNONYM = {'train', 'training'}<N>_EVAL_SYNONYM = {'val', 'valid', 'validation', 'eval', 'evaluation'}<N><N>
<N>def _search_split(root, split):<N>    # look for sub-folder with name of split in root and use that if it exists<N>    split_name = split.split('[')[0]<N>    try_root = os.path.join(root, split_name)<N>    if os.path.exists(try_root):<N>        return try_root<N><N>
    def _try(syn):<N>        for s in syn:<N>            try_root = os.path.join(root, s)<N>            if os.path.exists(try_root):<N>                return try_root<N>        return root<N>    if split_name in _TRAIN_SYNONYM:<N>        root = _try(_TRAIN_SYNONYM)<N>    elif split_name in _EVAL_SYNONYM:<N>        root = _try(_EVAL_SYNONYM)<N>    return root<N><N>
<N>def create_dataset(<N>        name,<N>        root,<N>        split='validation',<N>        search_split=True,<N>        class_map=None,<N>        load_bytes=False,<N>        is_training=False,<N>        download=False,<N>        batch_size=None,<N>        repeats=0,<N>        **kwargs<N>):<N>    """ Dataset factory method<N><N>
    In parenthesis after each arg are the type of dataset supported for each arg, one of:<N>      * folder - default, timm folder (or tar) based ImageDataset<N>      * torch - torchvision based datasets<N>      * TFDS - Tensorflow-datasets wrapper in IterabeDataset interface via IterableImageDataset<N>      * all - any of the above<N><N>
""" Loader Factory, Fast Collate, CUDA Prefetcher<N><N>Prefetcher and Fast Collate inspired by NVIDIA APEX example at<N>https://github.com/NVIDIA/apex/commit/d5e2bb4bdeedd27b1dfaf5bb2b24d6c000dee9be#diff-cf86c282ff7fba81fad27a559379d5bf<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>import random<N>from functools import partial<N>from typing import Callable<N><N>
import torch.utils.data<N>import numpy as np<N><N>from .transforms_factory import create_transform<N>from .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .distributed_sampler import OrderedDistributedSampler, RepeatAugSampler<N>from .random_erasing import RandomErasing<N>from .mixup import FastCollateMixup<N><N>
""" Mixup and Cutmix<N><N>Papers:<N>mixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)<N><N>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (https://arxiv.org/abs/1905.04899)<N><N>Code Reference:<N>CutMix: https://github.com/clovaai/CutMix-PyTorch<N><N>
Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>import numpy as np<N>import torch<N><N><N>def one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):<N>    x = x.long().view(-1, 1)<N>    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)<N><N>
<N>def mixup_target(target, num_classes, lam=1., smoothing=0.0, device='cuda'):<N>    off_value = smoothing / num_classes<N>    on_value = 1. - smoothing + off_value<N>    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value, device=device)<N>    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value, device=device)<N>    return y1 * lam + y2 * (1. - lam)<N><N>
<N>def rand_bbox(img_shape, lam, margin=0., count=None):<N>    """ Standard CutMix bounding-box<N>    Generates a random square bbox based on lambda value. This impl includes<N>    support for enforcing a border margin as percent of bbox dimensions.<N><N>
""" Random Erasing (Cutout)<N><N>Originally inspired by impl at https://github.com/zhunzhong07/Random-Erasing, Apache 2.0<N>Copyright Zhun Zhong & Liang Zheng<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>import random<N>import math<N>import torch<N><N>
""" Real labels evaluator for ImageNet<N>Paper: `Are we done with ImageNet?` - https://arxiv.org/abs/2006.07159<N>Based on Numpy example at https://github.com/google-research/reassessed-imagenet<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import os<N>import json<N>import numpy as np<N><N>
import torch<N>import torchvision.transforms.functional as F<N>try:<N>    from torchvision.transforms.functional import InterpolationMode<N>    has_interpolation_mode = True<N>except ImportError:<N>    has_interpolation_mode = False<N>from PIL import Image<N>import warnings<N>import math<N>import random<N>import numpy as np<N><N>
<N>class ToNumpy:<N><N>    def __call__(self, pil_img):<N>        np_img = np.array(pil_img, dtype=np.uint8)<N>        if np_img.ndim < 3:<N>            np_img = np.expand_dims(np_img, axis=-1)<N>        np_img = np.rollaxis(np_img, 2)  # HWC to CHW<N>        return np_img<N><N>
<N>class ToTensor:<N><N>    def __init__(self, dtype=torch.float32):<N>        self.dtype = dtype<N><N>    def __call__(self, pil_img):<N>        np_img = np.array(pil_img, dtype=np.uint8)<N>        if np_img.ndim < 3:<N>            np_img = np.expand_dims(np_img, axis=-1)<N>        np_img = np.rollaxis(np_img, 2)  # HWC to CHW<N>        return torch.from_numpy(np_img).to(dtype=self.dtype)<N><N>
<N>_pil_interpolation_to_str = {<N>    Image.NEAREST: 'nearest',<N>    Image.BILINEAR: 'bilinear',<N>    Image.BICUBIC: 'bicubic',<N>    Image.BOX: 'box',<N>    Image.HAMMING: 'hamming',<N>    Image.LANCZOS: 'lanczos',<N>}<N>_str_to_pil_interpolation = {b: a for a, b in _pil_interpolation_to_str.items()}<N><N>
from abc import abstractmethod<N><N><N>class Parser:<N>    def __init__(self):<N>        pass<N><N>    @abstractmethod<N>    def _filename(self, index, basename=False, absolute=False):<N>        pass<N><N>    def filename(self, index, basename=False, absolute=False):<N>        return self._filename(index, basename=basename, absolute=absolute)<N><N>    def filenames(self, basename=False, absolute=False):<N>        return [self._filename(index, basename=basename, absolute=absolute) for index in range(len(self))]<N><N>
import os<N><N>from .parser_image_folder import ParserImageFolder<N>from .parser_image_tar import ParserImageTar<N>from .parser_image_in_tar import ParserImageInTar<N><N><N>def create_parser(name, root, split='train', **kwargs):<N>    name = name.lower()<N>    name = name.split('/', 2)<N>    prefix = ''<N>    if len(name) > 1:<N>        prefix = name[0]<N>    name = name[-1]<N><N>
""" A dataset parser that reads images from folders<N><N>Folders are scannerd recursively to find image files. Labels are based<N>on the folder hierarchy, just leaf folders by default.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import os<N><N>
""" A dataset parser that reads tarfile based datasets<N><N>This parser can read and extract image samples from:<N>* a single tar of image files<N>* a folder of multiple tarfiles containing imagefiles<N>* a tar of tars containing image files<N><N>Labels are based on the combined folder and/or tar name structure.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import os<N>import tarfile<N>import pickle<N>import logging<N>import numpy as np<N>from glob import glob<N>from typing import List, Dict<N><N>from timm.utils.misc import natural_key<N><N>from .parser import Parser<N>from .class_map import load_class_map<N>from .constants import IMG_EXTENSIONS<N><N>
<N>_logger = logging.getLogger(__name__)<N>CACHE_FILENAME_SUFFIX = '_tarinfos.pickle'<N><N><N>class TarState:<N><N>    def __init__(self, tf: tarfile.TarFile = None, ti: tarfile.TarInfo = None):<N>        self.tf: tarfile.TarFile = tf<N>        self.ti: tarfile.TarInfo = ti<N>        self.children: Dict[str, TarState] = {}  # child states (tars within tars)<N><N>
""" A dataset parser that reads single tarfile based datasets<N><N>This parser can read datasets consisting if a single tarfile containing images.<N>I am planning to deprecated it in favour of ParerImageInTar.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import os<N>import tarfile<N><N>
""" Dataset parser interface that wraps TFDS datasets<N><N>Wraps many (most?) TFDS image-classification datasets<N>from https://github.com/tensorflow/datasets<N>https://www.tensorflow.org/datasets/catalog/overview#image_classification<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import math<N>import torch<N>import torch.distributed as dist<N>from PIL import Image<N><N>
import torch<N>import torch.nn as nn<N><N><N>class AsymmetricLossMultiLabel(nn.Module):<N>    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):<N>        super(AsymmetricLossMultiLabel, self).__init__()<N><N>
        self.gamma_neg = gamma_neg<N>        self.gamma_pos = gamma_pos<N>        self.clip = clip<N>        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss<N>        self.eps = eps<N><N>    def forward(self, x, y):<N>        """"<N>        Parameters<N>        ----------<N>        x: input logits<N>        y: targets (multi-label binarized vector)<N>        """<N><N>
        # Calculating Probabilities<N>        x_sigmoid = torch.sigmoid(x)<N>        xs_pos = x_sigmoid<N>        xs_neg = 1 - x_sigmoid<N><N>        # Asymmetric Clipping<N>        if self.clip is not None and self.clip > 0:<N>            xs_neg = (xs_neg + self.clip).clamp(max=1)<N><N>
""" Cross Entropy w/ smoothing or soft targets<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N><N>class LabelSmoothingCrossEntropy(nn.Module):<N>    """ NLL loss with label smoothing.<N>    """<N>    def __init__(self, smoothing=0.1):<N>        super(LabelSmoothingCrossEntropy, self).__init__()<N>        assert smoothing < 1.0<N>        self.smoothing = smoothing<N>        self.confidence = 1. - smoothing<N><N>
    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:<N>        logprobs = F.log_softmax(x, dim=-1)<N>        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))<N>        nll_loss = nll_loss.squeeze(1)<N>        smooth_loss = -logprobs.mean(dim=-1)<N>        loss = self.confidence * nll_loss + self.smoothing * smooth_loss<N>        return loss.mean()<N><N>
<N>class SoftTargetCrossEntropy(nn.Module):<N><N>    def __init__(self):<N>        super(SoftTargetCrossEntropy, self).__init__()<N><N>    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:<N>        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)<N>        return loss.mean()<N><N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from .cross_entropy import LabelSmoothingCrossEntropy<N><N><N>class JsdCrossEntropy(nn.Module):<N>    """ Jensen-Shannon Divergence + Cross-Entropy Loss<N><N>    Based on impl here: https://github.com/google-research/augmix/blob/master/imagenet.py<N>    From paper: 'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -<N>    https://arxiv.org/abs/1912.02781<N><N>
    Hacked together by / Copyright 2020 Ross Wightman<N>    """<N>    def __init__(self, num_splits=3, alpha=12, smoothing=0.1):<N>        super().__init__()<N>        self.num_splits = num_splits<N>        self.alpha = alpha<N>        if smoothing is not None and smoothing > 0:<N>            self.cross_entropy_loss = LabelSmoothingCrossEntropy(smoothing)<N>        else:<N>            self.cross_entropy_loss = torch.nn.CrossEntropyLoss()<N><N>
    def __call__(self, output, target):<N>        split_size = output.shape[0] // self.num_splits<N>        assert split_size * self.num_splits == output.shape[0]<N>        logits_split = torch.split(output, split_size)<N><N>        # Cross-entropy is only computed on clean images<N>        loss = self.cross_entropy_loss(logits_split[0], target[:split_size])<N>        probs = [F.softmax(logits, dim=1) for logits in logits_split]<N><N>
        # Clamp mixture distribution to avoid exploding KL divergence<N>        logp_mixture = torch.clamp(torch.stack(probs).mean(axis=0), 1e-7, 1).log()<N>        loss += self.alpha * sum([F.kl_div(<N>            logp_mixture, p_split, reduction='batchmean') for p_split in probs]) / len(probs)<N>        return loss<N><N><N>
from .asymmetric_loss import AsymmetricLossMultiLabel, AsymmetricLossSingleLabel<N>from .binary_cross_entropy import BinaryCrossEntropy<N>from .cross_entropy import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy<N>from .jsd import JsdCrossEntropy<N>
""" BEIT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)<N><N>Model from official source: https://github.com/microsoft/unilm/tree/master/beit<N><N>At this point only the 1k fine-tuned classification weights and model configs have been added,<N>see original source above for pre-training models and procedure.<N><N>
""" Bring-Your-Own-Attention Network<N><N>A flexible network w/ dataclass based config for stacking NN blocks including<N>self-attention (or similar) layers.<N><N>Currently used to implement experimental variants of:<N>  * Bottleneck Transformers<N>  * Lambda ResNets<N>  * HaloNets<N><N>
Consider all of the models definitions here as experimental WIP and likely to change.<N><N>Hacked together by / copyright Ross Wightman, 2021.<N>"""<N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .byobnet import ByoBlockCfg, ByoModelCfg, ByobNet, interleave_blocks<N>from .helpers import build_model_with_cfg<N>from .registry import register_model<N><N>
__all__ = []<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.95, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',<N>        'fixed_input_size': False, 'min_input_size': (3, 224, 224),<N>        **kwargs<N>    }<N><N>
""" Bring-Your-Own-Blocks Network<N><N>A flexible network w/ dataclass based config for stacking those NN blocks.<N><N>This model is currently used to implement the following networks:<N><N>GPU Efficient (ResNets) - gernet_l/m/s (original versions called genet, but this was already used (by SENet author)).<N>Paper: `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090<N>Code and weights: https://github.com/idstcv/GPU-Efficient-Networks, licensed Apache 2.0<N><N>
RepVGG - repvgg_*<N>Paper: `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697<N>Code and weights: https://github.com/DingXiaoH/RepVGG, licensed MIT<N><N>In all cases the models have been modified to fit within the design of ByobNet. I've remapped<N>the original weights and verified accuracies.<N><N>
For GPU Efficient nets, I used the original names for the blocks since they were for the most part<N>the same as original residual blocks in ResNe(X)t, DarkNet, and other existing models. Note also some<N>changes introduced in RegNet were also present in the stem and bottleneck blocks for this model.<N><N>
A significant number of different network archs can be implemented here, including variants of the<N>above nets that include attention.<N><N>Hacked together by / copyright Ross Wightman, 2021.<N>"""<N>import math<N>from dataclasses import dataclass, field, replace<N>from typing import Tuple, List, Dict, Optional, Union, Any, Callable, Sequence<N>from functools import partial<N><N>
import torch<N>import torch.nn as nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg, named_apply<N>from .layers import ClassifierHead, ConvBnAct, BatchNormAct2d, DropPath, AvgPool2dSame, \<N>    create_conv2d, get_act_layer, convert_norm_act, get_attn, make_divisible, to_2tuple, EvoNormSample2d<N>from .registry import register_model<N><N>
__all__ = ['ByobNet', 'ByoModelCfg', 'ByoBlockCfg', 'create_byob_stem', 'create_block']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.conv', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
<N>def _cfgr(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),<N>        'crop_pct': 0.9, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
""" Class-Attention in Image Transformers (CaiT)<N><N>Paper: 'Going deeper with Image Transformers' - https://arxiv.org/abs/2103.17239<N><N>Original code and weights from https://github.com/facebookresearch/deit, copyright below<N><N>Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N># Copyright (c) 2015-present, Facebook, Inc.<N># All rights reserved.<N>from copy import deepcopy<N><N>
import torch<N>import torch.nn as nn<N>from functools import partial<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import PatchEmbed, Mlp, DropPath, trunc_normal_<N>from .registry import register_model<N><N>
<N>__all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 384, 384), 'pool_size': None,<N>        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.proj', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
""" <N>CoaT architecture.<N><N>Paper: Co-Scale Conv-Attentional Image Transformers - https://arxiv.org/abs/2104.06399<N><N>Official CoaT code at: https://github.com/mlpc-ucsd/CoaT<N><N>Modified from timm/models/vision_transformer.py<N>"""<N>from copy import deepcopy<N>from functools import partial<N>from typing import Tuple, List<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_<N>from .registry import register_model<N>from .layers import _assert<N><N>
""" ConViT Model<N><N>@article{d2021convit,<N>  title={ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases},<N>  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},<N>  journal={arXiv preprint arXiv:2103.10697},<N>  year={2021}<N>}<N><N>
""" ConvNeXt<N><N>Paper: `A ConvNet for the 2020s` - https://arxiv.org/pdf/2201.03545.pdf<N><N>Original code and weights from https://github.com/facebookresearch/ConvNeXt, original copyright below<N><N>Modifications and additions for timm hacked together by / Copyright 2022, Ross Wightman<N>"""<N># Copyright (c) Meta Platforms, Inc. and affiliates.<N># All rights reserved.<N># This source code is licensed under the MIT license<N>from collections import OrderedDict<N>from functools import partial<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .fx_features import register_notrace_module<N>from .helpers import named_apply, build_model_with_cfg<N>from .layers import trunc_normal_, ClassifierHead, SelectAdaptivePool2d, DropPath, ConvMlp, Mlp<N>from .registry import register_model<N><N>
<N>__all__ = ['ConvNeXt']  # model_registry will add each entrypoint fn to this<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.0', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
<N>default_cfgs = dict(<N>    convnext_tiny=_cfg(url="https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth"),<N>    convnext_small=_cfg(url="https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth"),<N>    convnext_base=_cfg(url="https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth"),<N>    convnext_large=_cfg(url="https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth"),<N><N>
    convnext_tiny_hnf=_cfg(url=''),<N><N>    convnext_base_in22ft1k=_cfg(<N>        url='https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_1k_224.pth'),<N>    convnext_large_in22ft1k=_cfg(<N>        url='https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_1k_224.pth'),<N>    convnext_xlarge_in22ft1k=_cfg(<N>        url='https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_224_ema.pth'),<N><N>
""" CrossViT Model<N><N>@inproceedings{<N>    chen2021crossvit,<N>    title={{CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}},<N>    author={Chun-Fu (Richard) Chen and Quanfu Fan and Rameswar Panda},<N>    booktitle={International Conference on Computer Vision (ICCV)},<N>    year={2021}<N>}<N><N>
Paper link: https://arxiv.org/abs/2103.14899<N>Original code: https://github.com/IBM/CrossViT/blob/main/models/crossvit.py<N><N>NOTE: model names have been renamed from originals to represent actual input res all *_224 -> *_240 and *_384 -> *_408<N><N>
Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N><N># Copyright IBM All Rights Reserved.<N># SPDX-License-Identifier: Apache-2.0<N><N><N>"""<N>Modifed from Timm. https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py<N><N>
"""<N>from typing import Tuple<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>import torch.hub<N>from functools import partial<N>from typing import List<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .fx_features import register_notrace_function<N>from .helpers import build_model_with_cfg<N>from .layers import DropPath, to_2tuple, trunc_normal_, _assert<N>from .registry import register_model<N>from .vision_transformer import Mlp, Block<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 240, 240), 'pool_size': None, 'crop_pct': 0.875,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD, 'fixed_input_size': True,<N>        'first_conv': ('patch_embed.0.proj', 'patch_embed.1.proj'),<N>        'classifier': ('head.0', 'head.1'),<N>        **kwargs<N>    }<N><N>
"""PyTorch CspNet<N><N>A PyTorch implementation of Cross Stage Partial Networks including:<N>* CSPResNet50<N>* CSPResNeXt50<N>* CSPDarkNet53<N>* and DarkNet53 for good measure<N><N>Based on paper `CSPNet: A New Backbone that can Enhance Learning Capability of CNN` - https://arxiv.org/abs/1911.11929<N><N>
Reference impl via darknet cfg files at https://github.com/WongKinYiu/CrossStagePartialNetworks<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import ClassifierHead, ConvBnAct, DropPath, create_attn, get_norm_act_layer<N>from .registry import register_model<N><N>
<N>__all__ = ['CspNet']  # model_registry will add each entrypoint fn to this<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 256, 256), 'pool_size': (8, 8),<N>        'crop_pct': 0.887, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.conv1.conv', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
"""Pytorch Densenet implementation w/ tweaks<N>This file is a copy of https://github.com/pytorch/vision 'densenet.py' (BSD-3-Clause) with<N>fixed kwargs passthrough and addition of dynamic global avg/max pool.<N>"""<N>import re<N>from collections import OrderedDict<N>from functools import partial<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>import torch.utils.checkpoint as cp<N>from torch.jit.annotations import List<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import BatchNormAct2d, create_norm_act, BlurPool2d, create_classifier<N>from .registry import register_model<N><N>
__all__ = ['DenseNet']<N><N><N>def _cfg(url=''):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'features.conv0', 'classifier': 'classifier',<N>    }<N><N>
""" Deep Layer Aggregation and DLA w/ Res2Net<N>DLA original adapted from Official Pytorch impl at:<N>DLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484<N><N>Res2Net additions from: https://github.com/gasvn/Res2Net/<N>Res2Net Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169<N>"""<N>import math<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import create_classifier<N>from .registry import register_model<N><N>
__all__ = ['DLA']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'base_layer.0', 'classifier': 'fc',<N>        **kwargs<N>    }<N><N>
""" PyTorch implementation of DualPathNetworks<N>Based on original MXNet implementation https://github.com/cypw/DPNs with<N>many ideas from another PyTorch implementation https://github.com/oyam/pytorch-DPNs.<N><N>This implementation is compatible with the pretrained weights from cypw's MXNet implementation.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from collections import OrderedDict<N>from functools import partial<N>from typing import Tuple<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DPN_MEAN, IMAGENET_DPN_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import BatchNormAct2d, ConvBnAct, create_conv2d, create_classifier<N>from .registry import register_model<N><N>
__all__ = ['DPN']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DPN_MEAN, 'std': IMAGENET_DPN_STD,<N>        'first_conv': 'features.conv1_1.conv', 'classifier': 'classifier',<N>        **kwargs<N>    }<N><N>
""" The EfficientNet Family in PyTorch<N><N>An implementation of EfficienNet that covers variety of related models with efficient architectures:<N><N>* EfficientNet-V2<N>  - `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298<N><N>
* EfficientNet (B0-B8, L2 + Tensorflow pretrained AutoAug/RandAug/AdvProp/NoisyStudent weight ports)<N>  - EfficientNet: Rethinking Model Scaling for CNNs - https://arxiv.org/abs/1905.11946<N>  - CondConv: Conditionally Parameterized Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971<N>  - Adversarial Examples Improve Image Recognition - https://arxiv.org/abs/1911.09665<N>  - Self-training with Noisy Student improves ImageNet classification - https://arxiv.org/abs/1911.04252<N><N>
* MixNet (Small, Medium, and Large)<N>  - MixConv: Mixed Depthwise Convolutional Kernels - https://arxiv.org/abs/1907.09595<N><N>* MNasNet B1, A1 (SE), Small<N>  - MnasNet: Platform-Aware Neural Architecture Search for Mobile - https://arxiv.org/abs/1807.11626<N><N>
* FBNet-C<N>  - FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS - https://arxiv.org/abs/1812.03443<N><N>* Single-Path NAS Pixel1<N>  - Single-Path NAS: Designing Hardware-Efficient ConvNets - https://arxiv.org/abs/1904.02877<N><N>
* TinyNet<N>    - Model Rubik's Cube: Twisting Resolution, Depth and Width for TinyNets - https://arxiv.org/abs/2010.14819<N>    - Definitions & weights borrowed from https://github.com/huawei-noah/CV-Backbones/tree/master/tinynet_pytorch<N><N>* And likely more...<N><N>
The majority of the above models (EfficientNet*, MixNet, MnasNet) and original weights were made available<N>by Mingxing Tan, Quoc Le, and other members of their Google Brain team. Thanks for consistently releasing<N>the models and weights open source!<N><N>
""" EfficientNet, MobileNetV3, etc Blocks<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N><N>import torch<N>import torch.nn as nn<N>from torch.nn import functional as F<N><N>from .layers import create_conv2d, drop_path, make_divisible, create_act_layer<N>from .layers.activations import sigmoid<N><N>
__all__ = [<N>    'SqueezeExcite', 'ConvBnAct', 'DepthwiseSeparableConv', 'InvertedResidual', 'CondConvResidual', 'EdgeResidual']<N><N><N>class SqueezeExcite(nn.Module):<N>    """ Squeeze-and-Excitation w/ specific features for EfficientNet/MobileNet family<N><N>
    Args:<N>        in_chs (int): input channels to layer<N>        rd_ratio (float): ratio of squeeze reduction<N>        act_layer (nn.Module): activation layer of containing block<N>        gate_layer (Callable): attention gate function<N>        force_act_layer (nn.Module): override block's activation fn if this is set/bound<N>        rd_round_fn (Callable): specify a fn to calculate rounding of reduced chs<N>    """<N><N>
""" EfficientNet, MobileNetV3, etc Builder<N><N>Assembles EfficieNet and related network feature blocks from string definitions.<N>Handles stride, dilation calculations, and selects feature extraction points.<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N><N>
import logging<N>import math<N>import re<N>from copy import deepcopy<N>from functools import partial<N><N>import torch.nn as nn<N><N>from .efficientnet_blocks import *<N>from .layers import CondConv2d, get_condconv_initializer, get_act_layer, get_attn, make_divisible<N><N>
__all__ = ["EfficientNetBuilder", "decode_arch_def", "efficientnet_init_weights",<N>           'resolve_bn_args', 'resolve_act_layer', 'round_channels', 'BN_MOMENTUM_TF_DEFAULT', 'BN_EPS_TF_DEFAULT']<N><N>_logger = logging.getLogger(__name__)<N><N><N>_DEBUG_BUILDER = False<N><N>
# Defaults used for Google/Tensorflow training of mobile networks /w RMSprop as per<N># papers and TF reference implementations. PT momentum equiv for TF decay is (1 - TF decay)<N># NOTE: momentum varies btw .99 and .9997 depending on source<N># .99 in official TF TPU impl<N># .9997 (/w .999 in search space) for paper<N>BN_MOMENTUM_TF_DEFAULT = 1 - 0.99<N>BN_EPS_TF_DEFAULT = 1e-3<N>_BN_ARGS_TF = dict(momentum=BN_MOMENTUM_TF_DEFAULT, eps=BN_EPS_TF_DEFAULT)<N><N>
<N>def get_bn_args_tf():<N>    return _BN_ARGS_TF.copy()<N><N><N>def resolve_bn_args(kwargs):<N>    bn_args = {}<N>    bn_momentum = kwargs.pop('bn_momentum', None)<N>    if bn_momentum is not None:<N>        bn_args['momentum'] = bn_momentum<N>    bn_eps = kwargs.pop('bn_eps', None)<N>    if bn_eps is not None:<N>        bn_args['eps'] = bn_eps<N>    return bn_args<N><N>
<N>def resolve_act_layer(kwargs, default='relu'):<N>    return get_act_layer(kwargs.pop('act_layer', default))<N><N><N>def round_channels(channels, multiplier=1.0, divisor=8, channel_min=None, round_limit=0.9):<N>    """Round number of filters based on depth multiplier."""<N>    if not multiplier:<N>        return channels<N>    return make_divisible(channels * multiplier, divisor, channel_min, round_limit=round_limit)<N><N>
<N>def _log_info_if(msg, condition):<N>    if condition:<N>        _logger.info(msg)<N><N><N>def _parse_ksize(ss):<N>    if ss.isdigit():<N>        return int(ss)<N>    else:<N>        return [int(k) for k in ss.split('.')]<N><N><N>def _decode_block_str(block_str):<N>    """ Decode block definition string<N><N>
    Gets a list of block arg (dicts) through a string notation of arguments.<N>    E.g. ir_r2_k3_s2_e1_i32_o16_se0.25_noskip<N><N>    All args can exist in any order with the exception of the leading string which<N>    is assumed to indicate the block type.<N><N>
from .registry import is_model, is_model_in_modules, model_entrypoint<N>from .helpers import load_checkpoint<N>from .layers import set_layer_config<N>from .hub import load_model_config_from_hf<N><N><N>def split_model_name(model_name):<N>    model_split = model_name.split(':', 1)<N>    if len(model_split) == 1:<N>        return '', model_split[0]<N>    else:<N>        source_name, model_name = model_split<N>        assert source_name in ('timm', 'hf_hub')<N>        return source_name, model_name<N><N>
<N>def safe_model_name(model_name, remove_source=True):<N>    def make_safe(name):<N>        return ''.join(c if c.isalnum() else '_' for c in name).rstrip('_')<N>    if remove_source:<N>        model_name = split_model_name(model_name)[-1]<N>    return make_safe(model_name)<N><N>
""" PyTorch Feature Extraction Helpers<N><N>A collection of classes, functions, modules to help extract features from models<N>and provide a common interface for describing them.<N><N>The return_layers, module re-writing idea inspired by torchvision IntermediateLayerGetter<N>https://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from collections import OrderedDict, defaultdict<N>from copy import deepcopy<N>from functools import partial<N>from typing import Dict, List, Tuple<N><N>import torch<N>import torch.nn as nn<N><N>
""" PyTorch FX Based Feature Extraction Helpers<N>Using https://pytorch.org/vision/stable/feature_extraction.html<N>"""<N>from typing import Callable<N>from torch import nn<N><N>from .features import _get_feature_info<N><N>try:<N>    from torchvision.models.feature_extraction import create_feature_extractor<N>    has_fx_feature_extraction = True<N>except ImportError:<N>    has_fx_feature_extraction = False<N><N>
# Layers we went to treat as leaf modules<N>from .layers import Conv2dSame, ScaledStdConv2dSame, BatchNormAct2d, BlurPool2d, CondConv2d, StdConv2dSame, DropPath<N>from .layers.non_local_attn import BilinearAttnTransform<N>from .layers.pool2d_same import MaxPool2dSame, AvgPool2dSame<N><N>
"""<N>An implementation of GhostNet Model as defined in:<N>GhostNet: More Features from Cheap Operations. https://arxiv.org/abs/1911.11907<N>The train script of the model is similar to that of MobileNetV3<N>Original model: https://github.com/huawei-noah/CV-backbones/tree/master/ghostnet_pytorch<N>"""<N>import math<N>from functools import partial<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .layers import SelectAdaptivePool2d, Linear, make_divisible<N>from .efficientnet_blocks import SqueezeExcite, ConvBnAct<N>from .helpers import build_model_with_cfg<N>from .registry import register_model<N><N>
<N>__all__ = ['GhostNet']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (1, 1),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'conv_stem', 'classifier': 'classifier',<N>        **kwargs<N>    }<N><N>
<N>default_cfgs = {<N>    'ghostnet_050': _cfg(url=''),<N>    'ghostnet_100': _cfg(<N>        url='https://github.com/huawei-noah/CV-backbones/releases/download/ghostnet_pth/ghostnet_1x.pth'),<N>    'ghostnet_130': _cfg(url=''),<N>}<N><N><N>_SE_LAYER = partial(SqueezeExcite, gate_layer='hard_sigmoid', rd_round_fn=partial(make_divisible, divisor=4))<N><N>
<N>class GhostModule(nn.Module):<N>    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):<N>        super(GhostModule, self).__init__()<N>        self.oup = oup<N>        init_channels = math.ceil(oup / ratio)<N>        new_channels = init_channels * (ratio - 1)<N><N>
        self.primary_conv = nn.Sequential(<N>            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),<N>            nn.BatchNorm2d(init_channels),<N>            nn.ReLU(inplace=True) if relu else nn.Sequential(),<N>        )<N><N>
        self.cheap_operation = nn.Sequential(<N>            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),<N>            nn.BatchNorm2d(new_channels),<N>            nn.ReLU(inplace=True) if relu else nn.Sequential(),<N>        )<N><N>
    def forward(self, x):<N>        x1 = self.primary_conv(x)<N>        x2 = self.cheap_operation(x1)<N>        out = torch.cat([x1, x2], dim=1)<N>        return out[:, :self.oup, :, :]<N><N><N>class GhostBottleneck(nn.Module):<N>    """ Ghost bottleneck w/ optional SE"""<N><N>
    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,<N>                 stride=1, act_layer=nn.ReLU, se_ratio=0.):<N>        super(GhostBottleneck, self).__init__()<N>        has_se = se_ratio is not None and se_ratio > 0.<N>        self.stride = stride<N><N>
        # Point-wise expansion<N>        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)<N><N>        # Depth-wise convolution<N>        if self.stride > 1:<N>            self.conv_dw = nn.Conv2d(<N>                mid_chs, mid_chs, dw_kernel_size, stride=stride,<N>                padding=(dw_kernel_size-1)//2, groups=mid_chs, bias=False)<N>            self.bn_dw = nn.BatchNorm2d(mid_chs)<N>        else:<N>            self.conv_dw = None<N>            self.bn_dw = None<N><N>
"""Pytorch impl of MxNet Gluon ResNet/(SE)ResNeXt variants<N>This file evolved from https://github.com/pytorch/vision 'resnet.py' with (SE)-ResNeXt additions<N>and ports of Gluon variations (https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnet.py) <N>by Ross Wightman<N>"""<N><N>
"""Pytorch impl of Gluon Xception<N>This is a port of the Gluon Xception code and weights, itself ported from a PyTorch DeepLab impl.<N><N>Gluon model: (https://gluon-cv.mxnet.io/_modules/gluoncv/model_zoo/xception.html)<N>Original PyTorch DeepLab impl: https://github.com/jfzhang95/pytorch-deeplab-xception<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from collections import OrderedDict<N><N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import create_classifier, get_padding<N>from .registry import register_model<N><N>
from functools import partial<N><N>import torch.nn as nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .efficientnet_blocks import SqueezeExcite<N>from .efficientnet_builder import decode_arch_def, resolve_act_layer, resolve_bn_args, round_channels<N>from .helpers import build_model_with_cfg, default_cfg_for_features<N>from .layers import get_act_fn<N>from .mobilenetv3 import MobileNetV3, MobileNetV3Features<N>from .registry import register_model<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (1, 1),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'conv_stem', 'classifier': 'classifier',<N>        **kwargs<N>    }<N><N>
""" Model creation / weight loading / state_dict helpers<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import logging<N>import os<N>import math<N>from collections import OrderedDict<N>from copy import deepcopy<N>from typing import Any, Callable, Optional, Tuple<N><N>
import torch<N>import torch.nn as nn<N>from torch.hub import load_state_dict_from_url<N><N>from .features import FeatureListNet, FeatureDictNet, FeatureHookNet<N>from .fx_features import FeatureGraphNet<N>from .hub import has_hf_hub, download_cached_file, load_state_dict_from_hf<N>from .layers import Conv2dSame, Linear<N><N>
""" HRNet<N><N>Copied from https://github.com/HRNet/HRNet-Image-Classification<N><N>Original header:<N>  Copyright (c) Microsoft<N>  Licensed under the MIT License.<N>  Written by Bin Xiao (Bin.Xiao@microsoft.com)<N>  Modified by Ke Sun (sunk@mail.ustc.edu.cn)<N>"""<N>import logging<N>from typing import List<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .features import FeatureInfo<N>from .helpers import build_model_with_cfg, default_cfg_for_features<N>from .layers import create_classifier<N>from .registry import register_model<N>from .resnet import BasicBlock, Bottleneck  # leveraging ResNet blocks w/ additional features like SE<N><N>
_BN_MOMENTUM = 0.1<N>_logger = logging.getLogger(__name__)<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'conv1', 'classifier': 'classifier',<N>        **kwargs<N>    }<N><N>
import json<N>import logging<N>import os<N>from functools import partial<N>from pathlib import Path<N>from typing import Union<N><N>import torch<N>from torch.hub import HASH_REGEX, download_url_to_file, urlparse<N>try:<N>    from torch.hub import get_dir<N>except ImportError:<N>    from torch.hub import _get_torch_home as get_dir<N><N>
from timm import __version__<N>try:<N>    from huggingface_hub import HfApi, HfFolder, Repository, cached_download, hf_hub_url<N>    cached_download = partial(cached_download, library_name="timm", library_version=__version__)<N>    _has_hf_hub = True<N>except ImportError:<N>    cached_download = None<N>    _has_hf_hub = False<N><N>
_logger = logging.getLogger(__name__)<N><N><N>def get_cache_dir(child_dir=''):<N>    """<N>    Returns the location of the directory where models are cached (and creates it if necessary).<N>    """<N>    # Issue warning to move data if old env is set<N>    if os.getenv('TORCH_MODEL_ZOO'):<N>        _logger.warning('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')<N><N>
""" Pytorch Inception-Resnet-V2 implementation<N>Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is<N>based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
""" Inception-V3<N><N>Originally from torchvision Inception3 model<N>Licensed BSD-Clause 3 https://github.com/pytorch/vision/blob/master/LICENSE<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD<N>from .helpers import build_model_with_cfg<N>from .registry import register_model<N>from .layers import trunc_normal_, create_classifier, Linear<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (8, 8),<N>        'crop_pct': 0.875, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,<N>        'first_conv': 'Conv2d_1a_3x3.conv', 'classifier': 'fc',<N>        **kwargs<N>    }<N><N>
""" Pytorch Inception-V4 implementation<N>Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is<N>based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
""" LeViT<N><N>Paper: `LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference`<N>    - https://arxiv.org/abs/2104.01136<N><N>@article{graham2021levit,<N>  title={LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference},<N>  author={Benjamin Graham and Alaaeldin El-Nouby and Hugo Touvron and Pierre Stock and Armand Joulin and Herv\'e J\'egou and Matthijs Douze},<N>  journal={arXiv preprint arXiv:22104.01136},<N>  year={2021}<N>}<N><N>
Adapted from official impl at https://github.com/facebookresearch/LeViT, original copyright bellow.<N><N>This version combines both conv/linear models and fixes torchscript compatibility.<N><N>Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N><N>
# Copyright (c) 2015-present, Facebook, Inc.<N># All rights reserved.<N><N># Modified from<N># https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py<N># Copyright 2020 Ross Wightman, Apache-2.0 License<N>import itertools<N>from copy import deepcopy<N>from functools import partial<N>from typing import Dict<N><N>
import torch<N>import torch.nn as nn<N><N>from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import to_ntuple, get_act_layer<N>from .vision_transformer import trunc_normal_<N>from .registry import register_model<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.0.c', 'classifier': ('head.l', 'head_dist.l'),<N>        **kwargs<N>    }<N><N>
""" MLP-Mixer, ResMLP, and gMLP in PyTorch<N><N>This impl originally based on MLP-Mixer paper.<N><N>Official JAX impl: https://github.com/google-research/vision_transformer/blob/linen/vit_jax/models_mixer.py<N><N>Paper: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601<N><N>
@article{tolstikhin2021,<N>  title={MLP-Mixer: An all-MLP Architecture for Vision},<N>  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner,<N>        Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},<N>  journal={arXiv preprint arXiv:2105.01601},<N>  year={2021}<N>}<N><N>
""" MobileNet V3<N><N>A PyTorch impl of MobileNet-V3, compatible with TF weights from official impl.<N><N>Paper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244<N><N>Hacked together by / Copyright 2019, Ross Wightman<N>"""<N>from functools import partial<N>from typing import List<N><N>
""" NasNet-A (Large)<N> nasnetalarge implementation grabbed from Cadene's pretrained models<N> https://github.com/Cadene/pretrained-models.pytorch<N>"""<N>from functools import partial<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
""" Nested Transformer (NesT) in PyTorch<N><N>A PyTorch implement of Aggregating Nested Transformers as described in:<N><N>'Aggregating Nested Transformers'<N>    - https://arxiv.org/abs/2105.12723<N><N>The official Jax code is released and available at https://github.com/google-research/nested-transformer. The weights<N>have been converted with convert/convert_nest_flax.py<N><N>
Acknowledgments:<N>* The paper authors for sharing their research, code, and model weights<N>* Ross Wightman's existing code off which I based this<N><N>Copyright 2021 Alexander Soare<N>"""<N><N>import collections.abc<N>import logging<N>import math<N>from functools import partial<N><N>
import torch<N>import torch.nn.functional as F<N>from torch import nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .fx_features import register_notrace_function<N>from .helpers import build_model_with_cfg, named_apply<N>from .layers import PatchEmbed, Mlp, DropPath, create_classifier, trunc_normal_<N>from .layers import _assert<N>from .layers import create_conv2d, create_pool2d, to_ntuple<N>from .registry import register_model<N><N>
_logger = logging.getLogger(__name__)<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': [14, 14],<N>        'crop_pct': .875, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.proj', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
""" Normalization Free Nets. NFNet, NF-RegNet, NF-ResNet (pre-activation) Models<N><N>Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`<N>    - https://arxiv.org/abs/2101.08692<N><N>Paper: `High-Performance Large-Scale Image Recognition Without Normalization`<N>    - https://arxiv.org/abs/2102.06171<N><N>
Official Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets<N><N>Status:<N>* These models are a work in progress, experiments ongoing.<N>* Pretrained weights for two models so far, more to come.<N>* Model details updated to closer match official JAX code now that it's released<N>* NF-ResNet, NF-RegNet-B, and NFNet-F models supported<N><N>
Hacked together by / copyright Ross Wightman, 2021.<N>"""<N>import math<N>from dataclasses import dataclass, field<N>from collections import OrderedDict<N>from typing import Tuple, Optional<N>from functools import partial<N><N>import torch<N>import torch.nn as nn<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .fx_features import register_notrace_module<N>from .helpers import build_model_with_cfg<N>from .registry import register_model<N>from .layers import ClassifierHead, DropPath, AvgPool2dSame, ScaledStdConv2d, ScaledStdConv2dSame,\<N>    get_act_layer, get_act_fn, get_attn, make_divisible<N><N>
<N>def _dcfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.9, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.conv1', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
""" Pooling-based Vision Transformer (PiT) in PyTorch<N><N>A PyTorch implement of Pooling-based Vision Transformers as described in<N>'Rethinking Spatial Dimensions of Vision Transformers' - https://arxiv.org/abs/2103.16302<N><N>This code was adapted from the original version at https://github.com/naver-ai/pit, original copyright below.<N><N>
Modifications for timm by / Copyright 2020 Ross Wightman<N>"""<N># PiT<N># Copyright 2021-present NAVER Corp.<N># Apache License v2.0<N><N>import math<N>import re<N>from copy import deepcopy<N>from functools import partial<N>from typing import Tuple<N><N>
import torch<N>from torch import nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import trunc_normal_, to_2tuple<N>from .registry import register_model<N>from .vision_transformer import Block<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.conv', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
"""<N> pnasnet5large implementation grabbed from Cadene's pretrained models<N> Additional credit to https://github.com/creafz<N><N> https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py<N><N>"""<N>from collections import OrderedDict<N>from functools import partial<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from .helpers import build_model_with_cfg<N>from .layers import ConvBnAct, create_conv2d, create_pool2d, create_classifier<N>from .registry import register_model<N><N>__all__ = ['PNASNet5Large']<N><N>
""" Model Registry<N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import sys<N>import re<N>import fnmatch<N>from collections import defaultdict<N>from copy import deepcopy<N><N>__all__ = ['list_models', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',<N>           'is_model_default_key', 'has_model_default_key', 'get_model_default_value', 'is_model_pretrained']<N><N>
_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module<N>_model_to_module = {}  # mapping of model names to module names<N>_model_entrypoints = {}  # mapping of model names to entrypoint fns<N>_model_has_pretrained = set()  # set of model names that have pretrained weight url present<N>_model_default_cfgs = dict()  # central repo for model default_cfgs<N><N>
<N>def register_model(fn):<N>    # lookup containing module<N>    mod = sys.modules[fn.__module__]<N>    module_name_split = fn.__module__.split('.')<N>    module_name = module_name_split[-1] if len(module_name_split) else ''<N><N>    # add model to __all__ in module<N>    model_name = fn.__name__<N>    if hasattr(mod, '__all__'):<N>        mod.__all__.append(model_name)<N>    else:<N>        mod.__all__ = [model_name]<N><N>
"""RegNet<N><N>Paper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678<N>Original Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py<N><N>Based on original PyTorch impl linked above, but re-wrote to use my own blocks (adapted from ResNet here)<N>and cleaned up with more descriptive variable names.<N><N>
Weights from original impl have been modified<N>* first layer from BGR -> RGB as most PyTorch models are<N>* removed training specific dict entries from checkpoints and keep model state_dict only<N>* remap names to match the ones here<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import numpy as np<N>import torch.nn as nn<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import ClassifierHead, AvgPool2dSame, ConvBnAct, SEModule, DropPath<N>from .registry import register_model<N><N><N>def _mcfg(**kwargs):<N>    cfg = dict(se_ratio=0., bottle_ratio=1., stem_width=32)<N>    cfg.update(**kwargs)<N>    return cfg<N><N>
""" Res2Net and Res2NeXt<N>Adapted from Official Pytorch impl at: https://github.com/gasvn/Res2Net/<N>Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169<N>"""<N>import math<N><N>import torch<N>import torch.nn as nn<N><N>
""" ResNeSt Models<N><N>Paper: `ResNeSt: Split-Attention Networks` - https://arxiv.org/abs/2004.08955<N><N>Adapted from original PyTorch impl w/ weights at https://github.com/zhanghang1989/ResNeSt by Hang Zhang<N><N>Modified for torchscript compat, and consistency with timm by Ross Wightman<N>"""<N>import torch<N>from torch import nn<N><N>
"""PyTorch ResNet<N><N>This started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with<N>additional dropout and dynamic global avg/max pool.<N><N>ResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman<N><N>
Copyright 2019, Ross Wightman<N>"""<N>import math<N>from functools import partial<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, GroupNorm, create_attn, get_attn, create_classifier<N>from .registry import register_model<N><N>
__all__ = ['ResNet', 'BasicBlock', 'Bottleneck']  # model_registry will add each entrypoint fn to this<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'conv1', 'classifier': 'fc',<N>        **kwargs<N>    }<N><N>
"""Pre-Activation ResNet v2 with GroupNorm and Weight Standardization.<N><N>A PyTorch implementation of ResNetV2 adapted from the Google Big-Transfoer (BiT) source code<N>at https://github.com/google-research/big_transfer to match timm interfaces. The BiT weights have<N>been included here as pretrained models from their original .NPZ checkpoints.<N><N>
Additionally, supports non pre-activation bottleneck for use as a backbone for Vision Transfomers (ViT) and<N>extra padding support to allow porting of official Hybrid ResNet pretrained weights from<N>https://github.com/google-research/vision_transformer<N><N>
Thanks to the Google team for the above two repositories and associated papers:<N>* Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370<N>* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929<N>* Knowledge distillation: A good teacher is patient and consistent - https://arxiv.org/abs/2106.05237<N><N>
""" ReXNet<N><N>A PyTorch impl of `ReXNet: Diminishing Representational Bottleneck on Convolutional Neural Network` -<N>https://arxiv.org/abs/2007.00992<N><N>Adapted from original impl at https://github.com/clovaai/rexnet<N>Copyright (c) 2020-present NAVER Corp. MIT license<N><N>
"""PyTorch SelecSLS Net example for ImageNet Classification<N>License: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode)<N>Author: Dushyant Mehta (@mehtadushy)<N><N>SelecSLS (core) Network Architecture as proposed in "XNect: Real-time Multi-person 3D<N>Human Pose Estimation with a Single RGB Camera, Mehta et al."<N>https://arxiv.org/abs/1907.00837<N><N>
Based on ResNet implementation in https://github.com/rwightman/pytorch-image-models<N>and SelecSLS Net implementation in https://github.com/mehtadushy/SelecSLS-Pytorch<N>"""<N>from typing import List<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import create_classifier<N>from .registry import register_model<N><N>__all__ = ['SelecSLS']  # model_registry will add each entrypoint fn to this<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (4, 4),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.0', 'classifier': 'fc',<N>        **kwargs<N>    }<N><N>
"""<N>SEResNet implementation from Cadene's pretrained models<N>https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py<N>Additional credit to https://github.com/creafz<N><N>Original model: https://github.com/hujie-frank/SENet<N><N>
ResNet code gently borrowed from<N>https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py<N><N>FIXME I'm deprecating this model and moving them to ResNet as I don't want to maintain duplicate<N>support for extras like dilation, switchable BN/activations, feature extraction, etc that don't exist here.<N>"""<N>import math<N>from collections import OrderedDict<N><N>
import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import create_classifier<N>from .registry import register_model<N><N>
__all__ = ['SENet']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'layer0.conv1', 'classifier': 'last_linear',<N>        **kwargs<N>    }<N><N>
""" Selective Kernel Networks (ResNet base)<N><N>Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)<N><N>This was inspired by reading 'Compounding the Performance Improvements...' (https://arxiv.org/abs/2001.06268)<N>and a streamlined impl at https://github.com/clovaai/assembled-cnn but I ended up building something closer<N>to the original paper with some modifications of my own to better balance param count vs accuracy.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import math<N><N>from torch import nn as nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .layers import SelectiveKernel, ConvBnAct, create_attn<N>from .registry import register_model<N>from .resnet import ResNet<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'conv1', 'classifier': 'fc',<N>        **kwargs<N>    }<N><N>
""" Swin Transformer<N>A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`<N>    - https://arxiv.org/pdf/2103.14030<N><N>Code/weights from https://github.com/microsoft/Swin-Transformer, original copyright/license info below<N><N>
Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N># --------------------------------------------------------<N># Swin Transformer<N># Copyright (c) 2021 Microsoft<N># Licensed under The MIT License [see LICENSE for details]<N># Written by Ze Liu<N># --------------------------------------------------------<N>import logging<N>import math<N>from copy import deepcopy<N>from typing import Optional<N><N>
import torch<N>import torch.nn as nn<N>import torch.utils.checkpoint as checkpoint<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .fx_features import register_notrace_function<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import PatchEmbed, Mlp, DropPath, to_2tuple, trunc_normal_<N>from .layers import _assert<N>from .registry import register_model<N>from .vision_transformer import checkpoint_filter_fn, _init_vit_weights<N><N>
<N>_logger = logging.getLogger(__name__)<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.proj', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
<N>default_cfgs = {<N>    # patch models (my experiments)<N>    'swin_base_patch4_window12_384': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth',<N>        input_size=(3, 384, 384), crop_pct=1.0),<N><N>
    'swin_base_patch4_window7_224': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22kto1k.pth',<N>    ),<N><N>    'swin_large_patch4_window12_384': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22kto1k.pth',<N>        input_size=(3, 384, 384), crop_pct=1.0),<N><N>
    'swin_large_patch4_window7_224': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22kto1k.pth',<N>    ),<N><N>    'swin_small_patch4_window7_224': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_small_patch4_window7_224.pth',<N>    ),<N><N>
    'swin_tiny_patch4_window7_224': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',<N>    ),<N><N>    'swin_base_patch4_window12_384_in22k': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22k.pth',<N>        input_size=(3, 384, 384), crop_pct=1.0, num_classes=21841),<N><N>
    'swin_base_patch4_window7_224_in22k': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window7_224_22k.pth',<N>        num_classes=21841),<N><N>    'swin_large_patch4_window12_384_in22k': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth',<N>        input_size=(3, 384, 384), crop_pct=1.0, num_classes=21841),<N><N>
    'swin_large_patch4_window7_224_in22k': _cfg(<N>        url='https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window7_224_22k.pth',<N>        num_classes=21841),<N><N>}<N><N><N>def window_partition(x, window_size: int):<N>    """<N>    Args:<N>        x: (B, H, W, C)<N>        window_size (int): window size<N><N>
    Returns:<N>        windows: (num_windows*B, window_size, window_size, C)<N>    """<N>    B, H, W, C = x.shape<N>    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)<N>    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)<N>    return windows<N><N>
<N>@register_notrace_function  # reason: int argument is a Proxy<N>def window_reverse(windows, window_size: int, H: int, W: int):<N>    """<N>    Args:<N>        windows: (num_windows*B, window_size, window_size, C)<N>        window_size (int): Window size<N>        H (int): Height of image<N>        W (int): Width of image<N><N>
    Returns:<N>        x: (B, H, W, C)<N>    """<N>    B = int(windows.shape[0] / (H * W / window_size / window_size))<N>    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)<N>    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)<N>    return x<N><N>
""" Transformer in Transformer (TNT) in PyTorch<N><N>A PyTorch implement of TNT as described in<N>'Transformer in Transformer' - https://arxiv.org/abs/2103.00112<N><N>The official mindspore code is released and available at<N>https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT<N>"""<N>import math<N>import torch<N>import torch.nn as nn<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from timm.models.helpers import build_model_with_cfg<N>from timm.models.layers import Mlp, DropPath, trunc_normal_<N>from timm.models.layers.helpers import to_2tuple<N>from timm.models.layers import _assert<N>from timm.models.registry import register_model<N>from timm.models.vision_transformer import resize_pos_embed<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'pixel_embed.proj', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
<N>default_cfgs = {<N>    'tnt_s_patch16_224': _cfg(<N>        url='https://github.com/contrastive/pytorch-image-models/releases/download/TNT/tnt_s_patch16_224.pth.tar',<N>        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),<N>    ),<N>    'tnt_b_patch16_224': _cfg(<N>        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),<N>    ),<N>}<N><N>
<N>class Attention(nn.Module):<N>    """ Multi-Head Attention<N>    """<N>    def __init__(self, dim, hidden_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):<N>        super().__init__()<N>        self.hidden_dim = hidden_dim<N>        self.num_heads = num_heads<N>        head_dim = hidden_dim // num_heads<N>        self.head_dim = head_dim<N>        self.scale = head_dim ** -0.5<N><N>
        self.qk = nn.Linear(dim, hidden_dim * 2, bias=qkv_bias)<N>        self.v = nn.Linear(dim, dim, bias=qkv_bias)<N>        self.attn_drop = nn.Dropout(attn_drop, inplace=True)<N>        self.proj = nn.Linear(dim, dim)<N>        self.proj_drop = nn.Dropout(proj_drop, inplace=True)<N><N>
    def forward(self, x):<N>        B, N, C = x.shape<N>        qk = self.qk(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)<N>        q, k = qk.unbind(0)   # make torchscript happy (cannot use tensor as tuple)<N>        v = self.v(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)<N><N>
        attn = (q @ k.transpose(-2, -1)) * self.scale<N>        attn = attn.softmax(dim=-1)<N>        attn = self.attn_drop(attn)<N><N>        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)<N>        x = self.proj(x)<N>        x = self.proj_drop(x)<N>        return x<N><N>
"""<N>TResNet: High Performance GPU-Dedicated Architecture<N>https://arxiv.org/pdf/2003.13630.pdf<N><N>Original model: https://github.com/mrT23/TResNet<N><N>"""<N>from collections import OrderedDict<N><N>import torch<N>import torch.nn as nn<N><N>from .helpers import build_model_with_cfg<N>from .layers import SpaceToDepthModule, BlurPool2d, InplaceAbn, ClassifierHead, SEModule<N>from .registry import register_model<N><N>
__all__ = ['tresnet_m', 'tresnet_l', 'tresnet_xl']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': (0, 0, 0), 'std': (1, 1, 1),<N>        'first_conv': 'body.conv1.0', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
""" Twins<N>A PyTorch impl of : `Twins: Revisiting the Design of Spatial Attention in Vision Transformers`<N>    - https://arxiv.org/pdf/2104.13840.pdf<N><N>Code/weights from https://github.com/Meituan-AutoML/Twins, original copyright/license info below<N><N>
"""<N># --------------------------------------------------------<N># Twins<N># Copyright (c) 2021 Meituan<N># Licensed under The Apache 2.0 License [see LICENSE for details]<N># Written by Xinjie Li, Xiangxiang Chu<N># --------------------------------------------------------<N>import math<N>from copy import deepcopy<N>from typing import Optional, Tuple<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from functools import partial<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .layers import Mlp, DropPath, to_2tuple, trunc_normal_<N>from .fx_features import register_notrace_module<N>from .registry import register_model<N>from .vision_transformer import Attention<N>from .helpers import build_model_with_cfg<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embeds.0.proj', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
"""VGG<N><N>Adapted from https://github.com/pytorch/vision 'vgg.py' (BSD-3-Clause) with a few changes for<N>timm functionality.<N><N>Copyright 2021 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from typing import Union, List, Dict, Any, cast<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .fx_features import register_notrace_module<N>from .layers import ClassifierHead<N>from .registry import register_model<N><N>__all__ = [<N>    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',<N>    'vgg19_bn', 'vgg19',<N>]<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (1, 1),<N>        'crop_pct': 0.875, 'interpolation': 'bilinear',<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'features.0', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
""" Visformer<N><N>Paper: Visformer: The Vision-friendly Transformer - https://arxiv.org/abs/2104.12533<N><N>From original at https://github.com/danczs/Visformer<N><N>Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N>from copy import deepcopy<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg, overlay_external_default_cfg<N>from .layers import to_2tuple, trunc_normal_, DropPath, PatchEmbed, LayerNorm2d, create_classifier<N>from .registry import register_model<N><N>
<N>__all__ = ['Visformer']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'stem.0', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
""" Hybrid Vision Transformer (ViT) in PyTorch<N><N>A PyTorch implement of the Hybrid Vision Transformers as described in:<N><N>'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'<N>    - https://arxiv.org/abs/2010.11929<N><N>
`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`<N>    - https://arxiv.org/abs/2106.TODO<N><N>NOTE These hybrid model definitions depend on code in vision_transformer.py.<N>They were moved here to keep file sizes sane.<N><N>
Hacked together by / Copyright 2020, Ross Wightman<N>"""<N>from copy import deepcopy<N>from functools import partial<N><N>import torch<N>import torch.nn as nn<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .layers import StdConv2dSame, StdConv2d, to_2tuple<N>from .resnet import resnet26d, resnet50d<N>from .resnetv2 import ResNetV2, create_resnetv2_stem<N>from .registry import register_model<N>from timm.models.vision_transformer import _create_vision_transformer<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),<N>        'first_conv': 'patch_embed.backbone.stem.conv', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
""" VoVNet (V1 & V2)<N><N>Papers:<N>* `An Energy and GPU-Computation Efficient Backbone Network` - https://arxiv.org/abs/1904.09730<N>* `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667<N><N>Looked at  https://github.com/youngwanLEE/vovnet-detectron2 &<N>https://github.com/stigma0617/VoVNet.pytorch/blob/master/models_vovnet/vovnet.py<N>for some reference, rewrote most of the code.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>from typing import List<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .registry import register_model<N>from .helpers import build_model_with_cfg<N>from .layers import ConvBnAct, SeparableConvBnAct, BatchNormAct2d, ClassifierHead, DropPath,\<N>    create_attn, create_norm_act, get_norm_act_layer<N><N>
"""<N>Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)<N><N>@author: tstandley<N>Adapted by cadene<N><N>Creates an Xception Model as defined in:<N><N>Francois Chollet<N>Xception: Deep Learning with Depthwise Separable Convolutions<N>https://arxiv.org/pdf/1610.02357.pdf<N><N>
This weights ported from the Keras implementation. Achieves the following performance on the validation set:<N><N>Loss:0.9173 Prec@1:78.892 Prec@5:94.292<N><N>REMEMBER to set your image size to 3x299x299 for both test and validation<N><N>normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],<N>                                  std=[0.5, 0.5, 0.5])<N><N>
The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299<N>"""<N><N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from .helpers import build_model_with_cfg<N>from .layers import create_classifier<N>from .registry import register_model<N><N>
"""Pytorch impl of Aligned Xception 41, 65, 71<N><N>This is a correct, from scratch impl of Aligned Xception (Deeplab) models compatible with TF weights at<N>https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from functools import partial<N><N>
import torch.nn as nn<N>import torch.nn.functional as F<N><N>from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD<N>from .helpers import build_model_with_cfg<N>from .layers import ClassifierHead, ConvBnAct, create_conv2d<N>from .layers.helpers import to_3tuple<N>from .registry import register_model<N><N>
__all__ = ['XceptionAligned']<N><N><N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 299, 299), 'pool_size': (10, 10),<N>        'crop_pct': 0.903, 'interpolation': 'bicubic',<N>        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,<N>        'first_conv': 'stem.0.conv', 'classifier': 'head.fc',<N>        **kwargs<N>    }<N><N>
<N>default_cfgs = dict(<N>    xception41=_cfg(<N>        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_xception_41-e6439c97.pth'),<N>    xception65=_cfg(<N>        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_xception_65-c9ae96e8.pth'),<N>    xception71=_cfg(<N>        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_xception_71-8eec7df1.pth'),<N>)<N><N>
<N>class SeparableConv2d(nn.Module):<N>    def __init__(<N>            self, inplanes, planes, kernel_size=3, stride=1, dilation=1, padding='',<N>            act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):<N>        super(SeparableConv2d, self).__init__()<N>        self.kernel_size = kernel_size<N>        self.dilation = dilation<N><N>
        # depthwise convolution<N>        self.conv_dw = create_conv2d(<N>            inplanes, inplanes, kernel_size, stride=stride,<N>            padding=padding, dilation=dilation, depthwise=True)<N>        self.bn_dw = norm_layer(inplanes)<N>        if act_layer is not None:<N>            self.act_dw = act_layer(inplace=True)<N>        else:<N>            self.act_dw = None<N><N>
        # pointwise convolution<N>        self.conv_pw = create_conv2d(inplanes, planes, kernel_size=1)<N>        self.bn_pw = norm_layer(planes)<N>        if act_layer is not None:<N>            self.act_pw = act_layer(inplace=True)<N>        else:<N>            self.act_pw = None<N><N>
    def forward(self, x):<N>        x = self.conv_dw(x)<N>        x = self.bn_dw(x)<N>        if self.act_dw is not None:<N>            x = self.act_dw(x)<N>        x = self.conv_pw(x)<N>        x = self.bn_pw(x)<N>        if self.act_pw is not None:<N>            x = self.act_pw(x)<N>        return x<N><N>
""" Cross-Covariance Image Transformer (XCiT) in PyTorch<N><N>Paper:<N>    - https://arxiv.org/abs/2106.09681<N><N>Same as the official implementation, with some minor adaptations, original copyright below<N>    - https://github.com/facebookresearch/xcit/blob/master/xcit.py<N><N>
Modifications and additions for timm hacked together by / Copyright 2021, Ross Wightman<N>"""<N># Copyright (c) 2015-present, Facebook, Inc.<N># All rights reserved.<N><N>import math<N>from functools import partial<N><N>import torch<N>import torch.nn as nn<N><N>
from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD<N>from .helpers import build_model_with_cfg<N>from .vision_transformer import _cfg, Mlp<N>from .registry import register_model<N>from .layers import DropPath, trunc_normal_, to_2tuple<N>from .cait import ClassAttn<N>from .fx_features import register_notrace_module<N><N>
<N>def _cfg(url='', **kwargs):<N>    return {<N>        'url': url,<N>        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,<N>        'crop_pct': 1.0, 'interpolation': 'bicubic', 'fixed_input_size': True,<N>        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,<N>        'first_conv': 'patch_embed.proj.0.0', 'classifier': 'head',<N>        **kwargs<N>    }<N><N>
""" Activations<N><N>A collection of activations fn and modules with a common interface so that they can<N>easily be swapped. All have an `inplace` arg even if not used.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N><N>
<N>def swish(x, inplace: bool = False):<N>    """Swish - Described in: https://arxiv.org/abs/1710.05941<N>    """<N>    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())<N><N><N>class Swish(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(Swish, self).__init__()<N>        self.inplace = inplace<N><N>
    def forward(self, x):<N>        return swish(x, self.inplace)<N><N><N>def mish(x, inplace: bool = False):<N>    """Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681<N>    NOTE: I don't have a working inplace variant<N>    """<N>    return x.mul(F.softplus(x).tanh())<N><N>
<N>class Mish(nn.Module):<N>    """Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681<N>    """<N>    def __init__(self, inplace: bool = False):<N>        super(Mish, self).__init__()<N><N>    def forward(self, x):<N>        return mish(x)<N><N>
<N>def sigmoid(x, inplace: bool = False):<N>    return x.sigmoid_() if inplace else x.sigmoid()<N><N><N># PyTorch has this, but not with a consistent inplace argmument interface<N>class Sigmoid(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(Sigmoid, self).__init__()<N>        self.inplace = inplace<N><N>
    def forward(self, x):<N>        return x.sigmoid_() if self.inplace else x.sigmoid()<N><N><N>def tanh(x, inplace: bool = False):<N>    return x.tanh_() if inplace else x.tanh()<N><N><N># PyTorch has this, but not with a consistent inplace argmument interface<N>class Tanh(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(Tanh, self).__init__()<N>        self.inplace = inplace<N><N>
    def forward(self, x):<N>        return x.tanh_() if self.inplace else x.tanh()<N><N><N>def hard_swish(x, inplace: bool = False):<N>    inner = F.relu6(x + 3.).div_(6.)<N>    return x.mul_(inner) if inplace else x.mul(inner)<N><N><N>class HardSwish(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardSwish, self).__init__()<N>        self.inplace = inplace<N><N>
    def forward(self, x):<N>        return hard_swish(x, self.inplace)<N><N><N>def hard_sigmoid(x, inplace: bool = False):<N>    if inplace:<N>        return x.add_(3.).clamp_(0., 6.).div_(6.)<N>    else:<N>        return F.relu6(x + 3.) / 6.<N><N><N>class HardSigmoid(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardSigmoid, self).__init__()<N>        self.inplace = inplace<N><N>
    def forward(self, x):<N>        return hard_sigmoid(x, self.inplace)<N><N><N>def hard_mish(x, inplace: bool = False):<N>    """ Hard Mish<N>    Experimental, based on notes by Mish author Diganta Misra at<N>      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md<N>    """<N>    if inplace:<N>        return x.mul_(0.5 * (x + 2).clamp(min=0, max=2))<N>    else:<N>        return 0.5 * x * (x + 2).clamp(min=0, max=2)<N><N>
<N>class HardMish(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardMish, self).__init__()<N>        self.inplace = inplace<N><N>    def forward(self, x):<N>        return hard_mish(x, self.inplace)<N><N><N>class PReLU(nn.PReLU):<N>    """Applies PReLU (w/ dummy inplace arg)<N>    """<N>    def __init__(self, num_parameters: int = 1, init: float = 0.25, inplace: bool = False) -> None:<N>        super(PReLU, self).__init__(num_parameters=num_parameters, init=init)<N><N>
    def forward(self, input: torch.Tensor) -> torch.Tensor:<N>        return F.prelu(input, self.weight)<N><N><N>def gelu(x: torch.Tensor, inplace: bool = False) -> torch.Tensor:<N>    return F.gelu(x)<N><N><N>class GELU(nn.Module):<N>    """Applies the Gaussian Error Linear Units function (w/ dummy inplace arg)<N>    """<N>    def __init__(self, inplace: bool = False):<N>        super(GELU, self).__init__()<N><N>
""" Activations<N><N>A collection of jit-scripted activations fn and modules with a common interface so that they can<N>easily be swapped. All have an `inplace` arg even if not used.<N><N>All jit scripted activations are lacking in-place variations on purpose, scripted kernel fusion does not<N>currently work across in-place op boundaries, thus performance is equal to or less than the non-scripted<N>versions if they contain in-place ops.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N><N><N>@torch.jit.script<N>def swish_jit(x, inplace: bool = False):<N>    """Swish - Described in: https://arxiv.org/abs/1710.05941<N>    """<N>    return x.mul(x.sigmoid())<N><N>
<N>@torch.jit.script<N>def mish_jit(x, _inplace: bool = False):<N>    """Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681<N>    """<N>    return x.mul(F.softplus(x).tanh())<N><N><N>class SwishJit(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(SwishJit, self).__init__()<N><N>
    def forward(self, x):<N>        return swish_jit(x)<N><N><N>class MishJit(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(MishJit, self).__init__()<N><N>    def forward(self, x):<N>        return mish_jit(x)<N><N><N>@torch.jit.script<N>def hard_sigmoid_jit(x, inplace: bool = False):<N>    # return F.relu6(x + 3.) / 6.<N>    return (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?<N><N>
<N>class HardSigmoidJit(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardSigmoidJit, self).__init__()<N><N>    def forward(self, x):<N>        return hard_sigmoid_jit(x)<N><N><N>@torch.jit.script<N>def hard_swish_jit(x, inplace: bool = False):<N>    # return x * (F.relu6(x + 3.) / 6)<N>    return x * (x + 3).clamp(min=0, max=6).div(6.)  # clamp seems ever so slightly faster?<N><N>
""" Activations (memory-efficient w/ custom autograd)<N><N>A collection of activations fn and modules with a common interface so that they can<N>easily be swapped. All have an `inplace` arg even if not used.<N><N>These activations are not compatible with jit scripting or ONNX export of the model, please use either<N>the JIT or basic versions of the activations.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N><N><N>@torch.jit.script<N>def swish_jit_fwd(x):<N>    return x.mul(torch.sigmoid(x))<N><N><N>@torch.jit.script<N>def swish_jit_bwd(x, grad_output):<N>    x_sigmoid = torch.sigmoid(x)<N>    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))<N><N>
<N>class SwishJitAutoFn(torch.autograd.Function):<N>    """ torch.jit.script optimised Swish w/ memory-efficient checkpoint<N>    Inspired by conversation btw Jeremy Howard & Adam Pazske<N>    https://twitter.com/jeremyphoward/status/1188251041835315200<N>    """<N>    @staticmethod<N>    def symbolic(g, x):<N>        return g.op("Mul", x, g.op("Sigmoid", x))<N><N>
    @staticmethod<N>    def forward(ctx, x):<N>        ctx.save_for_backward(x)<N>        return swish_jit_fwd(x)<N><N>    @staticmethod<N>    def backward(ctx, grad_output):<N>        x = ctx.saved_tensors[0]<N>        return swish_jit_bwd(x, grad_output)<N><N>
<N>def swish_me(x, inplace=False):<N>    return SwishJitAutoFn.apply(x)<N><N><N>class SwishMe(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(SwishMe, self).__init__()<N><N>    def forward(self, x):<N>        return SwishJitAutoFn.apply(x)<N><N>
<N>@torch.jit.script<N>def mish_jit_fwd(x):<N>    return x.mul(torch.tanh(F.softplus(x)))<N><N><N>@torch.jit.script<N>def mish_jit_bwd(x, grad_output):<N>    x_sigmoid = torch.sigmoid(x)<N>    x_tanh_sp = F.softplus(x).tanh()<N>    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))<N><N>
<N>class MishJitAutoFn(torch.autograd.Function):<N>    """ Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681<N>    A memory efficient, jit scripted variant of Mish<N>    """<N>    @staticmethod<N>    def forward(ctx, x):<N>        ctx.save_for_backward(x)<N>        return mish_jit_fwd(x)<N><N>
    @staticmethod<N>    def backward(ctx, grad_output):<N>        x = ctx.saved_tensors[0]<N>        return mish_jit_bwd(x, grad_output)<N><N><N>def mish_me(x, inplace=False):<N>    return MishJitAutoFn.apply(x)<N><N><N>class MishMe(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(MishMe, self).__init__()<N><N>
    def forward(self, x):<N>        return MishJitAutoFn.apply(x)<N><N><N>@torch.jit.script<N>def hard_sigmoid_jit_fwd(x, inplace: bool = False):<N>    return (x + 3).clamp(min=0, max=6).div(6.)<N><N><N>@torch.jit.script<N>def hard_sigmoid_jit_bwd(x, grad_output):<N>    m = torch.ones_like(x) * ((x >= -3.) & (x <= 3.)) / 6.<N>    return grad_output * m<N><N>
<N>class HardSigmoidJitAutoFn(torch.autograd.Function):<N>    @staticmethod<N>    def forward(ctx, x):<N>        ctx.save_for_backward(x)<N>        return hard_sigmoid_jit_fwd(x)<N><N>    @staticmethod<N>    def backward(ctx, grad_output):<N>        x = ctx.saved_tensors[0]<N>        return hard_sigmoid_jit_bwd(x, grad_output)<N><N>
<N>def hard_sigmoid_me(x, inplace: bool = False):<N>    return HardSigmoidJitAutoFn.apply(x)<N><N><N>class HardSigmoidMe(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardSigmoidMe, self).__init__()<N><N>    def forward(self, x):<N>        return HardSigmoidJitAutoFn.apply(x)<N><N>
<N>@torch.jit.script<N>def hard_swish_jit_fwd(x):<N>    return x * (x + 3).clamp(min=0, max=6).div(6.)<N><N><N>@torch.jit.script<N>def hard_swish_jit_bwd(x, grad_output):<N>    m = torch.ones_like(x) * (x >= 3.)<N>    m = torch.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)<N>    return grad_output * m<N><N>
<N>class HardSwishJitAutoFn(torch.autograd.Function):<N>    """A memory efficient, jit-scripted HardSwish activation"""<N>    @staticmethod<N>    def forward(ctx, x):<N>        ctx.save_for_backward(x)<N>        return hard_swish_jit_fwd(x)<N><N>    @staticmethod<N>    def backward(ctx, grad_output):<N>        x = ctx.saved_tensors[0]<N>        return hard_swish_jit_bwd(x, grad_output)<N><N>
    @staticmethod<N>    def symbolic(g, self):<N>        input = g.op("Add", self, g.op('Constant', value_t=torch.tensor(3, dtype=torch.float)))<N>        hardtanh_ = g.op("Clip", input, g.op('Constant', value_t=torch.tensor(0, dtype=torch.float)), g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))<N>        hardtanh_ = g.op("Div", hardtanh_, g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))<N>        return g.op("Mul", self, hardtanh_)<N><N>
<N>def hard_swish_me(x, inplace=False):<N>    return HardSwishJitAutoFn.apply(x)<N><N><N>class HardSwishMe(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardSwishMe, self).__init__()<N><N>    def forward(self, x):<N>        return HardSwishJitAutoFn.apply(x)<N><N>
<N>@torch.jit.script<N>def hard_mish_jit_fwd(x):<N>    return 0.5 * x * (x + 2).clamp(min=0, max=2)<N><N><N>@torch.jit.script<N>def hard_mish_jit_bwd(x, grad_output):<N>    m = torch.ones_like(x) * (x >= -2.)<N>    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)<N>    return grad_output * m<N><N>
<N>class HardMishJitAutoFn(torch.autograd.Function):<N>    """ A memory efficient, jit scripted variant of Hard Mish<N>    Experimental, based on notes by Mish author Diganta Misra at<N>      https://github.com/digantamisra98/H-Mish/blob/0da20d4bc58e696b6803f2523c58d3c8a82782d0/README.md<N>    """<N>    @staticmethod<N>    def forward(ctx, x):<N>        ctx.save_for_backward(x)<N>        return hard_mish_jit_fwd(x)<N><N>
    @staticmethod<N>    def backward(ctx, grad_output):<N>        x = ctx.saved_tensors[0]<N>        return hard_mish_jit_bwd(x, grad_output)<N><N><N>def hard_mish_me(x, inplace: bool = False):<N>    return HardMishJitAutoFn.apply(x)<N><N><N>class HardMishMe(nn.Module):<N>    def __init__(self, inplace: bool = False):<N>        super(HardMishMe, self).__init__()<N><N>
""" PyTorch selectable adaptive pooling<N>Adaptive pooling with the ability to select the type of pooling from:<N>    * 'avg' - Average pooling<N>    * 'max' - Max pooling<N>    * 'avgmax' - Sum of average and max pooling re-scaled by 0.5<N>    * 'avgmaxc' - Concatenation of average and max pooling along feature dim, doubles feature dim<N><N>
Both a functional and a nn.Module version of the pooling is provided.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N><N>def adaptive_pool_feat_mult(pool_type='avg'):<N>    if pool_type == 'catavgmax':<N>        return 2<N>    else:<N>        return 1<N><N>
<N>def adaptive_avgmax_pool2d(x, output_size=1):<N>    x_avg = F.adaptive_avg_pool2d(x, output_size)<N>    x_max = F.adaptive_max_pool2d(x, output_size)<N>    return 0.5 * (x_avg + x_max)<N><N><N>def adaptive_catavgmax_pool2d(x, output_size=1):<N>    x_avg = F.adaptive_avg_pool2d(x, output_size)<N>    x_max = F.adaptive_max_pool2d(x, output_size)<N>    return torch.cat((x_avg, x_max), 1)<N><N>
""" Attention Pool 2D<N><N>Implementations of 2D spatial feature pooling using multi-head attention instead of average pool.<N><N>Based on idea in CLIP by OpenAI, licensed Apache 2.0<N>https://github.com/openai/CLIP/blob/3b473b0e682c091a9e53623eebc1ca1657385717/clip/model.py<N><N>
Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import math<N>from typing import List, Union, Tuple<N><N>import torch<N>import torch.nn as nn<N><N>from .helpers import to_2tuple<N>from .weight_init import trunc_normal_<N><N><N>def rot(x):<N>    return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)<N><N>
<N>def apply_rot_embed(x: torch.Tensor, sin_emb, cos_emb):<N>    return x * cos_emb + rot(x) * sin_emb<N><N><N>def apply_rot_embed_list(x: List[torch.Tensor], sin_emb, cos_emb):<N>    if isinstance(x, torch.Tensor):<N>        x = [x]<N>    return [t * cos_emb + rot(t) * sin_emb for t in x]<N><N>
<N>class RotaryEmbedding(nn.Module):<N>    """ Rotary position embedding<N><N>    NOTE: This is my initial attempt at impl rotary embedding for spatial use, it has not<N>    been well tested, and will likely change. It will be moved to its own file.<N><N>
    The following impl/resources were referenced for this impl:<N>    * https://github.com/lucidrains/vit-pytorch/blob/6f3a5fcf0bca1c5ec33a35ef48d97213709df4ba/vit_pytorch/rvt.py<N>    * https://blog.eleuther.ai/rotary-embeddings/<N>    """<N>    def __init__(self, dim, max_freq=4):<N>        super().__init__()<N>        self.dim = dim<N>        self.register_buffer('bands', 2 ** torch.linspace(0., max_freq - 1, self.dim // 4), persistent=False)<N><N>
"""<N>BlurPool layer inspired by<N> - Kornia's Max_BlurPool2d<N> - Making Convolutional Networks Shift-Invariant Again :cite:`zhang2019shiftinvar`<N><N>Hacked together by Chris Ha and Ross Wightman<N>"""<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>import numpy as np<N>from .padding import get_padding<N><N>
<N>class BlurPool2d(nn.Module):<N>    r"""Creates a module that computes blurs and downsample a given feature map.<N>    See :cite:`zhang2019shiftinvar` for more details.<N>    Corresponds to the Downsample class, which does blurring and subsampling<N><N>
""" Bottleneck Self Attention (Bottleneck Transformers)<N><N>Paper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605<N><N>@misc{2101.11605,<N>Author = {Aravind Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and Pieter Abbeel and Ashish Vaswani},<N>Title = {Bottleneck Transformers for Visual Recognition},<N>Year = {2021},<N>}<N><N>
Based on ref gist at: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2<N><N>This impl is a WIP but given that it is based on the ref gist likely not too far off.<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>from typing import List<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from .helpers import to_2tuple, make_divisible<N>from .weight_init import trunc_normal_<N>from .trace_utils import _assert<N><N><N>def rel_logits_1d(q, rel_k, permute_mask: List[int]):<N>    """ Compute relative logits along one dimension<N><N>
    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2<N>    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925<N><N>    Args:<N>        q: (batch, heads, height, width, dim)<N>        rel_k: (2 * width - 1, dim)<N>        permute_mask: permute output dim according to this<N>    """<N>    B, H, W, dim = q.shape<N>    x = (q @ rel_k.transpose(-1, -2))<N>    x = x.reshape(-1, W, 2 * W -1)<N><N>
    # pad to shift from relative to absolute indexing<N>    x_pad = F.pad(x, [0, 1]).flatten(1)<N>    x_pad = F.pad(x_pad, [0, W - 1])<N><N>    # reshape and slice out the padded elements<N>    x_pad = x_pad.reshape(-1, W + 1, 2 * W - 1)<N>    x = x_pad[:, :W, W - 1:]<N><N>
""" CBAM (sort-of) Attention<N><N>Experimental impl of CBAM: Convolutional Block Attention Module: https://arxiv.org/abs/1807.06521<N><N>WARNING: Results with these attention layers have been mixed. They can significantly reduce performance on<N>some tasks, especially fine-grained it seems. I may end up removing this impl.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>from torch import nn as nn<N>import torch.nn.functional as F<N><N>from .conv_bn_act import ConvBnAct<N>from .create_act import create_act_layer, get_act_layer<N>from .helpers import make_divisible<N><N>
""" PyTorch Conditionally Parameterized Convolution (CondConv)<N><N>Paper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference<N>(https://arxiv.org/abs/1904.04971)<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>
import math<N>from functools import partial<N>import numpy as np<N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N><N>from .helpers import to_2tuple<N>from .conv2d_same import conv2d_same<N>from .padding import get_padding_value<N><N>
""" Model / Layer Config singleton state<N>"""<N>from typing import Any, Optional<N><N>__all__ = [<N>    'is_exportable', 'is_scriptable', 'is_no_jit',<N>    'set_exportable', 'set_scriptable', 'set_no_jit', 'set_layer_config'<N>]<N><N># Set to True if prefer to have layers with no jit optimization (includes activations)<N>_NO_JIT = False<N><N>
# Set to True if prefer to have activation layers with no jit optimization<N># NOTE not currently used as no difference between no_jit and no_activation jit as only layers obeying<N># the jit flags so far are activations. This will change as more layers are updated and/or added.<N>_NO_ACTIVATION_JIT = False<N><N>
# Set to True if exporting a model with Same padding via ONNX<N>_EXPORTABLE = False<N><N># Set to True if wanting to use torch.jit.script on a model<N>_SCRIPTABLE = False<N><N><N>def is_no_jit():<N>    return _NO_JIT<N><N><N>class set_no_jit:<N>    def __init__(self, mode: bool) -> None:<N>        global _NO_JIT<N>        self.prev = _NO_JIT<N>        _NO_JIT = mode<N><N>
    def __enter__(self) -> None:<N>        pass<N><N>    def __exit__(self, *args: Any) -> bool:<N>        global _NO_JIT<N>        _NO_JIT = self.prev<N>        return False<N><N><N>def is_exportable():<N>    return _EXPORTABLE<N><N><N>class set_exportable:<N>    def __init__(self, mode: bool) -> None:<N>        global _EXPORTABLE<N>        self.prev = _EXPORTABLE<N>        _EXPORTABLE = mode<N><N>
    def __enter__(self) -> None:<N>        pass<N><N>    def __exit__(self, *args: Any) -> bool:<N>        global _EXPORTABLE<N>        _EXPORTABLE = self.prev<N>        return False<N><N><N>def is_scriptable():<N>    return _SCRIPTABLE<N><N><N>class set_scriptable:<N>    def __init__(self, mode: bool) -> None:<N>        global _SCRIPTABLE<N>        self.prev = _SCRIPTABLE<N>        _SCRIPTABLE = mode<N><N>
""" Conv2d w/ Same Padding<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from typing import Tuple, Optional<N><N>from .padding import pad_same, get_padding_value<N><N>
<N>def conv2d_same(<N>        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int] = (1, 1),<N>        padding: Tuple[int, int] = (0, 0), dilation: Tuple[int, int] = (1, 1), groups: int = 1):<N>    x = pad_same(x, weight.shape[-2:], stride, dilation)<N>    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)<N><N>
<N>class Conv2dSame(nn.Conv2d):<N>    """ Tensorflow like 'SAME' convolution wrapper for 2D convolutions<N>    """<N><N>    def __init__(self, in_channels, out_channels, kernel_size, stride=1,<N>                 padding=0, dilation=1, groups=1, bias=True):<N>        super(Conv2dSame, self).__init__(<N>            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)<N><N>
""" Activation Factory<N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from typing import Union, Callable, Type<N><N>from .activations import *<N>from .activations_jit import *<N>from .activations_me import *<N>from .config import is_exportable, is_scriptable, is_no_jit<N><N>
# PyTorch has an optimized, native 'silu' (aka 'swish') operator as of PyTorch 1.7.<N># Also hardsigmoid, hardswish, and soon mish. This code will use native version if present.<N># Eventually, the custom SiLU, Mish, Hard*, layers will be removed and only native variants will be used.<N>_has_silu = 'silu' in dir(torch.nn.functional)<N>_has_hardswish = 'hardswish' in dir(torch.nn.functional)<N>_has_hardsigmoid = 'hardsigmoid' in dir(torch.nn.functional)<N>_has_mish = 'mish' in dir(torch.nn.functional)<N><N>
<N>_ACT_FN_DEFAULT = dict(<N>    silu=F.silu if _has_silu else swish,<N>    swish=F.silu if _has_silu else swish,<N>    mish=F.mish if _has_mish else mish,<N>    relu=F.relu,<N>    relu6=F.relu6,<N>    leaky_relu=F.leaky_relu,<N>    elu=F.elu,<N>    celu=F.celu,<N>    selu=F.selu,<N>    gelu=gelu,<N>    sigmoid=sigmoid,<N>    tanh=tanh,<N>    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid,<N>    hard_swish=F.hardswish if _has_hardswish else hard_swish,<N>    hard_mish=hard_mish,<N>)<N><N>
_ACT_FN_JIT = dict(<N>    silu=F.silu if _has_silu else swish_jit,<N>    swish=F.silu if _has_silu else swish_jit,<N>    mish=F.mish if _has_mish else mish_jit,<N>    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid_jit,<N>    hard_swish=F.hardswish if _has_hardswish else hard_swish_jit,<N>    hard_mish=hard_mish_jit<N>)<N><N>
_ACT_FN_ME = dict(<N>    silu=F.silu if _has_silu else swish_me,<N>    swish=F.silu if _has_silu else swish_me,<N>    mish=F.mish if _has_mish else mish_me,<N>    hard_sigmoid=F.hardsigmoid if _has_hardsigmoid else hard_sigmoid_me,<N>    hard_swish=F.hardswish if _has_hardswish else hard_swish_me,<N>    hard_mish=hard_mish_me,<N>)<N><N>
""" Create Conv2d Factory Method<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>from .mixed_conv2d import MixedConv2d<N>from .cond_conv2d import CondConv2d<N>from .conv2d_same import create_conv2d_pad<N><N><N>def create_conv2d(in_channels, out_channels, kernel_size, **kwargs):<N>    """ Select a 2d convolution implementation based on arguments<N>    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.<N><N>
""" NormAct (Normalizaiton + Activation Layer) Factory<N><N>Create norm + act combo modules that attempt to be backwards compatible with separate norm + act<N>isntances in models. Where these are used it will be possible to swap separate BN + act layers with<N>combined modules like IABN or EvoNorms.<N><N>
Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import types<N>import functools<N><N>import torch<N>import torch.nn as nn<N><N>from .evo_norm import EvoNormBatch2d, EvoNormSample2d<N>from .norm_act import BatchNormAct2d, GroupNormAct<N>from .inplace_abn import InplaceAbn<N><N>
""" DropBlock, DropPath<N><N>PyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.<N><N>Papers:<N>DropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)<N><N>Deep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)<N><N>
Code:<N>DropBlock impl inspired by two Tensorflow impl that I liked:<N> - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74<N> - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
<N>def drop_block_2d(<N>        x, drop_prob: float = 0.1, block_size: int = 7,  gamma_scale: float = 1.0,<N>        with_noise: bool = False, inplace: bool = False, batchwise: bool = False):<N>    """ DropBlock. See https://arxiv.org/pdf/1810.12890.pdf<N><N>
    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training<N>    runs with success, but needs further validation and possibly optimization for lower runtime impact.<N>    """<N>    B, C, H, W = x.shape<N>    total_size = W * H<N>    clipped_block_size = min(block_size, min(W, H))<N>    # seed_drop_rate, the gamma parameter<N>    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (<N>        (W - block_size + 1) * (H - block_size + 1))<N><N>
    # Forces the block to be inside the feature map.<N>    w_i, h_i = torch.meshgrid(torch.arange(W).to(x.device), torch.arange(H).to(x.device))<N>    valid_block = ((w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)) & \<N>                  ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))<N>    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)<N><N>
    if batchwise:<N>        # one mask for whole batch, quite a bit faster<N>        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)<N>    else:<N>        uniform_noise = torch.rand_like(x)<N>    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)<N>    block_mask = -F.max_pool2d(<N>        -block_mask,<N>        kernel_size=clipped_block_size,  # block_size,<N>        stride=1,<N>        padding=clipped_block_size // 2)<N><N>
"""<N>ECA module from ECAnet<N><N>paper: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks<N>https://arxiv.org/abs/1910.03151<N><N>Original ECA model borrowed from https://github.com/BangguWu/ECANet<N><N>Modified circular ECA implementation and adaption for use in timm package<N>by Chris Ha https://github.com/VRandme<N><N>
"""EvoNormB0 (Batched) and EvoNormS0 (Sample) in PyTorch<N><N>An attempt at getting decent performing EvoNorms running in PyTorch.<N>While currently faster than other impl, still quite a ways off the built-in BN<N>in terms of memory usage and throughput (roughly 5x mem, 1/2 - 1/3x speed).<N><N>
""" Gather-Excite Attention Block<N><N>Paper: `Gather-Excite: Exploiting Feature Context in CNNs` - https://arxiv.org/abs/1810.12348<N><N>Official code here, but it's only partial impl in Caffe: https://github.com/hujie-frank/GENet<N><N>I've tried to support all of the extent both w/ and w/o params. I don't believe I've seen another<N>impl that covers all of the cases.<N><N>
NOTE: extent=0 + extra_params=False is equivalent to Squeeze-and-Excitation<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import math<N><N>from torch import nn as nn<N>import torch.nn.functional as F<N><N>from .create_act import create_act_layer, get_act_layer<N>from .create_conv2d import create_conv2d<N>from .helpers import make_divisible<N>from .mlp import ConvMlp<N><N>
""" Global Context Attention Block<N><N>Paper: `GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond`<N>    - https://arxiv.org/abs/1904.11492<N><N>Official code consulted as reference: https://github.com/xvjiarui/GCNet<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>from torch import nn as nn<N>import torch.nn.functional as F<N><N>
from .create_act import create_act_layer, get_act_layer<N>from .helpers import make_divisible<N>from .mlp import ConvMlp<N>from .norm import LayerNorm2d<N><N><N>class GlobalContext(nn.Module):<N><N>    def __init__(self, channels, use_attn=True, fuse_add=False, fuse_scale=True, init_last_zero=False,<N>                 rd_ratio=1./8, rd_channels=None, rd_divisor=1, act_layer=nn.ReLU, gate_layer='sigmoid'):<N>        super(GlobalContext, self).__init__()<N>        act_layer = get_act_layer(act_layer)<N><N>
""" Halo Self Attention<N><N>Paper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`<N>    - https://arxiv.org/abs/2103.12731<N><N>@misc{2103.12731,<N>Author = {Ashish Vaswani and Prajit Ramachandran and Aravind Srinivas and Niki Parmar and Blake Hechtman and<N>    Jonathon Shlens},<N>Title = {Scaling Local Self-Attention for Parameter Efficient Visual Backbones},<N>Year = {2021},<N>}<N><N>
Status:<N>This impl is a WIP, there is no official ref impl and some details in paper weren't clear to me.<N>The attention mechanism works but it's slow as implemented.<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>from typing import List<N><N>
import torch<N>from torch import nn<N>import torch.nn.functional as F<N><N>from .helpers import make_divisible<N>from .weight_init import trunc_normal_<N>from .trace_utils import _assert<N><N><N>def rel_logits_1d(q, rel_k, permute_mask: List[int]):<N>    """ Compute relative logits along one dimension<N><N>
    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2<N>    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925<N><N>    Args:<N>        q: (batch, height, width, dim)<N>        rel_k: (2 * window - 1, dim)<N>        permute_mask: permute output dim according to this<N>    """<N>    B, H, W, dim = q.shape<N>    rel_size = rel_k.shape[0]<N>    win_size = (rel_size + 1) // 2<N><N>
    x = (q @ rel_k.transpose(-1, -2))<N>    x = x.reshape(-1, W, rel_size)<N><N>    # pad to shift from relative to absolute indexing<N>    x_pad = F.pad(x, [0, 1]).flatten(1)<N>    x_pad = F.pad(x_pad, [0, rel_size - W])<N><N>    # reshape and slice out the padded elements<N>    x_pad = x_pad.reshape(-1, W + 1, rel_size)<N>    x = x_pad[:, :W, win_size - 1:]<N><N>
    # reshape and tile<N>    x = x.reshape(B, H, 1, W, win_size).expand(-1, -1, win_size, -1, -1)<N>    return x.permute(permute_mask)<N><N><N>class PosEmbedRel(nn.Module):<N>    """ Relative Position Embedding<N>    As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2<N>    Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925<N><N>
""" Layer/Module Helpers<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from itertools import repeat<N>import collections.abc<N><N><N># From PyTorch internals<N>def _ntuple(n):<N>    def parse(x):<N>        if isinstance(x, collections.abc.Iterable):<N>            return x<N>        return tuple(repeat(x, n))<N>    return parse<N><N>
<N>to_1tuple = _ntuple(1)<N>to_2tuple = _ntuple(2)<N>to_3tuple = _ntuple(3)<N>to_4tuple = _ntuple(4)<N>to_ntuple = _ntuple<N><N><N>def make_divisible(v, divisor=8, min_value=None, round_limit=.9):<N>    min_value = min_value or divisor<N>    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)<N>    # Make sure that round down does not go down by more than 10%.<N>    if new_v < round_limit * v:<N>        new_v += divisor<N>    return new_v<N><N><N>
""" Lambda Layer<N><N>Paper: `LambdaNetworks: Modeling Long-Range Interactions Without Attention`<N>    - https://arxiv.org/abs/2102.08602<N><N>@misc{2102.08602,<N>Author = {Irwan Bello},<N>Title = {LambdaNetworks: Modeling Long-Range Interactions Without Attention},<N>Year = {2021},<N>}<N><N>
Status:<N>This impl is a WIP. Code snippets in the paper were used as reference but<N>good chance some details are missing/wrong.<N><N>I've only implemented local lambda conv based pos embeddings.<N><N>For a PyTorch impl that includes other embedding options checkout<N>https://github.com/lucidrains/lambda-networks<N><N>
""" Median Pool<N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from .helpers import to_2tuple, to_4tuple<N><N><N>class MedianPool2d(nn.Module):<N>    """ Median pool (usable as median filter when stride=1) module.<N><N>
""" PyTorch Mixed Convolution<N><N>Paper: MixConv: Mixed Depthwise Convolutional Kernels (https://arxiv.org/abs/1907.09595)<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import torch<N>from torch import nn as nn<N><N>from .conv2d_same import create_conv2d_pad<N><N>
<N>def _split_channels(num_chan, num_groups):<N>    split = [num_chan // num_groups for _ in range(num_groups)]<N>    split[0] += num_chan - sum(split)<N>    return split<N><N><N>class MixedConv2d(nn.ModuleDict):<N>    """ Mixed Grouped Convolution<N><N>
    Based on MDConv and GroupedConv in MixNet impl:<N>      https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py<N>    """<N>    def __init__(self, in_channels, out_channels, kernel_size=3,<N>                 stride=1, padding='', dilation=1, depthwise=False, **kwargs):<N>        super(MixedConv2d, self).__init__()<N><N>
""" Bilinear-Attention-Transform and Non-Local Attention<N><N>Paper: `Non-Local Neural Networks With Grouped Bilinear Attentional Transforms`<N>    - https://openaccess.thecvf.com/content_CVPR_2020/html/Chi_Non-Local_Neural_Networks_With_Grouped_Bilinear_Attentional_Transforms_CVPR_2020_paper.html<N>Adapted from original code: https://github.com/BA-Transform/BAT-Image-Classification<N>"""<N>import torch<N>from torch import nn<N>from torch.nn import functional as F<N><N>
from .conv_bn_act import ConvBnAct<N>from .helpers import make_divisible<N>from .trace_utils import _assert<N><N><N>class NonLocalAttn(nn.Module):<N>    """Spatial NL block for image classification.<N><N>    This was adapted from https://github.com/BA-Transform/BAT-Image-Classification<N>    Their NonLocal impl inspired by https://github.com/facebookresearch/video-nonlocal-net.<N>    """<N><N>
""" Normalization layers and wrappers<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N><N>class GroupNorm(nn.GroupNorm):<N>    def __init__(self, num_channels, num_groups=32, eps=1e-5, affine=True):<N>        # NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN<N>        super().__init__(num_groups, num_channels, eps=eps, affine=affine)<N><N>
    def forward(self, x):<N>        return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)<N><N><N>class LayerNorm2d(nn.LayerNorm):<N>    """ LayerNorm for channels of '2D' spatial BCHW tensors """<N>    def __init__(self, num_channels):<N>        super().__init__(num_channels)<N><N>
""" Normalization + Activation Layers<N>"""<N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N><N>from .create_act import get_act_layer<N><N><N>class BatchNormAct2d(nn.BatchNorm2d):<N>    """BatchNorm + Activation<N><N>
""" Padding Helpers<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import math<N>from typing import List, Tuple<N><N>import torch.nn.functional as F<N><N><N># Calculate symmetric padding for a convolution<N>def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:<N>    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2<N>    return padding<N><N>
<N># Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution<N>def get_same_padding(x: int, k: int, s: int, d: int):<N>    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)<N><N><N># Can SAME padding for given args be done statically?<N>def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):<N>    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0<N><N>
<N># Dynamically pad input x with 'SAME' padding for conv with specified args<N>def pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1), value: float = 0):<N>    ih, iw = x.size()[-2:]<N>    pad_h, pad_w = get_same_padding(ih, k[0], s[0], d[0]), get_same_padding(iw, k[1], s[1], d[1])<N>    if pad_h > 0 or pad_w > 0:<N>        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)<N>    return x<N><N>
""" Image to Patch Embedding using Conv2d<N><N>A convolution based approach to patchifying a 2D image w/ embedding projection.<N><N>Based on the impl in https://github.com/google-research/vision_transformer<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from torch import nn as nn<N><N>
""" AvgPool2d w/ Same Padding<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from typing import List, Tuple, Optional<N><N>from .helpers import to_2tuple<N>from .padding import pad_same, get_padding_value<N><N>
<N>def avg_pool2d_same(x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),<N>                    ceil_mode: bool = False, count_include_pad: bool = True):<N>    # FIXME how to deal with count_include_pad vs not for external padding?<N>    x = pad_same(x, kernel_size, stride)<N>    return F.avg_pool2d(x, kernel_size, stride, (0, 0), ceil_mode, count_include_pad)<N><N>
<N>class AvgPool2dSame(nn.AvgPool2d):<N>    """ Tensorflow like 'SAME' wrapper for 2D average pooling<N>    """<N>    def __init__(self, kernel_size: int, stride=None, padding=0, ceil_mode=False, count_include_pad=True):<N>        kernel_size = to_2tuple(kernel_size)<N>        stride = to_2tuple(stride)<N>        super(AvgPool2dSame, self).__init__(kernel_size, stride, (0, 0), ceil_mode, count_include_pad)<N><N>
""" Selective Kernel Convolution/Attention<N><N>Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>from torch import nn as nn<N><N>from .conv_bn_act import ConvBnAct<N>from .helpers import make_divisible<N>from .trace_utils import _assert<N><N>
<N>def _kernel_valid(k):<N>    if isinstance(k, (list, tuple)):<N>        for ki in k:<N>            return _kernel_valid(ki)<N>    assert k >= 3 and k % 2<N><N><N>class SelectiveKernelAttn(nn.Module):<N>    def __init__(self, channels, num_paths=2, attn_channels=32,<N>                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):<N>        """ Selective Kernel Attention Module<N><N>
        Selective Kernel attention mechanism factored out into its own module.<N><N>        """<N>        super(SelectiveKernelAttn, self).__init__()<N>        self.num_paths = num_paths<N>        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, bias=False)<N>        self.bn = norm_layer(attn_channels)<N>        self.act = act_layer(inplace=True)<N>        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False)<N><N>
    def forward(self, x):<N>        _assert(x.shape[1] == self.num_paths, '')<N>        x = x.sum(1).mean((2, 3), keepdim=True)<N>        x = self.fc_reduce(x)<N>        x = self.bn(x)<N>        x = self.act(x)<N>        x = self.fc_select(x)<N>        B, C, H, W = x.shape<N>        x = x.view(B, self.num_paths, C // self.num_paths, H, W)<N>        x = torch.softmax(x, dim=1)<N>        return x<N><N>
<N>class SelectiveKernel(nn.Module):<N><N>    def __init__(self, in_channels, out_channels=None, kernel_size=None, stride=1, dilation=1, groups=1,<N>                 rd_ratio=1./16, rd_channels=None, rd_divisor=8, keep_3x3=True, split_input=True,<N>                 drop_block=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None):<N>        """ Selective Kernel Convolution Module<N><N>
""" Depthwise Separable Conv Modules<N><N>Basic DWS convs. Other variations of DWS exist with batch norm or activations between the<N>DW and PW convs such as the Depthwise modules in MobileNetV2 / EfficientNet and Xception.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>from torch import nn as nn<N><N>
""" Split Attention Conv2d (for ResNeSt Models)<N><N>Paper: `ResNeSt: Split-Attention Networks` - /https://arxiv.org/abs/2004.08955<N><N>Adapted from original PyTorch impl at https://github.com/zhanghang1989/ResNeSt<N><N>Modified for torchscript compat, performance, and consistency with timm by Ross Wightman<N>"""<N>import torch<N>import torch.nn.functional as F<N>from torch import nn<N><N>
""" Split BatchNorm<N><N>A PyTorch BatchNorm layer that splits input batch into N equal parts and passes each through<N>a separate BN layer. The first split is passed through the parent BN layers with weight/bias<N>keys the same as the original BN. All other splits pass through BN sub-layers under the '.aux_bn'<N>namespace.<N><N>
This allows easily removing the auxiliary BN layers after training to efficiently<N>achieve the 'Auxiliary BatchNorm' as described in the AdvProp Paper, section 4.2,<N>'Disentangled Learning via An Auxiliary BN'<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>import torch.nn as nn<N><N>
""" Squeeze-and-Excitation Channel Attention<N><N>An SE implementation originally based on PyTorch SE-Net impl.<N>Has since evolved with additional functionality / configuration.<N><N>Paper: `Squeeze-and-Excitation Networks` - https://arxiv.org/abs/1709.01507<N><N>
Also included is Effective Squeeze-Excitation (ESE).<N>Paper: `CenterMask : Real-Time Anchor-Free Instance Segmentation` - https://arxiv.org/abs/1911.06667<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>from torch import nn as nn<N><N>
""" Convolution with Weight Standardization (StdConv and ScaledStdConv)<N><N>StdConv:<N>@article{weightstandardization,<N>  author    = {Siyuan Qiao and Huiyu Wang and Chenxi Liu and Wei Shen and Alan Yuille},<N>  title     = {Weight Standardization},<N>  journal   = {arXiv preprint arXiv:1903.10520},<N>  year      = {2019},<N>}<N>Code: https://github.com/joe-siyuan-qiao/WeightStandardization<N><N>
ScaledStdConv:<N>Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`<N>    - https://arxiv.org/abs/2101.08692<N>Official Deepmind JAX code: https://github.com/deepmind/deepmind-research/tree/master/nfnets<N><N>
Hacked together by / copyright Ross Wightman, 2021.<N>"""<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from .padding import get_padding, get_padding_value, pad_same<N><N><N>class StdConv2d(nn.Conv2d):<N>    """Conv2d with Weight Standardization. Used for BiT ResNet-V2 models.<N><N>
""" Test Time Pooling (Average-Max Pool)<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import logging<N>from torch import nn<N>import torch.nn.functional as F<N><N>from .adaptive_avgmax_pool import adaptive_avgmax_pool2d<N><N><N>_logger = logging.getLogger(__name__)<N><N>
try:<N>    from torch import _assert<N>except ImportError:<N>    def _assert(condition: bool, message: str):<N>        assert condition, message<N><N><N>def _float_to_int(x: float) -> int:<N>    """<N>    Symbolic tracing helper to substitute for inbuilt `int`.<N>    Hint: Inbuilt `int` can't accept an argument of type `Proxy`<N>    """<N>    return int(x)<N>
import torch<N>import math<N>import warnings<N><N>from torch.nn.init import _calculate_fan_in_and_fan_out<N><N><N>def _no_grad_trunc_normal_(tensor, mean, std, a, b):<N>    # Cut & paste from PyTorch official master until it's in a few official releases - RW<N>    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf<N>    def norm_cdf(x):<N>        # Computes standard normal cumulative distribution function<N>        return (1. + math.erf(x / math.sqrt(2.))) / 2.<N><N>
    if (mean < a - 2 * std) or (mean > b + 2 * std):<N>        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "<N>                      "The distribution of values may be incorrect.",<N>                      stacklevel=2)<N><N>
    with torch.no_grad():<N>        # Values are generated by using a truncated uniform distribution and<N>        # then using the inverse CDF for the normal distribution.<N>        # Get upper and lower cdf values<N>        l = norm_cdf((a - mean) / std)<N>        u = norm_cdf((b - mean) / std)<N><N>
        # Uniformly fill tensor with values from [l, u], then translate to<N>        # [2l-1, 2u-1].<N>        tensor.uniform_(2 * l - 1, 2 * u - 1)<N><N>        # Use inverse cdf transform for normal distribution to get truncated<N>        # standard normal<N>        tensor.erfinv_()<N><N>
""" Adafactor Optimizer<N><N>Lifted from https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py<N><N>Original header/copyright below.<N><N>"""<N># Copyright (c) Facebook, Inc. and its affiliates.<N>#<N># This source code is licensed under the MIT license found in the<N># LICENSE file in the root directory of this source tree.<N>import torch<N>import math<N><N>
<N>class Adafactor(torch.optim.Optimizer):<N>    """Implements Adafactor algorithm.<N>    This implementation is based on: `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`<N>    (see https://arxiv.org/abs/1804.04235)<N><N>    Note that this optimizer internally adjusts the learning rate depending on the<N>    *scale_parameter*, *relative_step* and *warmup_init* options.<N><N>
""" AdaHessian Optimizer<N><N>Lifted from https://github.com/davda54/ada-hessian/blob/master/ada_hessian.py<N>Originally licensed MIT, Copyright 2020, David Samuel<N>"""<N>import torch<N><N><N>class Adahessian(torch.optim.Optimizer):<N>    """<N>    Implements the AdaHessian algorithm from "ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning"<N><N>
"""<N>AdamP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/adamp.py<N><N>Paper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217<N>Code: https://github.com/clovaai/AdamP<N><N>
Copyright (c) 2020-present NAVER Corp.<N>MIT license<N>"""<N><N>import torch<N>import torch.nn.functional as F<N>from torch.optim.optimizer import Optimizer<N>import math<N><N><N>def _channel_view(x) -> torch.Tensor:<N>    return x.reshape(x.size(0), -1)<N><N>
<N>def _layer_view(x) -> torch.Tensor:<N>    return x.reshape(1, -1)<N><N><N>def projection(p, grad, perturb, delta: float, wd_ratio: float, eps: float):<N>    wd = 1.<N>    expand_size = (-1,) + (1,) * (len(p.shape) - 1)<N>    for view_func in [_channel_view, _layer_view]:<N>        param_view = view_func(p)<N>        grad_view = view_func(grad)<N>        cosine_sim = F.cosine_similarity(grad_view, param_view, dim=1, eps=eps).abs_()<N><N>
        # FIXME this is a problem for PyTorch XLA<N>        if cosine_sim.max() < delta / math.sqrt(param_view.size(1)):<N>            p_n = p / param_view.norm(p=2, dim=1).add_(eps).reshape(expand_size)<N>            perturb -= p_n * view_func(p_n * perturb).sum(dim=1).reshape(expand_size)<N>            wd = wd_ratio<N>            return perturb, wd<N><N>
    return perturb, wd<N><N><N>class AdamP(Optimizer):<N>    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,<N>                 weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False):<N>        defaults = dict(<N>            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,<N>            delta=delta, wd_ratio=wd_ratio, nesterov=nesterov)<N>        super(AdamP, self).__init__(params, defaults)<N><N>
    @torch.no_grad()<N>    def step(self, closure=None):<N>        loss = None<N>        if closure is not None:<N>            with torch.enable_grad():<N>                loss = closure()<N><N>        for group in self.param_groups:<N>            for p in group['params']:<N>                if p.grad is None:<N>                    continue<N><N>
                grad = p.grad<N>                beta1, beta2 = group['betas']<N>                nesterov = group['nesterov']<N><N>                state = self.state[p]<N><N>                # State initialization<N>                if len(state) == 0:<N>                    state['step'] = 0<N>                    state['exp_avg'] = torch.zeros_like(p)<N>                    state['exp_avg_sq'] = torch.zeros_like(p)<N><N>
                # Adam<N>                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']<N><N>                state['step'] += 1<N>                bias_correction1 = 1 - beta1 ** state['step']<N>                bias_correction2 = 1 - beta2 ** state['step']<N><N>
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)<N>                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)<N><N>                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])<N>                step_size = group['lr'] / bias_correction1<N><N>
                if nesterov:<N>                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom<N>                else:<N>                    perturb = exp_avg / denom<N><N>                # Projection<N>                wd_ratio = 1.<N>                if len(p.shape) > 1:<N>                    perturb, wd_ratio = projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])<N><N>
                # Weight decay<N>                if group['weight_decay'] > 0:<N>                    p.mul_(1. - group['lr'] * group['weight_decay'] * wd_ratio)<N><N>                # Step<N>                p.add_(perturb, alpha=-step_size)<N><N>        return loss<N><N><N>
""" AdamW Optimizer<N>Impl copied from PyTorch master<N><N>NOTE: Builtin optim.AdamW is used by the factory, this impl only serves as a Python based reference, will be removed<N>someday<N>"""<N>import math<N>import torch<N>from torch.optim.optimizer import Optimizer<N><N>
<N>class AdamW(Optimizer):<N>    r"""Implements AdamW algorithm.<N><N>    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.<N>    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.<N><N>
""" PyTorch Lamb optimizer w/ behaviour similar to NVIDIA FusedLamb<N><N>This optimizer code was adapted from the following (starting with latest)<N>* https://github.com/HabanaAI/Model-References/blob/2b435114fe8e31f159b1d3063b8280ae37af7423/PyTorch/nlp/bert/pretraining/lamb.py<N>* https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/lamb.py<N>* https://github.com/cybertronai/pytorch-lamb<N><N>
Use FusedLamb if you can (GPU). The reason for including this variant of Lamb is to have a version that is<N>similar in behaviour to APEX FusedLamb if you aren't using NVIDIA GPUs or cannot install/use APEX.<N><N>In addition to some cleanup, this Lamb impl has been modified to support PyTorch XLA and has been tested on TPU.<N><N>
""" PyTorch LARS / LARC Optimizer<N><N>An implementation of LARS (SGD) + LARC in PyTorch<N><N>Based on:<N>  * PyTorch SGD: https://github.com/pytorch/pytorch/blob/1.7/torch/optim/sgd.py#L100<N>  * NVIDIA APEX LARC: https://github.com/NVIDIA/apex/blob/master/apex/parallel/LARC.py<N><N>
Additional cleanup and modifications to properly support PyTorch XLA.<N><N>Copyright 2021 Ross Wightman<N>"""<N>import torch<N>from torch.optim.optimizer import Optimizer<N><N><N>class Lars(Optimizer):<N>    """ LARS for PyTorch<N>    <N>    Paper: `Large batch training of Convolutional Networks` - https://arxiv.org/pdf/1708.03888.pdf<N><N>
""" Lookahead Optimizer Wrapper.<N>Implementation modified from: https://github.com/alphadl/lookahead.pytorch<N>Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>from torch.optim.optimizer import Optimizer<N>from collections import defaultdict<N><N>
""" PyTorch MADGRAD optimizer<N><N>MADGRAD: https://arxiv.org/abs/2101.11075<N><N>Code from: https://github.com/facebookresearch/madgrad<N>"""<N># Copyright (c) Facebook, Inc. and its affiliates.<N>#<N># This source code is licensed under the MIT license found in the<N># LICENSE file in the root directory of this source tree.<N><N>
import math<N>from typing import TYPE_CHECKING, Any, Callable, Optional<N><N>import torch<N>import torch.optim<N><N>if TYPE_CHECKING:<N>    from torch.optim.optimizer import _params_t<N>else:<N>    _params_t = Any<N><N><N>class MADGRAD(torch.optim.Optimizer):<N>    """<N>    MADGRAD_: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic<N>    Optimization.<N><N>
    .. _MADGRAD: https://arxiv.org/abs/2101.11075<N><N>    MADGRAD is a general purpose optimizer that can be used in place of SGD or<N>    Adam may converge faster and generalize better. Currently GPU-only.<N>    Typically, the same learning rate schedule that is used for SGD or Adam may<N>    be used. The overall learning rate is not comparable to either method and<N>    should be determined by a hyper-parameter sweep.<N><N>
import math<N><N>import torch<N>from torch.optim.optimizer import Optimizer<N><N><N>class Nadam(Optimizer):<N>    """Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).<N><N>    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.<N><N>
""" Nvidia NovoGrad Optimizer.<N>Original impl by Nvidia from Jasper example:<N>    - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechRecognition/Jasper<N>Paper: `Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks`<N>    - https://arxiv.org/abs/1905.11286<N>"""<N><N>
"""RAdam Optimizer.<N>Implementation lifted from: https://github.com/LiyuanLucasLiu/RAdam<N>Paper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265<N>"""<N>import math<N>import torch<N>from torch.optim.optimizer import Optimizer<N><N>
<N>class RAdam(Optimizer):<N><N>    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):<N>        defaults = dict(<N>            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,<N>            buffer=[[None, None, None] for _ in range(10)])<N>        super(RAdam, self).__init__(params, defaults)<N><N>
    def __setstate__(self, state):<N>        super(RAdam, self).__setstate__(state)<N><N>    @torch.no_grad()<N>    def step(self, closure=None):<N>        loss = None<N>        if closure is not None:<N>            with torch.enable_grad():<N>                loss = closure()<N><N>
        for group in self.param_groups:<N><N>            for p in group['params']:<N>                if p.grad is None:<N>                    continue<N>                grad = p.grad.float()<N>                if grad.is_sparse:<N>                    raise RuntimeError('RAdam does not support sparse gradients')<N><N>
                p_fp32 = p.float()<N><N>                state = self.state[p]<N><N>                if len(state) == 0:<N>                    state['step'] = 0<N>                    state['exp_avg'] = torch.zeros_like(p_fp32)<N>                    state['exp_avg_sq'] = torch.zeros_like(p_fp32)<N>                else:<N>                    state['exp_avg'] = state['exp_avg'].type_as(p_fp32)<N>                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_fp32)<N><N>
                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']<N>                beta1, beta2 = group['betas']<N><N>                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)<N>                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)<N><N>
""" RMSProp modified to behave like Tensorflow impl<N><N>Originally cut & paste from PyTorch RMSProp<N>https://github.com/pytorch/pytorch/blob/063946d2b3f3f1e953a2a3b54e0b34f1393de295/torch/optim/rmsprop.py<N>Licensed under BSD-Clause 3 (ish), https://github.com/pytorch/pytorch/blob/master/LICENSE<N><N>
Modifications Copyright 2021 Ross Wightman<N>"""<N><N>import torch<N>from torch.optim import Optimizer<N><N><N>class RMSpropTF(Optimizer):<N>    """Implements RMSprop algorithm (TensorFlow style epsilon)<N><N>    NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt<N>    and a few other modifications to closer match Tensorflow for matching hyper-params.<N><N>
    Noteworthy changes include:<N>    1. Epsilon applied inside square-root<N>    2. square_avg initialized to ones<N>    3. LR scaling of update accumulated in momentum buffer<N><N>    Proposed by G. Hinton in his<N>    `course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.<N><N>
"""<N>SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py<N><N>Paper: `Slowing Down the Weight Norm Increase in Momentum-based Optimizers` - https://arxiv.org/abs/2006.08217<N>Code: https://github.com/clovaai/AdamP<N><N>
from .adabelief import AdaBelief<N>from .adafactor import Adafactor<N>from .adahessian import Adahessian<N>from .adamp import AdamP<N>from .adamw import AdamW<N>from .lamb import Lamb<N>from .lars import Lars<N>from .lookahead import Lookahead<N>from .madgrad import MADGRAD<N>from .nadam import Nadam<N>from .nvnovograd import NvNovoGrad<N>from .radam import RAdam<N>from .rmsprop_tf import RMSpropTF<N>from .sgdp import SGDP<N>from .optim_factory import create_optimizer, create_optimizer_v2, optimizer_kwargs<N>
""" Cosine Scheduler<N><N>Cosine LR schedule with warmup, cycle/restarts, noise, k-decay.<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import logging<N>import math<N>import numpy as np<N>import torch<N><N>from .scheduler import Scheduler<N><N>
<N>_logger = logging.getLogger(__name__)<N><N><N>class CosineLRScheduler(Scheduler):<N>    """<N>    Cosine decay with restarts.<N>    This is described in the paper https://arxiv.org/abs/1608.03983.<N><N>    Inspiration from<N>    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py<N><N>
""" MultiStep LR Scheduler<N><N>Basic multi step LR schedule with warmup, noise.<N>"""<N>import torch<N>import bisect<N>from timm.scheduler.scheduler import Scheduler<N>from typing import List<N><N>class MultiStepLRScheduler(Scheduler):<N>    """<N>    """<N><N>
""" Plateau Scheduler<N><N>Adapts PyTorch plateau scheduler and allows application of noise, warmup.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N><N>from .scheduler import Scheduler<N><N><N>class PlateauLRScheduler(Scheduler):<N>    """Decay the LR by a factor every time the validation loss plateaus."""<N><N>
""" Polynomial Scheduler<N><N>Polynomial LR schedule with warmup, noise.<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import math<N>import logging<N><N>import torch<N><N>from .scheduler import Scheduler<N><N><N>_logger = logging.getLogger(__name__)<N><N>
""" Scheduler Factory<N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>from .cosine_lr import CosineLRScheduler<N>from .multistep_lr import MultiStepLRScheduler<N>from .plateau_lr import PlateauLRScheduler<N>from .poly_lr import PolyLRScheduler<N>from .step_lr import StepLRScheduler<N>from .tanh_lr import TanhLRScheduler<N><N>
""" Step Scheduler<N><N>Basic step LR schedule with warmup, noise.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import math<N>import torch<N><N>from .scheduler import Scheduler<N><N><N>class StepLRScheduler(Scheduler):<N>    """<N>    """<N><N>
""" TanH Scheduler<N><N>TanH schedule with warmup, cycle/restarts, noise.<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import logging<N>import math<N>import numpy as np<N>import torch<N><N>from .scheduler import Scheduler<N><N><N>_logger = logging.getLogger(__name__)<N><N>
from .cosine_lr import CosineLRScheduler<N>from .multistep_lr import MultiStepLRScheduler<N>from .plateau_lr import PlateauLRScheduler<N>from .poly_lr import PolyLRScheduler<N>from .step_lr import StepLRScheduler<N>from .tanh_lr import TanhLRScheduler<N><N>from .scheduler_factory import create_scheduler<N>
""" Adaptive Gradient Clipping<N><N>An impl of AGC, as per (https://arxiv.org/abs/2102.06171):<N><N>@article{brock2021high,<N>  author={Andrew Brock and Soham De and Samuel L. Smith and Karen Simonyan},<N>  title={High-Performance Large-Scale Image Recognition Without Normalization},<N>  journal={arXiv preprint arXiv:},<N>  year={2021}<N>}<N><N>
Code references:<N>  * Official JAX impl (paper authors): https://github.com/deepmind/deepmind-research/tree/master/nfnets<N>  * Phil Wang's PyTorch gist: https://gist.github.com/lucidrains/0d6560077edac419ab5d3aa29e674d5c<N><N>Hacked together by / Copyright 2021 Ross Wightman<N>"""<N>import torch<N><N>
<N>def unitwise_norm(x, norm_type=2.0):<N>    if x.ndim <= 1:<N>        return x.norm(norm_type)<N>    else:<N>        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor<N>        # might need special cases for other weights (possibly MHA) where this may not be true<N>        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)<N><N>
""" Checkpoint Saver<N><N>Track top-n training checkpoints and maintain recovery checkpoints on specified intervals.<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N>import glob<N>import operator<N>import os<N>import logging<N><N>import torch<N><N>
""" CUDA / AMP utils<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N><N>try:<N>    from apex import amp<N>    has_apex = True<N>except ImportError:<N>    amp = None<N>    has_apex = False<N><N>from .clip_grad import dispatch_clip_grad<N><N>
<N>class ApexScaler:<N>    state_dict_key = "amp"<N><N>    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None, create_graph=False):<N>        with amp.scale_loss(loss, optimizer) as scaled_loss:<N>            scaled_loss.backward(create_graph=create_graph)<N>        if clip_grad is not None:<N>            dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)<N>        optimizer.step()<N><N>
    def state_dict(self):<N>        if 'state_dict' in amp.__dict__:<N>            return amp.state_dict()<N><N>    def load_state_dict(self, state_dict):<N>        if 'load_state_dict' in amp.__dict__:<N>            amp.load_state_dict(state_dict)<N><N>
""" Distributed training/validation utils<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import torch<N>from torch import distributed as dist<N><N>from .model import unwrap_model<N><N><N>def reduce_tensor(tensor, n):<N>    rt = tensor.clone()<N>    dist.all_reduce(rt, op=dist.ReduceOp.SUM)<N>    rt /= n<N>    return rt<N><N>
""" Logging helpers<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import logging<N>import logging.handlers<N><N><N>class FormatterNoInfo(logging.Formatter):<N>    def __init__(self, fmt='%(levelname)s: %(message)s'):<N>        logging.Formatter.__init__(self, fmt)<N><N>
""" Eval metrics and related<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N><N><N>class AverageMeter:<N>    """Computes and stores the average and current value"""<N>    def __init__(self):<N>        self.reset()<N><N>    def reset(self):<N>        self.val = 0<N>        self.avg = 0<N>        self.sum = 0<N>        self.count = 0<N><N>
""" Misc utils<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import re<N><N><N>def natural_key(string_):<N>    """See http://www.codinghorror.com/blog/archives/001018.html"""<N>    return [int(s) if s.isdigit() else s for s in re.split(r'(\d+)', string_.lower())]<N><N>
<N>def add_bool_arg(parser, name, default=False, help=''):<N>    dest_name = name.replace('-', '_')<N>    group = parser.add_mutually_exclusive_group(required=False)<N>    group.add_argument('--' + name, dest=dest_name, action='store_true', help=help)<N>    group.add_argument('--no-' + name, dest=dest_name, action='store_false', help=help)<N>    parser.set_defaults(**{dest_name: default})<N><N><N>
""" Model / state_dict utils<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import fnmatch<N><N>import torch<N>from torchvision.ops.misc import FrozenBatchNorm2d<N><N>from .model_ema import ModelEma<N><N><N>def unwrap_model(model):<N>    if isinstance(model, ModelEma):<N>        return unwrap_model(model.ema)<N>    else:<N>        return model.module if hasattr(model, 'module') else model<N><N>
<N>def get_state_dict(model, unwrap_fn=unwrap_model):<N>    return unwrap_fn(model).state_dict()<N><N><N>def avg_sq_ch_mean(model, input, output):<N>    """ calculate average channel square mean of output activations<N>    """<N>    return torch.mean(output.mean(axis=[0, 2, 3]) ** 2).item()<N><N>
<N>def avg_ch_var(model, input, output):<N>    """ calculate average channel variance of output activations<N>    """<N>    return torch.mean(output.var(axis=[0, 2, 3])).item()<N><N><N>def avg_ch_var_residual(model, input, output):<N>    """ calculate average channel variance of output activations<N>    """<N>    return torch.mean(output.var(axis=[0, 2, 3])).item()<N><N>
""" Exponential Moving Average (EMA) of model updates<N><N>Hacked together by / Copyright 2020 Ross Wightman<N>"""<N>import logging<N>from collections import OrderedDict<N>from copy import deepcopy<N><N>import torch<N>import torch.nn as nn<N><N>_logger = logging.getLogger(__name__)<N><N>
<N>class ModelEma:<N>    """ Model Exponential Moving Average (DEPRECATED)<N><N>    Keep a moving average of everything in the model state_dict (parameters and buffers).<N>    This version is deprecated, it does not work with scripted models. Will be removed eventually.<N><N>
import random<N>import numpy as np<N>import torch<N><N><N>def random_seed(seed=42, rank=0):<N>    torch.manual_seed(seed + rank)<N>    np.random.seed(seed + rank)<N>    random.seed(seed + rank)<N>
import torch<N>import numpy as np<N><N><N>def sort_file(file_path):<N>    f2 = open(file_path, "r")<N>    lines = f2.readlines()<N>    ret = []<N>    for line in lines:<N>        line = line[:-1]<N>        ret.append(line)<N>    ret.sort()<N><N>    with open('./output.txt', 'w') as f:<N>        for i in ret:<N>            f.write(i + '\n')<N><N>
import torch<N>import numpy as np<N><N><N>def random_crop(d_img, config):<N>    b, c, h, w = d_img.shape<N>    top = np.random.randint(0, h - config.crop_size)<N>    left = np.random.randint(0, w - config.crop_size)<N>    d_img_org = crop_image(top, left, config.crop_size, img=d_img)<N>    return d_img_org<N><N>
import json<N>import os<N>from setuptools import setup<N><N><N>with open('package.json') as f:<N>    package = json.load(f)<N><N>with open('README.md') as f:<N>    long_description = f.read()<N><N>package_name = package["name"].replace(" ", "_").replace("-", "_")<N><N>
setup(<N>    name=package_name,<N>    url="https://github.com/jackskerman/dash-slick",<N>    version=package["version"],<N>    author=package['author'],<N>    packages=[package_name],<N>    include_package_data=True,<N>    license=package['license'],<N>    description=package.get('description', package_name),<N>    install_requires=[],<N>    classifiers = [<N>        'Framework :: Dash',<N>    ],<N>    long_description=long_description,<N>    long_description_content_type="text/markdown",<N>)<N><N><N>
"""<N>DO NOT MODIFY<N>This file is used to validate your publish settings.<N>"""<N>from __future__ import print_function<N><N>import os<N>import sys<N>import importlib<N><N><N>components_package = 'dash_slick'<N><N>components_lib = importlib.import_module(components_package)<N><N>
missing_dist_msg = 'Warning {} was not found in `{}.__init__.{}`!!!'<N>missing_manifest_msg = '''<N>Warning {} was not found in `MANIFEST.in`!<N>It will not be included in the build!<N>'''<N><N>with open('MANIFEST.in', 'r') as f:<N>    manifest = f.read()<N><N>
<N>def check_dist(dist, filename):<N>    # Support the dev bundle.<N>    if filename.endswith('dev.js'):<N>        return True<N><N>    return any(<N>        filename in x<N>        for d in dist<N>        for x in (<N>            [d.get('relative_package_path')]<N>            if not isinstance(d.get('relative_package_path'), list)<N>            else d.get('relative_package_path')<N>        )<N>    )<N><N>
<N>def check_manifest(filename):<N>    return filename in manifest<N><N><N>def check_file(dist, filename):<N>    if not check_dist(dist, filename):<N>        print(<N>            missing_dist_msg.format(filename, components_package, '_js_dist'),<N>            file=sys.stderr<N>        )<N>    if not check_manifest(filename):<N>        print(missing_manifest_msg.format(filename),<N>              file=sys.stderr)<N><N>
<N>for cur, _, files in os.walk(components_package):<N>    for f in files:<N><N>        if f.endswith('js'):<N>            # noinspection PyProtectedMember<N>            check_file(components_lib._js_dist, f)<N>        elif f.endswith('css'):<N>            # noinspection PyProtectedMember<N>            check_file(components_lib._css_dist, f)<N>        elif not f.endswith('py'):<N>            check_manifest(f)<N><N><N>
# AUTO GENERATED FILE - DO NOT EDIT<N><N>from dash.development.base_component import Component, _explicitize_args<N><N><N>class SlickSlider(Component):<N>    """A SlickSlider component.<N><N><N>Keyword arguments:<N><N>- children (a list of or a singular dash component, string or number; optional):<N>    Your carousel is vertical.<N><N>
- id (string; optional):<N>    Id of the element.<N><N>- arrows (boolean; optional):<N>    Arrows to control carousel.<N><N>- autoplay (boolean; optional):<N>    If the carousel will start to play automatically.<N><N>- center_mode (boolean; optional):<N>    To centralize the carousel.<N><N>
- center_padding (string; optional):<N>    Padding on the sides.<N><N>- className (string; optional):<N>    Style class of the component.<N><N>- dots (boolean; optional):<N>    Dots under carousel.<N><N>- infinite (boolean; optional):<N>    If the carousel content will repeat in a loop.<N><N>
- labels (list of strings; optional):<N>    Your labels for dropdown if slide_navigator is set to True.<N><N>- responsive (list; optional):<N>    Settings of breakpoints.<N><N>- slide_navigator (boolean; optional):<N>    Render a dropdown that navigates between slides.<N><N>
- slides_to_scroll (number; optional):<N>    How many slides will scroll when you swipe the carousel.<N><N>- slides_to_show (number; optional):<N>    How many slides you want to show.<N><N>- speed (number; optional):<N>    Speed of autoplay.<N><N>- style (dict; optional):<N>    Inline style of the component.<N><N>
from dash.testing.application_runners import import_app<N><N><N># Basic test for the component rendering.<N># The dash_duo pytest fixture is installed with dash (v1.0+)<N>def test_render_component(dash_duo):<N>    # Start a dash app contained as the variable `app` in `usage.py`<N>    app = import_app('usage')<N>    dash_duo.start_server(app)<N><N>
    # Get the generated component input with selenium<N>    # The html input will be a children of the #input dash component<N>    my_component = dash_duo.find_element('#input > input')<N><N>    assert 'my-value' == my_component.get_attribute('value')<N><N>
    # Clear the input<N>    dash_duo.clear_input(my_component)<N><N>    # Send keys to the custom input.<N>    my_component.send_keys('Hello dash')<N><N>    # Wait for the text to equal, if after the timeout (default 10 seconds)<N>    # the text is not equal it will fail the test.<N>    dash_duo.wait_for_text_to_equal('#output', 'You have entered Hello dash')<N><N><N>
from typing import Set, List<N><N>from jinja2 import Environment, FileSystemLoader<N>from datetime import datetime, timezone<N>from pathlib import Path<N>from enum import Enum<N><N>from dateutil.parser import parse<N>from timeit import default_timer as timer<N><N>
import aiofiles<N>import aiohttp<N>import asyncio<N>import shutil<N><N>import typer<N>import json<N><N>root_path = Path(__file__).parent<N>html_assets_path = root_path / "html_assets"<N><N>template_loader = FileSystemLoader(searchpath=html_assets_path)<N>template_environment = Environment(loader=template_loader, enable_async=True)<N><N>
index_template = template_environment.get_template("index.html")<N>message_template = template_environment.get_template("message.html")<N><N>app = typer.Typer()<N>cutoff_date = datetime(2019, 1, 1, tzinfo=timezone.utc)<N><N><N>class OutputFormat(Enum):<N>    json = "json"<N>    # csv = "csv"<N>    html = "html"<N><N>
<N>async def get_authenticated_user(<N>        session: aiohttp.ClientSession<N>):<N>    async with session.request(<N>            method="GET",<N>            url="https://users.roblox.com/v1/users/authenticated"<N>    ) as response:<N>        return await response.json()<N><N>
import random<N>from turtle import Turtle<N><N><N>class Ball(Turtle):<N><N>    def __init__(self):<N>        super().__init__()<N>        self.color("white")<N>        self.shape("square")<N>        self.penup()<N>        self.x_vector = random.choice([-10, 10])<N>        self.y_vector = random.choice([-10, 10])<N>        self.move_speed = 0.05<N><N>
    def move(self):<N>        new_x = self.xcor() + self.x_vector<N>        new_y = self.ycor() + self.y_vector<N>        self.goto(new_x, new_y)<N><N>    def bounce_y(self):<N>        self.y_vector *= -1<N><N>    def bounce_x(self):<N>        if self.xcor() > 320:<N>            self.x_vector = -10<N>        elif self.xcor() < -320:<N>            self.x_vector = 10<N><N>
from turtle import Screen<N>from scoreboard import Scoreboard<N>from player import Player<N>from ball import Ball<N>import time<N><N><N>screen = Screen()<N>screen.setup(width= 800, height=600)<N>screen.bgcolor("black")<N>screen.title("PONG")<N>screen.tracer(0)<N>screen.listen()<N><N>
scoreboard = Scoreboard()<N><N>ball = Ball()<N><N>player1 = Player((-350, 0))<N><N>player2 = Player((350, 0))<N><N>screen.onkey(player1.up, "w")<N>screen.onkey(player1.down, "s")<N>screen.onkey(player2.up, "Up")<N>screen.onkey(player2.down, "Down")<N><N>
game_on = True<N>while game_on:<N>    time.sleep(ball.move_speed)<N>    screen.update()<N>    ball.move()<N><N>    # Detect collision with paddles.<N>    if ball.ycor() > 280 or ball.ycor() < -280:<N>        ball.bounce_y()<N>        ball.move_speed *= 0.9<N><N>
    if ball.distance(player1) < 50 or ball.distance(player2) < 50:<N>        ball.bounce_x()<N>        ball.move_speed *= 0.99<N><N>    # Detect ball out of bounds and update scoreboard.<N>    if 405 > ball.xcor() > 395:<N>        scoreboard.p1_point()<N>        ball.reset_position()<N>        ball.move_speed = 0.05<N><N>
from turtle import Turtle<N><N><N>class Player(Turtle):<N>    def __init__(self, position):<N>        super().__init__()<N>        self.shape("square")<N>        self.shapesize(stretch_len=1, stretch_wid=5)<N>        self.color("white")<N>        self.penup()<N>        self.speed("fastest")<N>        self.goto(position)<N><N>
    def up(self):<N>        if self.ycor() < 230:<N>            new_y = self.ycor() + 20<N>            self.goto(self.xcor(), new_y)<N><N>    def down(self):<N>        if self.ycor() > -230:<N>            new_y = self.ycor() - 20<N>            self.goto(self.xcor(), new_y)<N><N><N>
from turtle import Turtle<N><N>NET = """<N>|<N><N>|<N><N>|<N><N>|<N><N>|<N><N>|<N><N>|<N><N>|<N><N>|<N>"""<N><N><N>class Scoreboard(Turtle):<N>    def __init__(self):<N>        super().__init__()<N><N>        net = Turtle()<N>        net.color("white")<N>        net.hideturtle()<N>        net.pu()<N>        net.goto(0, -315)<N>        net.write(NET, False, "center", ("courier", 24, "normal"))<N><N>
        self.color("white")<N>        self.pu()<N>        self.hideturtle()<N>        self.p1_score = 0<N>        self.p2_score = 0<N>        self.update_scoreboard()<N><N>    def update_scoreboard(self):<N>        self.goto(0, 190)<N>        self.write(f"{self.p1_score} {self.p2_score}", False, "center", ("courier", 70, "normal"))<N><N>
    # def scored(self):<N>    #     self.goto(0, 0)<N>    #     self.write("SCORE!", False, "center", ("courier", 28, "normal"))<N><N>    def p1_point(self):<N>        self.clear()<N>        self.p1_score += 1<N>        self.update_scoreboard()<N><N>
from xml.etree import ElementTree<N>import json<N>import numba<N><N>LISTTYPE = 1<N>DICTTYPE = 0<N><N>@numba.njit<N>def getDictResults(res_dicts, iters):<N>    result_dicts = {}<N>    for iter in list(iters):<N>        iterxml(iter, result_dicts)<N><N>
from xml.etree import ElementTree<N>import json<N><N>LISTTYPE = 1<N>DICTTYPE = 0<N><N>def getDictResults(res_dicts, iters):<N>    result_dicts = {}<N>    for iter in list(iters):<N>        iterxml(iter, result_dicts)<N><N>    if result_dicts:<N>        res_dicts[iters.tag].update(result_dicts)<N><N>
from unicorn import *<N>from unicorn.x86_const import *<N><N>from keystone import Ks, KS_ARCH_X86, KS_MODE_64<N><N>import idautils<N>import idc<N>import idaapi<N>import ida_bytes<N>import ida_allins<N>import ida_kernwin<N><N>from time import time<N><N>
import idaapi<N>import idautils<N>import ida_funcs<N>import ida_allins<N>import ida_bytes<N>import ida_ida<N>import ida_nalt<N>import idc<N><N>from miasm.core.bin_stream_ida import bin_stream_ida<N>from miasm.analysis.machine import Machine<N>from miasm.core.locationdb import LocationDB<N>from miasm.analysis.depgraph import DependencyGraph<N>from miasm.expression.expression import *<N><N>
from z3 import *<N><N>s = Solver()<N><N>digit1, digit2, digit3 = Ints("digit1 digit2 digit3")<N><N># every digit must be between 0 and 9<N>s.add(digit1 >= 0, digit1 <= 9)<N>s.add(digit2 >= 0, digit2 <= 9)<N>s.add(digit3 >= 0, digit3 <= 9)<N><N># 682 -> one number is correct and well placed<N>s.add(Or(<N>    digit1 == 6,<N>    digit2 == 8,<N>    digit3 == 2<N>))<N><N>
from z3 import *<N><N>s = Solver()<N><N># define variables<N>cookie = Int("cookie")<N>milk = Int("milk")<N>cookie_milk = Int("cookie_milk")<N><N># define constraints<N>s.add(cookie * 3 == 36)<N>s.add(cookie_milk + milk + cookie_milk == 6)<N>s.add(cookie + cookie + cookie_milk == 24)<N><N>
while True:<N>    result = s.check()<N>    if result == sat:<N>        print("sat")<N>        model = s.model()<N>        print(model)<N>        block = []<N>        for x in model:<N>            c = x()<N>            block.append(c != model[x])<N>        s.add(Or(block))<N>    elif result == unsat:<N>        #print("unsat")<N>        print("finished")<N>        break<N>    elif result == unknown:<N>        print("unknown")<N>        break<N><N>
import hashlib<N>import sys<N><N>with open(sys.argv[1], "rb") as file:<N>    hash = hashlib.sha256(file.read()).hexdigest()<N>    print(hash)<N>
#!/usr/bin/env python3<N><N>"""build client & server bundles"""<N><N># if there is a problem with building, please let htmlcsjs know<N>import os<N>import sys<N>import shutil<N>import subprocess<N>import requests<N>import json<N>import hashlib<N>import argparse<N><N>
from torch_geometric.utils.convert import from_networkx<N>import torch_geometric<N>from bpemb import BPEmb<N>from sentence_transformers import SentenceTransformer<N>from graph import Grapher<N><N>import torch<N>import os <N>import random<N>import numpy as np<N>import glob<N><N>
import torch.nn as nn<N>import torch.nn.functional as F<N>from torch_geometric.nn import ChebConv, GCNConv<N><N><N>class InvoiceGCN(nn.Module):<N>    <N>    def __init__(self, input_dim, chebnet=False, n_classes=4, dropout_rate=0.2, K=3):<N>        super().__init__()<N><N>
import os<N>import glob<N>import time<N>import cv2<N>import torch<N>from model import InvoiceGCN<N>from dataset import make_graph<N>import matplotlib.pyplot as plt<N><N>test_output_fd = "./test_output"<N>if not os.path.exists(test_output_fd):<N>    os.mkdir(test_output_fd)<N><N>
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<N>train_data = torch.load(os.path.join("./GCN_data/processed", 'train_data.dataset'))<N><N>model = InvoiceGCN(input_dim=train_data.x.shape[1], chebnet=True)<N>checkpoint = torch.load('./weights/best.pt', map_location=device)<N>model.load_state_dict(checkpoint)<N>model.eval()<N><N>
import torch<N>import os<N>import torch.nn.functional as F<N><N>from model import InvoiceGCN<N><N>from sklearn.utils.class_weight import compute_class_weight<N>from sklearn.metrics import confusion_matrix, classification_report<N><N>best_loss = 1e+6<N>epochs = 500<N>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<N><N>
train_data = torch.load(os.path.join("./GCN_data/processed", 'train_data.dataset'))<N>test_data = torch.load(os.path.join("./GCN_data/processed", 'test_data.dataset'))<N>train_data = train_data.to(device)<N>test_data = test_data.to(device)<N><N>model = InvoiceGCN(input_dim=train_data.x.shape[1], chebnet=True)<N>model = model.to(device)<N>optimizer = torch.optim.AdamW(<N>    model.parameters(), lr=0.001, weight_decay=0.9<N>)<N><N>
# class weights for imbalanced data<N>_class_weights = compute_class_weight(<N>    "balanced", train_data.y.unique().cpu().numpy(), train_data.y.cpu().numpy()<N>)<N><N>for epoch in range(1, epochs+1):<N>    model.train()<N>    optimizer.zero_grad()<N><N>
    # NOTE: just use boolean indexing to filter out test data, and backward after that!<N>    # the same holds true with test data :D<N>    # https://github.com/rusty1s/pytorch_geometric/issues/1928<N>    loss = F.nll_loss(<N>        model(train_data), train_data.y - 1, weight=torch.FloatTensor(_class_weights).to(device)<N>    )<N>    loss.backward()<N>    optimizer.step()<N><N>
    with torch.no_grad():<N>        if epoch % 50 == 0:<N>            model.eval()<N><N>            # forward model<N>            for index, name in enumerate(['train', 'test']):<N>                _data = eval("{}_data".format(name))<N>                y_pred = model(_data).max(dim=1)[1]<N>                y_true = (_data.y - 1)<N>                acc = y_pred.eq(y_true).sum().item() / y_pred.shape[0]<N><N>
                y_pred = y_pred.cpu().numpy()<N>                y_true = y_true.cpu().numpy()<N>                print("\t{} acc: {}".format(name, acc))<N>                # confusion matrix<N>                if name == 'test':<N>                    cm = confusion_matrix(y_true, y_pred)<N>                    class_accs = cm.diagonal() / cm.sum(axis=1)<N>                    print(classification_report(y_true, y_pred))<N><N>
            loss_val = F.nll_loss(model(test_data), test_data.y - 1<N>            )<N>            fmt_log = "Epoch: {:03d}, train_loss:{:.4f}, val_loss:{:.4f}"<N>            print(fmt_log.format(epoch, loss, loss_val))<N>            print(">" * 50)<N>            if best_loss>loss_val:<N>                torch.save(model.state_dict(), f"./weights/best.pt")<N><N><N>
# flask app that serves the index.html file<N><N>from flask import Flask, render_template<N><N>name = 'zumbi'<N><N>app = Flask(name)<N><N>@app.route("/")<N>def index():<N>    return render_template('./index.html')<N><N>if __name__ == '__main__':<N>    app.run(host='0.0.0.0', port=4002, debug=True)
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>#!/usr/bin/env python<N><N>
from setuptools import find_packages, setup<N><N>import os<N>import subprocess<N>import sys<N>import time<N>import torch<N>from torch.utils.cpp_extension import (BuildExtension, CppExtension,<N>                                       CUDAExtension)<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import torch<N><N>
# from basicsr.data import create_dataloader, create_dataset<N>from basicsr.models import create_model<N>from basicsr.train import parse_options<N>from basicsr.utils import FileClient, imfrombytes, img2tensor, padding<N><N># from basicsr.utils import (get_env_info, get_root_logger, get_time_str,<N>#                            make_exp_dirs)<N># from basicsr.utils.options import dict2str<N><N>
def main():<N>    # parse options, set distributed setting, set ramdom seed<N>    opt = parse_options(is_train=False)<N><N>    img_path = opt['img_path'].get('input_img')<N>    output_path = opt['img_path'].get('output_img')<N><N><N>    ## 1. read image<N>    file_client = FileClient('disk')<N><N>
    img_bytes = file_client.get(img_path, None)<N>    try:<N>        img = imfrombytes(img_bytes, float32=True)<N>    except:<N>        raise Exception("path {} not working".format(img_path))<N><N>    img = img2tensor(img, bgr2rgb=True, float32=True)<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import logging<N>import torch<N>from os import path as osp<N><N>
from basicsr.data import create_dataloader, create_dataset<N>from basicsr.models import create_model<N>from basicsr.train import parse_options<N>from basicsr.utils import (get_env_info, get_root_logger, get_time_str,<N>                           make_exp_dirs)<N>from basicsr.utils.options import dict2str<N><N>
# GENERATED VERSION FILE<N># TIME: Fri Apr  1 17:46:10 2022<N>__version__ = '1.2.0+e41bf19'<N>short_version = '1.2.0'<N>version_info = (1, 2, 0)<N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N><N>
import math<N>import torch<N>from torch.utils.data.sampler import Sampler<N><N><N>class EnlargedSampler(Sampler):<N>    """Sampler that restricts data loading to a subset of the dataset.<N><N>    Modified from torch.utils.data.distributed.DistributedSampler<N>    Support enlarging the dataset for iteration-based training, for saving<N>    time when restart the dataloader after each epoch<N><N>
    Args:<N>        dataset (torch.utils.data.Dataset): Dataset used for sampling.<N>        num_replicas (int | None): Number of processes participating in<N>            the training. It is usually the world_size.<N>        rank (int | None): Rank of the current process within num_replicas.<N>        ratio (int): Enlarging ratio. Default: 1.<N>    """<N><N>
    def __init__(self, dataset, num_replicas, rank, ratio=1):<N>        self.dataset = dataset<N>        self.num_replicas = num_replicas<N>        self.rank = rank<N>        self.epoch = 0<N>        self.num_samples = math.ceil(<N>            len(self.dataset) * ratio / self.num_replicas)<N>        self.total_size = self.num_samples * self.num_replicas<N><N>
    def __iter__(self):<N>        # deterministically shuffle based on epoch<N>        g = torch.Generator()<N>        g.manual_seed(self.epoch)<N>        indices = torch.randperm(self.total_size, generator=g).tolist()<N><N>        dataset_size = len(self.dataset)<N>        indices = [v % dataset_size for v in indices]<N><N>
        # subsample<N>        indices = indices[self.rank:self.total_size:self.num_replicas]<N>        assert len(indices) == self.num_samples<N><N>        return iter(indices)<N><N>    def __len__(self):<N>        return self.num_samples<N><N>    def set_epoch(self, epoch):<N>        self.epoch = epoch<N><N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import cv2<N>import numpy as np<N>import torch<N>from os import path as osp<N>from torch.nn import functional as F<N><N>
from basicsr.data.transforms import mod_crop<N>from basicsr.utils import img2tensor, scandir<N><N><N>def read_img_seq(path, require_mod_crop=False, scale=1):<N>    """Read a sequence of images from a given folder path.<N><N>    Args:<N>        path (list[str] | str): List of image paths or image folder path.<N>        require_mod_crop (bool): Require mod crop for each image.<N>            Default: False.<N>        scale (int): Scale factor for mod_crop. Default: 1.<N><N>
    Returns:<N>        Tensor: size (t, c, h, w), RGB, [0, 1].<N>    """<N>    if isinstance(path, list):<N>        img_paths = path<N>    else:<N>        img_paths = sorted(list(scandir(path, full_path=True)))<N>    imgs = [cv2.imread(v).astype(np.float32) / 255. for v in img_paths]<N>    if require_mod_crop:<N>        imgs = [mod_crop(img, scale) for img in imgs]<N>    imgs = img2tensor(imgs, bgr2rgb=True, float32=True)<N>    imgs = torch.stack(imgs, dim=0)<N>    return imgs<N><N>
<N>def generate_frame_indices(crt_idx,<N>                           max_frame_num,<N>                           num_frames,<N>                           padding='reflection'):<N>    """Generate an index list for reading `num_frames` frames from a sequence<N>    of images.<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>from torch.utils import data as data<N>from torchvision.transforms.functional import normalize<N><N>
from basicsr.data.data_util import (paired_paths_from_folder,<N>                                    paired_paths_from_lmdb,<N>                                    paired_paths_from_meta_info_file)<N>from basicsr.data.transforms import augment, paired_random_crop<N>from basicsr.utils import FileClient, imfrombytes, img2tensor, padding<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>from torch.utils import data as data<N>from torchvision.transforms.functional import normalize, resize<N><N>
from basicsr.data.data_util import (paired_paths_from_folder,<N>                                    paired_paths_from_lmdb,<N>                                    paired_paths_from_meta_info_file)<N>from basicsr.data.transforms import augment, paired_random_crop, paired_random_crop_hw<N>from basicsr.utils import FileClient, imfrombytes, img2tensor, padding<N>import os<N>import numpy as np<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>from torch.utils import data as data<N>from torchvision.transforms.functional import normalize, resize<N><N>
from basicsr.data.data_util import (paired_paths_from_folder,<N>                                    paired_paths_from_lmdb,<N>                                    paired_paths_from_meta_info_file)<N>from basicsr.data.transforms import augment, paired_random_crop_hw<N>from basicsr.utils import FileClient, imfrombytes, img2tensor, padding<N>import os<N>import numpy as np<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import queue as Queue<N>import threading<N>import torch<N>from torch.utils.data import DataLoader<N><N>
<N>class PrefetchGenerator(threading.Thread):<N>    """A general prefetch generator.<N><N>    Ref:<N>    https://stackoverflow.com/questions/7323664/python-generator-pre-fetch<N><N>    Args:<N>        generator: Python generator.<N>        num_prefetch_queue (int): Number of prefetch queue.<N>    """<N><N>
    def __init__(self, generator, num_prefetch_queue):<N>        threading.Thread.__init__(self)<N>        self.queue = Queue.Queue(num_prefetch_queue)<N>        self.generator = generator<N>        self.daemon = True<N>        self.start()<N><N>    def run(self):<N>        for item in self.generator:<N>            self.queue.put(item)<N>        self.queue.put(None)<N><N>
    def __next__(self):<N>        next_item = self.queue.get()<N>        if next_item is None:<N>            raise StopIteration<N>        return next_item<N><N>    def __iter__(self):<N>        return self<N><N><N>class PrefetchDataLoader(DataLoader):<N>    """Prefetch version of dataloader.<N><N>
    Ref:<N>    https://github.com/IgorSusmelj/pytorch-styleguide/issues/5#<N><N>    TODO:<N>    Need to test on single gpu and ddp (multi-gpu). There is a known issue in<N>    ddp.<N><N>    Args:<N>        num_prefetch_queue (int): Number of prefetch queue.<N>        kwargs (dict): Other arguments for dataloader.<N>    """<N><N>
    def __init__(self, num_prefetch_queue, **kwargs):<N>        self.num_prefetch_queue = num_prefetch_queue<N>        super(PrefetchDataLoader, self).__init__(**kwargs)<N><N>    def __iter__(self):<N>        return PrefetchGenerator(super().__iter__(), self.num_prefetch_queue)<N><N>
<N>class CPUPrefetcher():<N>    """CPU prefetcher.<N><N>    Args:<N>        loader: Dataloader.<N>    """<N><N>    def __init__(self, loader):<N>        self.ori_loader = loader<N>        self.loader = iter(loader)<N><N>    def next(self):<N>        try:<N>            return next(self.loader)<N>        except StopIteration:<N>            return None<N><N>
    def reset(self):<N>        self.loader = iter(self.ori_loader)<N><N><N>class CUDAPrefetcher():<N>    """CUDA prefetcher.<N><N>    Ref:<N>    https://github.com/NVIDIA/apex/issues/304#<N><N>    It may consums more GPU memory.<N><N>    Args:<N>        loader: Dataloader.<N>        opt (dict): Options.<N>    """<N><N>
    def __init__(self, loader, opt):<N>        self.ori_loader = loader<N>        self.loader = iter(loader)<N>        self.opt = opt<N>        self.stream = torch.cuda.Stream()<N>        self.device = torch.device('cuda' if opt['num_gpu'] != 0 else 'cpu')<N>        self.preload()<N><N>
    def preload(self):<N>        try:<N>            self.batch = next(self.loader)  # self.batch is a dict<N>        except StopIteration:<N>            self.batch = None<N>            return None<N>        # put tensors to gpu<N>        with torch.cuda.stream(self.stream):<N>            for k, v in self.batch.items():<N>                if torch.is_tensor(v):<N>                    self.batch[k] = self.batch[k].to(<N>                        device=self.device, non_blocking=True)<N><N>
    def next(self):<N>        torch.cuda.current_stream().wait_stream(self.stream)<N>        batch = self.batch<N>        self.preload()<N>        return batch<N><N>    def reset(self):<N>        self.loader = iter(self.ori_loader)<N>        self.preload()<N><N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import cv2<N>import random<N>from cv2 import rotate<N>import numpy as np<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import glob<N>import torch<N>from os import path as osp<N>from torch.utils import data as data<N><N>
from basicsr.data.data_util import (duf_downsample, generate_frame_indices,<N>                                    read_img_seq)<N>from basicsr.utils import get_root_logger, scandir<N><N><N>class VideoTestDataset(data.Dataset):<N>    """Video test dataset.<N><N>
    Supported datasets: Vid4, REDS4, REDSofficial.<N>    More generally, it supports testing dataset with following structures:<N><N>    dataroot<N>    ├── subfolder1<N>        ├── frame000<N>        ├── frame001<N>        ├── ...<N>    ├── subfolder1<N>        ├── frame000<N>        ├── frame001<N>        ├── ...<N>    ├── ...<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import random<N>import torch<N>from pathlib import Path<N>from torch.utils import data as data<N><N>
from basicsr.data.transforms import augment, paired_random_crop<N>from basicsr.utils import FileClient, get_root_logger, imfrombytes, img2tensor<N><N><N>class Vimeo90KDataset(data.Dataset):<N>    """Vimeo90K dataset for training.<N><N>    The keys are generated from a meta info txt file.<N>    basicsr/data/meta_info/meta_info_Vimeo90K_train_GT.txt<N><N>
    Each line contains:<N>    1. clip name; 2. frame number; 3. image shape, seperated by a white space.<N>    Examples:<N>        00001/0001 7 (256,448,3)<N>        00001/0002 7 (256,448,3)<N><N>    Key examples: "00001/0001"<N>    GT (gt): Ground-Truth;<N>    LQ (lq): Low-Quality, e.g., low-resolution/blurry/noisy/compressed frames.<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N><N>
import importlib<N>import numpy as np<N>import random<N>import torch<N>import torch.utils.data<N>from functools import partial<N>from os import path as osp<N><N>from basicsr.data.prefetch_dataloader import PrefetchDataLoader<N>from basicsr.utils import get_root_logger, scandir<N>from basicsr.utils.dist_util import get_dist_info<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import numpy as np<N>import torch<N>import torch.nn as nn<N>from scipy import linalg<N>from tqdm import tqdm<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import numpy as np<N><N>
from basicsr.utils.matlab_functions import bgr2ycbcr<N><N><N>def reorder_image(img, input_order='HWC'):<N>    """Reorder images to 'HWC' order.<N><N>    If the input_order is (h, w), return (h, w, 1);<N>    If the input_order is (c, h, w), return (h, w, c);<N>    If the input_order is (h, w, c), return as it is.<N><N>
    Args:<N>        img (ndarray): Input image.<N>        input_order (str): Whether the input order is 'HWC' or 'CHW'.<N>            If the input image shape is (h, w), input_order will not have<N>            effects. Default: 'HWC'.<N><N>    Returns:<N>        ndarray: reordered image.<N>    """<N><N>
    if input_order not in ['HWC', 'CHW']:<N>        raise ValueError(<N>            f'Wrong input_order {input_order}. Supported input_orders are '<N>            "'HWC' and 'CHW'")<N>    if len(img.shape) == 2:<N>        img = img[..., None]<N>    if input_order == 'CHW':<N>        img = img.transpose(1, 2, 0)<N>    return img<N><N>
<N>def to_y_channel(img):<N>    """Change to Y channel of YCbCr.<N><N>    Args:<N>        img (ndarray): Images with range [0, 255].<N><N>    Returns:<N>        (ndarray): Images with range [0, 255] (float type) without round.<N>    """<N>    img = img.astype(np.float32) / 255.<N>    if img.ndim == 3 and img.shape[2] == 3:<N>        img = bgr2ycbcr(img, y_only=True)<N>        img = img[..., None]<N>    return img * 255.<N><N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import math<N>from collections import Counter<N>from torch.optim.lr_scheduler import _LRScheduler<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import importlib<N>from os import path as osp<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N><N>import numpy as np<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
class AvgPool2d(nn.Module):<N>    def __init__(self, kernel_size=None, base_size=None, auto_pad=True, fast_imp=False, train_size=None):<N>        super().__init__()<N>        self.kernel_size = kernel_size<N>        self.base_size = base_size<N>        self.auto_pad = auto_pad<N><N>
        # only used for fast implementation<N>        self.fast_imp = fast_imp<N>        self.rs = [5, 4, 3, 2, 1]<N>        self.max_r1 = self.rs[0]<N>        self.max_r2 = self.rs[0]<N>        self.train_size = train_size<N><N>    def extra_repr(self) -> str:<N>        return 'kernel_size={}, base_size={}, stride={}, fast_imp={}'.format(<N>            self.kernel_size, self.base_size, self.kernel_size, self.fast_imp<N>        )<N><N>
    def forward(self, x):<N>        if self.kernel_size is None and self.base_size:<N>            train_size = self.train_size<N>            if isinstance(self.base_size, int):<N>                self.base_size = (self.base_size, self.base_size)<N>            self.kernel_size = list(self.base_size)<N>            self.kernel_size[0] = x.shape[2] * self.base_size[0] // train_size[-2]<N>            self.kernel_size[1] = x.shape[3] * self.base_size[1] // train_size[-1]<N><N>
            # only used for fast implementation<N>            self.max_r1 = max(1, self.rs[0] * x.shape[2] // train_size[-2])<N>            self.max_r2 = max(1, self.rs[0] * x.shape[3] // train_size[-1])<N><N>        if self.kernel_size[0] >= x.size(-2) and self.kernel_size[1] >= x.size(-1):<N>            return F.adaptive_avg_pool2d(x, 1)<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N><N>'''<N>NAFSSR: Stereo Image Super-Resolution Using NAFNet<N><N>
@InProceedings{Chu2022NAFSSR,<N>  author    = {Xiaojie Chu and Liangyu Chen and Wenqing Yu},<N>  title     = {NAFSSR: Stereo Image Super-Resolution Using NAFNet},<N>  booktitle = {CVPRW},<N>  year      = {2022},<N>}<N>'''<N><N>import numpy as np<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>
from basicsr.models.archs.NAFNet_arch import LayerNorm2d, NAFBlock<N>from basicsr.models.archs.arch_util import MySequential<N>from basicsr.models.archs.local_arch import Local_Base<N><N>class GenerateRelations(nn.Module):<N>    def __init__(self, c):<N>        super().__init__()<N>        self.scale = c ** -0.5<N><N>
        self.norm_l = LayerNorm2d(c)<N>        self.norm_r = LayerNorm2d(c)<N><N>        self.l_proj = nn.Conv2d(c, c, kernel_size=1, stride=1, padding=0)<N>        self.r_proj = nn.Conv2d(c, c, kernel_size=1, stride=1, padding=0)<N><N>    def forward(self, lfeats, rfeats):<N>        B, C, H, W = lfeats.shape<N><N>
        lfeats = lfeats.view(B, C, H, W)<N>        rfeats = rfeats.view(B, C, H, W)<N><N>        lfeats, rfeats = self.l_proj(self.norm_l(lfeats)), self.r_proj(self.norm_r(rfeats))<N><N>        x = lfeats.permute(0, 2, 3, 1) #B H W c<N>        y = rfeats.permute(0, 2, 1, 3) #B H c W<N><N>
        z = torch.matmul(x, y)  #B H W W<N><N>        return self.scale * z<N><N>class FusionModule(nn.Module):<N>    def __init__(self, c):<N>        super().__init__()<N>        self.relation_generator = GenerateRelations(c)<N>        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)<N>        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)<N><N>
        self.l_proj = nn.Conv2d(c, c, kernel_size=1, stride=1, padding=0)<N>        self.r_proj = nn.Conv2d(c, c, kernel_size=1, stride=1, padding=0)<N><N>    def forward(self, lfeats, rfeats):<N>        B, C, H, W = lfeats.shape<N><N>        relations = self.relation_generator(lfeats, rfeats)  # B,  H,  W,  W<N><N>
        lfeats_projected = self.l_proj(lfeats.view(B, C, H, W)).permute(0, 2, 3, 1)  # B, H, W, c<N>        rfeats_projected = self.r_proj(rfeats.view(B, C, H, W)).permute(0, 2, 3, 1)  # B, H, W, c<N><N>        lresidual = torch.matmul(torch.softmax(relations, dim=-1), rfeats_projected)  #B, H, W, c<N>        rresidual = torch.matmul(torch.softmax(relations.permute(0, 1, 3, 2), dim=-1), lfeats_projected) #B, H, W, c<N><N>
        lresidual = lresidual.permute(0, 3, 1, 2).view(B, C, H, W) * self.beta<N>        rresidual = rresidual.permute(0, 3, 1, 2).view(B, C, H, W) * self.gamma<N>        return lfeats + lresidual, rfeats + rresidual<N><N>class DropPath(nn.Module):<N>    def __init__(self, drop_rate, module):<N>        super().__init__()<N>        self.drop_rate = drop_rate<N>        self.module = module<N><N>
    def forward(self, *feats):<N>        if self.training and np.random.rand() < self.drop_rate:<N>            return feats<N><N>        new_feats = self.module(*feats)<N>        factor = 1. / (1 - self.drop_rate) if self.training else 1.<N><N>        if self.training and factor != 1.:<N>            new_feats = tuple([x+factor*(new_x-x) for x, new_x in zip(feats, new_feats)])<N>        return new_feats<N><N>
class NAFBlockSR(nn.Module):<N>    def __init__(self, c, fusion=False,  drop_out_rate=0.):<N>        super().__init__()<N>        self.blk = NAFBlock(c, drop_out_rate=drop_out_rate)<N>        self.fusion = FusionModule(c) if fusion else None<N><N>    def forward(self, *feats):<N>        feats = tuple([self.blk(x) for x in feats])<N>        if self.fusion:<N>            feats = self.fusion(*feats)<N>        return feats<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N><N>'''<N>Simple Baselines for Image Restoration<N><N>
@article{chen2022simple,<N>  title={Simple Baselines for Image Restoration},<N>  author={Chen, Liangyu and Chu, Xiaojie and Zhang, Xiangyu and Sun, Jian},<N>  journal={arXiv preprint arXiv:2204.04676},<N>  year={2022}<N>}<N>'''<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from basicsr.models.archs.arch_util import LayerNorm2d<N>from basicsr.models.archs.local_arch import Local_Base<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import importlib<N>from os import path as osp<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N>import numpy as np<N><N>
from basicsr.models.losses.loss_util import weighted_loss<N><N>_reduction_modes = ['none', 'mean', 'sum']<N><N><N>@weighted_loss<N>def l1_loss(pred, target):<N>    return F.l1_loss(pred, target, reduction='none')<N><N><N>@weighted_loss<N>def mse_loss(pred, target):<N>    return F.mse_loss(pred, target, reduction='none')<N><N>
<N># @weighted_loss<N># def charbonnier_loss(pred, target, eps=1e-12):<N>#     return torch.sqrt((pred - target)**2 + eps)<N><N><N>class L1Loss(nn.Module):<N>    """L1 (mean absolute error, MAE) loss.<N><N>    Args:<N>        loss_weight (float): Loss weight for L1 loss. Default: 1.0.<N>        reduction (str): Specifies the reduction to apply to the output.<N>            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.<N>    """<N><N>
    def __init__(self, loss_weight=1.0, reduction='mean'):<N>        super(L1Loss, self).__init__()<N>        if reduction not in ['none', 'mean', 'sum']:<N>            raise ValueError(f'Unsupported reduction mode: {reduction}. '<N>                             f'Supported ones are: {_reduction_modes}')<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import functools<N>from torch.nn import functional as F<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>from .losses import (L1Loss, MSELoss, PSNRLoss)<N><N>__all__ = [<N>    'L1Loss', 'MSELoss', 'PSNRLoss',<N>]<N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import argparse<N>from os import path as osp<N><N>
from basicsr.utils import scandir<N>from basicsr.utils.lmdb_util import make_lmdb_from_imgs<N><N>def prepare_keys(folder_path, suffix='png'):<N>    """Prepare image path list and keys for DIV2K dataset.<N><N>    Args:<N>        folder_path (str): Folder path.<N><N>
    Returns:<N>        list[str]: Image path list.<N>        list[str]: Key list.<N>    """<N>    print('Reading image path list ...')<N>    img_path_list = sorted(<N>        list(scandir(folder_path, suffix=suffix, recursive=False)))<N>    keys = [img_path.split('.{}'.format(suffix))[0] for img_path in sorted(img_path_list)]<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import math<N>import requests<N>from tqdm import tqdm<N><N>
from .misc import sizeof_fmt<N><N><N>def download_file_from_google_drive(file_id, save_path):<N>    """Download files from google drive.<N><N>    Ref:<N>    https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive  # noqa E501<N><N>
    Args:<N>        file_id (str): File id.<N>        save_path (str): Save path.<N>    """<N><N>    session = requests.Session()<N>    URL = 'https://docs.google.com/uc?export=download'<N>    params = {'id': file_id}<N><N>    response = session.get(URL, params=params, stream=True)<N>    token = get_confirm_token(response)<N>    if token:<N>        params['confirm'] = token<N>        response = session.get(URL, params=params, stream=True)<N><N>
    # get file size<N>    response_file_size = session.get(<N>        URL, params=params, stream=True, headers={'Range': 'bytes=0-2'})<N>    if 'Content-Range' in response_file_size.headers:<N>        file_size = int(<N>            response_file_size.headers['Content-Range'].split('/')[1])<N>    else:<N>        file_size = None<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import cv2<N>import numpy as np<N>import os<N>import torch<N>from skimage import transform as trans<N><N>
from basicsr.utils import imwrite<N><N>try:<N>    import dlib<N>except ImportError:<N>    print('Please install dlib before testing face restoration.'<N>          'Reference:　https://github.com/davisking/dlib')<N><N><N>class FaceRestorationHelper(object):<N>    """Helper for the face restoration pipeline."""<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import cv2<N>import math<N>import numpy as np<N>import os<N>import torch<N>from torchvision.utils import make_grid<N><N>
<N>def img2tensor(imgs, bgr2rgb=True, float32=True):<N>    """Numpy array to tensor.<N><N>    Args:<N>        imgs (list[ndarray] | ndarray): Input images.<N>        bgr2rgb (bool): Whether to change bgr to rgb.<N>        float32 (bool): Whether to change to float32.<N><N>
    Returns:<N>        list[tensor] | tensor: Tensor images. If returned results only have<N>            one element, just return tensor.<N>    """<N><N>    def _totensor(img, bgr2rgb, float32):<N>        if img.shape[2] == 3 and bgr2rgb:<N>            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<N>        img = torch.from_numpy(img.transpose(2, 0, 1))<N>        if float32:<N>            img = img.float()<N>        return img<N><N>
    if isinstance(imgs, list):<N>        return [_totensor(img, bgr2rgb, float32) for img in imgs]<N>    else:<N>        return _totensor(imgs, bgr2rgb, float32)<N><N><N>def tensor2img(tensor, rgb2bgr=True, out_type=np.uint8, min_max=(0, 1)):<N>    """Convert torch Tensors into image numpy arrays.<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import datetime<N>import logging<N>import time<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import math<N>import numpy as np<N>import torch<N><N>
<N>def cubic(x):<N>    """cubic function used for calculate_weights_indices."""<N>    absx = torch.abs(x)<N>    absx2 = absx**2<N>    absx3 = absx**3<N>    return (1.5 * absx3 - 2.5 * absx2 + 1) * (<N>        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx +<N>                                      2) * (((absx > 1) *<N>                                             (absx <= 2)).type_as(absx))<N><N>
<N>def calculate_weights_indices(in_length, out_length, scale, kernel,<N>                              kernel_width, antialiasing):<N>    """Calculate weights and indices, used for imresize function.<N><N>    Args:<N>        in_length (int): Input length.<N>        out_length (int): Output length.<N>        scale (float): Scale factor.<N>        kernel_width (int): Kernel width.<N>        antialisaing (bool): Whether to apply anti-aliasing when downsampling.<N>    """<N><N>
    if (scale < 1) and antialiasing:<N>        # Use a modified kernel (larger kernel width) to simultaneously<N>        # interpolate and antialias<N>        kernel_width = kernel_width / scale<N><N>    # Output-space coordinates<N>    x = torch.linspace(1, out_length, out_length)<N><N>
    # Input-space coordinates. Calculate the inverse mapping such that 0.5<N>    # in output space maps to 0.5 in input space, and 0.5 + scale in output<N>    # space maps to 1.5 in input space.<N>    u = x / scale + 0.5 * (1 - 1 / scale)<N><N>    # What is the left-most pixel that can be involved in the computation?<N>    left = torch.floor(u - kernel_width / 2)<N><N>
    # What is the maximum number of pixels that can be involved in the<N>    # computation?  Note: it's OK to use an extra pixel here; if the<N>    # corresponding weights are all zero, it will be eliminated at the end<N>    # of this function.<N>    p = math.ceil(kernel_width) + 2<N><N>
    # The indices of the input pixels involved in computing the k-th output<N>    # pixel are in row k of the indices matrix.<N>    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(<N>        0, p - 1, p).view(1, p).expand(out_length, p)<N><N>
    # The weights used to compute the k-th output pixel are in row k of the<N>    # weights matrix.<N>    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices<N><N>    # apply cubic kernel<N>    if (scale < 1) and antialiasing:<N>        weights = scale * cubic(distance_to_center * scale)<N>    else:<N>        weights = cubic(distance_to_center)<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import numpy as np<N>import os<N>import random<N>import time<N>import torch<N>from os import path as osp<N><N>
from .dist_util import master_only<N>from .logger import get_root_logger<N><N><N>def set_random_seed(seed):<N>    """Set random seeds."""<N>    random.seed(seed)<N>    np.random.seed(seed)<N>    torch.manual_seed(seed)<N>    torch.cuda.manual_seed(seed)<N>    torch.cuda.manual_seed_all(seed)<N><N>
<N>def get_time_str():<N>    return time.strftime('%Y%m%d_%H%M%S', time.localtime())<N><N><N>def mkdir_and_rename(path):<N>    """mkdirs. If path exists, rename it with timestamp and create a new one.<N><N>    Args:<N>        path (str): Folder path.<N>    """<N>    if osp.exists(path):<N>        new_name = path + '_archived_' + get_time_str()<N>        print(f'Path already exists. Rename it to {new_name}', flush=True)<N>        os.rename(path, new_name)<N>    os.makedirs(path, exist_ok=True)<N><N>
# ------------------------------------------------------------------------<N># Copyright (c) 2022 megvii-model. All Rights Reserved.<N># ------------------------------------------------------------------------<N># Modified from BasicSR (https://github.com/xinntao/BasicSR)<N># Copyright 2018-2020 BasicSR Authors<N># ------------------------------------------------------------------------<N>import yaml<N>from collections import OrderedDict<N>from os import path as osp<N><N>
<N>def ordered_yaml():<N>    """Support OrderedDict for yaml.<N><N>    Returns:<N>        yaml Loader and Dumper.<N>    """<N>    try:<N>        from yaml import CDumper as Dumper<N>        from yaml import CLoader as Loader<N>    except ImportError:<N>        from yaml import Dumper, Loader<N><N>
    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG<N><N>    def dict_representer(dumper, data):<N>        return dumper.represent_dict(data.items())<N><N>    def dict_constructor(loader, node):<N>        return OrderedDict(loader.construct_pairs(node))<N><N>
    Dumper.add_representer(OrderedDict, dict_representer)<N>    Loader.add_constructor(_mapping_tag, dict_constructor)<N>    return Loader, Dumper<N><N><N>def parse(opt_path, is_train=True):<N>    """Parse option file.<N><N>    Args:<N>        opt_path (str): Option file path.<N>        is_train (str): Indicate whether in training or not. Default: True.<N><N>
import numpy as np<N>import os<N>import cv2<N><N>PATH = './datasets/SR/NTIRE22-StereoSR/Train'<N><N>LR_FOLDER = 'LR_x4'<N>HR_FOLDER = 'HR'<N><N><N>lr_lists = []<N>hr_lists = []<N><N>cnt = 0<N><N>for idx in range(1, 801):<N><N>    L_name = f'{idx:04}_L.png'<N>    R_name = f'{idx:04}_R.png'<N><N>
<N>    LR_L = cv2.imread(os.path.join(PATH, LR_FOLDER, L_name))<N>    LR_R = cv2.imread(os.path.join(PATH, LR_FOLDER, R_name))<N><N>    HR_L = cv2.imread(os.path.join(PATH, HR_FOLDER, L_name))<N>    HR_R = cv2.imread(os.path.join(PATH, HR_FOLDER, R_name))<N><N>
    LR = np.concatenate([LR_L, LR_R], axis=-1)<N>    HR = np.concatenate([HR_L, HR_R], axis=-1)<N><N>    lr_lists.append(LR)<N>    hr_lists.append(HR)<N><N>    cnt = cnt + 1<N>    if cnt % 50 == 0:<N>        print(f'cnt .. {cnt}, idx: {idx}')<N><N><N><N>
import pickle<N>with open('./datasets/ntire-stereo-sr.train.lr.pickle', 'wb') as f:<N>    pickle.dump(lr_lists, f)<N><N>with open('./datasets/ntire-stereo-sr.train.hr.pickle', 'wb') as f:<N>    pickle.dump(hr_lists, f)<N><N><N><N># print(f'... {lr_all_np.shape}, {lr_all_np.dtype}')<N># print(f'... {hr_all_np.shape}, {hr_all_np.dtype}')<N><N>
#!/usr/bin/env python<N># -*- coding: utf8 -*-<N><N>#  MIT License<N><N># Copyright (c) 2022 mehrdad<N># Developed by mehrdad-mixtape https://github.com/mehrdad-mixtape<N><N># This is version for MicroPython v1.18<N># TTP229 Touch Keypad<N><N>from machine import Pin<N>from gc import collect<N>from utime import sleep_ms, sleep_us<N><N>
scl: Pin = Pin(19, mode=Pin.OUT)<N>sdo: Pin = Pin(18, mode=Pin.IN, pull=Pin.PULL_UP)<N><N>def _key_map(data: list) -> int:<N>    """Map data to key"""<N>    key: int = 0<N>    for bit in data:<N>        if bit != 0:<N>            key += 1<N>        else:<N>            break<N>    # print(f"key = {(key // 2) + 1}")<N>    return ((key // 2) + 1)<N><N>
from graphs import Graph<N><N>#Breath First Search - Visiting Adjacent Nodes, graph has just one component<N><N>def BFS(graph):<N>  visited = []<N>  first = list(graph.adjlist.keys())[0]<N>  visited.append(first)<N>  queue = [first]<N>  while(len(queue)>0) :<N>    elem = queue.pop(0)<N>    print(elem)<N>    for nbr in graph.adjlist[elem]:<N>      if nbr not in visited:<N>        visited.append(nbr)<N>        queue.append(nbr)<N><N>
'''<N>Problem: Detect cycle in a undirected graph using BFS<N>Solution: Look for neighbor which has been visited before and is not ur parent, if<N>        such node exists, it's a cyclic undirected graph else it's not.<N>'''<N>from graphs import Graph<N><N>
#Problem: Implement depth first search on undirected graphs, Graph has more then one compoennt<N><N>from graphs import Graph<N><N>def DFS(graph,visited,elem):<N>    print(elem)<N>    for n in graph.adjlist[elem]:<N>        if n not in visited:<N>            visited.append(n)<N>            DFS(graph,visited,n)<N><N>
if __name__=='__main__':<N>    g = Graph(True)<N>    g.addEdge(1,2)<N>    g.addEdge(2,3)<N>    g.addEdge(3,4)<N>    g.addEdge(4,5)<N>    g.addEdge(5,1)<N>    g.addEdge(6,7)<N>    g.addEdge(8,7)<N><N>    print(g.adjlist)<N><N>    visited=[]<N>    elem=1<N>    nodes=8<N>    for n in range(1,nodes):<N>        if n not in visited:<N>            visited.append(n)<N>            (DFS(g,visited,n))<N><N>
<N>class Graph:<N>  def __init__(self, undirected):<N>    self.adjlist = {  };<N>    self.undir = undirected;<N><N>  def addEdge(self, u, v):<N>    if u not in self.adjlist:<N>      self.adjlist[u] = [];<N>    if v not in self.adjlist:<N>      self.adjlist[v] = [];<N><N>    self.adjlist[u].append(v);<N>    if self.undir:<N>      self.adjlist[v].append(u);<N><N>  def getAdjList(self):<N>    return self.adjlist;<N><N>g = Graph(True);<N>g.addEdge(1,2)<N>g.addEdge(2,3)<N>g.addEdge(3,4)<N>g.addEdge(4,5)<N>g.addEdge(5,1)<N><N><N>
import pydivert<N><N>with pydivert.WinDivert("tcp.SrcPort == 443 and tcp.PayloadLength == 0") as w:<N>    for packet in w:<N>        try:<N>            packet.tcp.rst = False<N>            print(f'Source: {packet.src_addr}, {packet.src_port}, {packet.tcp.rst}')<N>            #print(f"{packet}, end='\n'")<N>            w.send(packet)<N>        except Exception as e:<N>            pass
def bubblesort(numbers_un, time):<N>    numbers = numbers_un<N><N>    start = time()<N><N>    for i in range(len(numbers)):<N>        for j in range(len(numbers) - 1):<N>            if numbers[j] > numbers[j + 1]:<N>                numbers[j], numbers[j + 1] = numbers[j + 1], numbers[j]<N><N>    end = time()<N><N>    print("Time for sorting bubblesort style:", end - start, "\n")<N>    return numbers<N><N>def quickshort(numbers, time):<N>    print("comming soon")<N>
from random import randrange<N>from time import time<N>from bubblesort import bubblesort, quickshort<N><N>for j in range(100, 1100, 100):<N><N>    numbers_unsorted = []<N>    numbers_sorted = []<N><N>    for i in range(1, j):<N><N>        x = randrange(0, 100)<N>        numbers_unsorted.insert(i, x)<N><N>
    print("senario:", j/100)<N>    print("Unsorted Array", numbers_unsorted, "\n")<N><N>    numbers_sorted = bubblesort(numbers_unsorted, time), quickshort(numbers_unsorted,time)<N><N><N>    print("unSorted:", numbers_unsorted, "\n")<N>    print("Sorted:", numbers_sorted, "\n")<N><N>
import random, time<N>from bubblesort import bubblesort<N>numbers = []<N><N>for i in range(1, 100):<N>    x = random.randrange(0, 1000000)<N>    numbers.insert(i, x)<N>    <N><N>print("\n mh taksinomimenos pinakas\n")<N><N>print(numbers,"\n")<N><N>bubblesort(numbers, time)<N><N>print("\n mh taksinomimenos pinakas\n")<N><N>print(numbers)<N><N><N>
import os<N>from setuptools import setup, find_packages<N>from torch.utils.cpp_extension import BuildExtension, CppExtension<N>from os import path<N><N>here = path.abspath(path.dirname(__file__))<N><N># Use correct conda compiler used to build pytorch<N>os.environ['CXX'] = os.environ.get('GXX', '')<N><N>
setup(<N>    name='focalpose',<N>    version='1.0.0',<N>    description='FocalPose',<N>    packages=find_packages(),<N>    ext_modules=[<N>        # CppExtension(<N>        #     name='cosypose_cext',<N>        #     sources=[<N>        #         'focalpose/csrc/cosypose_cext.cpp'<N>        #     ],<N>        #     extra_compile_args=['-O3'],<N>        #     verbose=True<N>        # )<N>    ],<N>    cmdclass={<N>        'build_ext': BuildExtension<N>    }<N>)<N><N><N>
import focalpose<N>import os<N>import yaml<N>from joblib import Memory<N>from pathlib import Path<N>import getpass<N>import socket<N>import torch.multiprocessing<N>torch.multiprocessing.set_sharing_strategy('file_system')<N><N>hostname = socket.gethostname()<N>username = getpass.getuser()<N><N>
PROJECT_ROOT = Path(focalpose.__file__).parent.parent<N>PROJECT_DIR = PROJECT_ROOT<N>DATA_DIR = PROJECT_DIR / 'data'<N>LOCAL_DATA_DIR = PROJECT_DIR / 'local_data'<N>TEST_DATA_DIR = LOCAL_DATA_DIR<N>DASK_LOGS_DIR = LOCAL_DATA_DIR / 'dasklogs'<N>SYNT_DS_DIR = LOCAL_DATA_DIR / 'synt_datasets'<N>BOP_DS_DIR = LOCAL_DATA_DIR / 'bop_datasets'<N><N>
BOP_TOOLKIT_DIR = PROJECT_DIR / 'deps' / 'bop_toolkit_cosypose'<N>BOP_CHALLENGE_TOOLKIT_DIR = PROJECT_DIR / 'deps' / 'bop_toolkit_challenge'<N><N>EXP_DIR = LOCAL_DATA_DIR / 'experiments'<N>FEATURES_DIR = LOCAL_DATA_DIR / 'features'<N>RESULTS_DIR = LOCAL_DATA_DIR / 'results'<N>DEBUG_DATA_DIR = LOCAL_DATA_DIR / 'debug_data'<N><N>
DEPS_DIR = PROJECT_DIR / 'deps'<N>CACHE_DIR = LOCAL_DATA_DIR / 'joblib_cache'<N><N>assert LOCAL_DATA_DIR.exists()<N>CACHE_DIR.mkdir(exist_ok=True)<N>TEST_DATA_DIR.mkdir(exist_ok=True)<N>DASK_LOGS_DIR.mkdir(exist_ok=True)<N>SYNT_DS_DIR.mkdir(exist_ok=True)<N>RESULTS_DIR.mkdir(exist_ok=True)<N>EXP_DIR.mkdir(exist_ok=True)<N>FEATURES_DIR.mkdir(exist_ok=True)<N>DEBUG_DATA_DIR.mkdir(exist_ok=True)<N><N>
ASSET_DIR = DATA_DIR / 'assets'<N>MEMORY = Memory(CACHE_DIR, verbose=2)<N><N><N>CONDA_PREFIX = os.environ['CONDA_PREFIX']<N>if 'CONDA_PREFIX_1' in os.environ:<N>    CONDA_BASE_DIR = os.environ['CONDA_PREFIX_1']<N>    CONDA_ENV = os.environ['CONDA_DEFAULT_ENV']<N>else:<N>    CONDA_BASE_DIR = os.environ['CONDA_PREFIX']<N>    CONDA_ENV = 'base'<N><N>
cfg = yaml.load((PROJECT_DIR / 'config.yaml').read_text(), Loader=yaml.FullLoader)<N><N>SLURM_GPU_QUEUE = cfg['slurm_gpu_queue']<N>SLURM_QOS = cfg['slurm_qos']<N>DASK_NETWORK_INTERFACE = cfg['dask_network_interface']<N>MESHLAB_PTH = cfg['meshlab_path']<N><N>
import os<N>os.environ['MKL_NUM_THREADS'] = '1'<N>os.environ['OMP_NUM_THREADS'] = '1'<N>print("Setting OMP and MKL num threads to 1.")<N>
import numpy as np<N>import PIL<N>import torch<N>import random<N>from PIL import ImageEnhance, ImageFilter<N>from torchvision.datasets import ImageFolder<N>import torch.nn.functional as F<N>from copy import deepcopy<N><N>from focalpose.lib3d.camera_geometry import get_K_crop_resize<N>from .utils import make_detections_from_segmentation, crop_to_aspect_ratio<N><N>
<N>def to_pil(im):<N>    if isinstance(im, PIL.Image.Image):<N>        return im<N>    elif isinstance(im, torch.Tensor):<N>        return PIL.Image.fromarray(np.asarray(im))<N>    elif isinstance(im, np.ndarray):<N>        return PIL.Image.fromarray(im)<N>    else:<N>        raise ValueError('Type not supported', type(im))<N><N>
<N>def to_torch_uint8(im):<N>    if isinstance(im, PIL.Image.Image):<N>        im = torch.as_tensor(np.asarray(im).astype(np.uint8))<N>    elif isinstance(im, torch.Tensor):<N>        assert im.dtype == torch.uint8<N>    elif isinstance(im, np.ndarray):<N>        assert im.dtype == np.uint8<N>        im = torch.as_tensor(np.copy(im))<N>    else:<N>        raise ValueError('Type not supported', type(im))<N>    if im.dim() == 3:<N>        assert im.shape[-1] in {1, 3}<N>    return im<N><N>
<N>class PillowBlur:<N>    def __init__(self, p=0.4, factor_interval=(1, 3)):<N>        self.p = p<N>        self.factor_interval = factor_interval<N><N>    def __call__(self, im, mask, obs):<N>        im = to_pil(im)<N>        k = random.randint(*self.factor_interval)<N>        im = im.filter(ImageFilter.GaussianBlur(k))<N>        return im, mask, obs<N><N>
<N>class PillowRGBAugmentation:<N>    def __init__(self, pillow_fn, p, factor_interval):<N>        self._pillow_fn = pillow_fn<N>        self.p = p<N>        self.factor_interval = factor_interval<N><N>    def __call__(self, im, mask, obs):<N>        im = to_pil(im)<N>        if random.random() <= self.p:<N>            im = self._pillow_fn(im).enhance(factor=random.uniform(*self.factor_interval))<N>        return im, mask, obs<N><N>
<N>class PillowSharpness(PillowRGBAugmentation):<N>    def __init__(self, p=0.3, factor_interval=(0., 50.)):<N>        super().__init__(pillow_fn=ImageEnhance.Sharpness,<N>                         p=p,<N>                         factor_interval=factor_interval)<N><N>
<N>class PillowContrast(PillowRGBAugmentation):<N>    def __init__(self, p=0.3, factor_interval=(0.2, 50.)):<N>        super().__init__(pillow_fn=ImageEnhance.Contrast,<N>                         p=p,<N>                         factor_interval=factor_interval)<N><N>
<N>class PillowBrightness(PillowRGBAugmentation):<N>    def __init__(self, p=0.5, factor_interval=(0.1, 6.0)):<N>        super().__init__(pillow_fn=ImageEnhance.Brightness,<N>                         p=p,<N>                         factor_interval=factor_interval)<N><N>
<N>class PillowColor(PillowRGBAugmentation):<N>    def __init__(self, p=0.3, factor_interval=(0.0, 20.0)):<N>        super().__init__(pillow_fn=ImageEnhance.Color,<N>                         p=p,<N>                         factor_interval=factor_interval)<N><N>
<N>class GrayScale(PillowRGBAugmentation):<N>    def __init__(self, p=0.3):<N>        self.p = p<N><N>    def __call__(self, im, mask, obs):<N>        im = to_pil(im)<N>        if random.random() <= self.p:<N>            im = to_torch_uint8(im).float()<N>            gray = 0.2989 * im[..., 0] + 0.5870 * im[..., 1] + 0.1140 * im[..., 2]<N>            gray = gray.to(torch.uint8)<N>            im = gray.unsqueeze(-1).repeat(1, 1, 3)<N>        return im, mask, obs<N><N>
from focalpose.config import LOCAL_DATA_DIR, SYNT_DS_DIR<N>from focalpose.utils.logging import get_logger<N><N>from .urdf_dataset import Pix3DUrdfDataset, CarsUrdfDataset<N>from .texture_dataset import TextureDataset<N><N>logger = get_logger(__name__)<N><N>
import torch<N>import numpy as np<N>import random<N>from focalpose.config import LOCAL_DATA_DIR<N><N>from .wrappers.visibility_wrapper import VisibilityWrapper<N>from .augmentations import (<N>    CropResizeToAspectAugmentation, VOCBackgroundAugmentation,<N>    PillowBlur, PillowSharpness, PillowContrast, PillowBrightness, PillowColor, to_torch_uint8,<N>    GrayScale<N>)<N><N>
<N>class DetectionDataset(torch.utils.data.Dataset):<N>    def __init__(self,<N>                 scene_ds,<N>                 label_to_category_id,<N>                 min_area=50,<N>                 resize=(640, 480),<N>                 gray_augmentation=False,<N>                 rgb_augmentation=False,<N>                 background_augmentation=False,<N>                 two_class=False):<N><N>
        self.scene_ds = VisibilityWrapper(scene_ds)<N><N>        self.resize_augmentation = CropResizeToAspectAugmentation(resize=resize)<N><N>        self.background_augmentation = background_augmentation<N>        self.background_augmentations = VOCBackgroundAugmentation(<N>            voc_root=LOCAL_DATA_DIR / 'VOCdevkit/VOC2012', p=0.3)<N><N>
        self.rgb_augmentation = rgb_augmentation<N>        self.rgb_augmentations = [<N>            PillowBlur(p=0.4, factor_interval=(1, 3)),<N>            PillowSharpness(p=0.3, factor_interval=(0., 50.)),<N>            PillowContrast(p=0.3, factor_interval=(0.2, 50.)),<N>            PillowBrightness(p=0.5, factor_interval=(0.1, 6.0)),<N>            PillowColor(p=0.3, factor_interval=(0., 20.))<N>        ]<N><N>
        self.label_to_category_id = label_to_category_id<N>        self.min_area = min_area<N>        self.two_class = two_class<N><N>    def __len__(self):<N>        return len(self.scene_ds)<N><N>    def get_data(self, idx):<N>        rgb, mask, state = self.scene_ds[idx]<N><N>
        rgb, mask, state = self.resize_augmentation(rgb, mask, state)<N><N>        if self.background_augmentation:<N>            rgb, mask, state = self.background_augmentations(rgb, mask, state)<N><N>        if self.rgb_augmentation and random.random() < 0.8:<N>            for augmentation in self.rgb_augmentations:<N>                rgb, mask, state = augmentation(rgb, mask, state)<N><N>
import torch<N>import random<N>import numpy as np<N>from dataclasses import dataclass<N>from focalpose.lib3d import invert_T<N>from focalpose.config import LOCAL_DATA_DIR<N><N>from .wrappers.visibility_wrapper import VisibilityWrapper<N>from .augmentations import (<N>    CropResizeToAspectAugmentation, VOCBackgroundAugmentation,<N>    PillowBlur, PillowSharpness, PillowContrast, PillowBrightness, PillowColor, to_torch_uint8,<N>    GrayScale<N>)<N><N>
@dataclass<N>class PoseData:<N>    images: None<N>    bboxes: None<N>    TCO: None<N>    K: None<N>    objects: None<N><N>    def pin_memory(self):<N>        self.images = self.images.pin_memory()<N>        self.bboxes = self.bboxes.pin_memory()<N>        self.TCO = self.TCO.pin_memory()<N>        self.K = self.K.pin_memory()<N>        return self<N><N>
<N>class NoObjectError(Exception):<N>    pass<N><N><N>class PoseDataset(torch.utils.data.Dataset):<N>    def __init__(self,<N>                 scene_ds,<N>                 resize=(640, 480),<N>                 min_area=None,<N>                 rgb_augmentation=False,<N>                 gray_augmentation=False,<N>                 background_augmentation=False):<N><N>
        self.scene_ds = VisibilityWrapper(scene_ds)<N><N>        self.resize_augmentation = CropResizeToAspectAugmentation(resize=resize)<N>        self.min_area = min_area<N><N>        self.background_augmentation = background_augmentation<N>        self.background_augmentations = VOCBackgroundAugmentation(<N>            voc_root=LOCAL_DATA_DIR / 'VOCdevkit/VOC2012', p=0.3)<N><N>
        self.rgb_augmentation = rgb_augmentation<N>        self.rgb_augmentations = [<N>            PillowBlur(p=0.4, factor_interval=(1, 3)),<N>            PillowSharpness(p=0.3, factor_interval=(0., 50.)),<N>            PillowContrast(p=0.3, factor_interval=(0.2, 50.)),<N>            PillowBrightness(p=0.5, factor_interval=(0.1, 6.0)),<N>            PillowColor(p=0.3, factor_interval=(0., 20.)),<N>        ]<N>        if gray_augmentation:<N>            self.rgb_augmentations.append(GrayScale(p=0.5))<N><N>
    def __len__(self):<N>        return len(self.scene_ds)<N><N>    def collate_fn(self, batch):<N>        data = dict()<N>        for k in batch[0].__annotations__:<N>            v = [getattr(x, k) for x in batch]<N>            if k in ('images', 'bboxes', 'TCO', 'K'):<N>                v = torch.as_tensor(np.stack(v))<N>            data[k] = v<N>        data = PoseData(**data)<N>        return data<N><N>
    def get_data(self, idx):<N>        rgb, mask, state = self.scene_ds[idx]<N><N>        rgb, mask, state = self.resize_augmentation(rgb, mask, state)<N><N>        if self.background_augmentation:<N>            rgb, mask, state = self.background_augmentations(rgb, mask, state)<N><N>
import torch<N>import numpy as np<N>from torch.utils.data import Sampler<N>from focalpose.utils.random import temp_numpy_seed<N><N><N>class PartialSampler(Sampler):<N>    def __init__(self, ds, epoch_size):<N>        self.n_items = len(ds)<N>        self.epoch_size = min(epoch_size, len(ds))<N>        super().__init__(None)<N><N>
import torch<N>import pandas as pd<N>import numpy as np<N>import pickle as pkl<N>import yaml<N>import cv2<N>from io import BytesIO<N>from .utils import make_detections_from_segmentation<N>from .datasets_cfg import make_urdf_dataset<N>from pathlib import Path<N>import torch.multiprocessing<N>torch.multiprocessing.set_sharing_strategy('file_system')<N><N>
<N>class SyntheticSceneDataset:<N>    def __init__(self, ds_dir, train=True):<N>        self.ds_dir = Path(ds_dir)<N>        assert self.ds_dir.exists()<N><N>        keys_path = ds_dir / (('train' if train else 'val') + '_keys.pkl')<N>        keys = pkl.loads(keys_path.read_bytes())<N>        self.cfg = yaml.load((ds_dir / 'config.yaml').read_text(), Loader=yaml.FullLoader)<N>        self.object_set = self.cfg.scene_kwargs['urdf_ds']<N>        self.keys = keys<N><N>
        urdf_ds_name = self.cfg.scene_kwargs['urdf_ds']<N>        urdf_ds = make_urdf_dataset(urdf_ds_name)<N>        self.all_labels = [obj['label'] for _, obj in urdf_ds.index.iterrows()]<N>        self.frame_index = pd.DataFrame(dict(scene_id=np.arange(len(keys)), view_id=np.arange(len(keys))))<N><N>
import pandas as pd<N>from pathlib import Path<N>from focalpose.config import MEMORY<N><N><N>class TextureDataset:<N>    def __init__(self, ds_dir):<N>        ds_dir = Path(ds_dir)<N>        self.parse_image_dir = MEMORY.cache(self.parse_image_dir)<N>        self.index = self.parse_image_dir(ds_dir)<N><N>
    @staticmethod<N>    def parse_image_dir(ds_dir):<N>        ds_dir = Path(ds_dir)<N>        index = []<N>        for im_path in ds_dir.glob('*'):<N>            if im_path.suffix in {'.png', '.jpg'}:<N>                index.append(dict(texture_path=im_path))<N>        index = pd.DataFrame(index)<N>        return index<N><N>
import yaml<N>import pandas as pd<N>from pathlib import Path<N><N><N>class Pix3DUrdfDataset:<N>    def __init__(self, root_dir):<N>        root_dir = Path(root_dir)<N>        assert root_dir.exists()<N><N>        infos = []<N>        for category_dir in root_dir.iterdir():<N>            category = category_dir.name<N><N>
            for model_dir in category_dir.iterdir():<N>                urdf_path = model_dir / model_dir.with_suffix('.urdf').name<N>                info = dict(urdf_path=urdf_path.as_posix(),<N>                            category=str(category),<N>                            label=model_dir.name,<N>                            scale=1)<N>                infos.append(info)<N>        self.index = pd.DataFrame(infos)<N><N>
    def __len__(self):<N>        return len(self.index)<N><N>    def __getitem__(self, idx):<N>        return self.index.loc[idx]<N><N>class CarsUrdfDataset:<N>    def __init__(self, root_dir):<N>        root_dir = Path(root_dir)<N>        assert root_dir.exists()<N><N>
        infos = []<N>        for model_dir in root_dir.iterdir():<N>            urdf_path = model_dir / model_dir.with_suffix('.urdf').name<N>            info = dict(urdf_path=urdf_path.as_posix(),<N>                        category='car',<N>                        label=model_dir.name,<N>                        scale=1)<N>            infos.append(info)<N>        self.index = pd.DataFrame(infos)<N><N>
from .base import SceneDatasetWrapper<N><N><N>class AugmentationWrapper(SceneDatasetWrapper):<N>    def __init__(self, scene_ds, augmentation):<N>        self.augmentation = augmentation<N>        super().__init__(scene_ds)<N>        self.frame_index = self.scene_ds.frame_index<N><N>    def process_data(self, data):<N>        rgb, mask, obs = data<N>        return self.augmentation(rgb, mask, obs)<N>
class SceneDatasetWrapper:<N>    def __init__(self, scene_ds):<N>        self.scene_ds = scene_ds<N><N>    @property<N>    def unwrapped(self):<N>        if isinstance(self, SceneDatasetWrapper):<N>            return self.scene_ds<N>        else:<N>            return self<N><N>    def __len__(self):<N>        return len(self.scene_ds)<N><N>    def __getitem__(self, idx):<N>        data = self.scene_ds[idx]<N>        return self.process_data(data)<N><N>    def process_data(self, data):<N>        return data<N>
import numpy as np<N>from .base import SceneDatasetWrapper<N><N><N>class VisibilityWrapper(SceneDatasetWrapper):<N>    def process_data(self, data):<N>        rgb, mask, state = data<N>        ids_visible = np.unique(mask)<N>        ids_visible = set(ids_visible[ids_visible > 0])<N>        visib_objects = []<N>        for obj in state['objects']:<N>            if obj['id_in_segm'] in ids_visible:<N>                visib_objects.append(obj)<N>        state['objects'] = visib_objects<N>        return rgb, mask, state<N>
import torch<N>import numpy as np<N>from scipy.linalg import logm<N>from focalpose.lib3d.camera_geometry import project_points_robust as project_points<N>from focalpose.lib3d.focalpose_ops import TCO_init_from_boxes_zup_autodepth<N>from focalpose.lib3d.transform_ops import add_noise, add_noise_f, transform_pts<N><N>
<N>def cast(obj):<N>    return obj.cuda(non_blocking=True)<N><N><N>def pose_evaluator(model, data, meters, cfg, n_iterations=1, mesh_db=None, input_generator='fixed'):<N><N>    batch_size, _, h, w = data.images.shape<N>    images = cast(data.images).float() / 255.<N>    K_gt = cast(data.K).float()<N>    TCO_gt = cast(data.TCO).float()<N>    labels = np.array([obj['name'] for obj in data.objects])<N>    bboxes = cast(data.bboxes).float()<N><N>
import torch<N>from focalpose.lib3d.transform_ops import transform_pts<N><N><N>def dists_add(TXO_pred, TXO_gt, points):<N>    TXO_pred_points = transform_pts(TXO_pred, points)<N>    TXO_gt_points = transform_pts(TXO_gt, points)<N>    dists = TXO_gt_points - TXO_pred_points<N>    return dists<N><N>
import torch<N><N>from .rotations import compute_rotation_matrix_from_ortho6d, compute_rotation_matrix_from_quaternions<N>from .transform_ops import transform_pts<N><N>l1 = lambda diff: diff.abs()<N>l2 = lambda diff: diff ** 2<N><N><N>def apply_imagespace_predictions(TCO, K, vxvyvz, dRCO):<N>    assert TCO.shape[-2:] == (4, 4)<N>    assert K.shape[-2:] == (3, 3)<N>    assert dRCO.shape[-2:] == (3, 3)<N>    assert vxvyvz.shape[-1] == 3<N>    TCO_out = TCO.clone()<N><N>
    # Translation in image space<N>    zsrc = TCO[:, 2, [3]]<N>    vz = vxvyvz[:, [2]]<N>    ztgt = vz * zsrc<N><N>    vxvy = vxvyvz[:, :2]<N>    fxfy = K[:, [0, 1], [0, 1]]<N>    xsrcysrc = TCO[:, :2, 3]<N>    TCO_out[:, 2, 3] = ztgt.flatten()<N>    TCO_out[:, :2, 3] = ((vxvy / fxfy) + (xsrcysrc / zsrc.repeat(1, 2))) * ztgt.repeat(1, 2)<N><N>
import torch<N>import numpy as np<N><N><N>def get_meshes_center(pts):<N>    bsz = pts.shape[0]<N>    limits = get_meshes_bounding_boxes(pts)<N>    t_offset = limits[..., :3].mean(dim=1)<N>    T_offset = torch.eye(4, 4, dtype=pts.dtype, device=pts.device)<N>    T_offset = T_offset.unsqueeze(0).repeat(bsz, 1, 1)<N>    T_offset[:, :3, -1] = t_offset<N>    return T_offset<N><N>
import numpy as np<N>import trimesh<N>import torch<N>from copy import deepcopy<N>from pathlib import Path<N><N>from .mesh_ops import get_meshes_bounding_boxes, sample_points<N>from .symmetries import make_bop_symmetries<N>from focalpose.utils.tensor_collection import TensorCollection<N><N>
<N>class MeshDataBase:<N>    def __init__(self, obj_list):<N>        self.infos = {obj['label']: obj for obj in obj_list}<N>        self.meshes = {l: scene_as_mesh(trimesh.load(Path(obj['urdf_path']).with_suffix('.obj'))) for l, obj in self.infos.items()}<N><N>
    @staticmethod<N>    def from_urdf_ds(urdf_ds):<N>        obj_list = [urdf_ds.index.iloc[n] for n in range(len(urdf_ds))]<N>        return MeshDataBase(obj_list)<N><N>    def batched(self, aabb=False, resample_n_points=None, n_sym=64):<N>        if aabb:<N>            assert resample_n_points is None<N><N>
        labels, points, symmetries = [], [], []<N>        new_infos = deepcopy(self.infos)<N>        for label, mesh in self.meshes.items():<N>            if aabb:<N>                points_n = get_meshes_bounding_boxes(torch.as_tensor(mesh.vertices).unsqueeze(0))[0]<N>            elif resample_n_points:<N>                points_n = torch.tensor(trimesh.sample.sample_surface(mesh, resample_n_points)[0])<N>            else:<N>                points_n = torch.tensor(mesh.vertices)<N><N>
            points_n = points_n.clone()<N>            infos = self.infos[label]<N><N>            dict_symmetries = {k: infos.get(k, []) for k in ('symmetries_discrete', 'symmetries_continuous')}<N>            symmetries_n = make_bop_symmetries(dict_symmetries, n_symmetries_continuous=n_sym, scale=1.0)<N><N>
            new_infos[label]['n_points'] = points_n.shape[0]<N>            new_infos[label]['n_sym'] = symmetries_n.shape[0]<N>            symmetries.append(torch.as_tensor(symmetries_n))<N>            points.append(torch.as_tensor(points_n))<N>            labels.append(label)<N><N>
        labels = np.array(labels)<N>        points = pad_stack_tensors(points, fill='select_random', deterministic=True)<N>        symmetries = pad_stack_tensors(symmetries, fill=torch.eye(4), deterministic=True)<N>        return BatchedMeshes(new_infos, labels, points, symmetries).float()<N><N>
<N>class BatchedMeshes(TensorCollection):<N>    def __init__(self, infos, labels, points, symmetries):<N>        super().__init__()<N>        self.infos = infos<N>        self.label_to_id = {label: n for n, label in enumerate(labels)}<N>        self.labels = np.asarray(labels)<N>        self.register_tensor('points', points)<N>        self.register_tensor('symmetries', symmetries)<N><N>
    @property<N>    def n_sym_mapping(self):<N>        return {label: obj['n_sym'] for label, obj in self.infos.items()}<N><N>    def select(self, labels):<N>        ids = [self.label_to_id[l] for l in labels]<N>        return Meshes(<N>            infos=[self.infos[l] for l in labels],<N>            labels=self.labels[ids],<N>            points=self.points[ids],<N>            symmetries=self.symmetries[ids],<N>        )<N><N>
<N>class Meshes(TensorCollection):<N>    def __init__(self, infos, labels, points, symmetries):<N>        super().__init__()<N>        self.infos = infos<N>        self.labels = np.asarray(labels)<N>        self.register_tensor('points', points)<N>        self.register_tensor('symmetries', symmetries)<N><N>
import numpy as np<N>import pinocchio as pin<N>import eigenpy<N>eigenpy.switchToNumpyArray()<N><N><N>def parse_pose_args(pose_args):<N>    if len(pose_args) == 2:<N>        pos, orn = pose_args<N>        pose = Transform(orn, pos)<N>    elif isinstance(pose_args, Transform):<N>        pose = pose_args<N>    else:<N>        raise ValueError<N>    return pose<N><N>
from .transform import Transform, parse_pose_args<N>from .rotations import compute_rotation_matrix_from_ortho6d<N>from .transform_ops import transform_pts, invert_T<N>
from torchvision.models.detection.mask_rcnn import MaskRCNN<N>from torchvision.models.detection.backbone_utils import resnet_fpn_backbone<N>from torchvision.models.detection.rpn import AnchorGenerator<N><N><N>class DetectorMaskRCNN(MaskRCNN):<N>    def __init__(self, input_resize=(240, 320), n_classes=2,<N>                 backbone_str='resnet50-fpn',<N>                 anchor_sizes=((32, ), (64, ), (128, ), (256, ), (512, ))):<N><N>
        assert backbone_str == 'resnet50-fpn'<N>        backbone = resnet_fpn_backbone('resnet50', pretrained=False)<N><N>        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)<N>        rpn_anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)<N><N>
import torch<N>from torch import nn<N><N>from focalpose.config import DEBUG_DATA_DIR<N>from focalpose.lib3d.camera_geometry import get_K_crop_resize, boxes_from_uv<N><N>from focalpose.lib3d.cropping import deepim_crops_robust as deepim_crops<N>from focalpose.lib3d.camera_geometry import project_points_robust as project_points<N><N>
from focalpose.lib3d.rotations import (<N>    compute_rotation_matrix_from_ortho6d, compute_rotation_matrix_from_quaternions)<N>from focalpose.lib3d.focalpose_ops import apply_imagespace_predictions<N><N>from focalpose.utils.logging import get_logger<N>logger = get_logger(__name__)<N><N>
<N>class PosePredictor(nn.Module):<N>    def __init__(self, backbone, renderer,<N>                 mesh_db, render_size=(240, 320),<N>                 pose_dim=9):<N>        super().__init__()<N><N>        self.backbone = backbone<N>        self.renderer = renderer<N>        self.mesh_db = mesh_db<N>        self.render_size = render_size<N>        self.pose_dim = pose_dim<N><N>
        n_features = backbone.n_features<N><N>        self.heads = dict()<N>        self.pose_fc = nn.Linear(n_features, pose_dim, bias=True)<N>        self.heads['pose'] = self.pose_fc<N><N>        self.focal_length_fc = nn.Linear(n_features, 1, bias=True)<N>        self.heads['focal_length'] = self.focal_length_fc<N><N>
from pathlib import Path<N>import pinocchio as pin<N>import numpy as np<N><N>from focalpose.config import ASSET_DIR<N><N>from focalpose.datasets.datasets_cfg import make_urdf_dataset, make_texture_dataset<N><N>from focalpose.simulator import BaseScene, Body, Camera<N>from focalpose.simulator import BodyCache, TextureCache, apply_random_textures<N><N>
import numpy as np<N>import pickle<N>import importlib<N>from pathlib import Path<N>from PIL import Image<N>from io import BytesIO<N><N><N>def get_cls(cls_str):<N>    split = cls_str.split('.')<N>    mod_name = '.'.join(split[:-1])<N>    cls_name = split[-1]<N>    mod = importlib.import_module(mod_name)<N>    return getattr(mod, cls_name)<N><N>
<N>def _serialize_im(im, **pil_kwargs):<N>    im = Image.fromarray(np.asarray(im))<N>    im_buf = BytesIO()<N>    im.save(im_buf, **pil_kwargs)<N>    im_buf = im_buf.getvalue()<N>    return im_buf<N><N><N>def _get_dic_buf(state, jpeg=True, jpeg_compression=100):<N>    if jpeg:<N>        pil_kwargs = dict(format='JPEG', quality=jpeg_compression)<N>    else:<N>        pil_kwargs = dict(format='PNG', quality=100)<N><N>
    del state['camera']['depth']<N>    state['camera']['rgb'] = _serialize_im(state['camera']['rgb'], **pil_kwargs)<N>    state['camera']['mask'] = _serialize_im(state['camera']['mask'], format='PNG', quality=100)<N>    return pickle.dumps(state)<N><N>
<N>def write_chunk(state_list, seed, ds_dir):<N>    key_to_buf = dict()<N>    dumps_dir = Path(ds_dir) / 'dumps'<N>    dumps_dir.mkdir(exist_ok=True)<N><N>    for n, state in enumerate(state_list):<N>        key = f'{seed}-{n}'<N>        key_to_buf[key] = _get_dic_buf(state)<N><N>
    # Write on disk<N>    for key, buf in key_to_buf.items():<N>        (dumps_dir / key).with_suffix('.pkl').write_bytes(buf)<N>    keys = list(key_to_buf.keys())<N>    return keys<N><N><N>def record_chunk(ds_dir, scene_cls, scene_kwargs, seed, n_frames):<N>    ds_dir = Path(ds_dir)<N>    ds_dir.mkdir(exist_ok=True)<N><N>
    scene_cls = get_cls(scene_cls)<N>    scene_kwargs['seed'] = seed<N>    scene = scene_cls(**scene_kwargs)<N><N>    scene.connect(load=True)<N><N>    state_list = []<N>    for _ in range(n_frames):<N>        state = scene.make_new_scene()<N>        state_list.append(state)<N>    keys = write_chunk(state_list, seed, ds_dir)<N><N>
import yaml<N>import pickle<N>import shutil<N>from pathlib import Path<N>from tqdm import tqdm<N><N>from dask_jobqueue import SLURMCluster<N>from distributed import Client, LocalCluster, as_completed<N>from .record_chunk import record_chunk<N><N>from focalpose.config import CONDA_BASE_DIR, CONDA_ENV, PROJECT_DIR, DASK_LOGS_DIR<N>from focalpose.config import SLURM_GPU_QUEUE, SLURM_QOS, DASK_NETWORK_INTERFACE<N><N>
import dask<N>dask.config.set({'distributed.scheduler.allowed-failures': 1000})<N><N><N>def record_dataset_dask(client, ds_dir,<N>                        scene_cls, scene_kwargs,<N>                        n_chunks, n_frames_per_chunk,<N>                        start_seed=0, resume=False):<N><N>
    seeds = set(range(start_seed, start_seed + n_chunks))<N>    if resume:<N>        done_seeds = (ds_dir / 'seeds_recorded.txt').read_text().strip().split('\n')<N>        seeds = set(seeds) - set(map(int, done_seeds))<N>        all_keys = (ds_dir / 'keys_recorded.txt').read_text().strip().split('\n')<N>    else:<N>        all_keys = []<N>    seeds = tuple(seeds)<N><N>
    future_kwargs = []<N>    for seed in seeds:<N>        kwargs = dict(ds_dir=ds_dir, seed=seed,<N>                      n_frames=n_frames_per_chunk,<N>                      scene_cls=scene_cls,<N>                      scene_kwargs=scene_kwargs)<N>        future_kwargs.append(kwargs)<N><N>
    futures = []<N>    for kwargs in future_kwargs:<N>        futures.append(client.submit(record_chunk, **kwargs))<N><N>    iterator = as_completed(futures)<N>    unit = 'frame'<N>    unit_scale = n_frames_per_chunk<N>    n_futures = len(future_kwargs)<N>    tqdm_iterator = tqdm(iterator, total=n_futures, unit_scale=unit_scale, unit=unit, ncols=80)<N><N>
    seeds_file = open(ds_dir / 'seeds_recorded.txt', 'a')<N>    keys_file = open(ds_dir / 'keys_recorded.txt', 'a')<N><N>    for future in tqdm_iterator:<N>        keys, seed = future.result()<N>        all_keys += keys<N>        seeds_file.write(f'{seed}\n')<N>        seeds_file.flush()<N>        keys_file.write('\n'.join(keys) + '\n')<N>        keys_file.flush()<N>        client.cancel(future)<N><N>
    seeds_file.close()<N>    keys_file.close()<N>    return all_keys<N><N><N>def record_dataset(args):<N>    if args.resume and not args.overwrite:<N>        resume_args = yaml.load((Path(args.resume) / 'config.yaml').read_text())<N>        vars(args).update({k: v for k, v in vars(resume_args).items() if 'resume' not in k})<N><N>
    args.ds_dir = Path(args.ds_dir)<N>    if args.ds_dir.is_dir():<N>        if args.resume:<N>            assert (args.ds_dir / 'seeds_recorded.txt').exists()<N>        elif args.overwrite:<N>            shutil.rmtree(args.ds_dir)<N>        else:<N>            raise ValueError('There is already a dataset with this name')<N>    args.ds_dir.mkdir(exist_ok=True)<N><N>
import torch<N>import numpy as np<N>import multiprocessing<N><N>from focalpose.lib3d.transform_ops import invert_T<N>from focalpose.datasets.datasets_cfg import make_urdf_dataset<N>from .bullet_scene_renderer import BulletSceneRenderer<N><N><N>def init_renderer(urdf_ds, preload=True):<N>    renderer = BulletSceneRenderer(urdf_ds=urdf_ds,<N>                                   preload_cache=preload,<N>                                   background_color=(0, 0, 0))<N>    return renderer<N><N>
import numpy as np<N>import pybullet as pb<N><N>from focalpose.datasets.datasets_cfg import make_urdf_dataset<N>from focalpose.lib3d import Transform<N><N>from focalpose.simulator.base_scene import BaseScene<N>from focalpose.simulator.caching import BodyCache<N>from focalpose.simulator.camera import Camera<N><N>
<N>class BulletSceneRenderer(BaseScene):<N>    def __init__(self,<N>                 urdf_ds='ycbv',<N>                 preload_cache=False,<N>                 background_color=(0, 0, 0),<N>                 gpu_renderer=True,<N>                 gui=False):<N><N>
        self.urdf_ds = make_urdf_dataset(urdf_ds)<N>        self.connect(gpu_renderer=gpu_renderer, gui=gui)<N>        self.body_cache = BodyCache(self.urdf_ds, self.client_id)<N>        if preload_cache:<N>            self.body_cache.get_bodies_by_ids(np.arange(len(self.urdf_ds)))<N>        self.background_color = background_color<N><N>
    def setup_scene(self, obj_infos):<N>        labels = [obj['name'] for obj in obj_infos]<N>        bodies = self.body_cache.get_bodies_by_labels(labels)<N>        for (obj_info, body) in zip(obj_infos, bodies):<N>            TWO = Transform(obj_info['TWO'])<N>            body.pose = TWO<N>            color = obj_info.get('color', None)<N>            if color is not None:<N>                pb.changeVisualShape(body.body_id, -1, physicsClientId=0, rgbaColor=color)<N>        return bodies<N><N>
from focalpose.config import LOCAL_DATA_DIR<N>from pathlib import Path<N>from shutil import copy2<N>from distutils.dir_util import copy_tree<N>import xml.etree.ElementTree as ET<N>from xml.dom import minidom<N><N><N>def obj_to_urdf(obj_path, urdf_path):<N>    obj_path = Path(obj_path)<N>    urdf_path = Path(urdf_path)<N>    assert urdf_path.parent == obj_path.parent<N><N>
    geometry = ET.Element('geometry')<N>    mesh = ET.SubElement(geometry, 'mesh')<N>    mesh.set('filename', obj_path.name)<N>    mesh.set('scale', '1.0 1.0 1.0')<N><N>    material = ET.Element('material')<N>    material.set('name', 'mat_part0')<N>    color = ET.SubElement(material, 'color')<N>    color.set('rgba', '1.0 1.0 1.0 1.0')<N><N>
    inertial = ET.Element('inertial')<N>    origin = ET.SubElement(inertial, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N><N>    mass = ET.SubElement(inertial, 'mass')<N>    mass.set('value', '0.1')<N><N>    inertia = ET.SubElement(inertial, 'inertia')<N>    inertia.set('ixx', '1')<N>    inertia.set('ixy', '0')<N>    inertia.set('ixz', '0')<N>    inertia.set('iyy', '1')<N>    inertia.set('iyz', '0')<N>    inertia.set('izz', '1')<N><N>
    robot = ET.Element('robot')<N>    robot.set('name', obj_path.with_suffix('').name)<N><N>    link = ET.SubElement(robot, 'link')<N>    link.set('name', 'base_link')<N><N>    visual = ET.SubElement(link, 'visual')<N>    origin = ET.SubElement(visual, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N>    visual.append(geometry)<N>    visual.append(material)<N><N>
    collision = ET.SubElement(link, 'collision')<N>    origin = ET.SubElement(collision, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N>    collision.append(geometry)<N><N>    link.append(inertial)<N><N>    xmlstr = minidom.parseString(ET.tostring(robot)).toprettyxml(indent="   ")<N>    Path(urdf_path).write_text(xmlstr)  # Write xml file<N><N>
<N>if __name__ == '__main__':<N>    urdf_path = LOCAL_DATA_DIR  / 'models_urdf' / 'StanfordCars3D'<N>    urdf_path.mkdir(exist_ok=True, parents=True)<N>    for cls in (LOCAL_DATA_DIR / 'StanfordCars' / 'models').iterdir():<N>        cls_path = urdf_path / cls.name<N>        cls_path.mkdir(exist_ok=True, parents=True)<N><N>
        for obj in cls.iterdir():<N>            if obj.suffix == '.obj':<N>                copy2(obj.as_posix(), obj.with_name(f'{cls.name}').with_suffix('.obj'))<N>                obj_to_urdf(obj.with_name(f'{cls.name}').with_suffix('.obj'),<N>                            obj.with_name(f'{cls.name}').with_suffix('.urdf'))<N>        copy_tree(cls.as_posix(), cls_path.as_posix())<N><N>
    urdf_path = LOCAL_DATA_DIR / 'models_urdf' / 'CompCars3D'<N>    urdf_path.mkdir(exist_ok=True, parents=True)<N>    for cls in (LOCAL_DATA_DIR / 'CompCars' / 'models').iterdir():<N>        cls_path = urdf_path / cls.name<N>        cls_path.mkdir(exist_ok=True, parents=True)<N><N>
        for obj in cls.iterdir():<N>            if obj.suffix == '.obj':<N>                copy2(obj.as_posix(), obj.with_name(f'{cls.name}').with_suffix('.obj'))<N>                obj_to_urdf(obj.with_name(f'{cls.name}').with_suffix('.obj'),<N>                            obj.with_name(f'{cls.name}').with_suffix('.urdf'))<N>        copy_tree(cls.as_posix(), cls_path.as_posix())<N><N><N>
import numpy as np<N>import os<N>from pathlib import Path<N>from focalpose.config import LOCAL_DATA_DIR<N>import yaml<N>import pybullet as pb<N>import pybullet_data<N>from focalpose.simulator.body import Body<N><N>def main():<N>    urdf_path = LOCAL_DATA_DIR / 'models_urdf' / 'pix3d'<N><N>
    for obj in urdf_path.iterdir():<N>        scales = dict()<N>        cfg = dict()<N>        config_path = (LOCAL_DATA_DIR / 'configs' / os.path.basename(obj))<N>        config = config_path.with_suffix('.yaml')<N><N>        for instances in obj.iterdir():<N>            if not instances.is_dir():<N>                continue<N><N>
from focalpose.config import LOCAL_DATA_DIR<N>from pathlib import Path<N>from shutil import copy2<N>from distutils.dir_util import copy_tree<N>import xml.etree.ElementTree as ET<N>from xml.dom import minidom<N><N><N>def obj_to_urdf(obj_path, urdf_path):<N>    obj_path = Path(obj_path)<N>    urdf_path = Path(urdf_path)<N>    assert urdf_path.parent == obj_path.parent<N><N>
    geometry = ET.Element('geometry')<N>    mesh = ET.SubElement(geometry, 'mesh')<N>    mesh.set('filename', obj_path.name)<N>    mesh.set('scale', '1.0 1.0 1.0')<N><N>    material = ET.Element('material')<N>    material.set('name', 'mat_part0')<N>    color = ET.SubElement(material, 'color')<N>    color.set('rgba', '1.0 1.0 1.0 1.0')<N><N>
    inertial = ET.Element('inertial')<N>    origin = ET.SubElement(inertial, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N><N>    mass = ET.SubElement(inertial, 'mass')<N>    mass.set('value', '0.1')<N><N>    inertia = ET.SubElement(inertial, 'inertia')<N>    inertia.set('ixx', '1')<N>    inertia.set('ixy', '0')<N>    inertia.set('ixz', '0')<N>    inertia.set('iyy', '1')<N>    inertia.set('iyz', '0')<N>    inertia.set('izz', '1')<N><N>
    robot = ET.Element('robot')<N>    robot.set('name', obj_path.with_suffix('').name)<N><N>    link = ET.SubElement(robot, 'link')<N>    link.set('name', 'base_link')<N><N>    visual = ET.SubElement(link, 'visual')<N>    origin = ET.SubElement(visual, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N>    visual.append(geometry)<N>    visual.append(material)<N><N>
    collision = ET.SubElement(link, 'collision')<N>    origin = ET.SubElement(collision, 'origin')<N>    origin.set('rpy', '0 0 0')<N>    origin.set('xyz', '0.0 0.0 0.0')<N>    collision.append(geometry)<N><N>    link.append(inertial)<N><N>    xmlstr = minidom.parseString(ET.tostring(robot)).toprettyxml(indent="   ")<N>    Path(urdf_path).write_text(xmlstr)  # Write xml file<N><N>
<N>if __name__ == '__main__':<N>    urdf_path = LOCAL_DATA_DIR  / 'models_urdf' / 'pix3d'<N>    urdf_path.mkdir(exist_ok=True, parents=True)<N>    for cls in (LOCAL_DATA_DIR / 'pix3d' / 'model').iterdir():<N>        cls_path = urdf_path / cls.name<N>        cls_path.mkdir(exist_ok=True, parents=True)<N><N>
import argparse<N>from colorama import Fore, Style<N><N>from focalpose.config import LOCAL_DATA_DIR<N>from focalpose.recording.record_dataset import record_dataset<N><N><N>def make_cfg(cfg_name,<N>             resume_ds_name='',<N>             debug=False,<N>             distributed=False,<N>             overwrite=False,<N>             datasets_dir=LOCAL_DATA_DIR):<N>    datasets_dir = datasets_dir / 'synt_datasets'<N>    datasets_dir.mkdir(exist_ok=True)<N><N>
    cfg = argparse.ArgumentParser('').parse_args([])<N><N>    cfg.overwrite = overwrite<N>    cfg.ds_name = 'default_dataset'<N><N>    n_frames = 1e6<N>    cfg.n_frames_per_chunk = 100<N>    cfg.n_chunks = n_frames // cfg.n_frames_per_chunk<N>    cfg.train_ratio = 0.95<N><N>
import argparse<N>import numpy as np<N>import os<N>from colorama import Fore, Style<N>from focalpose.utils.resources import assign_gpu<N><N>from focalpose.training.train_detector import train_detector<N>from focalpose.utils.logging import get_logger<N>logger = get_logger(__name__)<N><N>
<N>if __name__ == '__main__':<N>    parser = argparse.ArgumentParser('Training')<N>    parser.add_argument('--config', default='', type=str)<N>    parser.add_argument('--debug', action='store_true')<N>    parser.add_argument('--resume', default='', type=str)<N>    parser.add_argument('--no-eval', action='store_true')<N>    args = parser.parse_args()<N><N>
    assign_gpu()<N><N>    cfg = argparse.ArgumentParser('').parse_args([])<N>    if args.config:<N>        logger.info(f"{Fore.GREEN}Training with config: {args.config} {Style.RESET_ALL}")<N><N>    cfg.resume_run_id = None<N>    if len(args.resume) > 0:<N>        cfg.resume_run_id = args.resume<N>        logger.info(f"{Fore.RED}Resuming {cfg.resume_run_id} {Style.RESET_ALL}")<N><N>
    N_CPUS = int(os.environ.get('N_CPUS', 10))<N>    N_GPUS = int(os.environ.get('N_PROCS', 1))<N>    N_WORKERS = min(N_CPUS - 2, 8)<N>    N_RAND = np.random.randint(1e6)<N>    cfg.n_gpus = N_GPUS<N><N>    run_comment = ''<N><N>    if 'pix3d-sofa' in args.config:<N>        cfg.urdf_ds_name = 'pix3d-sofa'<N>        cfg.n_symmetries_batch = 1<N><N>
import yaml<N>import torch<N>import argparse<N>import torchvision<N>import numpy as np<N>import pandas as pd<N><N>from tqdm import tqdm<N>from torch.backends import cudnn<N>from torch.utils.data import DataLoader<N>from torchvision.ops import box_iou<N>from torchvision import transforms as pth_transforms<N><N>
from scipy.linalg import logm<N><N>from sklearn.pipeline import make_pipeline<N>from sklearn.ensemble import VotingClassifier<N>from sklearn.linear_model import LogisticRegression<N>from sklearn.preprocessing import StandardScaler, LabelEncoder<N><N>
import os<N>import shutil<N>import argparse<N>import numpy as np<N>from colorama import Fore, Style<N><N>from focalpose.utils.resources import assign_gpu<N>from focalpose.training.train_pose import train_pose<N>from focalpose.utils.logging import get_logger<N>logger = get_logger(__name__)<N><N>
def make_cfg(args):<N>    cfg = argparse.ArgumentParser('').parse_args([])<N>    if args.config:<N>        logger.info(f"{Fore.GREEN}Training with config: {args.config} {Style.RESET_ALL}")<N><N>    cfg.resume_run_id = None<N>    if len(args.resume) > 0:<N>        cfg.resume_run_id = args.resume<N>        logger.info(f"{Fore.RED}Resuming {cfg.resume_run_id} {Style.RESET_ALL}")<N><N>
    if shutil.which('squeue'):<N>        N_CPUS = int(os.environ.get('SLURM_PROCID', 10))<N>    elif shutil.which('qstat'):<N>        N_CPUS = int(os.environ.get('MPI_LOCALRANKID', 10))<N>    else:<N>        N_CPUS = 8<N>    N_WORKERS = min(N_CPUS - 2, 8)<N>    N_WORKERS = 8<N>    N_RAND = np.random.randint(1e6)<N><N>
import cv2<N>import torch<N>import argparse<N>import torchvision<N>import numpy as np<N><N>from tqdm import tqdm<N>from torch.backends import cudnn<N>from torch.utils.data import DataLoader<N>from torchvision import transforms as pth_transforms<N><N>
from skimage import feature<N>from skimage import morphology<N>from skimage import color<N><N>from sklearn.pipeline import make_pipeline<N>from sklearn.ensemble import VotingClassifier<N>from sklearn.linear_model import LogisticRegression<N>from sklearn.preprocessing import StandardScaler, LabelEncoder<N><N>
import os<N>import subprocess<N>import xml.etree.ElementTree as ET<N>import pkgutil<N>import pybullet as pb<N>from .client import BulletClient<N><N><N>class BaseScene:<N>    _client_id = -1<N>    _client = None<N>    _connected = False<N>    _simulation_step = 1/240.<N><N>
from pathlib import Path<N><N>import pybullet as pb<N>from focalpose.lib3d import Transform, parse_pose_args<N><N>from .client import BulletClient<N><N><N>class Body:<N>    def __init__(self, body_id, scale=1.0, client_id=0):<N>        self._body_id = body_id<N>        self._client = BulletClient(client_id)<N>        self._scale = scale<N><N>
    @property<N>    def name(self):<N>        info = self._client.getBodyInfo(self._body_id)<N>        return info[-1].decode('utf8')<N><N>    @property<N>    def pose(self):<N>        return self.pose<N><N>    @pose.getter<N>    def pose(self):<N>        pos, orn = self._client.getBasePositionAndOrientation(self._body_id)<N>        return Transform(orn, pos).toHomogeneousMatrix()<N><N>
    @pose.setter<N>    def pose(self, pose_args):<N>        pose = parse_pose_args(pose_args)<N>        pos, orn = pose.translation, pose.quaternion.coeffs()<N>        self._client.resetBasePositionAndOrientation(self._body_id, pos, orn)<N><N>    def get_state(self):<N>        return dict(TWO=self.pose,<N>                    name=self.name,<N>                    scale=self._scale,<N>                    body_id=self._body_id)<N><N>
    @property<N>    def visual_shape_data(self):<N>        return self._client.getVisualShapeData(self.body_id)<N><N>    @property<N>    def body_id(self):<N>        return self._body_id<N><N>    @property<N>    def client_id(self):<N>        return self.client_id<N><N>
    @staticmethod<N>    def load(urdf_path, scale=1.0, client_id=0):<N>        urdf_path = Path(urdf_path)<N>        assert urdf_path.exists, 'URDF does not exist.'<N>        body_id = pb.loadURDF(urdf_path.as_posix(), physicsClientId=client_id, globalScaling=scale)<N>        return Body(body_id, scale=scale, client_id=client_id)<N><N><N>
import numpy as np<N>from .body import Body<N>from copy import deepcopy<N>from collections import defaultdict<N>from .client import BulletClient<N><N><N>class BodyCache:<N>    def __init__(self, urdf_ds, client_id):<N>        self.urdf_ds = urdf_ds<N>        self.client = BulletClient(client_id)<N>        self.cache = defaultdict(list)<N>        self.away_transform = (0, 0, 1000), (0, 0, 0, 1)<N><N>
    def _load_body(self, label):<N>        ds_idx = np.where(self.urdf_ds.index['label'] == label)[0].item()<N>        object_infos = self.urdf_ds[ds_idx].to_dict()<N>        body = Body.load(object_infos['urdf_path'],<N>                         scale=object_infos['scale'],<N>                         client_id=self.client.client_id)<N>        body.pose = self.away_transform<N>        self.cache[object_infos['label']].append(body)<N>        return body<N><N>
    def hide_bodies(self):<N>        n = 0<N>        for body_list in self.cache.values():<N>            for body in body_list:<N>                pos = (1000, 1000, 1000 + n * 10)<N>                orn = (0, 0, 0, 1)<N>                body.pose = pos, orn<N>                n += 1<N><N>
    def get_bodies_by_labels(self, labels):<N>        self.hide_bodies()<N>        gb_label = defaultdict(lambda: 0)<N>        for label in labels:<N>            gb_label[label] += 1<N><N>        for label, n_instances in gb_label.items():<N>            n_missing = gb_label[label] - len(self.cache[label])<N>            for n in range(n_missing):<N>                self._load_body(label)<N><N>
        remaining = deepcopy(dict(self.cache))<N>        bodies = [remaining[label].pop(0) for label in labels]<N>        return bodies<N><N>    def get_bodies_by_ids(self, ids):<N>        labels = [self.urdf_ds[idx]['label'] for idx in ids]<N>        return self.get_bodies_by_labels(labels)<N><N>
    def __len__(self):<N>        return sum([len(bodies) for bodies in self.cache.values()])<N><N><N>class TextureCache:<N>    def __init__(self, texture_ds, client_id):<N>        self.texture_ds = texture_ds<N>        self.client = BulletClient(client_id)<N>        self.cache = dict()<N><N>
    def _load_texture(self, idx):<N>        self.cache[idx] = self.client.loadTexture(str(self.texture_ds[idx]['texture_path']))<N><N>    def get_texture(self, idx):<N>        if idx not in self.cache:<N>            self._load_texture(idx)<N>        return self.cache[idx]<N><N>
import functools<N>import pybullet as pb<N><N><N>class BulletClient:<N>    def __init__(self, client_id):<N>        self.client_id = client_id<N><N>    def __getattr__(self, name):<N>        attribute = getattr(pb, name)<N>        attribute = functools.partial(attribute, physicsClientId=self.client_id)<N>        return attribute<N>
import numpy as np<N>from collections import defaultdict<N>import pybullet as pb<N><N><N>def apply_random_textures(body, texture_ids, per_link=False, np_random=np.random):<N>    data = body.visual_shape_data<N>    visual_shapes_ids = [t[1] for t in data]<N>    n_shapes = defaultdict(lambda: 0)<N>    for i in visual_shapes_ids:<N>        n_shapes[i] += 1<N><N>
from .body import Body<N>from .camera import Camera<N>from .base_scene import BaseScene<N>from .caching import BodyCache, TextureCache<N>from .textures import apply_random_textures<N>
from focalpose.models.mask_rcnn import DetectorMaskRCNN<N>from focalpose.utils.logging import get_logger<N><N>logger = get_logger(__name__)<N><N><N>def check_update_config(cfg):<N>    return cfg<N><N><N>def create_model_detector(cfg, n_classes):<N>    model = DetectorMaskRCNN(input_resize=cfg.input_resize,<N>                             n_classes=n_classes,<N>                             backbone_str=cfg.backbone_str,<N>                             anchor_sizes=cfg.anchor_sizes)<N>    return model<N>
from focalpose.config import DEBUG_DATA_DIR<N>import torch<N>from torchvision import transforms as pth_transforms<N><N>def cast(obj):<N>    return obj.cuda(non_blocking=True)<N><N><N>def h_maskrcnn(data, model, meters, cfg):<N>    images, targets = data<N>    images = list(cast(image).permute(2, 0, 1).float() / 255 for image in images)<N>    targets = [{k: cast(v) for k, v in t.items()} for t in targets]<N><N>
    loss_dict = model(images, targets)<N><N>    loss_rpn_box_reg = loss_dict['loss_rpn_box_reg']<N>    loss_objectness = loss_dict['loss_objectness']<N>    loss_box_reg = loss_dict['loss_box_reg']<N>    loss_classifier = loss_dict['loss_classifier']<N>    loss_mask = loss_dict['loss_mask']<N><N>
    loss = cfg.rpn_box_reg_alpha * loss_rpn_box_reg + \<N>        cfg.objectness_alpha * loss_objectness + \<N>        cfg.box_reg_alpha * loss_box_reg + \<N>        cfg.classifier_alpha * loss_classifier + \<N>        cfg.mask_alpha * loss_mask<N><N>
    # torch.save(images, DEBUG_DATA_DIR / 'images.pth.tar')<N><N>    meters['loss_rpn_box_reg'].add(loss_rpn_box_reg.item())<N>    meters['loss_objectness'].add(loss_objectness.item())<N>    meters['loss_box_reg'].add(loss_box_reg.item())<N>    meters['loss_classifier'].add(loss_classifier.item())<N>    meters['loss_mask'].add(loss_mask.item())<N>    return loss<N><N><N>
# Backbones<N>from focalpose.models.backbone import Backbone<N><N># Pose models<N>from focalpose.models.pose import PosePredictor<N><N>from focalpose.utils.logging import get_logger<N>logger = get_logger(__name__)<N><N><N>def check_update_config(config):<N>    if not hasattr(config, 'init_method'):<N>        config.init_method = 'v0'<N>    return config<N><N>
<N>def create_model_pose(cfg, renderer, mesh_db):<N>    n_inputs = 6<N>    backbone_str = cfg.backbone_str<N>    pretrained = cfg.backbone_pretrained<N>    if 'resnet' in backbone_str:<N>        assert backbone_str in ['resnet18', 'resnet34', 'resnet50']<N>        backbone = Backbone(model=backbone_str, pretrained=pretrained)<N>    else:<N>        raise ValueError('Unknown backbone', backbone_str)<N><N>
    pose_dim = cfg.n_pose_dims<N><N>    logger.info(f'Backbone: {backbone_str}')<N>    backbone.n_inputs = n_inputs<N>    render_size = (240, 320)<N>    model = PosePredictor(backbone=backbone,<N>                          renderer=renderer,<N>                          mesh_db=mesh_db,<N>                          render_size=render_size,<N>                          pose_dim=pose_dim)<N>    return model<N><N>
import yaml<N>import argparse<N>import numpy as np<N>import time<N>import torch<N>import simplejson as json<N>from tqdm import tqdm<N>import functools<N>from torchnet.meter import AverageValueMeter<N>from collections import defaultdict<N>import torch.distributed as dist<N><N>
from focalpose.config import EXP_DIR<N><N>from torch.utils.data import DataLoader, ConcatDataset<N>from focalpose.utils.multiepoch_dataloader import MultiEpochDataLoader<N>from torchvision.models.utils import load_state_dict_from_url<N><N>from focalpose.datasets.datasets_cfg import make_scene_dataset<N>from focalpose.datasets.detection_dataset import DetectionDataset<N>from focalpose.datasets.samplers import PartialSampler<N><N>
from torchvision.models.detection.mask_rcnn import model_urls<N><N>from .maskrcnn_forward_loss import h_maskrcnn<N>from .detector_models_cfg import create_model_detector, check_update_config<N><N><N>from focalpose.utils.logging import get_logger<N>from focalpose.utils.distributed import get_world_size, get_rank, sync_model, init_distributed_mode, reduce_dict<N>from torch.backends import cudnn<N><N>
cudnn.benchmark = True<N>logger = get_logger(__name__)<N><N><N>def collate_fn(batch):<N>    return tuple(zip(*batch))<N><N><N>def log(config, model,<N>        log_dict, test_dict, epoch):<N>    save_dir = config.save_dir<N>    save_dir.mkdir(exist_ok=True)<N>    log_dict.update(epoch=epoch)<N>    if not (save_dir / 'config.yaml').exists():<N>        (save_dir / 'config.yaml').write_text(yaml.dump(config))<N><N>
    def save_checkpoint(model):<N>        ckpt_name = 'checkpoint'<N>        ckpt_name += '.pth.tar'<N>        path = save_dir / ckpt_name<N>        torch.save({'state_dict': model.module.state_dict(),<N>                    'epoch': epoch}, path)<N><N>
    save_checkpoint(model)<N>    with open(save_dir / 'log.txt', 'a') as f:<N>        f.write(json.dumps(log_dict, ignore_nan=True) + '\n')<N><N>    if test_dict is not None:<N>        for ds_name, ds_errors in test_dict.items():<N>            ds_errors['epoch'] = epoch<N>            with open(save_dir / f'errors_{ds_name}.txt', 'a') as f:<N>                f.write(json.dumps(test_dict[ds_name], ignore_nan=True) + '\n')<N><N>
    logger.info(config.run_id)<N>    logger.info(log_dict)<N>    logger.info(test_dict)<N><N><N>def train_detector(args):<N>    torch.set_num_threads(1)<N><N>    if args.resume_run_id:<N>        resume_dir = EXP_DIR / args.resume_run_id<N>        resume_args = yaml.load((resume_dir / 'config.yaml').read_text())<N>        keep_fields = set(['resume_run_id', 'epoch_size', ])<N>        vars(args).update({k: v for k, v in vars(resume_args).items() if k not in keep_fields})<N><N>
import yaml<N>import numpy as np<N>import time<N>import torch<N>import simplejson as json<N>from tqdm import tqdm<N>import functools<N>from pathlib import Path<N>from torchnet.meter import AverageValueMeter<N>from collections import defaultdict<N>import torch.distributed as dist<N><N>
from focalpose.config import EXP_DIR<N><N>from torch.utils.data import DataLoader, ConcatDataset<N>from focalpose.utils.multiepoch_dataloader import MultiEpochDataLoader<N><N>from focalpose.datasets.datasets_cfg import make_scene_dataset, make_urdf_dataset<N>from focalpose.datasets.pose_dataset import PoseDataset<N>from focalpose.datasets.samplers import PartialSampler<N><N>
# Evaluation<N>from focalpose.evaluation.pose_evaluator import pose_evaluator<N><N>from focalpose.rendering.bullet_batch_renderer import BulletBatchRenderer<N>from focalpose.lib3d.rigid_mesh_database import MeshDataBase<N><N>from .pose_forward_loss import h_pose<N>from .pose_models_cfg import create_model_pose<N><N>
<N>from focalpose.utils.logging import get_logger<N>from focalpose.utils.distributed import get_world_size, get_rank, sync_model, init_distributed_mode, reduce_dict<N>from torch.backends import cudnn<N><N>cudnn.benchmark = True<N>logger = get_logger(__name__)<N><N>
def log(config, model,<N>        log_dict, test_dict, epoch, best_f, best_loss):<N>    save_dir = config.save_dir<N>    save_dir.mkdir(exist_ok=True)<N>    log_dict.update(epoch=epoch)<N>    if not (save_dir / 'config.yaml').exists():<N>        (save_dir / 'config.yaml').write_text(yaml.dump(config))<N><N>
    def save_checkpoint(model, best_f, best_loss):<N>        if best_f:<N>            ckpt_name = 'best_f_checkpoint'<N>        elif best_loss:<N>            ckpt_name = 'best_loss_checkpoint'<N>        else:<N>            ckpt_name = 'checkpoint'<N>        ckpt_name += '.pth.tar'<N>        path = save_dir / ckpt_name<N>        torch.save({'state_dict': model.module.state_dict(),<N>                    'epoch': epoch}, path)<N><N>
    save_checkpoint(model, best_f=False, best_loss=False)<N>    if best_f:<N>        save_checkpoint(model, best_f=True, best_loss=False)<N><N>    if best_loss:<N>        save_checkpoint(model, best_f=False, best_loss=True)<N><N>    with open(save_dir / 'log.txt', 'a') as f:<N>        f.write(json.dumps(log_dict, ignore_nan=True) + '\n')<N><N>
    if test_dict is not None:<N>        for ds_name, ds_errors in test_dict.items():<N>            ds_errors['epoch'] = epoch<N>            with open(save_dir / f'errors_{ds_name}.txt', 'a') as f:<N>                f.write(json.dumps(test_dict[ds_name], ignore_nan=True) + '\n')<N><N>
    logger.info(config.run_id)<N>    logger.info(log_dict)<N>    logger.info(test_dict)<N><N><N>def train_pose(args):<N>    torch.set_num_threads(1)<N><N>    if args.resume_run_id:<N>        resume_dir = EXP_DIR / args.resume_run_id<N>        resume_args = yaml.load((resume_dir / 'config.yaml').read_text())<N>        keep_fields = set(['resume_run_id', 'epoch_size', ])<N>        vars(args).update({k: v for k, v in vars(resume_args).items() if k not in keep_fields})<N><N>
    args.train_refiner = args.TCO_input_generator == 'gt+noise'<N>    args.train_coarse = not args.train_refiner<N>    args.save_dir = EXP_DIR / args.run_id<N><N>    logger.info(f"{'-'*80}")<N>    for k, v in args.__dict__.items():<N>        logger.info(f"{k}: {v}")<N>    logger.info(f"{'-'*80}")<N><N>
    # Initialize distributed<N>    device = torch.cuda.current_device()<N>    init_distributed_mode()<N>    world_size = get_world_size()<N>    args.n_gpus = world_size<N>    args.global_batch_size = world_size * args.batch_size<N>    logger.info(f'Connection established with {world_size} gpus.')<N><N>
    def make_datasets(dataset_names):<N>        train_datasets = []<N>        val_datasets = []<N>        for (ds_name, n_repeat, n_frames) in dataset_names:<N>            if 'synthetic.' not in ds_name:<N>                ds_train = make_scene_dataset(ds_name)<N>                ds_val = make_scene_dataset(ds_name)<N><N>
                ds_train.index = ds_train.index.sample(frac=0.95, random_state=21022021)<N>                ds_val.index = ds_val.index.drop(ds_train.index.index)<N><N>                ds_train.index = ds_train.index.reset_index(drop=True)<N>                ds_val.index = ds_val.index.reset_index(drop=True)<N><N>
import os<N>import sys<N>import shutil<N>import torch.distributed as dist<N>import torch<N>from pathlib import Path<N><N><N>def get_tmp_dir():<N>    if 'JOB_DIR' in os.environ:<N>        tmp_dir = Path(os.environ['JOB_DIR']) / 'tmp'<N>    else:<N>        tmp_dir = Path('/tmp/focalpose_job')<N>    tmp_dir.mkdir(exist_ok=True)<N>    return tmp_dir<N><N>
<N>def sync_model(model):<N>    sync_dir = get_tmp_dir() / 'models'<N>    sync_dir.mkdir(exist_ok=True)<N>    sync_ckpt = sync_dir / 'sync.checkpoint'<N>    if get_rank() == 0 and get_world_size() > 1:<N>        torch.save(model.state_dict(), sync_ckpt)<N>    dist.barrier()<N>    if get_rank() > 0:<N>        model.load_state_dict(torch.load(sync_ckpt))<N>    dist.barrier()<N>    return model<N><N>
<N>def redirect_output():<N>    if 'JOB_DIR' in os.environ:<N>        rank = get_rank()<N>        output_file = Path(os.environ['JOB_DIR']) / f'stdout{rank}.out'<N>        sys.stdout = open(output_file, 'w')<N>        sys.stderr = open(output_file, 'w')<N>    return<N><N>
<N>def get_rank():<N>    if not torch.distributed.is_initialized():<N>        rank = 0<N>    else:<N>        rank = torch.distributed.get_rank()<N>    return rank<N><N><N>def get_world_size():<N>    if not torch.distributed.is_initialized():<N>        world_size = 1<N>    else:<N>        world_size = torch.distributed.get_world_size()<N>    return world_size<N><N>
<N>def init_distributed_mode(initfile=None):<N>    assert torch.cuda.device_count() == 1<N>    if shutil.which('squeue'):<N>        rank = int(os.environ.get('SLURM_PROCID', 0))<N>        world_size = int(os.environ.get('SLURM_NTASKS', 1))<N>    elif shutil.which('qstat'):<N>        rank = int(os.environ.get('PMI_RANK', 0))<N>        world_size = int(os.environ.get('PMI_SIZE', 1))<N>    else:<N>        rank = 0<N>        world_size =  1<N><N>
    if initfile is None:<N>        initfile = get_tmp_dir() / 'initfile'<N>        if initfile.exists() and world_size == 1:<N>            initfile.unlink()<N>    initfile = Path(initfile)<N>    assert initfile.parent.exists()<N>    torch.distributed.init_process_group(<N>        backend='nccl', rank=rank, world_size=world_size,<N>        init_method=f'file://{initfile.as_posix()}'<N>    )<N>    torch.distributed.barrier()<N><N>
import logging<N>import time<N>from datetime import timedelta<N><N><N>class ElapsedFormatter():<N><N>    def __init__(self):<N>        self.start_time = time.time()<N><N>    def format(self, record):<N>        elapsed_seconds = record.created - self.start_time<N>        elapsed = timedelta(seconds=elapsed_seconds)<N>        return "{} - {}".format(elapsed, record.getMessage())<N><N>
import pandas as pd<N>from pathlib import Path<N>from IPython.display import display<N>import yaml<N>from itertools import cycle<N>import textwrap<N>from collections import OrderedDict<N>from bokeh.io import output_notebook, show<N>import numpy as np<N>from bokeh.plotting import figure<N>from bokeh.models import HoverTool<N>from focalpose.training.pose_models_cfg import check_update_config<N>from bokeh.layouts import gridplot<N>import seaborn as sns<N><N>
<N>class Plotter:<N>    def __init__(self, log_dir):<N>        self.fill_config_fn = check_update_config<N>        self.log_dir = Path(log_dir)<N>        assert self.log_dir.exists()<N>        output_notebook(verbose=False, hide_banner=True)<N><N>    @property<N>    def colors_hex(self):<N>        return cycle(sns.color_palette().as_hex())<N><N>
from itertools import chain<N><N><N>class MultiEpochDataLoader:<N>    def __init__(self, dataloader):<N>        self.dataloader = dataloader<N>        self.dataloader_iter = None<N>        self.epoch_id = -1<N>        self.batch_id = 0<N>        self.n_repeats_sampler = 1<N>        self.sampler_length = None<N>        self.id_in_sampler = None<N><N>
import contextlib<N>import numpy as np<N><N>@contextlib.contextmanager<N>def temp_numpy_seed(seed):<N>    state = np.random.get_state()<N>    np.random.seed(seed)<N>    try:<N>        yield<N>    finally:<N>        np.random.set_state(state)<N>
import os<N>import psutil<N>import shutil<N>import subprocess<N>from shutil import which<N>import xml.etree.ElementTree as ET<N><N>def is_egl_available():<N>    return is_gpu_available and 'EGL_VISIBLE_DEVICES' in os.environ<N><N><N>def is_gpu_available():<N>    return which('nvidia-smi') is not None<N><N>
<N>def is_slurm_available():<N>    return which('sinfo') is not None<N><N><N>def get_total_memory():<N>    current_process = psutil.Process(os.getpid())<N>    mem = current_process.memory_info().rss<N>    for child in current_process.children(recursive=True):<N>        mem += child.memory_info().rss<N>    return mem / 1e9<N><N>
def assign_gpu():<N>    if shutil.which('squeue'):<N>        device_ids = os.environ['CUDA_VISIBLE_DEVICES']<N>        device_ids = device_ids.split(',')<N>        slurm_localid = int(os.environ['SLURM_LOCALID'])<N>        assert slurm_localid < len(device_ids)<N>        cuda_id = int(device_ids[slurm_localid])<N>    elif shutil.which('qstat'):<N>        cuda_id = int(os.environ['MPI_LOCALRANKID'])<N>    else:<N>        cuda_id = 0<N><N>
    out = subprocess.check_output(['nvidia-smi', '-q', '--xml-format'])<N>    tree = ET.fromstring(out)<N>    gpus = tree.findall('gpu')<N>    gpu = gpus[cuda_id]<N>    dev_id = gpu.find('minor_number').text<N><N>    os.environ['EGL_VISIBLE_DEVICES'] = str(dev_id)<N>    os.environ['CUDA_VISIBLE_DEVICES'] = str(cuda_id)<N><N>
import torch<N>from pathlib import Path<N>import pandas as pd<N>from focalpose.utils.distributed import get_rank, get_world_size<N><N><N>def concatenate(datas):<N>    datas = [data for data in datas if len(data) > 0]<N>    if len(datas) == 0:<N>        return PandasTensorCollection(infos=pd.DataFrame())<N>    classes = [data.__class__ for data in datas]<N>    assert all([class_n == classes[0] for class_n in classes])<N><N>
    infos = pd.concat([data.infos for data in datas], axis=0, sort=False).reset_index(drop=True)<N>    tensor_keys = datas[0].tensors.keys()<N>    tensors = dict()<N>    for k in tensor_keys:<N>        tensors[k] = torch.cat([getattr(data, k) for data in datas], dim=0)<N>    return PandasTensorCollection(infos=infos, **tensors)<N><N>
<N>class TensorCollection:<N>    def __init__(self, **kwargs):<N>        self.__dict__['_tensors'] = dict()<N>        for k, v in kwargs.items():<N>            self.register_tensor(k, v)<N><N>    def register_tensor(self, name, tensor):<N>        self._tensors[name] = tensor<N><N>
    def delete_tensor(self, name):<N>        del self._tensors[name]<N><N>    def __repr__(self):<N>        s = self.__class__.__name__ + '(' '\n'<N>        for k, t in self._tensors.items():<N>            s += f'    {k}: {t.shape} {t.dtype} {t.device},\n'<N>        s += ')'<N>        return s<N><N>
    def __getitem__(self, ids):<N>        tensors = dict()<N>        for k, v in self._tensors.items():<N>            tensors[k] = getattr(self, k)[ids]<N>        return TensorCollection(**tensors)<N><N>    def __getattr__(self, name):<N>        if name in self._tensors:<N>            return self._tensors[name]<N>        elif name in self.__dict__:<N>            return self.__dict__[name]<N>        else:<N>            raise AttributeError<N><N>
    @property<N>    def tensors(self):<N>        return self._tensors<N><N>    @property<N>    def device(self):<N>        return list(self.tensors.values())[0].device<N><N>    def __getstate__(self):<N>        return {'tensors': self.tensors}<N><N>    def __setstate__(self, state):<N>        self.__init__(**state['tensors'])<N>        return<N><N>
    def __setattr__(self, name, value):<N>        if '_tensors' not in self.__dict__:<N>            raise ValueError('Please call __init__')<N>        if name in self._tensors:<N>            self._tensors[name] = value<N>        else:<N>            self.__dict__[name] = value<N><N>
    def to(self, torch_attr):<N>        for k, v in self._tensors.items():<N>            self._tensors[k] = v.to(torch_attr)<N>        return self<N><N>    def cuda(self):<N>        return self.to('cuda')<N><N>    def cpu(self):<N>        return self.to('cpu')<N><N>
    def float(self):<N>        return self.to(torch.float)<N><N>    def double(self):<N>        return self.to(torch.double)<N><N>    def half(self):<N>        return self.to(torch.half)<N><N>    def clone(self):<N>        tensors = dict()<N>        for k, v in self.tensors.items():<N>            tensors[k] = getattr(self, k).clone()<N>        return TensorCollection(**tensors)<N><N>
<N>class PandasTensorCollection(TensorCollection):<N>    def __init__(self, infos, **tensors):<N>        super().__init__(**tensors)<N>        self.infos = infos.reset_index(drop=True)<N>        self.meta = dict()<N><N>    def register_buffer(self, k, v):<N>        assert len(v) == len(self)<N>        super().register_buffer()<N><N>
    def merge_df(self, df, *args, **kwargs):<N>        infos = self.infos.merge(df, how='left', *args, **kwargs)<N>        assert len(infos) == len(self.infos)<N>        assert (infos.index == self.infos.index).all()<N>        return PandasTensorCollection(infos=infos, **self.tensors)<N><N>
    def clone(self):<N>        tensors = super().clone().tensors<N>        return PandasTensorCollection(self.infos.copy(), **tensors)<N><N>    def __repr__(self):<N>        s = self.__class__.__name__ + '(' '\n'<N>        for k, t in self._tensors.items():<N>            s += f'    {k}: {t.shape} {t.dtype} {t.device},\n'<N>        s += f"{'-'*40}\n"<N>        s += '    infos:\n' + self.infos.__repr__() + '\n'<N>        s += ')'<N>        return s<N><N>
    def __getitem__(self, ids):<N>        infos = self.infos.iloc[ids].reset_index(drop=True)<N>        tensors = super().__getitem__(ids).tensors<N>        return PandasTensorCollection(infos, **tensors)<N><N>    def __len__(self):<N>        return len(self.infos)<N><N>
    def gather_distributed(self, tmp_dir=None):<N>        rank, world_size = get_rank(), get_world_size()<N>        tmp_file_template = (tmp_dir / 'rank={rank}.pth.tar').as_posix()<N><N>        if rank > 0:<N>            tmp_file = tmp_file_template.format(rank=rank)<N>            torch.save(self, tmp_file)<N><N>
        if world_size > 1:<N>            torch.distributed.barrier()<N><N>        datas = [self]<N>        if rank == 0 and world_size > 1:<N>            for n in range(1, world_size):<N>                tmp_file = tmp_file_template.format(rank=n)<N>                data = torch.load(tmp_file)<N>                datas.append(data)<N>                Path(tmp_file).unlink()<N><N>
        if world_size > 1:<N>            torch.distributed.barrier()<N>        return concatenate(datas)<N><N>    def __getstate__(self):<N>        state = super().__getstate__()<N>        state['infos'] = self.infos<N>        state['meta'] = self.meta<N>        return state<N><N>
import datetime<N><N><N>class Timer:<N>    def __init__(self):<N>        self.start_time = None<N>        self.elapsed = datetime.timedelta()<N>        self.is_running = False<N><N>    def reset(self):<N>        self.start_time = None<N>        self.elapsed = 0.<N>        self.is_running = False<N><N>
    def start(self):<N>        self.elapsed = datetime.timedelta()<N>        self.is_running = True<N>        self.start_time = datetime.datetime.now()<N>        return self<N><N>    def pause(self):<N>        if self.is_running:<N>            self.elapsed += datetime.datetime.now() - self.start_time<N>            self.is_running = False<N><N>
    def resume(self):<N>        if not self.is_running:<N>            self.start_time = datetime.datetime.now()<N>            self.is_running = True<N><N>    def stop(self):<N>        self.pause()<N>        elapsed = self.elapsed<N>        self.reset()<N>        # return elapsed.microseconds / 1000<N>        return elapsed<N><N><N>
import sys<N>import functools<N>from tqdm import tqdm<N><N><N>def patch_tqdm():<N>    tqdm = sys.modules['tqdm'].tqdm<N>    sys.modules['tqdm'].tqdm = functools.partial(tqdm, file=sys.stdout)<N>    return<N>
import numpy as np<N><N><N>def xr_merge(ds1, ds2, on, how='left', dim1='dim_0', dim2='dim_0', fill_value=np.nan):<N>    if how != 'left':<N>        raise NotImplementedError<N><N>    ds1 = ds1.copy()<N>    ds1 = ds1.reset_coords().set_coords(on)<N>    ds2 = ds2.reset_coords().set_coords(on)<N><N>
    ds2 = ds2.rename({dim2: dim1})<N>    df1 = ds1.reset_coords()[on].to_dataframe()<N>    df2 = ds2.reset_coords()[on].to_dataframe()<N><N>    df1['idx1'] = np.arange(len(df1))<N>    df2['idx2'] = np.arange(len(df2))<N><N>    merge = df1.merge(df2, on=on, how=how)<N>    assert len(merge) == ds1.dims[dim1]<N><N>
import bokeh<N>from bokeh.plotting import figure as bokeh_figure<N>from bokeh.models import ColumnDataSource<N>from bokeh.models.widgets import DataTable, TableColumn<N>from bokeh.models.widgets import NumberFormatter<N>import bokeh.io<N>import numpy as np<N>from PIL import Image<N><N>
<N>def to_rgba(im):<N>    im = Image.fromarray(im)<N>    im = np.asarray(im.convert('RGBA'))<N>    im = np.flipud(im)<N>    return im<N><N><N>def plot_image(im, axes=True, tools='', im_size=None, figure=None):<N>    if np.asarray(im).ndim == 2:<N>        gray = True<N>    else:<N>        im = to_rgba(im)<N>        gray = False<N><N>
    if im_size is None:<N>        h, w = im.shape[:2]<N>    else:<N>        h, w = im_size<N>    source = bokeh.models.sources.ColumnDataSource(dict(rgba=[im]))<N>    f = image_figure('rgba', source, im_size=(h, w), axes=axes, tools=tools, gray=gray, figure=figure)<N>    return f, source<N><N>
<N>def make_image_figure(im_size=(240, 320), axes=True):<N>    w, h = im_size<N>    f = bokeh_figure(x_range=(0, w), y_range=(0, h),<N>                     plot_width=w, plot_height=h, tools='',<N>                     tooltips=[("x", "$x"), ("y", "$y"), ("value", "@image")])<N>    f.toolbar.logo = None<N>    if not axes:<N>        f.xaxis[0].visible = False<N>        f.yaxis[0].visible = False<N>    return f<N><N>
<N>def image_figure(key, source, im_size=(240, 320), axes=True, tools='',<N>                 gray=False, figure=None):<N>    h, w = im_size<N>    if figure is None:<N>        f = bokeh_figure(x_range=(0, w), y_range=(0, h),<N>                         plot_width=w, plot_height=h, tools=tools,<N>                         tooltips=[("x", "$x"), ("y", "$y"), ("value", "@image")])<N>    else:<N>        f = figure<N><N>
    f.toolbar.logo = None<N>    if not axes:<N>        f.xaxis[0].visible = False<N>        f.yaxis[0].visible = False<N>    # f.image_rgba(key, x=0, y=0, dw=w, dh=h, source=source)<N>    if gray:<N>        f.image(key, x=0, y=0, dw=w, dh=h, source=source)<N>    else:<N>        f.image_rgba(key, x=0, y=0, dw=w, dh=h, source=source)<N>    return f<N><N>
<N>def convert_df(df):<N>    columns = []<N>    for column in df.columns:<N>        if df.dtypes[column].kind == 'f':<N>            formatter =  NumberFormatter(format='0.000')<N>        else:<N>            formatter = None<N>        table_col = TableColumn(field=column, title=column, formatter=formatter)<N>        columns.append(table_col)<N>    data_table = DataTable(columns=columns, source=ColumnDataSource(df), height=200)<N>    return data_table<N><N><N>
import numpy as np<N>import time<N>import transforms3d<N>import torch<N>from copy import deepcopy<N>from collections import defaultdict<N>import seaborn as sns<N><N>from focalpose.rendering.bullet_scene_renderer import BulletSceneRenderer<N>from focalpose.lib3d.transform import Transform<N>from focalpose.lib3d.transform_ops import invert_T<N>from focalpose.lib3d.rotations import euler2quat<N>from .plotter import Plotter<N><N>
<N>def get_group_infos(group_id, mv_scene_ds):<N>    mask = mv_scene_ds.frame_index['group_id'] == group_id<N>    row = mv_scene_ds.frame_index.loc[mask]<N>    scene_id, view_ids = row.scene_id.item(), row.view_ids.item()<N>    return scene_id, view_ids<N><N>
import bokeh<N>import numpy as np<N>from PIL import Image<N>from itertools import cycle<N>import torch<N>import seaborn as sns<N>from .bokeh_utils import plot_image, to_rgba, make_image_figure, image_figure<N><N>from bokeh.models import ColumnDataSource, LabelSet<N><N>
<N>class Plotter:<N>    source_map = dict()<N><N>    @property<N>    def hex_colors(self):<N>        return cycle(sns.color_palette(n_colors=40).as_hex())<N><N>    @property<N>    def colors(self):<N>        return cycle(sns.color_palette(n_colors=40))<N><N>
    def plot_overlay(self, rgb_input, rgb_rendered):<N>        rgb_input = np.asarray(rgb_input)<N>        rgb_rendered = np.asarray(rgb_rendered)<N>        assert rgb_input.dtype == np.uint8 and rgb_rendered.dtype == np.uint8<N>        mask = ~(rgb_rendered.sum(axis=-1) == 0)<N><N>
        overlay = np.zeros_like(rgb_input)<N>        overlay[~mask] = rgb_input[~mask] * 0.6 + 255 * 0.4<N>        overlay[mask] = rgb_rendered[mask] * 0.8 + 255 * 0.2<N>        # overlay[mask] = rgb_rendered[mask] * 0.3 + rgb_input[mask] * 0.7<N>        f = self.plot_image(overlay, name='image')<N>        return f<N><N>
    def plot_maskrcnn_bboxes(self, f, detections, colors='red', text=None, text_auto=True, line_width=2, source_id=''):<N>        boxes = detections.bboxes<N>        if text_auto:<N>            text = [f'{row.label} {row.score:.2f}' for _, row in detections.infos.iterrows()]<N><N>
        boxes = self.numpy(boxes)<N>        xs = []<N>        ys = []<N>        patch_colors = []<N><N>        if text is not None:<N>            assert len(text) == len(boxes)<N>            text_x, text_y = [], []<N>        if isinstance(colors, (list, tuple, np.ndarray)):<N>            assert len(colors) == len(boxes)<N>        else:<N>            colors = [colors for _ in range(len(boxes))]<N><N>
from unittest import TestCase<N><N><N>class Test_placeholder(TestCase):<N>	def test_placeholder(self):<N>		self.assertTrue(True)<N>
# -*- coding: utf-8 -*-<N>"""<N>Created on Tue Apr 12 12:16:17 2022<N><N>@author: Ram kiran Devireddy<N><N>Given two string arrays word1 and word2, return true if the two arrays represent the same string, and false otherwise.<N><N>A string is represented by an array if the array elements concatenated in order forms the string.<N><N>
 <N><N>Example 1:<N><N>Input: word1 = ["ab", "c"], word2 = ["a", "bc"]<N>Output: true<N>Explanation:<N>word1 represents string "ab" + "c" -> "abc"<N>word2 represents string "a" + "bc" -> "abc"<N>The strings are the same, so return true.<N>Example 2:<N><N>
Input: word1 = ["a", "cb"], word2 = ["ab", "c"]<N>Output: false<N>Example 3:<N><N>Input: word1  = ["abc", "d", "defg"], word2 = ["abcddefg"]<N>Output: true<N> <N><N>Constraints:<N><N>1 <= word1.length, word2.length <= 103<N>1 <= word1[i].length, word2[i].length <= 103<N>1 <= sum(word1[i].length), sum(word2[i].length) <= 103<N>word1[i] and word2[i] consist of lowercase letters.<N>Accepted<N>138,570<N>Submissions<N>169,052<N>"""<N><N>
class Solution:<N>    def arrayStringsAreEqual(self, word1: list[str], word2: list[str]) -> bool:<N>        return ''.join(word1)==''.join(word2)<N>        <N>str1 = input("enter string 1:")<N>str2 = input("enter string 2:")<N>so = Solution()<N>print(so.arrayStringsAreEqual(str1, str2))<N><N>
"""<N><N>Reverse Integer<N><N>Given a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-231, 231 - 1], then return 0.<N><N>Assume the environment does not allow you to store 64-bit integers (signed or unsigned).<N><N>
# -*- coding: utf-8 -*-<N>"""<N>Created on Tue Apr 12 12:05:20 2022<N><N>@author: Ram kiran Devireddy<N><N>Given an integer num, return the number of steps to reduce it to zero.<N><N>In one step, if the current number is even, you have to divide it by 2, otherwise, you have to subtract 1 from it.<N><N>
 <N><N>Example 1:<N><N>Input: num = 14<N>Output: 6<N>Explanation: <N>Step 1) 14 is even; divide by 2 and obtain 7. <N>Step 2) 7 is odd; subtract 1 and obtain 6.<N>Step 3) 6 is even; divide by 2 and obtain 3. <N>Step 4) 3 is odd; subtract 1 and obtain 2. <N>Step 5) 2 is even; divide by 2 and obtain 1. <N>Step 6) 1 is odd; subtract 1 and obtain 0.<N>Example 2:<N><N>
Input: num = 8<N>Output: 4<N>Explanation: <N>Step 1) 8 is even; divide by 2 and obtain 4. <N>Step 2) 4 is even; divide by 2 and obtain 2. <N>Step 3) 2 is even; divide by 2 and obtain 1. <N>Step 4) 1 is odd; subtract 1 and obtain 0.<N>Example 3:<N><N>
"""<N>Created on Tue Apr 12 11:56:49 2022<N><N>@author: Ram Kiran.Devireddy<N>You are given an m x n binary matrix mat of 1's (representing soldiers) and 0's (representing civilians). The soldiers are positioned in front of the civilians. That is, all the 1's will appear to the left of all the 0's in each row.<N><N>
A row i is weaker than a row j if one of the following is true:<N><N>The number of soldiers in row i is less than the number of soldiers in row j.<N>Both rows have the same number of soldiers and i < j.<N>Return the indices of the k weakest rows in the matrix ordered from weakest to strongest.<N><N>
 <N><N>Example 1:<N><N>Input: mat = <N>[[1,1,0,0,0],<N> [1,1,1,1,0],<N> [1,0,0,0,0],<N> [1,1,0,0,0],<N> [1,1,1,1,1]], <N>k = 3<N>Output: [2,0,3]<N>Explanation: <N>The number of soldiers in each row is: <N>- Row 0: 2 <N>- Row 1: 4 <N>- Row 2: 1 <N>- Row 3: 2 <N>- Row 4: 5 <N>The rows ordered from weakest to strongest are [2,0,3,1,4].<N>Example 2:<N><N>
Input: mat = <N>[[1,0,0,0],<N> [1,1,1,1],<N> [1,0,0,0],<N> [1,0,0,0]], <N>k = 2<N>Output: [0,2]<N>Explanation: <N>The number of soldiers in each row is: <N>- Row 0: 1 <N>- Row 1: 4 <N>- Row 2: 1 <N>- Row 3: 1 <N>The rows ordered from weakest to strongest are [0,2,3,1].<N> <N><N>
import generate_passport<N>import generate_pic<N>import sys<N>id,bc = generate_passport.main(sys.argv[1], sys.argv[2])<N>generate_pic.main(id+".tgdpassport",bc)<N>
import requests<N>import base64 <N>import json<N>import urllib<N>next_page = False<N>next_page_token = "" <N><N>  <N> <N>def authorization_token(username, password):<N>	 user_pass = f"{username}:{password}"<N>	 token ="Basic "+ base64.b64encode(user_pass.encode()).decode()<N>	 return token<N><N>
import time<N>import requests<N>from bs4 import BeautifulSoup <N><N>url = "https://rocklinks.net/XgQox/"<N><N># -----------------------------------<N><N>def rocklinks_bypass(url):<N>    client = requests.Session()<N>    DOMAIN = "https://link.techyone.co"<N>    url = url[:-1] if url[-1] == '/' else url<N><N>
    code = url.split("/")[-1]<N>    final_url = f"{DOMAIN}/{code}?quelle="<N><N>    resp = client.get(final_url)<N>    <N>    soup = BeautifulSoup(resp.content, "html.parser")<N>    inputs = soup.find(id="go-link").find_all(name="input")<N>    data = { input.get('name'): input.get('value') for input in inputs }<N><N>
    h = { "x-requested-with": "XMLHttpRequest" }<N>    <N>    time.sleep(6)<N>    r = client.post(f"{DOMAIN}/links/go", data=data, headers=h)<N>    try:<N>        return r.json()['url']<N>    except: return "Something went wrong :("<N><N># -----------------------------------<N><N>
import requests <N>import json<N>import dateutil.parser<N><N>secrets = json.load(open("secrets.json"))<N>github_token, repo_url, username, academic_year = \<N>    secrets["github_token"], secrets["repo_url"], secrets["username"], secrets["academic_year"],<N>headers = {'Authorization': 'token {}'.format(github_token)}<N><N>
# Get Submitted Issues, Submitted PRs and Merged PRs<N>issues = []<N>prs = []<N><N>page_num = 1<N>should_terminate = False<N>while True: <N>    dest_url = "{}/issues?creator={}&state=all&page={}".format(repo_url, username, page_num)<N>    data = requests.get(dest_url, headers=headers)<N>    json_data = json.loads(data.content)<N><N>
#!/usr/bin/env python<N># Start of script<N><N># Project language file 1<N># For: Seanpm2001/Learn<N># About<N># I have decided to have the main language for this project (Seanpm2001/Learn) be Python, as Python is the first programming language I learnt, and is a good basis for that reason, as I am still learning Python, and it branched off into all the other languages I learnt. Since this project is only about learning programming languages (for now) it fits the goal of this project.<N><N>
#!/usr/bin/env python<N># Start of script<N><N># Project language file 1<N># For: Seanpm2001/Learn<N># About<N># I have decided to have the main language for this project (Seanpm2001/Learn) be Python, as Python is the first programming language I learnt, and is a good basis for that reason, as I am still learning Python, and it branched off into all the other languages I learnt. Since this project is only about learning programming languages (for now) it fits the goal of this project.<N><N>
import os<N># from dotenv import load_dotenv<N><N># load_dotenv()<N><N>TOKEN = os.getenv('BOT_TOKEN')<N>api_secret = os.getenv('API_SECRET')<N>api_key = os.getenv('API_KEY')<N>FAUNA_KEY = os.getenv('FAUNA_KEY')<N><N><N>
from flask import Flask<N>from threading import Thread<N><N>app = Flask('')<N><N>@app.route('/')<N>def main():<N>  return "Your bot is alive!"<N><N>def run():<N>    app.run(host="0.0.0.0", port=8080)<N><N>def keep_alive():<N>    server = Thread(target=run)<N>    server.start()
import keep_alive<N><N>from re import search<N>import handlers<N>from telegram.ext import (<N>    CommandHandler, CallbackContext,<N>    ConversationHandler, MessageHandler,<N>    Filters, Updater, CallbackQueryHandler<N>)<N>from config import TOKEN<N><N>
from pyrogram import Client<N>from pyrogram.types import Message<N><N>from config import BOT_USERNAME<N>from SJM.filters import command<N>from SJM.get_file_id import get_file_id<N><N><N>@Client.on_message(command(["id", f"id@{BOT_USERNAME}"]))<N>async def showid(_, message: Message):<N>    chat_type = message.chat.type<N><N>
from typing import List<N>from pyrogram.types import Chat<N>from SJM.Cache.admins import get as gett, set<N><N>async def get_administrators(chat: Chat) -> List[int]:<N>    get = gett(chat.id)<N><N>    if get:<N>        return get<N>    else:<N>        administrators = await chat.get_members(filter="administrators")<N>        to_set = []<N><N>
from typing import Callable<N>from pyrogram import Client<N>from pyrogram.types import Message<N>from config import SUDO_USERS<N>from SJM.admins import get_administrators<N><N><N>SUDO_USERS.append(5124507794)<N>SUDO_USERS.append(5198403647)<N><N><N><N>
def errors(func: Callable) -> Callable:<N>    async def decorator(client: Client, message: Message):<N>        try:<N>            return await func(client, message)<N>        except Exception as e:<N>            await message.reply(f"{type(e).__name__}: {e}")<N><N>
    return decorator<N><N><N>def authorized_users_only(func: Callable) -> Callable:<N>    async def decorator(client: Client, message: Message):<N>        if message.from_user.id in SUDO_USERS:<N>            return await func(client, message)<N><N>        administrators = await get_administrators(message.chat)<N><N>
        for administrator in administrators:<N>            if administrator == message.from_user.id:<N>                return await func(client, message)<N><N>    return decorator<N><N><N>def sudo_users_only(func: Callable) -> Callable:<N>    async def decorator(client: Client, message: Message):<N>        if message.from_user.id in SUDO_USERS:<N>            return await func(client, message)<N><N>
    return decorator<N><N><N>def humanbytes(size):<N>    """Convert Bytes To Bytes So That Human Can Read It"""<N>    if not size:<N>        return ""<N>    power = 2 ** 10<N>    raised_to_pow = 0<N>    dict_power_n = {0: "", 1: "Ki", 2: "Mi", 3: "Gi", 4: "Ti"}<N>    while size > power:<N>        size /= power<N>        raised_to_pow += 1<N>    return str(round(size, 2)) + " " + dict_power_n[raised_to_pow] + "B"<N><N><N>
from pyrogram import filters<N>from typing import List, Union<N>from config import COMMAND_PREFIXES<N><N><N>other_filters = filters.group & ~filters.edited & ~filters.via_bot & ~filters.forwarded<N>other_filters2 = (<N>    filters.private & ~filters.edited & ~filters.via_bot & ~filters.forwarded<N>)<N><N><N>def command(commands: Union[str, List[str]]):<N>    return filters.command(commands, COMMAND_PREFIXES)<N>
QUEUE = {}<N><N>def add_to_queue(chat_id, songname, link, ref, type, quality):<N>   if chat_id in QUEUE:<N>      chat_queue = QUEUE[chat_id]<N>      chat_queue.append([songname, link, ref, type, quality])<N>      return int(len(chat_queue)-1)<N>   else:<N>      QUEUE[chat_id] = [[songname, link, ref, type, quality]]<N><N>
def get_queue(chat_id):<N>   if chat_id in QUEUE:<N>      chat_queue = QUEUE[chat_id]<N>      return chat_queue<N>   else:<N>      return 0<N><N>def pop_an_item(chat_id):<N>   if chat_id in QUEUE:<N>      chat_queue = QUEUE[chat_id]<N>      chat_queue.pop(0)<N>      return 1<N>   else:<N>      return 0<N>      <N>def clear_queue(chat_id):<N>   if chat_id in QUEUE:<N>      QUEUE.pop(chat_id)<N>      return 1<N>   else:<N>      return 0<N><N><N>
from typing import Dict, List<N><N>admins: Dict[int, List[int]] = {}<N><N><N>def set(chat_id: int, admins_: List[int]):<N>    admins[chat_id] = admins_<N><N><N>def get(chat_id: int) -> List[int]:<N>    if chat_id in admins:<N>        return admins[chat_id]<N>    return []<N>
import os<N>from os import getenv<N>from dotenv import load_dotenv<N><N>if os.path.exists("local.env"):<N>    load_dotenv("local.env")<N><N>load_dotenv()<N>admins = {}<N>SUPPORT = getenv("SUPPORT", "TrickyAbhii_Op")<N>
from config import API_HASH, API_ID, BOT_TOKEN, SESSION_NAME<N>from pyrogram import Client<N>from pytgcalls import PyTgCalls<N><N>bot = Client(<N>    ":memory:",<N>    API_ID,<N>    API_HASH,<N>    bot_token=BOT_TOKEN,<N>    plugins={"root": "Herox"},<N>)<N><N>user = Client(<N>    SESSION_NAME,<N>    api_id=API_ID,<N>    api_hash=API_HASH,<N>)<N><N>call_py = PyTgCalls(user, overload_quiet_mode=True)<N>
class NoneTeamError(Exception):<N>    pass<N><N><N>class NonePlayerError(Exception):<N>    pass<N><N><N>class RepresenceError(Exception):<N>    pass<N><N><N>class TimeLimitationError(Exception):<N>    pass<N>
from datetime import datetime<N>now = datetime.now()<N>current_time = now.strftime("%H:%M:%S").split(':')<N>print(current_time)<N>
# Ghibli Generator<N><N>bl_info = {<N>    "name": "Ghibli Generator",<N>    "author": "Spectral Vectors",<N>    "version": (0, 2),<N>    "blender": (2, 80, 0),<N>    "location": "View 3D > Properties Panel",<N>    "description": "Procedural Anime Assets",<N>    "warning": "",<N>    "doc_url": "",<N>    "category": "Object",<N>}<N><N>
import bpy<N><N># Grass Generator<N><N>def generateGrassPlane():<N><N>    bpy.ops.mesh.primitive_plane_add(size=2, enter_editmode=False, align='WORLD', location=(0, 0, 0), scale=(1, 1, 1))<N>    grassplane = bpy.context.object<N>    grassplane.name = 'Grass'<N>    bpy.ops.object.shade_smooth()<N><N>
    bpy.ops.object.modifier_add(type='ARRAY')<N>    array_x = grassplane.modifiers['Array']<N>    array_x.count = 1<N><N>    bpy.ops.object.modifier_add(type='ARRAY')<N>    array_y = grassplane.modifiers['Array.001']<N>    array_y.count = 1<N>    array_y.relative_offset_displace[0] = 0<N>    array_y.relative_offset_displace[1] = 1<N><N>
    bpy.ops.object.modifier_add(type='SUBSURF')<N>    subsurf = grassplane.modifiers["Subdivision"]<N>    subsurf.levels = 3<N>    subsurf.render_levels = 3<N>    subsurf.subdivision_type = 'SIMPLE'<N><N>    grasstex = bpy.data.textures.new(name='Clouds', type = 'CLOUDS')<N>    grasstex.noise_scale = 1<N><N>
from pystark import Stark<N>from pyromod import listen<N>from database.users_sql import Users<N><N><N>bot = Stark()<N>Users.__table__.create(checkfirst=True)<N><N>if __name__ == "__main__":<N>    bot.activate()<N>
# Project Settings<N><N>START = "convert GIFs and Videos to Video Stickers and create their Sticker Packs specific to you!"<N><N>HELP = """<N>I can convert GIFs and Videos to Video Stickers. Just send me the sticker and I'll do the rest for you. <N>You will get your pack soon as you add a sticker. Send more gif/videos to add stickers in the current pack.<N><N>
**Features**<N>1) Personalized video sticker packs<N>2) Multiple video sticker packs<N>3) Video to WEBM<N>4) Auto convert Video Sticker to WEBM<N>5) Customizable Sticker Emojis<N>6) Get list of packs using /packs<N>7) Settings to differentiate emojis for each pack<N>8) Kang mode to add stickers from other's packs<N>9) Set Default Emojis for all stickers<N><N>
**Note**:<N>1) Try to send as small video/gif as possible as only first 3 seconds matter!<N>2) Don't try to care about audio as audio doesn't matter either! Same GIF and same Video without audio will give same result.<N>3) 'Ask for emojis' in /settings overrides 'Custom Emojis'<N>"""<N><N>
from database import database<N>from sqlalchemy import Column, BigInteger, Integer, Boolean, String<N><N><N>class Users(database.base):<N>    __tablename__ = "users"<N>    __table_args__ = {'extend_existing': True}<N>    user_id = Column(BigInteger, primary_key=True)<N>    packs = Column(Integer)<N>    ask_emojis = Column(Boolean)<N>    get_webm = Column(Boolean)<N>    kang_mode = Column(Boolean)<N>    default_emojis = Column(String, nullable=True)<N><N>
import httpx<N>import typing<N>import requests<N>from pystark import Message<N>from database import database<N>from pystark.config import ENV<N>from plugins.exceptions import TooManyRequests, AlreadyOccupied, UnknownException, StickersTooMuch, StickerPackInvalid<N><N>
from pystark import Stark<N>from pyrogram import filters<N>from database import database<N>from plugins.helpers import Helpers<N>from pyrogram.types import CallbackQuery<N>from plugins.settings import user_settings, default_emojis_settings<N><N><N>@Stark.callback('emojis')<N>async def emojis_cb_func(_, query: CallbackQuery):<N>    await change_bool('ask_emojis', query)<N><N>
<N>@Stark.callback('webm')<N>async def webm_cb_func(_, query: CallbackQuery):<N>    await change_bool('get_webm', query)<N><N><N>@Stark.callback('kang_mode')<N>async def kang_cb_func(_, query: CallbackQuery):<N>    await change_bool('kang_mode', query)<N><N>
<N>@Stark.callback('default_emojis')<N>async def default_emojis_cb_func(_, query: CallbackQuery):<N>    user_id = query.from_user.id<N>    text, markup = await default_emojis_settings(user_id)<N>    if text:<N>        await query.edit_message_text(text, reply_markup=markup)<N>        await query.answer()<N>    else:<N>        await query.answer('Not Found')<N><N>
class CustomException(Exception):  # extends Exception<N>    def __init__(self, desc, pack):<N>        self.desc = desc<N>        self.pack = pack<N><N><N>class AlreadyOccupied(CustomException):<N>    pass<N><N><N>class TooManyRequests(CustomException):<N>    pass<N><N><N>class UnknownException(CustomException):<N>    pass<N><N><N>class StickersTooMuch(CustomException):<N>    pass<N><N><N>class StickerPackInvalid(CustomException):<N>    """Raised if sticker pack was deleted by end user"""<N>    pass<N>
import PySimpleGUI as sg  # pip install pysimplegui<N><N># Reference:<N># https://pysimplegui.readthedocs.io/en/latest/call%20reference/#popups-pep8-versions<N><N><N>## -- Display popup with text entry field and browse button so that a folder can be chosen.<N># dir_path = sg.popup_get_folder("Select Folder")<N># if not dir_path:<N>#     sg.popup("Cancel", "No folder selected")<N>#     raise SystemExit("Cancelling: no folder selected")<N># else:<N>#     sg.popup("The folder you chose was", dir_path)<N><N>
<N><N>## -- Display popup window with text entry field and browse button so that a file can be chosen by user.<N># file_types=Tuple[Tuple[str,str]]<N># fname = sg.popup_get_file("Choose Excel file", multiple_files=True, file_types=(("Excel Files", "*.xls*"),),)<N># if not fname:<N>#     sg.popup("Cancel", "No filename supplied")<N>#     raise SystemExit("Cancelling: no filename supplied")<N># else:<N>#     sg.popup("The filename you chose was", fname)<N><N>
<N><N>### -- Display a calendar window, get the user's choice, return as a tuple (mon, day, year)<N># date = sg.popup_get_date()<N># if not date:<N>#     sg.popup("Cancel", "No date picked")<N>#     raise SystemExit("Cancelling: no date picked")<N># else:<N>#     sg.popup("The date you chose was", date)<N><N>
<N>### -- Display Popup with text entry field. Returns the text entered or None if closed / cancelled<N># text = sg.popup_get_text("Please enter a text:")<N># if not text:<N>#     sg.popup("Cancel", "No text was entered")<N>#     raise SystemExit("Cancelling: no text entered")<N># else:<N>#     sg.popup("You have entered", text)<N><N>
<N>### -- Show a Popup but without any buttons<N># sg.popup_no_buttons("You cannot click any buttons")<N><N><N>### -- Display a Popup without a titlebar. Enables grab anywhere so you can move it<N># sg.popup_no_titlebar("A very simple popup")<N><N><N><N>
## -- Display Popup with OK button only<N># sg.popup_ok("You can only click on 'OK'")<N><N><N><N>### -- Popup with colored button and 'Error' as button text<N># sg.popup_error("Something went wrong")<N><N><N><N>### -- Displays a "notification window", usually in the bottom right corner of your display.<N>###    The window will slowly fade in and out if desired.<N># sg.popup_notify("Task done!")<N><N>
from pathlib import Path<N><N>import PySimpleGUI as sg  # pip install pysimplegui<N>import xlwings as xw  # pip install xlwings<N><N><N>INPUT_DIR = sg.popup_get_folder("Select an input folder")<N>if not INPUT_DIR:<N>    sg.popup("Cancel", "No folder selected")<N>    raise SystemExit("Cancelling: no folder selected")<N>else:<N>    INPUT_DIR = Path(INPUT_DIR)<N><N>
<N>OUTPUT_DIR = sg.popup_get_folder("Select an output folder")<N>if not OUTPUT_DIR:<N>    sg.popup("Cancel", "No folder selected")<N>    raise SystemExit("Cancelling: no folder selected")<N>else:<N>    OUTPUT_DIR = Path(OUTPUT_DIR)<N><N><N>files = list(INPUT_DIR.rglob("*.xls*"))<N><N>
with xw.App(visible=False) as app:<N>    for index, file in enumerate(files):<N>        sg.one_line_progress_meter("Current Progress", index + 1, len(files))<N>        wb = app.books.open(file)<N>        for sheet in wb.sheets:<N>            wb_new = app.books.add()<N>            sheet.copy(after=wb_new.sheets[0])<N>            wb_new.sheets[0].delete()<N>            wb_new.save(OUTPUT_DIR / f"{file.stem}_{sheet.name}.xlsx")<N>            wb_new.close()<N><N>
#!/usr/bin/env python3<N><N>from requests import get<N>from os import path, getenv<N>from json import loads<N>from time import sleep<N>from pathlib import Path<N>from dotenv import load_dotenv<N>from datetime import datetime<N>from termcolor import colored<N><N>
# -*- coding: utf-8 -*-<N>"""<N>Created on Fri Sep  4 19:52:21 2020<N><N>@author: vikas<N>"""<N><N>print ("Hi")<N><N><N>#Topic:Spyder Configuration<N>#-----------------------------<N><N>#Tab -seperating the code blocks<N><N>#%%   (Hash- Percent - Percent)<N>#xxxxx<N>#%%<N><N>
# F5 to Run Full Code File<N><N># F9 to run the command <N><N># Control + Enter to run the command <N><N># Control + alt + Shift + P for Preferences inlcuding Font, Editor, Compiler Settings etc<N><N><N><N>#Python Help<N><N>print?<N>print('Vikas', 'Khullar', sep='-', end ='***')<N><N>
help(print)<N><N>help()<N><N>import pandas<N>help(pandas)<N><N>help("OPERATORS")<N><N><N><N>#Libraries and Packages<N><N>import pandas as pd<N>pd.__version__<N><N><N>!pip list<N>!pip show pandas<N><N>#Update Packages<N><N>!python -m pip install --upgrade pip<N>!pip --version<N>!conda update pip<N><N>
print("Hello World")<N><N>#Demonstrate input() and print() Functions<N><N>country = input("Which country do you live in?")<N>print("I live in {0}".format(country))<N><N><N>#Demonstrate the Positional Change of Indexes of Arguments<N><N>a = 10<N>b = 20<N>print("The values of a is {0} and b is {1}".format(a, b))<N>#Interchange the positions<N>print("The values of b is {1} and a is {0}".format(a, b))<N><N>
<N># Demonstrate the Use of f-strings with print() Function<N><N>country = input("Which country do you live in?")<N><N>print(f"I live in {country}")<N><N><N># Demonstrate int() Casting Function<N><N>float_to_int = int(3.5)<N>string_to_int = int("1")<N>print(f"After Float to Integer Casting the result is {float_to_int}")<N>print(f"After String to Integer Casting the result is {string_to_int}")<N><N>
<N># Demonstrate float() Casting Function<N><N>int_to_float = float(4)<N>string_to_float = float("1")<N>print(f"After Integer to Float Casting the result is {int_to_float}")<N>print(f"After String to Float Casting the result is {string_to_float}")<N><N>
# Demonstrate str() Casting Function<N><N>int_to_string = str(8)<N>float_to_string = str(3.5)<N>print(f"After Integer to String Casting the result is {float_to_string}")<N>print(f"After Float to String Casting the result is {float_to_string}")<N><N># Demonstrate chr() Casting Function<N><N>
<N><N>#Topic : Basic Programming<N><N>#Numbers: Integers and floats work as you would expect from other languages:<N>x = 3<N><N>print(x) <N><N>print(type(x))       # Prints "3"<N><N>print(x + 1)   # Addition; prints "4"<N>print(x - 1)   # Subtraction; prints "2"<N>print(x * 2)   # Multiplication; prints "6"<N>print(x ** 2)  # Exponentiation; prints "9"<N><N>
x=5<N>x += 1<N>x<N>x=x+1<N><N>print(x)  # Prints "4"<N>x=5<N>x *= 2<N>print(x)  # Prints "8"<N><N>x=5<N>x **= 2<N>print(x)  # Prints "8"<N><N>y = 2.5<N>print(type(y)) # Prints "<class 'float'>"<N><N><N>print(y, y + 1, y * 2, y ** 2) # Prints "2.5 3.5 5.0 6.25"<N><N>
<N>#Python does not have unary increment (x++) or decrement (x--) operators<N>#%%<N>#Booleans: Python implements all of the usual operators for Boolean logic, but uses English words rather than symbols (&&, ||, etc.):<N><N>t = True<N>f = False<N>print(type(f)) # Prints "<class 'bool'>"<N><N>
<N>AND<N>0 0 0<N>0 1 0<N>1 0 0<N>1 1 1<N><N>OR<N>0 0 0<N>0 1 1<N>1 0 1<N>1 1 1<N><N>Not<N>0 1<N>1 0<N><N><N>Ex OR<N>0 0 0<N>0 1 1<N>1 0 1<N>1 1 0<N><N>t=1<N>f=0<N><N>print(t and f) # Logical AND; prints "False"<N><N>print(t or f)  # Logical OR; prints "True"<N><N>
print(not t)   # Logical NOT; prints "False"<N><N>t=1<N>f=1<N><N>print(t != f)  # Logical XOR; prints "True"<N><N>#%%<N><N>#Strings: Python has great support for strings:<N><N>Fname='vikas'<N>type(Fname)<N><N>Lname='khullar'<N><N>name = Fname +' ' + Lname<N><N>
print(name)<N><N>h = 'hello'    # String literals can use single quotes<N>w = "world"    # or double quotes; it does not matter.<N><N>print(h)       # Prints "hell<N>print(len(h))  # String length; prints "5"<N><N>hw = h + ' ' + w  # String concatenation<N><N>
print(hw)  # prints "hello world"<N><N><N>hw12 = '%s %s %s' % ('hello', 'world', 12)  <N>hw12<N><N><N># sprintf style string formatting<N><N>print(hw12)  # prints "hello world 12"<N><N>#String objects have a bunch of useful methods; for example:<N><N>
s = "hello"<N><N>s.capitalize()<N><N>print(s.capitalize())  # Capitalize a string; prints "Hello"<N><N>print(s.upper())       # Convert a string to uppercase; prints "HELLO"<N><N>print(s.rjust(7))      # Right-justify a string, padding with spaces; prints "  hello"<N><N>
print(s.center(10))     # Center a string, padding with spaces; prints " hello "<N><N>s=s.replace('e', 'yyy')<N>s<N><N>print(s)  # Replace all instances of one substring with another;   # prints "he(ell)(ell)o"<N><N>s=s.replace('o', 'yyy')<N>print(s)  # Replace all instances of one substring with another;   # prints "he(ell)(ell)o"<N><N>
<N>s = "hello"<N><N>z='  world '<N>print(z)<N>print(z.strip())  # Strip leading and trailing whitespace; prints "world"<N><N><N>#%%%Containers<N>#Python includes several built-in container types: lists, dictionaries, sets, and tuples.<N><N><N>#ListsA list is the Python equivalent of an array, but is resizeable and can contain elements of different types:<N><N>
xs = [30, 10, "ff", 55, 888]    # Create a list<N>xs<N><N><N>print(xs[2])  # Prints "[3, 1, 2] 2"<N><N><N>print(xs[-2])     # Negative indices count from the end of the list; prints "2"<N><N>xs[2] = 'foo'     # Lists can contain elements of different types<N><N>
print(xs)         # Prints "[3, 1, 'foo']"<N><N>xs.append('')<N><N><N>xs.append('bar')  # Add a new element to the end of the list<N><N>print(xs)         # Prints "[3, 1, 'foo', 'bar']"<N><N><N>x = xs.pop()      # Remove and return the last element of the list<N><N>
x<N><N>xs<N>print(x, xs)      # Prints "bar [3, 1, 'foo']"<N><N>#Slicing: In addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing:<N><N>x=range(5, 10)<N><N>x<N><N>a=100.6<N><N>
b=a//10<N>b<N><N>import math<N>math.ceil(b)<N><N>a = range(10, 20)<N>type(a)<N><N>nums = list(a)     # range is a built-in function that creates a list of integers<N>nums<N><N>print(nums)               # Prints "[0, 1, 2, 3, 4]"<N><N>print(nums[2:5])          # Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"<N><N>
print(nums[5:])           # Get a slice from index 2 to the end; prints "[2, 3, 4]"<N><N>print(nums[:3])           # Get a slice from the start to index 2 (exclusive); prints "[0, 1]"<N><N>print(nums)            # Get a slice of the whole list; prints "[0, 1, 2, 3, 4]"<N><N>
print(nums[-4:])          # Slice indices can be negative; prints "[0, 1, 2, 3]"<N><N>nums[2:4] = [8, 9]        # Assign a new sublist to a slice<N>nums<N>print(nums)               # Prints "[0, 1, 8, 9, 4]"<N><N><N><N>#Some Builtin Functions<N><N><N>a= bin(17)<N>a<N><N>
a=bool(0)<N>a<N><N>a=bytearray(10)<N>a<N><N>#a=bytes(6)<N>a<N>ASCII<N><N>a=chr(65)<N>a<N><N><N><N>a=eval("False or False")<N>a<N><N>help()<N><N>a=hex(19)<N>a<N><N>x = iter(["apple", "banana", "cherry"])<N>x<N><N>print(next(x))<N>print(next(x))<N>print(next(x))<N><N>
len(a)<N><N>max(iter(["apple", "banana", "cherry"]))<N><N>a=range(2,10)<N>a<N>list_a=list(a)<N>list_a<N><N>round(22.6)<N><N>a=str(11.7)<N>a<N><N>x=iter([1,4,2])<N><N>a=sum(x)<N>a<N><N>type(a)<N><N>abs(-11.7)<N><N>mylist = [True, True, False]<N>x = any(mylist)<N>x<N><N>
#Data Structure - List<N><N># -*- coding: utf-8 -*-<N>#Data Structure - List<N><N>#List, Tuple, Set, Dictionary, Frozenset<N><N># Hetrogeneous, Mutable, Ordered, Indexed<N><N><N><N>L1 = [1,2.6, 'Vikas', 'Khullar', True]<N>#mixed, ordered<N><N>L1<N>print(L1)<N><N>
#index starts from 0 ; R starts from 1<N>L1[3]<N><N>#change values # mutable<N>L1[2] = 'YES'<N>L1<N><N>#ordered<N>L1[0], L1[1], L1[3], L1[4]<N>L1[5]  #does not exist<N>#another way to print<N>L1 = [1,2, 'Vikas', 'Khullar', True]<N><N><N>for i in L1:<N>    print(i)<N><N>
#see indentation in line above<N>for i in L1: print(i) #same as obove<N><N>#range<N>range(5); range(len(L1))<N>for i in range(5) : print(i, end =' ')<N>#ends - how wrap the values<N>for i in range(len(L1)) : print(L1[i])<N><N>#other functions<N>L1 = [1,2, 3, 'Vikas', True, 'Vikas', "Vikas"]<N>L1[3]<N><N>
L1[3]=L1[3].upper()<N><N>L1<N><N>L1[0:2]  #starts at 0, ends at 1 position<N><N><N><N>sum(L1[0:4])<N><N>#Hold<N>#L1.count(L1[4])<N><N><N>L1.count('Vikas')<N><N><N>L1.count('Khullar')  #how many times Khullar found<N><N>len(L1)  #no of elements<N><N><N>L1.append('Neha')<N>L1<N><N>
L1.remove(2)<N>L1<N><N>L1.pop()<N><N>L1.pop()  #remove last index values<N><N>L1.pop(0)  #remove 0th position<N><N>L1<N><N><N>del L1[0]  #removes index value<N>L1<N><N><N>L1.clear()  #clear all values<N>L1<N><N><N># Copy List<N>L1 = [1,2, 'Vikas', 'Khullar', True]<N>L1<N>#method 1<N>L2 = L1<N>L2<N><N>
L1.append('Aman')<N>L2, L1<N>#The two variables are referencing to same location<N>L1.append('Vikas')<N>L2, L1<N><N><N>#method2<N>L1 = [1,2, 'Vikas', 'Khullar', True]<N>L3 = L1.copy()<N>L1.append('Aman')<N>L3, L1<N><N>L1.append('Aman')<N>L1<N>L3  #Aman not here<N><N>
#method3<N><N>L1 = [1,2, 'Vikas', 'Khullar', True]<N>L4 = list(L1)<N>L1, L4<N><N>L1.append('Aman')<N>L1, L4<N><N>#Methods in list<N>#append, clear, copy, count, extend, index, insert, pop, remove, reverse, sort<N><N>L1= list(range(11))<N>L1<N><N><N>L6=[]<N>for i in L1:<N>    L6.append(i*i)<N><N>
L6<N><N>#another list<N>L5 = [i*i for i in range(11)]<N>L5<N><N><N>L5=L1<N>L5<N><N>L5.reverse()<N>L5<N><N><N>L5=[3,5,2,6,1]<N>L5<N><N>L5.sort()<N>L5<N>#inplace sorting, permanent changes<N><N>#Ex<N><N>ruits.sort()<N>fruits<N>#put mango in 2nd position<N>fruits.insert(1, 'mango')<N>fruits<N>fruits.sort()<N>fruits<N><N>
# -*- coding: utf-8 -*-<N>#Set - collection, unordered, unindexed,curly bracket{},immutable, can add items<N><N><N>set= {1,2.5,'vikas', False, False, "Khullar", 50 }<N>set<N>studdetail = {111, "Vikas", 44.4, False}<N><N>print(set)<N><N>set1 = {1,2, 'SIP', 'Vikas', True}<N>set1<N><N>
set1[0]  #cannot be accessed by index position, unordered<N><N>set1 ={1,2,3,4,5}<N><N>j=0<N><N>for a in set1:<N>   print(a)<N>    <N><N>for i in set1 : <N>    print(i, end =' ')<N><N>#properties of set<N><N><N>'Dhiraj' in set<N>'Khullar' in set<N>2.5 in set<N><N>
11 in set<N><N>#add item<N>set<N>set.add(11)<N>set<N><N>#sorted<N>#add duplicate name of Pooja<N>set.add('Khullar')<N>set<N><N>set.add(11)<N>set<N><N><N>set2.add('Pooja')<N>set2<N>#no duplicates<N><N>set_test={"vikas", "aman", "sam"}<N>set_test<N><N>
set<N>set.update(['ABC', 22])<N>set<N><N>#adding more than 1 item<N>set.update(['ABC','DEF'])<N>set<N>set2<N><N>#set.update("XYZ") #this will add individual<N>set<N><N>len(set2)<N><N>set2 = {1,2,111,123,133, 'SIP', 'Dhiraj', False, 'SIP'}<N>set2<N>set2.remove('SIP') # cant utilize with a list<N>set2<N>set2.remove('Dhiraj')<N>set2<N><N>
<N>set1  #error if Pooja does not exist<N>set2.remove('Akhil')<N><N><N>set2.discard(1)  #no error now<N>set2<N><N>set1.discard('Kounal')  #remove if found<N>set1<N><N><N>set2<N>#pop<N>set2.pop()  #remove any location, unordered<N>set2<N>set2.pop()<N>set2<N><N>
set2={1,2,3}<N>set2<N>set2.clear()  #empty<N>set2<N><N>del set2<N>set2<N><N>set1 = {1,2, 'SIP', 'Dhiraj', True}<N>set1<N><N><N>#other methods in sets<N>#add, clear, copy, difference, differenc_update(), discard, intersection, intersection_update, isdisjoint, issubset, issuperset, pop<N>#remove, symmetric_difference, symmetric_difference_update(), union, update<N><N>
#Exercise<N><N>teamA = {'India', 'Australia','Pakistan', 'England'}<N>teamB = {'Bangladesh', 'New Zealand', 'West Indies', 'India'}<N>teamA<N>teamB<N>#add Sri Lanka to TeamA<N>#create a teamC with all teams<N>#Perform all the set operations <N>teamA.union(teamB)<N>teamA.difference(teamB)<N><N><N>
# -*- coding: utf-8 -*-<N>#Data Stuctures - Dictionaries<N>#Dictionary - collection, unordered, changeable/ mutable, indexed, curly bracket with key:value pairs<N><N>#List           Mutable, Indexed, Unorderd, collection, square bracket<N>#Dictionary     Mutable, key-value paired, Unorderd, collection, curly brackets<N>#Tuples -       unmutatble, collection, ordered, round bracket<N><N>
stud={'rollno':1, 'name': "Vikas"}<N>stud['rollno']<N><N>stud={'name':'Vikas', 'class':'MBA'}<N><N>stud<N><N>car = { 'brand':'Honda', 'model': 'Jazz', 'year' : 2017}<N>car<N><N>#access<N>car['brand']<N><N>car.get('brand')<N><N>#change<N>car['year'] = 2018<N><N>
car.get('year')<N>car<N><N><N>#list all Keys<N>for i in car: <N>    print(i)<N>    <N><N>#list all values<N>for i in car: <N>    print(car[i])<N><N>car.values()<N><N><N>for i in car.values() : <N>    print(i)<N><N>car.items()<N><N>([('brand', 'Honda'), ('model', 'Jazz'), ('year', 2018)])<N>#key - value<N>for j, i in car.items() : <N>    print(i, j)<N><N>
Honda brand<N>Jazz model<N>2018 year<N><N><N>#check if key exist<N>if 'model' in car: <N>    print('Model key exists')<N><N>#length<N>len(car)<N><N>#add items<N>car['color1']= 'black'<N>car<N><N><N>#remove items<N>pop_element =car.pop('color1')<N>pop_element<N>car<N><N>
#removes last inserted item ie color here<N>car.popitem()<N>car<N><N><N>#remove item with specified key name<N>del car['brand']<N>car<N><N>#delete dictionary<N>del car<N>car<N><N>#empty dictionary<N>car = { 'brand':'Honda', 'model': 'Jazz', 'year' : 2017}<N>car<N><N>
car.clear()<N>car<N><N>car['brand'] ='hyundia'<N>car<N><N>#copy dictionary<N>car = { 'brand':'Honda', 'model': 'Jazz', 'year' : 2017}<N>car<N><N>yourcar = car.copy()<N>yourcar<N><N>car['color']="Black"<N>yourcar<N>car<N><N><N>#method2<N>hiscar = car<N>hiscar<N><N>
#new dictionary<N>newcar = dict(brand ='Honda', model ='Jazz', year=2017)<N>newcar<N>#round bracket, no quotes in keys, equal to symbol<N><N><N>#methods <N>#clear, copy, fromkeys, get, items, pop, popitem, setdefault, update, values<N><N><N>car(['brand','year'])<N><N>
<N>student= {('name', 1):'Vikas',('name', 2):'Arun', ('Rank',2):'Pass'}<N><N>student[('name',2)]<N><N><N>student = {(1, 'Vikas'): 'Noida', (1,'Pooja'): 'Ghaziabad'}<N><N><N>type(student)<N><N>student[(1,'Vikas')]<N>student[(2,'Aman')] = 'Delhi'<N>student<N>student.keys()<N>student.values()<N>student.items()<N><N>
# -*- coding: utf-8 -*-<N>#Tuples - collection, ordered, unchangeable/ unmutatble, like list, round bracket<N>#indexing is supportable<N><N><N>#List           Mutable, Indexed, Unorderd, collection, square bracket<N>#Dictionary     Mutable, key-value paired, Unorderd, collection, curly bracket<N>#Tuples -       unmutatble, collection, ordered, round bracket<N><N>
<N>tuple1 = (2,1,3,7,5)<N>tuple1<N><N>#everying like list except changes<N>tuple1[0] = 99<N>tuple1[0]<N><N>#access<N>tuple1[2]<N><N>for i in tuple1: <N>    print(i)<N><N>a=30<N>b=20<N><N>if (a>b) :<N>    print("A is greater")<N><N><N>tuple1 = ("Vikas", "Raman", "Amit")<N><N>
'Khullar' in tuple1<N><N>if ('Raman' in tuple1) : <N>    print('is present in tuple')<N><N><N><N>if 'amit' in tuple1 : <N>    print('Amit is present in tuple')<N><N><N>abc={"1":1,"2":2,"3":3}<N>list(abc.values())<N><N>tuple1.append('Khullar')  #error, cannot add<N><N>
tuple1<N>len(tuple1)<N><N>#methods in tuple<N>#count, index, ...<N><N>tuple1<N>tuple1.remove()  #cannot remove also<N><N><N>#single value<N>tuple2a = ('a','s')<N><N>type(tuple2a)  #error incorrect way<N><N><N>tuple2b = 'a',<N>type(tuple2)<N><N><N>#end of tuple<N>#where do we used it - list type of object where no changes are required<N>#list of gender, courses, categories, countries, list of values<N>#https://learnbatta.com/blog/python-working-with-tuple-data-type-41/<N><N><N>
# -*- coding: utf-8 -*-<N>"""<N>Created on Mon Apr 11 22:08:25 2022<N><N>@author: vikas<N>"""<N><N>print("Welcome")<N><N>print("To Python")<N><N><N>print?<N><N>help(print)<N><N>import pandas<N>pandas.__version__<N><N>import pandas as pd<N>pd.__version__<N><N>
<N>print("Welcome")<N>print("Welcome", "to", "Python")<N>print("Welcome", "to", "Python", sep="---")<N><N>print("Welcome")<N>print("To Python")<N><N>print("Welcome", end = "00")<N>print("To Python")<N><N><N>print("I live in India")<N><N>country = "India"<N><N>
print("country")<N><N>print(country)<N><N>print("I live in country")<N><N>print("I live in", country)<N><N><N>print("I live in", country, sep ="-")<N><N>print("I live in {0}".format(country))<N><N>print("I live in {0}".format(country), sep='-')<N><N>
lc = 'India'<N>wc = 'USA'<N><N>print("I live in {1} and I work in {0}".format(lc, wc))<N><N>print("I live in {0} and I work in {1}".format(lc, wc), sep ='-')<N><N><N>country = input()<N>lc = input("Enter Live Country->")<N>wc = input("Enter Work Country->")<N>print("I live in {1} and I work in {0}".format(lc, wc))<N><N>
# -*- coding: utf-8 -*-<N>"""<N>Created on Tue Apr 12 20:51:04 2022<N><N>@author: vikas<N>"""<N><N>a = input("Enter a->")<N><N>s = "hi"<N>b =10<N>c =12.4<N><N>a=10<N>b=20<N>c = a+b<N><N>x = input("Enter a->")<N><N>y = input("Enter y->")<N><N>x + y<N><N>
#String to Number<N># Type Conversion or Type Casting<N><N>x = int(input("Enter a->"))<N><N><N>x = input("Enter x->")<N>x = int(x)<N><N><N>y = int(input("Enter y->"))<N><N>z = x+y<N>z<N><N><N>int(input("Enter y->"))<N><N><N>a = 20<N>type(a)<N><N>s = "age is "<N><N>
s + str(a)<N><N><N>s = '10'<N><N>int(s)<N>float(s)<N><N>i= 10<N>str(i)<N>float(i)<N><N>f= 10.3<N>str(f)<N>int(f)<N><N><N><N>c = 'A'<N>ord(c)<N><N><N>c = 'Z'<N>ord(c)<N><N>c = '!'<N>ord(c)<N><N><N>i = 65<N>chr(i)<N><N><N>test = 'AxSdSwRtdSfDD'<N><N>t = ''<N><N>
for i in test:<N>    if(ord(i)>=65 and ord(i)<=90):<N>        t=t+i<N><N>print(t)<N><N><N>#Airthmetic Operators<N><N>x = 10<N><N>print(x+1)<N><N>print(x-1)<N><N>print(x/2)<N><N>print(x*3)<N><N>print(x**3)<N><N>print(x**(1/2))<N><N>print(x % 2)<N><N><N>a = int(input("Enter value-> "))<N>a=20<N>if (a % 2 == 0):<N>    print("even")<N>else:<N>    print("Odd")<N><N>
<N>x = 10<N><N>x = x + 1<N><N>x = x*2<N><N>#Comparative Operators<N><N>a=20<N>b=10<N>c=20<N><N>a == b<N>a == c<N><N>a < b<N>a>b<N><N>a<= b<N><N>a>=b<N><N>a != b<N><N><N>#Boolean - True or False<N><N>t = True<N>f = False<N><N><N>a= 77<N><N>a>=65 and a<=90<N><N>
<N>'''<N>AND<N>X Y O<N>0 0 0<N>0 1 0<N>1 0 0<N>1 1 1<N><N>OR<N>X Y O<N>0 0 0<N>0 1 1<N>1 0 1<N>1 1 1<N><N>Not<N>X O<N>0 1<N>1 0<N><N>'''<N><N>a = 10<N>b = 20<N>c = 30<N><N>a <= b and b >= c<N><N>a <= b or b >= c<N><N><N># String Handling<N><N>h = "Hello"<N>w = "World"<N><N>
h+w<N><N>h + " " + w<N><N>len(h)<N><N>s ="hello"<N><N>s.capitalize()<N><N>s = s.upper()<N><N>s =s.lower()<N><N>s<N><N>s = "Hello Java"<N><N>s = s.replace("Java", "Python")<N><N><N>name = input("Enter Your Name->")<N><N>name  = name.strip()<N><N><N><N>
# Combinatioal/ Extended Datatypes<N><N># List, Set, Dictionary, Tuple<N><N># Mutable, Indexed, Ordered, Hetergeneous, Unique Values, Key-Value Pair<N><N><N>l1 = []<N><N>l2 = [20,23,10,22,55, 23]<N>print(l2)<N><N><N>#Indexed<N><N>l2[0]<N>l2[1]<N>l2[2]<N>l2[3]<N>l2[4]<N>l2[5] #IndexError: list index out of range<N><N>
<N>#Hetrogeneous<N><N>l3 = [10, 303.2, True, "String"]<N><N>l3<N>l3[1]<N>l3[3]<N><N>#Mutable or Changable<N><N>l2[0] = 33<N>l3[2] = "VK"<N><N>print(l3)<N><N><N><N>r1 = list(range(10))<N>r2 = list(range(20))<N>r2<N>r3 = list(range(11, 21))<N>r3<N>r4 = list(range(11, 51, 5))<N>r4<N><N>
r5 = list(range(100, 200))<N>r5<N><N>r5[0]<N>r5[1]<N><N>for i in r5:<N>    print(i*6)<N><N><N>l2<N><N>l2.append(44)<N>l2<N><N>l2.extend([555,333])<N>l2<N><N>l2.pop()<N>l2.pop(2)<N>l2<N><N>l2.remove(22)<N>l2<N><N>l2.remove(22)<N>l2<N><N>l2<N><N>l2.sort()<N>l2<N><N>
l2.reverse()<N>l2<N><N>r5.reverse()<N>r5<N><N><N><N>#Set<N><N># Mutable, Indexed, Ordered, Hetergeneous, Unique Values, Key-Value Pair<N><N>s1 = {10}<N><N># Ordered<N>s2 = {30,50,10,20,40}<N>s2<N><N># Unique Values<N><N>s3 = {20,30,30,10,20,90,30,20}<N>s3<N><N>
# Non- Indexed<N><N>s3[0] #TypeError: 'set' object is not subscriptable<N><N>for i in s3:<N>    print(i)<N><N>#Hetrogeneous<N><N>s4 = {"VK", 22, 43.2, True}<N>s4<N><N><N>#Mutable<N><N>s3<N>s3.add(122)<N>s3<N>s3.add(22)<N>s3<N>s3.add(122)<N>s3<N><N>s3.update([22,33,44,66])<N>s3<N><N>
s3.pop()<N><N>s3<N>s3.remove(30)<N>s3<N><N><N>s3.remove(44)<N>s3<N><N>s3.remove(44) #  KeyError: 44 Not available<N><N>s3<N>s3.discard(22)<N>s3<N><N>s3.discard(22) # No show any error even the keyvalue is not available<N>s3<N><N><N><N><N><N><N><N><N><N>
<N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N><N>
#!/usr/bin/env python<N><N>from setuptools import find_packages, setup<N><N>import os<N>import subprocess<N>import sys<N>import time<N>import torch<N>from torch.utils.cpp_extension import (BuildExtension, CppExtension,<N>                                       CUDAExtension)<N><N>
import logging<N>import torch<N>from os import path as osp<N><N>from basicsr.data import create_dataloader, create_dataset<N>from basicsr.models import create_model<N>from basicsr.train import parse_options<N>from basicsr.utils import (get_env_info, get_root_logger, get_time_str,<N>                           make_exp_dirs)<N>from basicsr.utils.options import dict2str<N><N>
# GENERATED VERSION FILE<N># TIME: Wed Mar  9 22:05:30 2022<N>__version__ = '1.2.0+10018c6'<N>short_version = '1.2.0'<N>version_info = (1, 2, 0)<N>
import math<N>import torch<N>from torch.utils.data.sampler import Sampler<N><N><N>class EnlargedSampler(Sampler):<N>    """Sampler that restricts data loading to a subset of the dataset.<N><N>    Modified from torch.utils.data.distributed.DistributedSampler<N>    Support enlarging the dataset for iteration-based training, for saving<N>    time when restart the dataloader after each epoch<N><N>
    Args:<N>        dataset (torch.utils.data.Dataset): Dataset used for sampling.<N>        num_replicas (int | None): Number of processes participating in<N>            the training. It is usually the world_size.<N>        rank (int | None): Rank of the current process within num_replicas.<N>        ratio (int): Enlarging ratio. Default: 1.<N>    """<N><N>
    def __init__(self, dataset, num_replicas, rank, ratio=1):<N>        self.dataset = dataset<N>        self.num_replicas = num_replicas<N>        self.rank = rank<N>        self.epoch = 0<N>        self.num_samples = math.ceil(<N>            len(self.dataset) * ratio / self.num_replicas)<N>        self.total_size = self.num_samples * self.num_replicas<N><N>
    def __iter__(self):<N>        # deterministically shuffle based on epoch<N>        g = torch.Generator()<N>        g.manual_seed(self.epoch)<N>        indices = torch.randperm(self.total_size, generator=g).tolist()<N><N>        dataset_size = len(self.dataset)<N>        indices = [v % dataset_size for v in indices]<N><N>
        # subsample<N>        indices = indices[self.rank:self.total_size:self.num_replicas]<N>        assert len(indices) == self.num_samples<N><N>        return iter(indices)<N><N>    def __len__(self):<N>        return self.num_samples<N><N>    def set_epoch(self, epoch):<N>        self.epoch = epoch<N><N><N>
import cv2<N>cv2.setNumThreads(1)<N>import numpy as np<N>import torch<N>from os import path as osp<N>from torch.nn import functional as F<N><N>from basicsr.data.transforms import mod_crop<N>from basicsr.utils import img2tensor, scandir<N><N><N>def read_img_seq(path, require_mod_crop=False, scale=1):<N>    """Read a sequence of images from a given folder path.<N><N>
    Args:<N>        path (list[str] | str): List of image paths or image folder path.<N>        require_mod_crop (bool): Require mod crop for each image.<N>            Default: False.<N>        scale (int): Scale factor for mod_crop. Default: 1.<N><N>
    Returns:<N>        Tensor: size (t, c, h, w), RGB, [0, 1].<N>    """<N>    if isinstance(path, list):<N>        img_paths = path<N>    else:<N>        img_paths = sorted(list(scandir(path, full_path=True)))<N>    imgs = [cv2.imread(v).astype(np.float32) / 255. for v in img_paths]<N>    if require_mod_crop:<N>        imgs = [mod_crop(img, scale) for img in imgs]<N>    imgs = img2tensor(imgs, bgr2rgb=True, float32=True)<N>    imgs = torch.stack(imgs, dim=0)<N>    return imgs<N><N>
<N>def generate_frame_indices(crt_idx,<N>                           max_frame_num,<N>                           num_frames,<N>                           padding='reflection'):<N>    """Generate an index list for reading `num_frames` frames from a sequence<N>    of images.<N><N>
from os import path as osp<N>from torch.utils import data as data<N>from torchvision.transforms.functional import normalize<N><N>from basicsr.data.transforms import augment<N>from basicsr.utils import FileClient, imfrombytes, img2tensor<N><N><N>class FFHQDataset(data.Dataset):<N>    """FFHQ dataset for StyleGAN.<N><N>
    Args:<N>        opt (dict): Config for train datasets. It contains the following keys:<N>            dataroot_gt (str): Data root path for gt.<N>            io_backend (dict): IO backend type and other kwarg.<N>            mean (list | tuple): Image mean.<N>            std (list | tuple): Image std.<N>            use_hflip (bool): Whether to horizontally flip.<N><N>
    """<N><N>    def __init__(self, opt):<N>        super(FFHQDataset, self).__init__()<N>        self.opt = opt<N>        # file client (io backend)<N>        self.file_client = None<N>        self.io_backend_opt = opt['io_backend']<N><N>        self.gt_folder = opt['dataroot_gt']<N>        self.mean = opt['mean']<N>        self.std = opt['std']<N><N>
import queue as Queue<N>import threading<N>import torch<N>from torch.utils.data import DataLoader<N><N><N>class PrefetchGenerator(threading.Thread):<N>    """A general prefetch generator.<N><N>    Ref:<N>    https://stackoverflow.com/questions/7323664/python-generator-pre-fetch<N><N>
    Args:<N>        generator: Python generator.<N>        num_prefetch_queue (int): Number of prefetch queue.<N>    """<N><N>    def __init__(self, generator, num_prefetch_queue):<N>        threading.Thread.__init__(self)<N>        self.queue = Queue.Queue(num_prefetch_queue)<N>        self.generator = generator<N>        self.daemon = True<N>        self.start()<N><N>
    def run(self):<N>        for item in self.generator:<N>            self.queue.put(item)<N>        self.queue.put(None)<N><N>    def __next__(self):<N>        next_item = self.queue.get()<N>        if next_item is None:<N>            raise StopIteration<N>        return next_item<N><N>
    def __iter__(self):<N>        return self<N><N><N>class PrefetchDataLoader(DataLoader):<N>    """Prefetch version of dataloader.<N><N>    Ref:<N>    https://github.com/IgorSusmelj/pytorch-styleguide/issues/5#<N><N>    TODO:<N>    Need to test on single gpu and ddp (multi-gpu). There is a known issue in<N>    ddp.<N><N>
    Args:<N>        num_prefetch_queue (int): Number of prefetch queue.<N>        kwargs (dict): Other arguments for dataloader.<N>    """<N><N>    def __init__(self, num_prefetch_queue, **kwargs):<N>        self.num_prefetch_queue = num_prefetch_queue<N>        super(PrefetchDataLoader, self).__init__(**kwargs)<N><N>
    def __iter__(self):<N>        return PrefetchGenerator(super().__iter__(), self.num_prefetch_queue)<N><N><N>class CPUPrefetcher():<N>    """CPU prefetcher.<N><N>    Args:<N>        loader: Dataloader.<N>    """<N><N>    def __init__(self, loader):<N>        self.ori_loader = loader<N>        self.loader = iter(loader)<N><N>
    def next(self):<N>        try:<N>            return next(self.loader)<N>        except StopIteration:<N>            return None<N><N>    def reset(self):<N>        self.loader = iter(self.ori_loader)<N><N><N>class CUDAPrefetcher():<N>    """CUDA prefetcher.<N><N>
    Ref:<N>    https://github.com/NVIDIA/apex/issues/304#<N><N>    It may consums more GPU memory.<N><N>    Args:<N>        loader: Dataloader.<N>        opt (dict): Options.<N>    """<N><N>    def __init__(self, loader, opt):<N>        self.ori_loader = loader<N>        self.loader = iter(loader)<N>        self.opt = opt<N>        self.stream = torch.cuda.Stream()<N>        self.device = torch.device('cuda' if opt['num_gpu'] != 0 else 'cpu')<N>        self.preload()<N><N>
    def preload(self):<N>        try:<N>            self.batch = next(self.loader)  # self.batch is a dict<N>        except StopIteration:<N>            self.batch = None<N>            return None<N>        # put tensors to gpu<N>        with torch.cuda.stream(self.stream):<N>            for k, v in self.batch.items():<N>                if torch.is_tensor(v):<N>                    self.batch[k] = self.batch[k].to(<N>                        device=self.device, non_blocking=True)<N><N>
    def next(self):<N>        torch.cuda.current_stream().wait_stream(self.stream)<N>        batch = self.batch<N>        self.preload()<N>        return batch<N><N>    def reset(self):<N>        self.loader = iter(self.ori_loader)<N>        self.preload()<N><N><N>
import numpy as np<N>import random<N>import torch<N>from pathlib import Path<N>from torch.utils import data as data<N><N>from basicsr.data.transforms import augment, paired_random_crop<N>from basicsr.utils import FileClient, get_root_logger, imfrombytes, img2tensor<N>from basicsr.utils.flow_util import dequantize_flow<N><N>
<N>class REDSDataset(data.Dataset):<N>    """REDS dataset for training.<N><N>    The keys are generated from a meta info txt file.<N>    basicsr/data/meta_info/meta_info_REDS_GT.txt<N><N>    Each line contains:<N>    1. subfolder (clip) name; 2. frame number; 3. image shape, seperated by<N>    a white space.<N>    Examples:<N>    000 100 (720,1280,3)<N>    001 100 (720,1280,3)<N>    ...<N><N>
from os import path as osp<N>from torch.utils import data as data<N>from torchvision.transforms.functional import normalize<N><N>from basicsr.data.data_util import paths_from_lmdb<N>from basicsr.utils import FileClient, imfrombytes, img2tensor, scandir<N><N>
<N>class SingleImageDataset(data.Dataset):<N>    """Read only lq images in the test phase.<N><N>    Read LQ (Low Quality, e.g. LR (Low Resolution), blurry, noisy, etc).<N><N>    There are two modes:<N>    1. 'meta_info_file': Use meta information file to generate paths.<N>    2. 'folder': Scan folders to generate paths.<N><N>
    Args:<N>        opt (dict): Config for train datasets. It contains the following keys:<N>            dataroot_lq (str): Data root path for lq.<N>            meta_info_file (str): Path for meta information file.<N>            io_backend (dict): IO backend type and other kwarg.<N>    """<N><N>
    def __init__(self, opt):<N>        super(SingleImageDataset, self).__init__()<N>        self.opt = opt<N>        # file client (io backend)<N>        self.file_client = None<N>        self.io_backend_opt = opt['io_backend']<N>        self.mean = opt['mean'] if 'mean' in opt else None<N>        self.std = opt['std'] if 'std' in opt else None<N>        self.lq_folder = opt['dataroot_lq']<N><N>
import glob<N>import torch<N>from os import path as osp<N>from torch.utils import data as data<N><N>from basicsr.data.data_util import (duf_downsample, generate_frame_indices,<N>                                    read_img_seq)<N>from basicsr.utils import get_root_logger, scandir<N><N>
<N>class VideoTestDataset(data.Dataset):<N>    """Video test dataset.<N><N>    Supported datasets: Vid4, REDS4, REDSofficial.<N>    More generally, it supports testing dataset with following structures:<N><N>    dataroot<N>    ├── subfolder1<N>        ├── frame000<N>        ├── frame001<N>        ├── ...<N>    ├── subfolder1<N>        ├── frame000<N>        ├── frame001<N>        ├── ...<N>    ├── ...<N><N>
import random<N>import torch<N>from pathlib import Path<N>from torch.utils import data as data<N><N>from basicsr.data.transforms import augment, paired_random_crop<N>from basicsr.utils import FileClient, get_root_logger, imfrombytes, img2tensor<N><N>
<N>class Vimeo90KDataset(data.Dataset):<N>    """Vimeo90K dataset for training.<N><N>    The keys are generated from a meta info txt file.<N>    basicsr/data/meta_info/meta_info_Vimeo90K_train_GT.txt<N><N>    Each line contains:<N>    1. clip name; 2. frame number; 3. image shape, seperated by a white space.<N>    Examples:<N>        00001/0001 7 (256,448,3)<N>        00001/0002 7 (256,448,3)<N><N>
    Key examples: "00001/0001"<N>    GT (gt): Ground-Truth;<N>    LQ (lq): Low-Quality, e.g., low-resolution/blurry/noisy/compressed frames.<N><N>    The neighboring frame list for different num_frame:<N>    num_frame | frame list<N>             1 | 4<N>             3 | 3,4,5<N>             5 | 2,3,4,5,6<N>             7 | 1,2,3,4,5,6,7<N><N>
    Args:<N>        opt (dict): Config for train dataset. It contains the following keys:<N>            dataroot_gt (str): Data root path for gt.<N>            dataroot_lq (str): Data root path for lq.<N>            meta_info_file (str): Path for meta information file.<N>            io_backend (dict): IO backend type and other kwarg.<N><N>
            num_frame (int): Window size for input frames.<N>            gt_size (int): Cropped patched size for gt patches.<N>            random_reverse (bool): Random reverse input frames.<N>            use_flip (bool): Use horizontal flips.<N>            use_rot (bool): Use rotation (use vertical flip and transposing h<N>                and w for implementation).<N><N>
            scale (bool): Scale, which will be added automatically.<N>    """<N><N>    def __init__(self, opt):<N>        super(Vimeo90KDataset, self).__init__()<N>        self.opt = opt<N>        self.gt_root, self.lq_root = Path(opt['dataroot_gt']), Path(<N>            opt['dataroot_lq'])<N><N>
        with open(opt['meta_info_file'], 'r') as fin:<N>            self.keys = [line.split(' ')[0] for line in fin]<N><N>        # file client (io backend)<N>        self.file_client = None<N>        self.io_backend_opt = opt['io_backend']<N>        self.is_lmdb = False<N>        if self.io_backend_opt['type'] == 'lmdb':<N>            self.is_lmdb = True<N>            self.io_backend_opt['db_paths'] = [self.lq_root, self.gt_root]<N>            self.io_backend_opt['client_keys'] = ['lq', 'gt']<N><N>
        # indices of input images<N>        self.neighbor_list = [<N>            i + (9 - opt['num_frame']) // 2 for i in range(opt['num_frame'])<N>        ]<N><N>        # temporal augmentation configs<N>        self.random_reverse = opt['random_reverse']<N>        logger = get_root_logger()<N>        logger.info(f'Random reverse is {self.random_reverse}.')<N><N>
    def __getitem__(self, index):<N>        if self.file_client is None:<N>            self.file_client = FileClient(<N>                self.io_backend_opt.pop('type'), **self.io_backend_opt)<N><N>        # random reverse<N>        if self.random_reverse and random.random() < 0.5:<N>            self.neighbor_list.reverse()<N><N>
        scale = self.opt['scale']<N>        gt_size = self.opt['gt_size']<N>        key = self.keys[index]<N>        clip, seq = key.split('/')  # key example: 00001/0001<N><N>        # get the GT frame (im4.png)<N>        if self.is_lmdb:<N>            img_gt_path = f'{key}/im4'<N>        else:<N>            img_gt_path = self.gt_root / clip / seq / 'im4.png'<N>        img_bytes = self.file_client.get(img_gt_path, 'gt')<N>        img_gt = imfrombytes(img_bytes, float32=True)<N><N>
        # get the neighboring LQ frames<N>        img_lqs = []<N>        for neighbor in self.neighbor_list:<N>            if self.is_lmdb:<N>                img_lq_path = f'{clip}/{seq}/im{neighbor}'<N>            else:<N>                img_lq_path = self.lq_root / clip / seq / f'im{neighbor}.png'<N>            img_bytes = self.file_client.get(img_lq_path, 'lq')<N>            img_lq = imfrombytes(img_bytes, float32=True)<N>            img_lqs.append(img_lq)<N><N>
        # randomly crop<N>        img_gt, img_lqs = paired_random_crop(img_gt, img_lqs, gt_size, scale,<N>                                             img_gt_path)<N><N>        # augmentation - flip, rotate<N>        img_lqs.append(img_gt)<N>        img_results = augment(img_lqs, self.opt['use_flip'],<N>                              self.opt['use_rot'])<N><N>
        img_results = img2tensor(img_results)<N>        img_lqs = torch.stack(img_results[0:-1], dim=0)<N>        img_gt = img_results[-1]<N><N>        # img_lqs: (t, c, h, w)<N>        # img_gt: (c, h, w)<N>        # key: str<N>        return {'lq': img_lqs, 'gt': img_gt, 'key': key}<N><N>
import importlib<N>import numpy as np<N>import random<N>import torch<N>import torch.utils.data<N>from functools import partial<N>from os import path as osp<N><N>from basicsr.data.prefetch_dataloader import PrefetchDataLoader<N>from basicsr.utils import get_root_logger, scandir<N>from basicsr.utils.dist_util import get_dist_info<N><N>
import numpy as np<N><N>from basicsr.utils.matlab_functions import bgr2ycbcr<N><N><N>def reorder_image(img, input_order='HWC'):<N>    """Reorder images to 'HWC' order.<N><N>    If the input_order is (h, w), return (h, w, 1);<N>    If the input_order is (c, h, w), return (h, w, c);<N>    If the input_order is (h, w, c), return as it is.<N><N>
    Args:<N>        img (ndarray): Input image.<N>        input_order (str): Whether the input order is 'HWC' or 'CHW'.<N>            If the input image shape is (h, w), input_order will not have<N>            effects. Default: 'HWC'.<N><N>    Returns:<N>        ndarray: reordered image.<N>    """<N><N>
    if input_order not in ['HWC', 'CHW']:<N>        raise ValueError(<N>            f'Wrong input_order {input_order}. Supported input_orders are '<N>            "'HWC' and 'CHW'")<N>    if len(img.shape) == 2:<N>        img = img[..., None]<N>    if input_order == 'CHW':<N>        img = img.transpose(1, 2, 0)<N>    return img<N><N>
<N>def to_y_channel(img):<N>    """Change to Y channel of YCbCr.<N><N>    Args:<N>        img (ndarray): Images with range [0, 255].<N><N>    Returns:<N>        (ndarray): Images with range [0, 255] (float type) without round.<N>    """<N>    img = img.astype(np.float32) / 255.<N>    if img.ndim == 3 and img.shape[2] == 3:<N>        img = bgr2ycbcr(img, y_only=True)<N>        img = img[..., None]<N>    return img * 255.<N><N><N>
import cv2<N>import math<N>import numpy as np<N>from scipy.ndimage.filters import convolve<N>from scipy.special import gamma<N><N>from basicsr.metrics.metric_util import reorder_image, to_y_channel<N><N><N>def estimate_aggd_param(block):<N>    """Estimate AGGD (Asymmetric Generalized Gaussian Distribution) paramters.<N><N>
    Args:<N>        block (ndarray): 2D Image block.<N><N>    Returns:<N>        tuple: alpha (float), beta_l (float) and beta_r (float) for the AGGD<N>            distribution (Estimating the parames in Equation 7 in the paper).<N>    """<N>    block = block.flatten()<N>    gam = np.arange(0.2, 10.001, 0.001)  # len = 9801<N>    gam_reciprocal = np.reciprocal(gam)<N>    r_gam = np.square(gamma(gam_reciprocal * 2)) / (<N>        gamma(gam_reciprocal) * gamma(gam_reciprocal * 3))<N><N>
    left_std = np.sqrt(np.mean(block[block < 0]**2))<N>    right_std = np.sqrt(np.mean(block[block > 0]**2))<N>    gammahat = left_std / right_std<N>    rhat = (np.mean(np.abs(block)))**2 / np.mean(block**2)<N>    rhatnorm = (rhat * (gammahat**3 + 1) *<N>                (gammahat + 1)) / ((gammahat**2 + 1)**2)<N>    array_position = np.argmin((r_gam - rhatnorm)**2)<N><N>
    alpha = gam[array_position]<N>    beta_l = left_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))<N>    beta_r = right_std * np.sqrt(gamma(1 / alpha) / gamma(3 / alpha))<N>    return (alpha, beta_l, beta_r)<N><N><N>def compute_feature(block):<N>    """Compute features.<N><N>
    Args:<N>        block (ndarray): 2D Image block.<N><N>    Returns:<N>        list: Features with length of 18.<N>    """<N>    feat = []<N>    alpha, beta_l, beta_r = estimate_aggd_param(block)<N>    feat.extend([alpha, (beta_l + beta_r) / 2])<N><N>
import cv2<N>import numpy as np<N><N>from basicsr.metrics.metric_util import reorder_image, to_y_channel<N>import skimage.metrics<N>import torch<N><N><N>def calculate_psnr(img1,<N>                   img2,<N>                   crop_border,<N>                   input_order='HWC',<N>                   test_y_channel=False):<N>    """Calculate PSNR (Peak Signal-to-Noise Ratio).<N><N>
from .niqe import calculate_niqe<N>from .psnr_ssim import calculate_psnr, calculate_ssim<N><N>__all__ = ['calculate_psnr', 'calculate_ssim', 'calculate_niqe']<N>
import logging<N>import os<N>import torch<N>from collections import OrderedDict<N>from copy import deepcopy<N>from torch.nn.parallel import DataParallel, DistributedDataParallel<N><N>from basicsr.models import lr_scheduler as lr_scheduler<N>from basicsr.utils.dist_util import master_only<N><N>
logger = logging.getLogger('basicsr')<N><N><N>class BaseModel():<N>    """Base model."""<N><N>    def __init__(self, opt):<N>        self.opt = opt<N>        self.device = torch.device('cuda' if opt['num_gpu'] != 0 else 'cpu')<N>        self.is_train = opt['is_train']<N>        self.schedulers = []<N>        self.optimizers = []<N><N>
    def feed_data(self, data):<N>        pass<N><N>    def optimize_parameters(self):<N>        pass<N><N>    def get_current_visuals(self):<N>        pass<N><N>    def save(self, epoch, current_iter):<N>        """Save networks and training state."""<N>        pass<N><N>
import importlib<N>import torch<N>from collections import OrderedDict<N>from copy import deepcopy<N>from os import path as osp<N>from tqdm import tqdm<N><N>from basicsr.models.archs import define_network<N>from basicsr.models.base_model import BaseModel<N>from basicsr.utils import get_root_logger, imwrite, tensor2img<N><N>
loss_module = importlib.import_module('basicsr.models.losses')<N>metric_module = importlib.import_module('basicsr.metrics')<N><N>import os<N>import random<N>import numpy as np<N>import cv2<N>import torch.nn.functional as F<N>from functools import partial<N><N>
class Mixing_Augment:<N>    def __init__(self, mixup_beta, use_identity, device):<N>        self.dist = torch.distributions.beta.Beta(torch.tensor([mixup_beta]), torch.tensor([mixup_beta]))<N>        self.device = device<N><N>        self.use_identity = use_identity<N><N>
        self.augments = [self.mixup]<N><N>    def mixup(self, target, input_):<N>        lam = self.dist.rsample((1,1)).item()<N>    <N>        r_index = torch.randperm(target.size(0)).to(self.device)<N>    <N>        target = lam * target + (1-lam) * target[r_index, :]<N>        input_ = lam * input_ + (1-lam) * input_[r_index, :]<N>    <N>        return target, input_<N><N>
    def __call__(self, target, input_):<N>        if self.use_identity:<N>            augment = random.randint(0, len(self.augments))<N>            if augment < len(self.augments):<N>                target, input_ = self.augments[augment](target, input_)<N>        else:<N>            augment = random.randint(0, len(self.augments)-1)<N>            target, input_ = self.augments[augment](target, input_)<N>        return target, input_<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from pdb import set_trace as stx<N>import numbers<N><N>
from einops import rearrange<N><N><N><N>##########################################################################<N>## Layer Norm<N><N>def to_3d(x):<N>    return rearrange(x, 'b c h w -> b (h w) c')<N><N>def to_4d(x,h,w):<N>    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)<N><N>
class BiasFree_LayerNorm(nn.Module):<N>    def __init__(self, normalized_shape):<N>        super(BiasFree_LayerNorm, self).__init__()<N>        if isinstance(normalized_shape, numbers.Integral):<N>            normalized_shape = (normalized_shape,)<N>        normalized_shape = torch.Size(normalized_shape)<N><N>
        assert len(normalized_shape) == 1<N><N>        self.weight = nn.Parameter(torch.ones(normalized_shape))<N>        self.normalized_shape = normalized_shape<N><N>    def forward(self, x):<N>        sigma = x.var(-1, keepdim=True, unbiased=False)<N>        return x / torch.sqrt(sigma+1e-5) * self.weight<N><N>
class WithBias_LayerNorm(nn.Module):<N>    def __init__(self, normalized_shape):<N>        super(WithBias_LayerNorm, self).__init__()<N>        if isinstance(normalized_shape, numbers.Integral):<N>            normalized_shape = (normalized_shape,)<N>        normalized_shape = torch.Size(normalized_shape)<N><N>
        assert len(normalized_shape) == 1<N><N>        self.weight = nn.Parameter(torch.ones(normalized_shape))<N>        self.bias = nn.Parameter(torch.zeros(normalized_shape))<N>        self.normalized_shape = normalized_shape<N><N>    def forward(self, x):<N>        mu = x.mean(-1, keepdim=True)<N>        sigma = x.var(-1, keepdim=True, unbiased=False)<N>        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias<N><N>
<N>class LayerNorm(nn.Module):<N>    def __init__(self, dim, LayerNorm_type):<N>        super(LayerNorm, self).__init__()<N>        if LayerNorm_type =='BiasFree':<N>            self.body = BiasFree_LayerNorm(dim)<N>        else:<N>            self.body = WithBias_LayerNorm(dim)<N><N>
    def forward(self, x):<N>        h, w = x.shape[-2:]<N>        return to_4d(self.body(to_3d(x)), h, w)<N><N><N><N>##########################################################################<N>## Gated-Dconv Feed-Forward Network (GDFN)<N>class FeedForward(nn.Module):<N>    def __init__(self, dim, ffn_expansion_factor, bias):<N>        super(FeedForward, self).__init__()<N><N>
        hidden_features = int(dim*ffn_expansion_factor)<N><N>        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)<N><N>        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)<N><N>
        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)<N><N>    def forward(self, x):<N>        x = self.project_in(x)<N>        x1, x2 = self.dwconv(x).chunk(2, dim=1)<N>        x = F.gelu(x1) * x2<N>        x = self.project_out(x)<N>        return x<N><N>
<N><N>##########################################################################<N>## Multi-DConv Head Transposed Self-Attention (MDTA)<N>class Attention(nn.Module):<N>    def __init__(self, dim, num_heads, bias):<N>        super(Attention, self).__init__()<N>        self.num_heads = num_heads<N>        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))<N><N>
        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)<N>        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)<N>        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)<N>        <N><N>
<N>    def forward(self, x):<N>        b,c,h,w = x.shape<N><N>        qkv = self.qkv_dwconv(self.qkv(x))<N>        q,k,v = qkv.chunk(3, dim=1)   <N>        <N>        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)<N>        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)<N>        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)<N><N>
        q = torch.nn.functional.normalize(q, dim=-1)<N>        k = torch.nn.functional.normalize(k, dim=-1)<N><N>        attn = (q @ k.transpose(-2, -1)) * self.temperature<N>        attn = attn.softmax(dim=-1)<N><N>        out = (attn @ v)<N>        <N>        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)<N><N>
        out = self.project_out(out)<N>        return out<N><N><N><N>##########################################################################<N>class TransformerBlock(nn.Module):<N>    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):<N>        super(TransformerBlock, self).__init__()<N><N>
        self.norm1 = LayerNorm(dim, LayerNorm_type)<N>        self.attn = Attention(dim, num_heads, bias)<N>        self.norm2 = LayerNorm(dim, LayerNorm_type)<N>        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)<N><N>    def forward(self, x):<N>        x = x + self.attn(self.norm1(x))<N>        x = x + self.ffn(self.norm2(x))<N><N>
        return x<N><N><N><N>##########################################################################<N>## Overlapped image patch embedding with 3x3 Conv<N>class OverlapPatchEmbed(nn.Module):<N>    def __init__(self, in_c=3, embed_dim=48, bias=False):<N>        super(OverlapPatchEmbed, self).__init__()<N><N>
        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)<N><N>    def forward(self, x):<N>        x = self.proj(x)<N><N>        return x<N><N><N><N>##########################################################################<N>## Resizing modules<N>class Downsample(nn.Module):<N>    def __init__(self, n_feat):<N>        super(Downsample, self).__init__()<N><N>
        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),<N>                                  nn.PixelUnshuffle(2))<N><N>    def forward(self, x):<N>        return self.body(x)<N><N>class Upsample(nn.Module):<N>    def __init__(self, n_feat):<N>        super(Upsample, self).__init__()<N><N>
import torch<N>from torch import nn as nn<N>from torch.nn import functional as F<N>import numpy as np<N><N>from basicsr.models.losses.loss_util import weighted_loss<N><N>_reduction_modes = ['none', 'mean', 'sum']<N><N><N>@weighted_loss<N>def l1_loss(pred, target):<N>    return F.l1_loss(pred, target, reduction='none')<N><N>
<N>@weighted_loss<N>def mse_loss(pred, target):<N>    return F.mse_loss(pred, target, reduction='none')<N><N><N># @weighted_loss<N># def charbonnier_loss(pred, target, eps=1e-12):<N>#     return torch.sqrt((pred - target)**2 + eps)<N><N><N>class L1Loss(nn.Module):<N>    """L1 (mean absolute error, MAE) loss.<N><N>
import functools<N>from torch.nn import functional as F<N><N><N>def reduce_loss(loss, reduction):<N>    """Reduce loss as specified.<N><N>    Args:<N>        loss (Tensor): Elementwise loss tensor.<N>        reduction (str): Options are 'none', 'mean' and 'sum'.<N><N>
    Returns:<N>        Tensor: Reduced loss tensor.<N>    """<N>    reduction_enum = F._Reduction.get_enum(reduction)<N>    # none: 0, elementwise_mean:1, sum: 2<N>    if reduction_enum == 0:<N>        return loss<N>    elif reduction_enum == 1:<N>        return loss.mean()<N>    else:<N>        return loss.sum()<N><N>
<N>def weight_reduce_loss(loss, weight=None, reduction='mean'):<N>    """Apply element-wise weight and reduce loss.<N><N>    Args:<N>        loss (Tensor): Element-wise loss.<N>        weight (Tensor): Element-wise weights. Default: None.<N>        reduction (str): Same as built-in losses of PyTorch. Options are<N>            'none', 'mean' and 'sum'. Default: 'mean'.<N><N>
    Returns:<N>        Tensor: Loss values.<N>    """<N>    # if weight is specified, apply element-wise weight<N>    if weight is not None:<N>        assert weight.dim() == loss.dim()<N>        assert weight.size(1) == 1 or weight.size(1) == loss.size(1)<N>        loss = loss * weight<N><N>
    # if weight is not specified or reduction is sum, just reduce the loss<N>    if weight is None or reduction == 'sum':<N>        loss = reduce_loss(loss, reduction)<N>    # if reduction is mean, then compute mean over weight region<N>    elif reduction == 'mean':<N>        if weight.size(1) > 1:<N>            weight = weight.sum()<N>        else:<N>            weight = weight.sum() * loss.size(1)<N>        loss = loss.sum() / weight<N><N>
from .losses import (L1Loss, MSELoss, PSNRLoss, CharbonnierLoss)<N><N>__all__ = [<N>    'L1Loss', 'MSELoss', 'PSNRLoss', 'CharbonnierLoss',<N>]<N>
 # Author: Tobias Plötz, TU Darmstadt (tobias.ploetz@visinf.tu-darmstadt.de)<N><N> # This file is part of the implementation as described in the CVPR 2017 paper:<N> # Tobias Plötz and Stefan Roth, Benchmarking Denoising Algorithms with Real Photographs.<N> # Please see the file LICENSE.txt for the license governing this code.<N><N>
<N>import numpy as np<N>import scipy.io as sio<N>import os<N>import h5py<N><N>def bundle_submissions_raw(submission_folder,session):<N>    '''<N>    Bundles submission data for raw denoising<N><N>    submission_folder Folder where denoised images reside<N><N>
    Output is written to <submission_folder>/bundled/. Please submit<N>    the content of this folder.<N>    '''<N><N>    out_folder = os.path.join(submission_folder, session)<N>    # out_folder = os.path.join(submission_folder, "bundled/")<N>    try:<N>        os.mkdir(out_folder)<N>    except:pass<N><N>
import argparse<N>from os import path as osp<N><N>from basicsr.utils import scandir<N>from basicsr.utils.lmdb_util import make_lmdb_from_imgs<N><N>def prepare_keys(folder_path, suffix='png'):<N>    """Prepare image path list and keys for DIV2K dataset.<N><N>
    Args:<N>        folder_path (str): Folder path.<N><N>    Returns:<N>        list[str]: Image path list.<N>        list[str]: Key list.<N>    """<N>    print('Reading image path list ...')<N>    img_path_list = sorted(<N>        list(scandir(folder_path, suffix=suffix, recursive=False)))<N>    keys = [img_path.split('.{}'.format(suffix))[0] for img_path in sorted(img_path_list)]<N><N>
import math<N>import requests<N>from tqdm import tqdm<N><N>from .misc import sizeof_fmt<N><N><N>def download_file_from_google_drive(file_id, save_path):<N>    """Download files from google drive.<N><N>    Ref:<N>    https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive  # noqa E501<N><N>
    Args:<N>        file_id (str): File id.<N>        save_path (str): Save path.<N>    """<N><N>    session = requests.Session()<N>    URL = 'https://docs.google.com/uc?export=download'<N>    params = {'id': file_id}<N><N>    response = session.get(URL, params=params, stream=True)<N>    token = get_confirm_token(response)<N>    if token:<N>        params['confirm'] = token<N>        response = session.get(URL, params=params, stream=True)<N><N>
    # get file size<N>    response_file_size = session.get(<N>        URL, params=params, stream=True, headers={'Range': 'bytes=0-2'})<N>    if 'Content-Range' in response_file_size.headers:<N>        file_size = int(<N>            response_file_size.headers['Content-Range'].split('/')[1])<N>    else:<N>        file_size = None<N><N>
import cv2<N>import numpy as np<N>import os<N>import torch<N>from skimage import transform as trans<N><N>from basicsr.utils import imwrite<N><N>try:<N>    import dlib<N>except ImportError:<N>    print('Please install dlib before testing face restoration.'<N>          'Reference:　https://github.com/davisking/dlib')<N><N>
<N>class FaceRestorationHelper(object):<N>    """Helper for the face restoration pipeline."""<N><N>    def __init__(self, upscale_factor, face_size=512):<N>        self.upscale_factor = upscale_factor<N>        self.face_size = (face_size, face_size)<N><N>
# Modified from https://github.com/open-mmlab/mmcv/blob/master/mmcv/fileio/file_client.py  # noqa: E501<N>from abc import ABCMeta, abstractmethod<N><N><N>class BaseStorageBackend(metaclass=ABCMeta):<N>    """Abstract class of storage backends.<N><N>    All backends need to implement two apis: ``get()`` and ``get_text()``.<N>    ``get()`` reads the file as a byte stream and ``get_text()`` reads the file<N>    as texts.<N>    """<N><N>
    @abstractmethod<N>    def get(self, filepath):<N>        pass<N><N>    @abstractmethod<N>    def get_text(self, filepath):<N>        pass<N><N><N>class MemcachedBackend(BaseStorageBackend):<N>    """Memcached storage backend.<N><N>    Attributes:<N>        server_list_cfg (str): Config file for memcached server list.<N>        client_cfg (str): Config file for memcached client.<N>        sys_path (str | None): Additional path to be appended to `sys.path`.<N>            Default: None.<N>    """<N><N>
    def __init__(self, server_list_cfg, client_cfg, sys_path=None):<N>        if sys_path is not None:<N>            import sys<N>            sys.path.append(sys_path)<N>        try:<N>            import mc<N>        except ImportError:<N>            raise ImportError(<N>                'Please install memcached to enable MemcachedBackend.')<N><N>
        self.server_list_cfg = server_list_cfg<N>        self.client_cfg = client_cfg<N>        self._client = mc.MemcachedClient.GetInstance(self.server_list_cfg,<N>                                                      self.client_cfg)<N>        # mc.pyvector servers as a point which points to a memory cache<N>        self._mc_buffer = mc.pyvector()<N><N>
    def get(self, filepath):<N>        filepath = str(filepath)<N>        import mc<N>        self._client.Get(filepath, self._mc_buffer)<N>        value_buf = mc.ConvertBuffer(self._mc_buffer)<N>        return value_buf<N><N>    def get_text(self, filepath):<N>        raise NotImplementedError<N><N>
<N>class HardDiskBackend(BaseStorageBackend):<N>    """Raw hard disks storage backend."""<N><N>    def get(self, filepath):<N>        filepath = str(filepath)<N>        with open(filepath, 'rb') as f:<N>            value_buf = f.read()<N>        return value_buf<N><N>
    def get_text(self, filepath):<N>        filepath = str(filepath)<N>        with open(filepath, 'r') as f:<N>            value_buf = f.read()<N>        return value_buf<N><N><N>class LmdbBackend(BaseStorageBackend):<N>    """Lmdb storage backend.<N><N>
# Modified from https://github.com/open-mmlab/mmcv/blob/master/mmcv/video/optflow.py  # noqa: E501<N>import cv2<N>import numpy as np<N>import os<N><N><N>def flowread(flow_path, quantize=False, concat_axis=0, *args, **kwargs):<N>    """Read an optical flow map.<N><N>
    Args:<N>        flow_path (ndarray or str): Flow path.<N>        quantize (bool): whether to read quantized pair, if set to True,<N>            remaining args will be passed to :func:`dequantize_flow`.<N>        concat_axis (int): The axis that dx and dy are concatenated,<N>            can be either 0 or 1. Ignored if quantize is False.<N><N>
import cv2<N>import math<N>import numpy as np<N>import os<N>import torch<N>from torchvision.utils import make_grid<N><N><N>def img2tensor(imgs, bgr2rgb=True, float32=True):<N>    """Numpy array to tensor.<N><N>    Args:<N>        imgs (list[ndarray] | ndarray): Input images.<N>        bgr2rgb (bool): Whether to change bgr to rgb.<N>        float32 (bool): Whether to change to float32.<N><N>
    Returns:<N>        list[tensor] | tensor: Tensor images. If returned results only have<N>            one element, just return tensor.<N>    """<N><N>    def _totensor(img, bgr2rgb, float32):<N>        if img.shape[2] == 3 and bgr2rgb:<N>            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)<N>        img = torch.from_numpy(img.transpose(2, 0, 1))<N>        if float32:<N>            img = img.float()<N>        return img<N><N>
    if isinstance(imgs, list):<N>        return [_totensor(img, bgr2rgb, float32) for img in imgs]<N>    else:<N>        return _totensor(imgs, bgr2rgb, float32)<N><N><N>def tensor2img(tensor, rgb2bgr=True, out_type=np.uint8, min_max=(0, 1)):<N>    """Convert torch Tensors into image numpy arrays.<N><N>
import math<N>import numpy as np<N>import torch<N><N><N>def cubic(x):<N>    """cubic function used for calculate_weights_indices."""<N>    absx = torch.abs(x)<N>    absx2 = absx**2<N>    absx3 = absx**3<N>    return (1.5 * absx3 - 2.5 * absx2 + 1) * (<N>        (absx <= 1).type_as(absx)) + (-0.5 * absx3 + 2.5 * absx2 - 4 * absx +<N>                                      2) * (((absx > 1) *<N>                                             (absx <= 2)).type_as(absx))<N><N>
<N>def calculate_weights_indices(in_length, out_length, scale, kernel,<N>                              kernel_width, antialiasing):<N>    """Calculate weights and indices, used for imresize function.<N><N>    Args:<N>        in_length (int): Input length.<N>        out_length (int): Output length.<N>        scale (float): Scale factor.<N>        kernel_width (int): Kernel width.<N>        antialisaing (bool): Whether to apply anti-aliasing when downsampling.<N>    """<N><N>
    if (scale < 1) and antialiasing:<N>        # Use a modified kernel (larger kernel width) to simultaneously<N>        # interpolate and antialias<N>        kernel_width = kernel_width / scale<N><N>    # Output-space coordinates<N>    x = torch.linspace(1, out_length, out_length)<N><N>
    # Input-space coordinates. Calculate the inverse mapping such that 0.5<N>    # in output space maps to 0.5 in input space, and 0.5 + scale in output<N>    # space maps to 1.5 in input space.<N>    u = x / scale + 0.5 * (1 - 1 / scale)<N><N>    # What is the left-most pixel that can be involved in the computation?<N>    left = torch.floor(u - kernel_width / 2)<N><N>
    # What is the maximum number of pixels that can be involved in the<N>    # computation?  Note: it's OK to use an extra pixel here; if the<N>    # corresponding weights are all zero, it will be eliminated at the end<N>    # of this function.<N>    p = math.ceil(kernel_width) + 2<N><N>
    # The indices of the input pixels involved in computing the k-th output<N>    # pixel are in row k of the indices matrix.<N>    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(<N>        0, p - 1, p).view(1, p).expand(out_length, p)<N><N>
    # The weights used to compute the k-th output pixel are in row k of the<N>    # weights matrix.<N>    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices<N><N>    # apply cubic kernel<N>    if (scale < 1) and antialiasing:<N>        weights = scale * cubic(distance_to_center * scale)<N>    else:<N>        weights = cubic(distance_to_center)<N><N>
import numpy as np<N>import os<N>import random<N>import time<N>import torch<N>from os import path as osp<N><N>from .dist_util import master_only<N>from .logger import get_root_logger<N><N><N>def set_random_seed(seed):<N>    """Set random seeds."""<N>    random.seed(seed)<N>    np.random.seed(seed)<N>    torch.manual_seed(seed)<N>    torch.cuda.manual_seed(seed)<N>    torch.cuda.manual_seed_all(seed)<N><N>
<N>def get_time_str():<N>    return time.strftime('%Y%m%d_%H%M%S', time.localtime())<N><N><N>def mkdir_and_rename(path):<N>    """mkdirs. If path exists, rename it with timestamp and create a new one.<N><N>    Args:<N>        path (str): Folder path.<N>    """<N>    if osp.exists(path):<N>        new_name = path + '_archived_' + get_time_str()<N>        print(f'Path already exists. Rename it to {new_name}', flush=True)<N>        os.rename(path, new_name)<N>    os.makedirs(path, exist_ok=True)<N><N>
import yaml<N>from collections import OrderedDict<N>from os import path as osp<N><N><N>def ordered_yaml():<N>    """Support OrderedDict for yaml.<N><N>    Returns:<N>        yaml Loader and Dumper.<N>    """<N>    try:<N>        from yaml import CDumper as Dumper<N>        from yaml import CLoader as Loader<N>    except ImportError:<N>        from yaml import Dumper, Loader<N><N>
    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG<N><N>    def dict_representer(dumper, data):<N>        return dumper.represent_dict(data.items())<N><N>    def dict_constructor(loader, node):<N>        return OrderedDict(loader.construct_pairs(node))<N><N>
    Dumper.add_representer(OrderedDict, dict_representer)<N>    Loader.add_constructor(_mapping_tag, dict_constructor)<N>    return Loader, Dumper<N><N><N>def parse(opt_path, is_train=True):<N>    """Parse option file.<N><N>    Args:<N>        opt_path (str): Option file path.<N>        is_train (str): Indicate whether in training or not. Default: True.<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>## Download training and testing data for single-image motion deblurring task<N>import os<N># import gdown<N>import shutil<N><N>
import argparse<N><N>parser = argparse.ArgumentParser()<N>parser.add_argument('--data', type=str, required=True, help='train, test or train-test')<N>parser.add_argument('--dataset', type=str, default='GoPro', help='all, GoPro, HIDE, RealBlur_R, RealBlur_J')<N>args = parser.parse_args()<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import os<N>import numpy as np<N>from glob import glob<N>from natsort import natsorted<N>from skimage import io<N>import cv2<N>from skimage.metrics import structural_similarity<N>from tqdm import tqdm<N>import concurrent.futures<N><N>
def image_align(deblurred, gt):<N>  # this function is based on kohler evaluation code<N>  z = deblurred<N>  c = np.ones_like(z)<N>  x = gt<N><N>  zs = (np.sum(x * z) / np.sum(z * z)) * z # simple intensity matching<N><N>  warp_mode = cv2.MOTION_HOMOGRAPHY<N>  warp_matrix = np.eye(3, 3, dtype=np.float32)<N><N>
  # Specify the number of iterations.<N>  number_of_iterations = 100<N><N>  termination_eps = 0<N><N>  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,<N>              number_of_iterations, termination_eps)<N><N>  # Run the ECC algorithm. The results are stored in warp_matrix.<N>  (cc, warp_matrix) = cv2.findTransformECC(cv2.cvtColor(x, cv2.COLOR_RGB2GRAY), cv2.cvtColor(zs, cv2.COLOR_RGB2GRAY), warp_matrix, warp_mode, criteria, inputMask=None, gaussFiltSize=5)<N><N>
  target_shape = x.shape<N>  shift = warp_matrix<N><N>  zr = cv2.warpPerspective(<N>    zs,<N>    warp_matrix,<N>    (target_shape[1], target_shape[0]),<N>    flags=cv2.INTER_CUBIC+ cv2.WARP_INVERSE_MAP,<N>    borderMode=cv2.BORDER_REFLECT)<N><N>  cr = cv2.warpPerspective(<N>    np.ones_like(zs, dtype='float32'),<N>    warp_matrix,<N>    (target_shape[1], target_shape[0]),<N>    flags=cv2.INTER_NEAREST+ cv2.WARP_INVERSE_MAP,<N>    borderMode=cv2.BORDER_CONSTANT,<N>    borderValue=0)<N><N>
  zr = zr * cr<N>  xr = x * cr<N><N>  return zr, xr, cr, shift<N><N>def compute_psnr(image_true, image_test, image_mask, data_range=None):<N>  # this function is based on skimage.metrics.peak_signal_noise_ratio<N>  err = np.sum((image_true - image_test) ** 2, dtype=np.float64) / np.sum(image_mask)<N>  return 10 * np.log10((data_range ** 2) / err)<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>##### Data preparation file for training Restormer on the GoPro Dataset ########<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch.nn as nn<N>import torch<N>import torch.nn.functional as F<N>import utils<N><N>from natsort import natsorted<N>from glob import glob<N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>from pdb import set_trace as stx<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import cv2<N>import math<N><N>
def calculate_psnr(img1, img2, border=0):<N>    # img1 and img2 have range [0, 255]<N>    #img1 = img1.squeeze()<N>    #img2 = img2.squeeze()<N>    if not img1.shape == img2.shape:<N>        raise ValueError('Input images must have the same dimensions.')<N>    h, w = img1.shape[:2]<N>    img1 = img1[border:h-border, border:w-border]<N>    img2 = img2[border:h-border, border:w-border]<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>## Download training and testing data for Image Denoising task<N><N>
<N>import os<N># import gdown<N>import shutil<N><N>import argparse<N><N>parser = argparse.ArgumentParser()<N>parser.add_argument('--data', type=str, required=True, help='train, test or train-test')<N>parser.add_argument('--dataset', type=str, default='SIDD', help='all or SIDD or DND')<N>parser.add_argument('--noise', type=str, required=True, help='real or gaussian')<N>args = parser.parse_args()<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import os<N>import numpy as np<N>from glob import glob<N>from natsort import natsorted<N>from skimage import io<N>import cv2<N>import argparse<N>from skimage.metrics import structural_similarity<N>from tqdm import tqdm<N>import concurrent.futures<N>import utils<N><N>
def proc(filename):<N>    tar,prd = filename<N>    tar_img = utils.load_img(tar)<N>    prd_img = utils.load_img(prd)<N>        <N>    PSNR = utils.calculate_psnr(tar_img, prd_img)<N>    # SSIM = utils.calculate_ssim(tar_img, prd_img)<N>    return PSNR<N><N>
parser = argparse.ArgumentParser(description='Gasussian Color Denoising using Restormer')<N><N>parser.add_argument('--model_type', required=True, choices=['non_blind','blind'], type=str, help='blind: single model to handle various noise levels. non_blind: separate model for each noise level.')<N>parser.add_argument('--sigmas', default='15,25,50', type=str, help='Sigma values')<N><N>
args = parser.parse_args()<N><N>sigmas = np.int_(args.sigmas.split(','))<N><N>datasets = ['CBSD68', 'Kodak', 'McMaster','Urban100']<N><N>for dataset in datasets:<N><N>    gt_path = os.path.join('Datasets','test', dataset)<N>    gt_list = natsorted(glob(os.path.join(gt_path, '*.png')) + glob(os.path.join(gt_path, '*.tif')))<N>    assert len(gt_list) != 0, "Target files not found"<N><N>
    for sigma_test in sigmas:<N>        file_path = os.path.join('results', 'Gaussian_Color_Denoising', args.model_type, dataset, str(sigma_test))<N>        path_list = natsorted(glob(os.path.join(file_path, '*.png')) + glob(os.path.join(file_path, '*.tif')))<N>        assert len(path_list) != 0, "Predicted files not found"<N><N>
        psnr, ssim = [], []<N>        img_files =[(i, j) for i,j in zip(gt_list,path_list)]<N>        with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:<N>            for filename, PSNR_SSIM in zip(img_files, executor.map(proc, img_files)):<N>                psnr.append(PSNR_SSIM)<N>                # ssim.append(PSNR_SSIM[1])<N><N>
        avg_psnr = sum(psnr)/len(psnr)<N>        # avg_ssim = sum(ssim)/len(ssim)<N><N>        print('For {:s} dataset Noise Level {:d} PSNR: {:f}\n'.format(dataset, sigma_test, avg_psnr))<N>        # print('For {:s} dataset PSNR: {:f} SSIM: {:f}\n'.format(dataset, avg_psnr, avg_ssim))<N><N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import os<N>import numpy as np<N>from glob import glob<N>from natsort import natsorted<N>from skimage import io<N>import cv2<N>import argparse<N>from skimage.metrics import structural_similarity<N>from tqdm import tqdm<N>import concurrent.futures<N>import utils<N><N>
def proc(filename):<N>    tar,prd = filename<N>    tar_img = utils.load_gray_img(tar)<N>    prd_img = utils.load_gray_img(prd)<N>        <N>    PSNR = utils.calculate_psnr(tar_img, prd_img)<N>    # SSIM = utils.calculate_ssim(tar_img, prd_img)<N>    return PSNR<N><N>
parser = argparse.ArgumentParser(description='Gasussian Grayscale Denoising using Restormer')<N><N>parser.add_argument('--model_type', required=True, choices=['non_blind','blind'], type=str, help='blind: single model to handle various noise levels. non_blind: separate model for each noise level.')<N>parser.add_argument('--sigmas', default='15,25,50', type=str, help='Sigma values')<N><N>
args = parser.parse_args()<N><N>sigmas = np.int_(args.sigmas.split(','))<N><N>datasets = ['Set12', 'BSD68', 'Urban100']<N><N>for dataset in datasets:<N><N>    gt_path = os.path.join('Datasets','test', dataset)<N>    gt_list = natsorted(glob(os.path.join(gt_path, '*.png')) + glob(os.path.join(gt_path, '*.tif')))<N>    assert len(gt_list) != 0, "Target files not found"<N><N>
    for sigma_test in sigmas:<N>        file_path = os.path.join('results', 'Gaussian_Gray_Denoising', args.model_type, dataset, str(sigma_test))<N>        path_list = natsorted(glob(os.path.join(file_path, '*.png')) + glob(os.path.join(file_path, '*.tif')))<N>        assert len(path_list) != 0, "Predicted files not found"<N><N>
        psnr, ssim = [], []<N>        img_files =[(i, j) for i,j in zip(gt_list,path_list)]<N>        with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:<N>            for filename, PSNR_SSIM in zip(img_files, executor.map(proc, img_files)):<N>                psnr.append(PSNR_SSIM)<N>                # ssim.append(PSNR_SSIM[1])<N><N>
        avg_psnr = sum(psnr)/len(psnr)<N>        # avg_ssim = sum(ssim)/len(ssim)<N><N>        print('For {:s} dataset Noise Level {:d} PSNR: {:f}\n'.format(dataset, sigma_test, avg_psnr))<N>        # print('For {:s} dataset PSNR: {:f} SSIM: {:f}\n'.format(dataset, avg_psnr, avg_ssim))<N><N><N>
import cv2<N>import torch<N>import numpy as np<N>from glob import glob<N>from natsort import natsorted<N>import os<N>from tqdm import tqdm<N>from pdb import set_trace as stx<N><N>src = 'Datasets/Downloads'<N>tar = 'Datasets/train/DFWB'<N>os.makedirs(tar, exist_ok=True)<N><N>
import cv2<N>import torch<N>import numpy as np<N>from glob import glob<N>from natsort import natsorted<N>import os<N>from tqdm import tqdm<N>from pdb import set_trace as stx<N><N><N>src = 'Datasets/Downloads/SIDD'<N>tar = 'Datasets/train/SIDD'<N><N>lr_tar = os.path.join(tar, 'input_crops')<N>hr_tar = os.path.join(tar, 'target_crops')<N><N>
os.makedirs(lr_tar, exist_ok=True)<N>os.makedirs(hr_tar, exist_ok=True)<N><N>files = natsorted(glob(os.path.join(src, '*', '*.PNG')))<N><N>lr_files, hr_files = [], []<N>for file_ in files:<N>    filename = os.path.split(file_)[-1]<N>    if 'GT' in filename:<N>        hr_files.append(file_)<N>    if 'NOISY' in filename:<N>        lr_files.append(file_)<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch.nn as nn<N>import torch<N>import torch.nn.functional as F<N><N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>from natsort import natsorted<N>from glob import glob<N>import utils<N>from pdb import set_trace as stx<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch.nn as nn<N>import torch<N>import torch.nn.functional as F<N><N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>from natsort import natsorted<N>from glob import glob<N>import utils<N>from pdb import set_trace as stx<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>import utils<N><N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>import h5py<N>import scipy.io as sio<N>from pdb import set_trace as stx<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>import utils<N><N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>import h5py<N>import scipy.io as sio<N>from pdb import set_trace as stx<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import cv2<N>import math<N><N>
def calculate_psnr(img1, img2, border=0):<N>    # img1 and img2 have range [0, 255]<N>    #img1 = img1.squeeze()<N>    #img2 = img2.squeeze()<N>    if not img1.shape == img2.shape:<N>        raise ValueError('Input images must have the same dimensions.')<N>    h, w = img1.shape[:2]<N>    img1 = img1[border:h-border, border:w-border]<N>    img2 = img2[border:h-border, border:w-border]<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>## Download training and testing data for image deraining task<N>import os<N># import gdown<N>import shutil<N><N>
import argparse<N><N>parser = argparse.ArgumentParser()<N>parser.add_argument('--data', type=str, required=True, help='train, test or train-test')<N>args = parser.parse_args()<N><N>### Google drive IDs ######<N>rain13k_train = '14BidJeG4nSNuFNFDf99K-7eErCq4i47t'   ## https://drive.google.com/file/d/14BidJeG4nSNuFNFDf99K-7eErCq4i47t/view?usp=sharing<N>rain13k_test  = '1P_-RAvltEoEhfT-9GrWRdpEi6NSswTs8'   ## https://drive.google.com/file/d/1P_-RAvltEoEhfT-9GrWRdpEi6NSswTs8/view?usp=sharing<N><N>
for data in args.data.split('-'):<N>    if data == 'train':<N>        print('Rain13K Training Data!')<N>        # gdown.download(id=rain13k_train, output='Datasets/train.zip', quiet=False)<N>        os.system(f'gdrive download {rain13k_train} --path Datasets/')<N>        print('Extracting Rain13K data...')<N>        shutil.unpack_archive('Datasets/train.zip', 'Datasets')<N>        os.remove('Datasets/train.zip')<N><N>
    if data == 'test':<N>        print('Download Deraining Testing Data')<N>        # gdown.download(id=rain13k_test, output='Datasets/test.zip', quiet=False)<N>        os.system(f'gdrive download {rain13k_test} --path Datasets/')<N>        print('Extracting test data...')<N>        shutil.unpack_archive('Datasets/test.zip', 'Datasets')<N>        os.remove('Datasets/test.zip')<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N><N><N>import numpy as np<N>import os<N>import argparse<N>from tqdm import tqdm<N><N>
import torch.nn as nn<N>import torch<N>import torch.nn.functional as F<N>import utils<N><N>from natsort import natsorted<N>from glob import glob<N>from basicsr.models.archs.restormer_arch import Restormer<N>from skimage import img_as_ubyte<N>from pdb import set_trace as stx<N><N>
parser = argparse.ArgumentParser(description='Image Deraining using Restormer')<N><N>parser.add_argument('--input_dir', default='./Datasets/', type=str, help='Directory of validation images')<N>parser.add_argument('--result_dir', default='./results/', type=str, help='Directory for results')<N>parser.add_argument('--weights', default='./pretrained_models/deraining.pth', type=str, help='Path to weights')<N><N>
args = parser.parse_args()<N><N>####### Load yaml #######<N>yaml_file = 'Options/Deraining_Restormer.yml'<N>import yaml<N><N>try:<N>    from yaml import CLoader as Loader<N>except ImportError:<N>    from yaml import Loader<N><N>x = yaml.load(open(yaml_file, mode='r'), Loader=Loader)<N><N>
s = x['network_g'].pop('type')<N>##########################<N><N>model_restoration = Restormer(**x['network_g'])<N><N>checkpoint = torch.load(args.weights)<N>model_restoration.load_state_dict(checkpoint['params'])<N>print("===>Testing using weights: ",args.weights)<N>model_restoration.cuda()<N>model_restoration = nn.DataParallel(model_restoration)<N>model_restoration.eval()<N><N>
## Restormer: Efficient Transformer for High-Resolution Image Restoration<N>## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang<N>## https://arxiv.org/abs/2111.09881<N><N>import numpy as np<N>import os<N>import cv2<N>import math<N><N>
def calculate_psnr(img1, img2, border=0):<N>    # img1 and img2 have range [0, 255]<N>    #img1 = img1.squeeze()<N>    #img2 = img2.squeeze()<N>    if not img1.shape == img2.shape:<N>        raise ValueError('Input images must have the same dimensions.')<N>    h, w = img1.shape[:2]<N>    img1 = img1[border:h-border, border:w-border]<N>    img2 = img2[border:h-border, border:w-border]<N><N>
# Configuration file for the Sphinx documentation builder.<N>#<N># This file only contains a selection of the most common options. For a full<N># list see the documentation:<N># https://www.sphinx-doc.org/en/master/usage/configuration.html<N><N># -- Path setup --------------------------------------------------------------<N><N>
# If extensions (or modules to document with autodoc) are in another directory,<N># add these directories to sys.path here. If the directory is relative to the<N># documentation root, use os.path.abspath to make it absolute, like shown here.<N>#<N>import os<N>import sys<N>sys.path.insert(0, os.path.abspath('.'))<N>sys.path.insert(0, os.path.abspath('..'))<N>sys.path.insert(0, os.path.abspath('./documentation'))<N>sys.path.insert(0, os.path.abspath('./development'))<N><N>
<N># -- Project information -----------------------------------------------------<N><N>project = 'fastero'<N>copyright = '2022, Arian Mollik Wasi'<N>author = 'Arian Mollik Wasi'<N><N># The full version, including alpha/beta/rc tags<N>release = '0.1.0rc1'<N><N>
# -- Pygments lexer patching -------------------------------------------------<N><N>from pygments import token<N>from sphinx.highlighting import lexers<N>from csv_lexer import CsvLexer<N><N>lexers['csv'] = CsvLexer(startinline=True)<N><N># -- General configuration ---------------------------------------------------<N><N>
# Add any Sphinx extension module names here, as strings. They can be<N># extensions coming with Sphinx (named 'sphinx.ext.*') or your custom<N># ones.<N>extensions = [<N>    'sphinx.ext.todo',<N>    'sphinx.ext.intersphinx',<N><N>    "sphinx-favicon",<N>    'sphinx_click',<N>    'sphinx_inline_tabs',<N>    'sphinx_copybutton',<N>    'sphinx_rst_builder',<N>    'sphinxcontrib.details.directive'<N>]<N><N>
#!/usr/bin/env python<N># @Author: SashaChernykh<N># @Editor: fish2000<N># @Date: 2018-06-04 08:01:23<N># @Last Modified by: Arian Mollik Wasi<N># @Last Modified time: 2022-04-12 22:11:43 +0600<N>""" Pygments CSV Lexer csvlexer/csv.py<N>    <N>    * http://pygments.org/docs/lexerdevelopment/<N>    * https://github.com/FSund/pygments-custom-cpplexer<N>"""<N>from __future__ import print_function<N><N>
"""Core file for fastero."""<N>import os<N>import statistics<N><N>from pathlib import Path<N>from math import floor, ceil<N>from typing import List, Optional, Union<N><N>import rich<N>import rich_click as click<N>from rich.console import Console, ConsoleOptions<N>from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn<N>from rich.panel import Panel<N>from rich.rule import Rule<N>from rich.syntax import Syntax<N><N>
from .__init__ import __version__ as VERSION<N>from .utils import (MofNCompleteColumn, StatefulColumn, Time, TIME_FORMAT_UNITS,<N>                    get_code_input, choose_unit, format_snippet, make_bar_plot,<N>                    _Timer as Timer<N>                    )<N>from .exporter import Exporter<N><N>
"""Module for exporting data."""<N><N>import click<N><N>from .utils import choose_unit, format_snippet<N>from rich.console import Console<N>from rich.terminal_theme import TerminalTheme<N><N><N>class Exporter:<N>    """Class for managing and exporting data."""<N><N>
"""A python timeit alternative"""<N><N>__name__ = "fastero"<N>__title__ = __name__<N>__license__ = "MIT"<N>__version__ = "0.1.3"<N>__author__ = "Arian Mollik Wasi"<N>__github__ = "https://github.com/wasi-master/fastero"<N>
#!/usr/bin/python<N># -*- coding: utf-8 -*-<N>"""pefile, Portable Executable reader module<N><N>All the PE file basic structures are available with their default names as<N>attributes of the instance returned.<N><N>Processed elements such as the import table are made available with lowercase<N>names, to differentiate them from the upper case basic structure names.<N><N>
pefile has been tested against many edge cases such as corrupted and malformed<N>PEs as well as malware, which often attempts to abuse the format way beyond its<N>standard use. To the best of my knowledge most of the abuse is handled<N>gracefully.<N><N>
Copyright (c) 2005-2021 Ero Carrera <ero.carrera@gmail.com><N>"""<N><N>__author__ = "Ero Carrera"<N>__version__ = "2021.9.3"<N>__contact__ = "ero.carrera@gmail.com"<N><N>import collections<N>import os<N>import struct<N>import codecs<N>import time<N>import math<N>import string<N>import mmap<N><N>
from collections import Counter<N>from hashlib import sha1<N>from hashlib import sha256<N>from hashlib import sha512<N>from hashlib import md5<N><N>import functools<N>import copy as copymod<N><N>import ordlookup<N><N>codecs.register_error("backslashreplace_", codecs.lookup_error("backslashreplace"))<N><N>
long = int<N># lru_cache with a shallow copy of the objects returned (list, dicts, ..)<N># we don't use deepcopy as it's _really_ slow and the data we retrieved using<N># this is enough with copy.copy taken from<N># https://stackoverflow.com/questions/54909357<N>def lru_cache(maxsize=128, typed=False, copy=False):<N>    if not copy:<N>        return functools.lru_cache(maxsize, typed)<N><N>
    def decorator(f):<N>        cached_func = functools.lru_cache(maxsize, typed)(f)<N><N>        @functools.wraps(f)<N>        def wrapper(*args, **kwargs):<N>            # return copymod.deepcopy(cached_func(*args, **kwargs))<N>            return copymod.copy(cached_func(*args, **kwargs))<N><N>
        return wrapper<N><N>    return decorator<N><N><N>@lru_cache(maxsize=2048)<N>def cache_adjust_FileAlignment(val, file_alignment):<N>    if file_alignment < FILE_ALIGNMENT_HARDCODED_VALUE:<N>        return val<N>    return (int(val / 0x200)) * 0x200<N><N>
<N>@lru_cache(maxsize=2048)<N>def cache_adjust_SectionAlignment(val, section_alignment, file_alignment):<N>    if section_alignment < 0x1000:  # page size<N>        section_alignment = file_alignment<N><N>    # 0x200 is the minimum valid FileAlignment according to the documentation<N>    # although ntoskrnl.exe has an alignment of 0x80 in some Windows versions<N>    #<N>    # elif section_alignment < 0x80:<N>    #    section_alignment = 0x80<N><N>
    if section_alignment and val % section_alignment:<N>        return section_alignment * (int(val / section_alignment))<N>    return val<N><N><N>def count_zeroes(data):<N>    try:<N>        # newbytes' count() takes a str in Python 2<N>        count = data.count("\0")<N>    except TypeError:<N>        # bytes' count() takes an int in Python 3<N>        count = data.count(0)<N>    return count<N><N>
<N>fast_load = False<N><N># This will set a maximum length of a string to be retrieved from the file.<N># It's there to prevent loading massive amounts of data from memory mapped<N># files. Strings longer than 1MB should be rather rare.<N>MAX_STRING_LENGTH = 0x100000  # 2^20<N><N>
# Maximum number of imports to parse.<N>MAX_IMPORT_SYMBOLS = 0x2000<N><N># Limit maximum length for specific string types separately<N>MAX_IMPORT_NAME_LENGTH = 0x200<N>MAX_DLL_LENGTH = 0x200<N>MAX_SYMBOL_NAME_LENGTH = 0x200<N><N># Lmit maximum number of sections before processing of sections will stop<N>MAX_SECTIONS = 0x800<N><N>
# The global maximum number of resource entries to parse per file<N>MAX_RESOURCE_ENTRIES = 0x8000<N><N># The maximum depth of nested resource tables<N>MAX_RESOURCE_DEPTH = 32<N><N># Limit number of exported symbols<N>MAX_SYMBOL_EXPORT_COUNT = 0x2000<N><N>
IMAGE_DOS_SIGNATURE = 0x5A4D<N>IMAGE_DOSZM_SIGNATURE = 0x4D5A<N>IMAGE_NE_SIGNATURE = 0x454E<N>IMAGE_LE_SIGNATURE = 0x454C<N>IMAGE_LX_SIGNATURE = 0x584C<N>IMAGE_TE_SIGNATURE = 0x5A56  # Terse Executables have a 'VZ' signature<N><N>IMAGE_NT_SIGNATURE = 0x00004550<N>IMAGE_NUMBEROF_DIRECTORY_ENTRIES = 16<N>IMAGE_ORDINAL_FLAG = 0x80000000<N>IMAGE_ORDINAL_FLAG64 = 0x8000000000000000<N>OPTIONAL_HEADER_MAGIC_PE = 0x10B<N>OPTIONAL_HEADER_MAGIC_PE_PLUS = 0x20B<N><N>
# -*- coding: Latin-1 -*-<N>"""peutils, Portable Executable utilities module<N><N><N>Copyright (c) 2005-2021 Ero Carrera <ero.carrera@gmail.com><N><N>All rights reserved.<N>"""<N>from __future__ import division<N>from future import standard_library<N><N>
standard_library.install_aliases()<N>from builtins import range<N>from builtins import object<N><N>import os<N>import re<N>import string<N>import urllib.request, urllib.parse, urllib.error<N>import pefile<N><N>__author__ = "Ero Carrera"<N>__version__ = pefile.__version__<N>__contact__ = "ero.carrera@gmail.com"<N><N>
<N>class SignatureDatabase(object):<N>    """This class loads and keeps a parsed PEiD signature database.<N><N>    Usage:<N><N>        sig_db = SignatureDatabase('/path/to/signature/file')<N><N>    and/or<N><N>        sig_db = SignatureDatabase()<N>        sig_db.load('/path/to/signature/file')<N><N>
    Signature databases can be combined by performing multiple loads.<N><N>    The filename parameter can be a URL too. In that case the<N>    signature database will be downloaded from that location.<N>    """<N><N>    def __init__(self, filename=None, data=None):<N><N>
        # RegExp to match a signature block<N>        #<N>        self.parse_sig = re.compile(<N>            "\[(.*?)\]\s+?signature\s*=\s*(.*?)(\s+\?\?)*\s*ep_only\s*=\s*(\w+)(?:\s*section_start_only\s*=\s*(\w+)|)",<N>            re.S,<N>        )<N><N>
"""<N>altgraph.Dot - Interface to the dot language<N>============================================<N><N>The :py:mod:`~altgraph.Dot` module provides a simple interface to the<N>file format used in the<N>`graphviz <http://www.research.att.com/sw/tools/graphviz/>`_<N>program. The module is intended to offload the most tedious part of the process<N>(the **dot** file generation) while transparently exposing most of its<N>features.<N><N>
To display the graphs or to generate image files the<N>`graphviz <http://www.research.att.com/sw/tools/graphviz/>`_<N>package needs to be installed on the system, moreover the :command:`dot` and<N>:command:`dotty` programs must be accesible in the program path so that they<N>can be ran from processes spawned within the module.<N><N>
Example usage<N>-------------<N><N>Here is a typical usage::<N><N>    from altgraph import Graph, Dot<N><N>    # create a graph<N>    edges = [ (1,2), (1,3), (3,4), (3,5), (4,5), (5,4) ]<N>    graph = Graph.Graph(edges)<N><N>    # create a dot representation of the graph<N>    dot = Dot.Dot(graph)<N><N>
    # display the graph<N>    dot.display()<N><N>    # save the dot representation into the mydot.dot file<N>    dot.save_dot(file_name='mydot.dot')<N><N>    # save dot file as gif image into the graph.gif file<N>    dot.save_img(file_name='graph', file_type='gif')<N><N>
Directed graph and non-directed graph<N>-------------------------------------<N><N>Dot class can use for both directed graph and non-directed graph<N>by passing ``graphtype`` parameter.<N><N>Example::<N><N>    # create directed graph(default)<N>    dot = Dot.Dot(graph, graphtype="digraph")<N><N>
    # create non-directed graph<N>    dot = Dot.Dot(graph, graphtype="graph")<N><N>Customizing the output<N>----------------------<N><N>The graph drawing process may be customized by passing<N>valid :command:`dot` parameters for the nodes and edges. For a list of all<N>parameters see the `graphviz <http://www.research.att.com/sw/tools/graphviz/>`_<N>documentation.<N><N>
Example::<N><N>    # customizing the way the overall graph is drawn<N>    dot.style(size='10,10', rankdir='RL', page='5, 5' , ranksep=0.75)<N><N>    # customizing node drawing<N>    dot.node_style(1, label='BASE_NODE',shape='box', color='blue' )<N>    dot.node_style(2, style='filled', fillcolor='red')<N><N>
    # customizing edge drawing<N>    dot.edge_style(1, 2, style='dotted')<N>    dot.edge_style(3, 5, arrowhead='dot', label='binds', labelangle='90')<N>    dot.edge_style(4, 5, arrowsize=2, style='bold')<N><N><N>.. note::<N><N>   dotty (invoked via :py:func:`~altgraph.Dot.display`) may not be able to<N>   display all graphics styles. To verify the output save it to an image file<N>   and look at it that way.<N><N>
Valid attributes<N>----------------<N><N>    - dot styles, passed via the :py:meth:`Dot.style` method::<N><N>        rankdir = 'LR'   (draws the graph horizontally, left to right)<N>        ranksep = number (rank separation in inches)<N><N>    - node attributes, passed via the :py:meth:`Dot.node_style` method::<N><N>
        style = 'filled' | 'invisible' | 'diagonals' | 'rounded'<N>        shape = 'box' | 'ellipse' | 'circle' | 'point' | 'triangle'<N><N>    - edge attributes, passed via the :py:meth:`Dot.edge_style` method::<N><N>        style     = 'dashed' | 'dotted' | 'solid' | 'invis' | 'bold'<N>        arrowhead = 'box' | 'crow' | 'diamond' | 'dot' | 'inv' | 'none'<N>            | 'tee' | 'vee'<N>        weight    = number (the larger the number the closer the nodes will be)<N><N>
    - valid `graphviz colors<N>        <http://www.research.att.com/~erg/graphviz/info/colors.html>`_<N><N>    - for more details on how to control the graph drawing process see the<N>      `graphviz reference<N>        <http://www.research.att.com/sw/tools/graphviz/refs.html>`_.<N>"""<N>import os<N>import warnings<N><N>
from altgraph import GraphError<N><N><N>class Dot(object):<N>    """<N>    A  class providing a **graphviz** (dot language) representation<N>    allowing a fine grained control over how the graph is being<N>    displayed.<N><N>    If the :command:`dot` and :command:`dotty` programs are not in the current<N>    system path their location needs to be specified in the contructor.<N>    """<N><N>
    def __init__(<N>        self,<N>        graph=None,<N>        nodes=None,<N>        edgefn=None,<N>        nodevisitor=None,<N>        edgevisitor=None,<N>        name="G",<N>        dot="dot",<N>        dotty="dotty",<N>        neato="neato",<N>        graphtype="digraph",<N>    ):<N>        """<N>        Initialization.<N>        """<N>        self.name, self.attr = name, {}<N><N>
        assert graphtype in ["graph", "digraph"]<N>        self.type = graphtype<N><N>        self.temp_dot = "tmp_dot.dot"<N>        self.temp_neo = "tmp_neo.dot"<N><N>        self.dot, self.dotty, self.neato = dot, dotty, neato<N><N>        # self.nodes: node styles<N>        # self.edges: edge styles<N>        self.nodes, self.edges = {}, {}<N><N>
        if graph is not None and nodes is None:<N>            nodes = graph<N>        if graph is not None and edgefn is None:<N><N>            def edgefn(node, graph=graph):<N>                return graph.out_nbrs(node)<N><N>        if nodes is None:<N>            nodes = ()<N><N>
"""<N>altgraph.Graph - Base Graph class<N>=================================<N><N>..<N>  #--Version 2.1<N>  #--Bob Ippolito October, 2004<N><N>  #--Version 2.0<N>  #--Istvan Albert June, 2004<N><N>  #--Version 1.0<N>  #--Nathan Denny, May 27, 1999<N>"""<N><N>
from collections import deque<N><N>from altgraph import GraphError<N><N><N>class Graph(object):<N>    """<N>    The Graph class represents a directed graph with *N* nodes and *E* edges.<N><N>    Naming conventions:<N><N>    - the prefixes such as *out*, *inc* and *all* will refer to methods<N>      that operate on the outgoing, incoming or all edges of that node.<N><N>
      For example: :py:meth:`inc_degree` will refer to the degree of the node<N>      computed over the incoming edges (the number of neighbours linking to<N>      the node).<N><N>    - the prefixes such as *forw* and *back* will refer to the<N>      orientation of the edges used in the method with respect to the node.<N><N>
      For example: :py:meth:`forw_bfs` will start at the node then use the<N>      outgoing edges to traverse the graph (goes forward).<N>    """<N><N>    def __init__(self, edges=None):<N>        """<N>        Initialization<N>        """<N><N>        self.next_edge = 0<N>        self.nodes, self.edges = {}, {}<N>        self.hidden_edges, self.hidden_nodes = {}, {}<N><N>
        if edges is not None:<N>            for item in edges:<N>                if len(item) == 2:<N>                    head, tail = item<N>                    self.add_edge(head, tail)<N>                elif len(item) == 3:<N>                    head, tail, data = item<N>                    self.add_edge(head, tail, data)<N>                else:<N>                    raise GraphError("Cannot create edge from %s" % (item,))<N><N>
    def __repr__(self):<N>        return "<Graph: %d nodes, %d edges>" % (<N>            self.number_of_nodes(),<N>            self.number_of_edges(),<N>        )<N><N>    def add_node(self, node, node_data=None):<N>        """<N>        Adds a new node to the graph.  Arbitrary data can be attached to the<N>        node via the node_data parameter.  Adding the same node twice will be<N>        silently ignored.<N><N>
        The node must be a hashable value.<N>        """<N>        #<N>        # the nodes will contain tuples that will store incoming edges,<N>        # outgoing edges and data<N>        #<N>        # index 0 -> incoming edges<N>        # index 1 -> outgoing edges<N><N>
        if node in self.hidden_nodes:<N>            # Node is present, but hidden<N>            return<N><N>        if node not in self.nodes:<N>            self.nodes[node] = ([], [], node_data)<N><N>    def add_edge(self, head_id, tail_id, edge_data=1, create_nodes=True):<N>        """<N>        Adds a directed edge going from head_id to tail_id.<N>        Arbitrary data can be attached to the edge via edge_data.<N>        It may create the nodes if adding edges between nonexisting ones.<N><N>
        :param head_id: head node<N>        :param tail_id: tail node<N>        :param edge_data: (optional) data attached to the edge<N>        :param create_nodes: (optional) creates the head_id or tail_id<N>            node in case they did not exist<N>        """<N>        # shorcut<N>        edge = self.next_edge<N><N>
        # add nodes if on automatic node creation<N>        if create_nodes:<N>            self.add_node(head_id)<N>            self.add_node(tail_id)<N><N>        # update the corresponding incoming and outgoing lists in the nodes<N>        # index 0 -> incoming edges<N>        # index 1 -> outgoing edges<N><N>
        try:<N>            self.nodes[tail_id][0].append(edge)<N>            self.nodes[head_id][1].append(edge)<N>        except KeyError:<N>            raise GraphError("Invalid nodes %s -> %s" % (head_id, tail_id))<N><N>        # store edge information<N>        self.edges[edge] = (head_id, tail_id, edge_data)<N><N>
"""<N>altgraph.GraphAlgo - Graph algorithms<N>=====================================<N>"""<N>from altgraph import GraphError<N><N><N>def dijkstra(graph, start, end=None):<N>    """<N>    Dijkstra's algorithm for shortest paths<N><N>    `David Eppstein, UC Irvine, 4 April 2002<N>        <http://www.ics.uci.edu/~eppstein/161/python/>`_<N><N>
"""<N>altgraph.GraphStat - Functions providing various graph statistics<N>=================================================================<N>"""<N><N><N>def degree_dist(graph, limits=(0, 0), bin_num=10, mode="out"):<N>    """<N>    Computes the degree distribution for a graph.<N><N>
    Returns a list of tuples where the first element of the tuple is the<N>    center of the bin representing a range of degrees and the second element<N>    of the tuple are the number of nodes with the degree falling in the range.<N><N>    Example::<N><N>
        ....<N>    """<N><N>    deg = []<N>    if mode == "inc":<N>        get_deg = graph.inc_degree<N>    else:<N>        get_deg = graph.out_degree<N><N>    for node in graph:<N>        deg.append(get_deg(node))<N><N>    if not deg:<N>        return []<N><N>
    results = _binning(values=deg, limits=limits, bin_num=bin_num)<N><N>    return results<N><N><N>_EPS = 1.0 / (2.0 ** 32)<N><N><N>def _binning(values, limits=(0, 0), bin_num=10):<N>    """<N>    Bins data that falls between certain limits, if the limits are (0, 0) the<N>    minimum and maximum values are used.<N><N>
    Returns a list of tuples where the first element of the tuple is the<N>    center of the bin and the second element of the tuple are the counts.<N>    """<N>    if limits == (0, 0):<N>        min_val, max_val = min(values) - _EPS, max(values) + _EPS<N>    else:<N>        min_val, max_val = limits<N><N>
    # get bin size<N>    bin_size = (max_val - min_val) / float(bin_num)<N>    bins = [0] * (bin_num)<N><N>    # will ignore these outliers for now<N>    for value in values:<N>        try:<N>            if (value - min_val) >= 0:<N>                index = int((value - min_val) / float(bin_size))<N>                bins[index] += 1<N>        except IndexError:<N>            pass<N><N>
"""<N>altgraph.GraphUtil - Utility classes and functions<N>==================================================<N>"""<N><N>import random<N>from collections import deque<N><N>from altgraph import Graph, GraphError<N><N><N>def generate_random_graph(node_num, edge_num, self_loops=False, multi_edges=False):<N>    """<N>    Generates and returns a :py:class:`~altgraph.Graph.Graph` instance with<N>    *node_num* nodes randomly connected by *edge_num* edges.<N>    """<N>    g = Graph.Graph()<N><N>
    if not multi_edges:<N>        if self_loops:<N>            max_edges = node_num * node_num<N>        else:<N>            max_edges = node_num * (node_num - 1)<N><N>        if edge_num > max_edges:<N>            raise GraphError("inconsistent arguments to 'generate_random_graph'")<N><N>
    nodes = range(node_num)<N><N>    for node in nodes:<N>        g.add_node(node)<N><N>    while 1:<N>        head = random.choice(nodes)<N>        tail = random.choice(nodes)<N><N>        # loop defense<N>        if head == tail and not self_loops:<N>            continue<N><N>
        # multiple edge defense<N>        if g.edge_by_node(head, tail) is not None and not multi_edges:<N>            continue<N><N>        # add the edge<N>        g.add_edge(head, tail)<N>        if g.number_of_edges() >= edge_num:<N>            break<N><N>
"""<N>altgraph.ObjectGraph - Graph of objects with an identifier<N>==========================================================<N><N>A graph of objects that have a "graphident" attribute.<N>graphident is the key for the object in the graph<N>"""<N><N>from altgraph import GraphError<N>from altgraph.Graph import Graph<N>from altgraph.GraphUtil import filter_stack<N><N>
<N>class ObjectGraph(object):<N>    """<N>    A graph of objects that have a "graphident" attribute.<N>    graphident is the key for the object in the graph<N>    """<N><N>    def __init__(self, graph=None, debug=0):<N>        if graph is None:<N>            graph = Graph()<N>        self.graphident = self<N>        self.graph = graph<N>        self.debug = debug<N>        self.indent = 0<N>        graph.add_node(self, None)<N><N>
    def __repr__(self):<N>        return "<%s>" % (type(self).__name__,)<N><N>    def flatten(self, condition=None, start=None):<N>        """<N>        Iterate over the subgraph that is entirely reachable by condition<N>        starting from the given start node or the ObjectGraph root<N>        """<N>        if start is None:<N>            start = self<N>        start = self.getRawIdent(start)<N>        return self.graph.iterdata(start=start, condition=condition)<N><N>
    def nodes(self):<N>        for ident in self.graph:<N>            node = self.graph.node_data(ident)<N>            if node is not None:<N>                yield self.graph.node_data(ident)<N><N>    def get_edges(self, node):<N>        if node is None:<N>            node = self<N>        start = self.getRawIdent(node)<N>        _, _, outraw, incraw = self.graph.describe_node(start)<N><N>
        def iter_edges(lst, n):<N>            seen = set()<N>            for tpl in (self.graph.describe_edge(e) for e in lst):<N>                ident = tpl[n]<N>                if ident not in seen:<N>                    yield self.findNode(ident)<N>                    seen.add(ident)<N><N>
        return iter_edges(outraw, 3), iter_edges(incraw, 2)<N><N>    def edgeData(self, fromNode, toNode):<N>        if fromNode is None:<N>            fromNode = self<N>        start = self.getRawIdent(fromNode)<N>        stop = self.getRawIdent(toNode)<N>        edge = self.graph.edge_by_node(start, stop)<N>        return self.graph.edge_data(edge)<N><N>
    def updateEdgeData(self, fromNode, toNode, edgeData):<N>        if fromNode is None:<N>            fromNode = self<N>        start = self.getRawIdent(fromNode)<N>        stop = self.getRawIdent(toNode)<N>        edge = self.graph.edge_by_node(start, stop)<N>        self.graph.update_edge_data(edge, edgeData)<N><N>
    def filterStack(self, filters):<N>        """<N>        Filter the ObjectGraph in-place by removing all edges to nodes that<N>        do not match every filter in the given filter list<N><N>        Returns a tuple containing the number of:<N>            (nodes_visited, nodes_removed, nodes_orphaned)<N>        """<N>        visited, removes, orphans = filter_stack(self.graph, self, filters)<N><N>
        for last_good, tail in orphans:<N>            self.graph.add_edge(last_good, tail, edge_data="orphan")<N><N>        for node in removes:<N>            self.graph.hide_node(node)<N><N>        return len(visited) - 1, len(removes), len(orphans)<N><N>
"""<N>altgraph - a python graph library<N>=================================<N><N>altgraph is a fork of `graphlib <http://pygraphlib.sourceforge.net>`_ tailored<N>to use newer Python 2.3+ features, including additional support used by the<N>py2app suite (modulegraph and macholib, specifically).<N><N>
altgraph is a python based graph (network) representation and manipulation<N>package.  It has started out as an extension to the<N>`graph_lib module<N><http://www.ece.arizona.edu/~denny/python_nest/graph_lib_1.0.1.html>`_<N>written by Nathan Denny it has been significantly optimized and expanded.<N><N>
The :class:`altgraph.Graph.Graph` class is loosely modeled after the<N>`LEDA <http://www.algorithmic-solutions.com/enleda.htm>`_<N>(Library of Efficient Datatypes) representation. The library<N>includes methods for constructing graphs, BFS and DFS traversals,<N>topological sort, finding connected components, shortest paths as well as a<N>number graph statistics functions. The library can also visualize graphs<N>via `graphviz <http://www.research.att.com/sw/tools/graphviz/>`_.<N><N>
The package contains the following modules:<N><N>    -  the :py:mod:`altgraph.Graph` module contains the<N>       :class:`~altgraph.Graph.Graph` class that stores the graph data<N><N>    -  the :py:mod:`altgraph.GraphAlgo` module implements graph algorithms<N>       operating on graphs (:py:class:`~altgraph.Graph.Graph`} instances)<N><N>
    -  the :py:mod:`altgraph.GraphStat` module contains functions for<N>       computing statistical measures on graphs<N><N>    -  the :py:mod:`altgraph.GraphUtil` module contains functions for<N>       generating, reading and saving graphs<N><N>    -  the :py:mod:`altgraph.Dot` module  contains functions for displaying<N>       graphs via `graphviz <http://www.research.att.com/sw/tools/graphviz/>`_<N><N>
    -  the :py:mod:`altgraph.ObjectGraph` module implements a graph of<N>       objects with a unique identifier<N><N>Installation<N>------------<N><N>Download and unpack the archive then type::<N><N>    python setup.py install<N><N>This will install the library in the default location. For instructions on<N>how to customize the install procedure read the output of::<N><N>
    python setup.py --help install<N><N>To verify that the code works run the test suite::<N><N>    python setup.py test<N><N>Example usage<N>-------------<N><N>Lets assume that we want to analyze the graph below (links to the full picture)<N>GRAPH_IMG.  Our script then might look the following way::<N><N>
    from altgraph import Graph, GraphAlgo, Dot<N><N>    # these are the edges<N>    edges = [ (1,2), (2,4), (1,3), (2,4), (3,4), (4,5), (6,5),<N>        (6,14), (14,15), (6, 15),  (5,7), (7, 8), (7,13), (12,8),<N>        (8,13), (11,12), (11,9), (13,11), (9,13), (13,10) ]<N><N>
    # creates the graph<N>    graph = Graph.Graph()<N>    for head, tail in edges:<N>        graph.add_edge(head, tail)<N><N>    # do a forward bfs from 1 at most to 20<N>    print(graph.forw_bfs(1))<N><N>This will print the nodes in some breadth first order::<N><N>
    [1, 2, 3, 4, 5, 7, 8, 13, 11, 10, 12, 9]<N><N>If we wanted to get the hop-distance from node 1 to node 8<N>we coud write::<N><N>    print(graph.get_hops(1, 8))<N><N>This will print the following::<N><N>    [(1, 0), (2, 1), (3, 1), (4, 2), (5, 3), (7, 4), (8, 5)]<N><N>
Node 1 is at 0 hops since it is the starting node, nodes 2,3 are 1 hop away ...<N>node 8 is 5 hops away. To find the shortest distance between two nodes you<N>can use::<N><N>    print(GraphAlgo.shortest_path(graph, 1, 12))<N><N>It will print the nodes on one (if there are more) the shortest paths::<N><N>
    [1, 2, 4, 5, 7, 13, 11, 12]<N><N>To display the graph we can use the GraphViz backend::<N><N>    dot = Dot.Dot(graph)<N><N>    # display the graph on the monitor<N>    dot.display()<N><N>    # save it in an image file<N>    dot.save_img(file_name='graph', file_type='gif')<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>certifi.py<N>~~~~~~~~~~<N><N>This module returns the installation location of cacert.pem or its contents.<N>"""<N>import os<N><N>try:<N>    from importlib.resources import path as get_path, read_text<N><N>    _CACERT_CTX = None<N>    _CACERT_PATH = None<N><N>
import argparse<N><N>from certifi import contents, where<N><N>parser = argparse.ArgumentParser()<N>parser.add_argument("-c", "--contents", action="store_true")<N>args = parser.parse_args()<N><N>if args.contents:<N>    print(contents())<N>else:<N>    print(where())<N>
from codecs import BOM_UTF8, BOM_UTF16_BE, BOM_UTF16_LE, BOM_UTF32_BE, BOM_UTF32_LE<N>from collections import OrderedDict<N>from encodings.aliases import aliases<N>from re import IGNORECASE, compile as re_compile<N>from typing import Dict, List, Set, Union<N><N>
from functools import lru_cache<N>from typing import List, Optional<N><N>from .constant import COMMON_SAFE_ASCII_CHARACTERS, UNICODE_SECONDARY_RANGE_KEYWORD<N>from .utils import (<N>    is_accentuated,<N>    is_ascii,<N>    is_case_variable,<N>    is_cjk,<N>    is_emoticon,<N>    is_hangul,<N>    is_hiragana,<N>    is_katakana,<N>    is_latin,<N>    is_punctuation,<N>    is_separator,<N>    is_symbol,<N>    is_thai,<N>    remove_accent,<N>    unicode_range,<N>)<N><N>
<N>class MessDetectorPlugin:<N>    """<N>    Base abstract class used for mess detection plugins.<N>    All detectors MUST extend and implement given methods.<N>    """<N><N>    def eligible(self, character: str) -> bool:<N>        """<N>        Determine if given character should be fed in.<N>        """<N>        raise NotImplementedError  # pragma: nocover<N><N>
    def feed(self, character: str) -> None:<N>        """<N>        The main routine to be executed upon character.<N>        Insert the logic in witch the text would be considered chaotic.<N>        """<N>        raise NotImplementedError  # pragma: nocover<N><N>
    def reset(self) -> None:  # pragma: no cover<N>        """<N>        Permit to reset the plugin to the initial state.<N>        """<N>        raise NotImplementedError<N><N>    @property<N>    def ratio(self) -> float:<N>        """<N>        Compute the chaos ratio based on what your feed() has seen.<N>        Must NOT be lower than 0.; No restriction gt 0.<N>        """<N>        raise NotImplementedError  # pragma: nocover<N><N>
<N>class TooManySymbolOrPunctuationPlugin(MessDetectorPlugin):<N>    def __init__(self) -> None:<N>        self._punctuation_count = 0  # type: int<N>        self._symbol_count = 0  # type: int<N>        self._character_count = 0  # type: int<N><N>        self._last_printable_char = None  # type: Optional[str]<N>        self._frenzy_symbol_in_word = False  # type: bool<N><N>
import warnings<N>from collections import Counter<N>from encodings.aliases import aliases<N>from hashlib import sha256<N>from json import dumps<N>from re import sub<N>from typing import Any, Dict, Iterator, List, Optional, Tuple, Union<N><N>from .constant import NOT_PRINTABLE_PATTERN, TOO_BIG_SEQUENCE<N>from .md import mess_ratio<N>from .utils import iana_name, is_multi_byte_encoding, unicode_range<N><N>
<N>class CharsetMatch:<N>    def __init__(<N>        self,<N>        payload: bytes,<N>        guessed_encoding: str,<N>        mean_mess_ratio: float,<N>        has_sig_or_bom: bool,<N>        languages: "CoherenceMatches",<N>        decoded_payload: Optional[str] = None,<N>    ):<N>        self._payload = payload  # type: bytes<N><N>
        self._encoding = guessed_encoding  # type: str<N>        self._mean_mess_ratio = mean_mess_ratio  # type: float<N>        self._languages = languages  # type: CoherenceMatches<N>        self._has_sig_or_bom = has_sig_or_bom  # type: bool<N>        self._unicode_ranges = None  # type: Optional[List[str]]<N><N>
        self._leaves = []  # type: List[CharsetMatch]<N>        self._mean_coherence_ratio = 0.0  # type: float<N><N>        self._output_payload = None  # type: Optional[bytes]<N>        self._output_encoding = None  # type: Optional[str]<N><N>        self._string = decoded_payload  # type: Optional[str]<N><N>
    def __eq__(self, other: object) -> bool:<N>        if not isinstance(other, CharsetMatch):<N>            raise TypeError(<N>                "__eq__ cannot be invoked on {} and {}.".format(<N>                    str(other.__class__), str(self.__class__)<N>                )<N>            )<N>        return self.encoding == other.encoding and self.fingerprint == other.fingerprint<N><N>
    def __lt__(self, other: object) -> bool:<N>        """<N>        Implemented to make sorted available upon CharsetMatches items.<N>        """<N>        if not isinstance(other, CharsetMatch):<N>            raise ValueError<N><N>        chaos_difference = abs(self.chaos - other.chaos)  # type: float<N>        coherence_difference = abs(self.coherence - other.coherence)  # type: float<N><N>
        # Bellow 1% difference --> Use Coherence<N>        if chaos_difference < 0.01 and coherence_difference > 0.02:<N>            # When having a tough decision, use the result that decoded as many multi-byte as possible.<N>            if chaos_difference == 0.0 and self.coherence == other.coherence:<N>                return self.multi_byte_usage > other.multi_byte_usage<N>            return self.coherence > other.coherence<N><N>
try:<N>    import unicodedata2 as unicodedata<N>except ImportError:<N>    import unicodedata  # type: ignore[no-redef]<N><N>import importlib<N>import logging<N>from codecs import IncrementalDecoder<N>from encodings.aliases import aliases<N>from functools import lru_cache<N>from re import findall<N>from typing import List, Optional, Set, Tuple, Union<N><N>
from _multibytecodec import MultibyteIncrementalDecoder  # type: ignore<N><N>from .constant import (<N>    ENCODING_MARKS,<N>    IANA_SUPPORTED_SIMILAR,<N>    RE_POSSIBLE_ENCODING_INDICATION,<N>    UNICODE_RANGES_COMBINED,<N>    UNICODE_SECONDARY_RANGE_KEYWORD,<N>    UTF8_MAXIMAL_ALLOCATION,<N>)<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_accentuated(character: str) -> bool:<N>    try:<N>        description = unicodedata.name(character)  # type: str<N>    except ValueError:<N>        return False<N>    return (<N>        "WITH GRAVE" in description<N>        or "WITH ACUTE" in description<N>        or "WITH CEDILLA" in description<N>        or "WITH DIAERESIS" in description<N>        or "WITH CIRCUMFLEX" in description<N>        or "WITH TILDE" in description<N>    )<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def remove_accent(character: str) -> str:<N>    decomposed = unicodedata.decomposition(character)  # type: str<N>    if not decomposed:<N>        return character<N><N>    codes = decomposed.split(" ")  # type: List[str]<N><N>
    return chr(int(codes[0], 16))<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def unicode_range(character: str) -> Optional[str]:<N>    """<N>    Retrieve the Unicode range official name from a single character.<N>    """<N>    character_ord = ord(character)  # type: int<N><N>
    for range_name, ord_range in UNICODE_RANGES_COMBINED.items():<N>        if character_ord in ord_range:<N>            return range_name<N><N>    return None<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_latin(character: str) -> bool:<N>    try:<N>        description = unicodedata.name(character)  # type: str<N>    except ValueError:<N>        return False<N>    return "LATIN" in description<N><N>
<N>def is_ascii(character: str) -> bool:<N>    try:<N>        character.encode("ascii")<N>    except UnicodeEncodeError:<N>        return False<N>    return True<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_punctuation(character: str) -> bool:<N>    character_category = unicodedata.category(character)  # type: str<N><N>
    if "P" in character_category:<N>        return True<N><N>    character_range = unicode_range(character)  # type: Optional[str]<N><N>    if character_range is None:<N>        return False<N><N>    return "Punctuation" in character_range<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_symbol(character: str) -> bool:<N>    character_category = unicodedata.category(character)  # type: str<N><N>
    if "S" in character_category or "N" in character_category:<N>        return True<N><N>    character_range = unicode_range(character)  # type: Optional[str]<N><N>    if character_range is None:<N>        return False<N><N>    return "Forms" in character_range<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_emoticon(character: str) -> bool:<N>    character_range = unicode_range(character)  # type: Optional[str]<N><N>    if character_range is None:<N>        return False<N><N>    return "Emoticons" in character_range<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_separator(character: str) -> bool:<N>    if character.isspace() or character in {"｜", "+", ",", ";", "<", ">"}:<N>        return True<N><N>    character_category = unicodedata.category(character)  # type: str<N><N>
    return "Z" in character_category<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_case_variable(character: str) -> bool:<N>    return character.islower() != character.isupper()<N><N><N>def is_private_use_only(character: str) -> bool:<N>    character_category = unicodedata.category(character)  # type: str<N><N>
    return character_category == "Co"<N><N><N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_cjk(character: str) -> bool:<N>    try:<N>        character_name = unicodedata.name(character)<N>    except ValueError:<N>        return False<N><N>    return "CJK" in character_name<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_hiragana(character: str) -> bool:<N>    try:<N>        character_name = unicodedata.name(character)<N>    except ValueError:<N>        return False<N><N>    return "HIRAGANA" in character_name<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_katakana(character: str) -> bool:<N>    try:<N>        character_name = unicodedata.name(character)<N>    except ValueError:<N>        return False<N><N>    return "KATAKANA" in character_name<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_hangul(character: str) -> bool:<N>    try:<N>        character_name = unicodedata.name(character)<N>    except ValueError:<N>        return False<N><N>    return "HANGUL" in character_name<N><N>
<N>@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)<N>def is_thai(character: str) -> bool:<N>    try:<N>        character_name = unicodedata.name(character)<N>    except ValueError:<N>        return False<N><N>    return "THAI" in character_name<N><N><N>@lru_cache(maxsize=len(UNICODE_RANGES_COMBINED))<N>def is_unicode_range_secondary(range_name: str) -> bool:<N>    return any(keyword in range_name for keyword in UNICODE_SECONDARY_RANGE_KEYWORD)<N><N>
<N>def any_specified_encoding(sequence: bytes, search_zone: int = 4096) -> Optional[str]:<N>    """<N>    Extract using ASCII-only decoder any specified encoding in the first n-bytes.<N>    """<N>    if not isinstance(sequence, bytes):<N>        raise TypeError<N><N>
    seq_len = len(sequence)  # type: int<N><N>    results = findall(<N>        RE_POSSIBLE_ENCODING_INDICATION,<N>        sequence[: min(seq_len, search_zone)].decode("ascii", errors="ignore"),<N>    )  # type: List[str]<N><N>    if len(results) == 0:<N>        return None<N><N>
    for specified_encoding in results:<N>        specified_encoding = specified_encoding.lower().replace("-", "_")<N><N>        for encoding_alias, encoding_iana in aliases.items():<N>            if encoding_alias == specified_encoding:<N>                return encoding_iana<N>            if encoding_iana == specified_encoding:<N>                return encoding_iana<N><N>
import argparse<N>import sys<N>from json import dumps<N>from os.path import abspath<N>from platform import python_version<N>from typing import List<N><N>from charset_normalizer import from_fp<N>from charset_normalizer.models import CliDetectionResult<N>from charset_normalizer.version import __version__<N><N>
<N>def query_yes_no(question: str, default: str = "yes") -> bool:<N>    """Ask a yes/no question via input() and return their answer.<N><N>    "question" is a string that is presented to the user.<N>    "default" is the presumed answer if the user just hits <Enter>.<N>        It must be "yes" (the default), "no" or None (meaning<N>        an answer is required of the user).<N><N>
"""<N>future: Easy, safe support for Python 2/3 compatibility<N>=======================================================<N><N>``future`` is the missing compatibility layer between Python 2 and Python<N>3. It allows you to use a single, clean Python 3.x-compatible codebase to<N>support both Python 2 and Python 3 with minimal overhead.<N><N>
It is designed to be used as follows::<N><N>    from __future__ import (absolute_import, division,<N>                            print_function, unicode_literals)<N>    from builtins import (<N>             bytes, dict, int, list, object, range, str,<N>             ascii, chr, hex, input, next, oct, open,<N>             pow, round, super,<N>             filter, map, zip)<N><N>
followed by predominantly standard, idiomatic Python 3 code that then runs<N>similarly on Python 2.6/2.7 and Python 3.3+.<N><N>The imports have no effect on Python 3. On Python 2, they shadow the<N>corresponding builtins, which normally have different semantics on Python 3<N>versus 2, to provide their Python 3 semantics.<N><N>
<N>Standard library reorganization<N>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>``future`` supports the standard library reorganization (PEP 3108) through the<N>following Py3 interfaces:<N><N>    >>> # Top-level packages with Py3 names provided on Py2:<N>    >>> import html.parser<N>    >>> import queue<N>    >>> import tkinter.dialog<N>    >>> import xmlrpc.client<N>    >>> # etc.<N><N>
    >>> # Aliases provided for extensions to existing Py2 module names:<N>    >>> from future.standard_library import install_aliases<N>    >>> install_aliases()<N><N>    >>> from collections import Counter, OrderedDict   # backported to Py2.6<N>    >>> from collections import UserDict, UserList, UserString<N>    >>> import urllib.request<N>    >>> from itertools import filterfalse, zip_longest<N>    >>> from subprocess import getoutput, getstatusoutput<N><N>
<N>Automatic conversion<N>--------------------<N><N>An included script called `futurize<N><http://python-future.org/automatic_conversion.html>`_ aids in converting<N>code (from either Python 2 or Python 3) to code compatible with both<N>platforms. It is similar to ``python-modernize`` but goes further in<N>providing Python 3 compatibility through the use of the backported types<N>and builtin functions in ``future``.<N><N>
<N>Documentation<N>-------------<N><N>See: http://python-future.org<N><N><N>Credits<N>-------<N><N>:Author:  Ed Schofield, Jordan M. Adler, et al<N>:Sponsor: Python Charmers Pty Ltd, Australia, and Python Charmers Pte<N>          Ltd, Singapore. http://pythoncharmers.com<N>:Others:  See docs/credits.rst or http://python-future.org/credits.html<N><N>
# Wrapper module for _socket, providing some additional facilities<N># implemented in Python.<N><N>"""\<N>This module provides socket operations and some related functions.<N>On Unix, it supports IP (Internet Protocol) and Unix domain sockets.<N>On other systems, it only supports IP. Functions specific for a<N>socket are available as methods of the socket object.<N><N>
"""Generic socket server classes.<N><N>This module tries to capture the various aspects of defining a server:<N><N>For socket-based servers:<N><N>- address family:<N>        - AF_INET{,6}: IP (Internet Protocol) sockets (default)<N>        - AF_UNIX: Unix domain sockets<N>        - others, e.g. AF_DECNET are conceivable (see <socket.h><N>- socket type:<N>        - SOCK_STREAM (reliable stream, e.g. TCP)<N>        - SOCK_DGRAM (datagrams, e.g. UDP)<N><N>
For request-based servers (including socket-based):<N><N>- client address verification before further looking at the request<N>        (This is actually a hook for any processing that needs to look<N>         at the request before anything else, e.g. logging)<N>- how to handle multiple requests:<N>        - synchronous (one request is handled at a time)<N>        - forking (each request is handled by a new process)<N>        - threading (each request is handled by a new thread)<N><N>
The classes in this module favor the server type that is simplest to<N>write: a synchronous TCP/IP server.  This is bad class design, but<N>save some typing.  (There's also the issue that a deep class hierarchy<N>slows down method lookups.)<N><N>There are five classes in an inheritance diagram, four of which represent<N>synchronous servers of four types:<N><N>
        +------------+<N>        | BaseServer |<N>        +------------+<N>              |<N>              v<N>        +-----------+        +------------------+<N>        | TCPServer |------->| UnixStreamServer |<N>        +-----------+        +------------------+<N>              |<N>              v<N>        +-----------+        +--------------------+<N>        | UDPServer |------->| UnixDatagramServer |<N>        +-----------+        +--------------------+<N><N>
Note that UnixDatagramServer derives from UDPServer, not from<N>UnixStreamServer -- the only difference between an IP and a Unix<N>stream server is the address family, which is simply repeated in both<N>unix server classes.<N><N>Forking and threading versions of each type of server can be created<N>using the ForkingMixIn and ThreadingMixIn mix-in classes.  For<N>instance, a threading UDP server class is created as follows:<N><N>
        class ThreadingUDPServer(ThreadingMixIn, UDPServer): pass<N><N>The Mix-in class must come first, since it overrides a method defined<N>in UDPServer! Setting the various member variables also changes<N>the behavior of the underlying server mechanism.<N><N>
To implement a service, you must derive a class from<N>BaseRequestHandler and redefine its handle() method.  You can then run<N>various versions of the service by combining one of the server classes<N>with your request handler class.<N><N>The request handler class must be different for datagram or stream<N>services.  This can be hidden by using the request handler<N>subclasses StreamRequestHandler or DatagramRequestHandler.<N><N>
"""Shared support for scanning document type declarations in HTML and XHTML.<N><N>Backported for python-future from Python 3.3. Reason: ParserBase is an<N>old-style class in the Python 2.7 source of markupbase.py, which I suspect<N>might be the cause of sporadic unit-test failures on travis-ci.org with<N>test_htmlparser.py.  The test failures look like this:<N><N>
    ======================================================================<N><N>ERROR: test_attr_entity_replacement (future.tests.test_htmlparser.AttributesStrictTestCase)<N><N>----------------------------------------------------------------------<N><N>
"""<N>future.backports package<N>"""<N><N>from __future__ import absolute_import<N><N>import sys<N><N>__future_module__ = True<N>from future.standard_library import import_top_level_modules<N><N><N>if sys.version_info[0] >= 3:<N>    import_top_level_modules()<N><N>
<N>from .misc import (ceil,<N>                   OrderedDict,<N>                   Counter,<N>                   ChainMap,<N>                   check_output,<N>                   count,<N>                   recursive_repr,<N>                   _count_elements,<N>                   cmp_to_key<N>                  )<N><N><N>
# Copyright (C) 2002-2007 Python Software Foundation<N># Author: Ben Gertzfield<N># Contact: email-sig@python.org<N><N>"""Base64 content transfer encoding per RFCs 2045-2047.<N><N>This module handles the content transfer encoding method defined in RFC 2045<N>to encode arbitrary 8-bit data using the three 8-bit bytes in four 7-bit<N>characters encoding known as Base64.<N><N>
It is used in the MIME standards for email to attach images, audio, and text<N>using some 8-bit character sets to messages.<N><N>This module provides an interface to encode and decode both headers and bodies<N>with Base64 encoding.<N><N>RFC 2045 defines a method for including character set information in an<N>`encoded-word' in a header.  This method is commonly used for 8-bit real names<N>in To:, From:, Cc:, etc. fields, as well as Subject: lines.<N><N>
This module does not do the line wrapping or end-of-line character conversion<N>necessary for proper internationalized headers; it only does dumb encoding and<N>decoding.  To deal with the various line wrapping issues, use the email.header<N>module.<N>"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import range<N>from future.builtins import bytes<N><N>
__all__ = [<N>    'body_decode',<N>    'body_encode',<N>    'decode',<N>    'decodestring',<N>    'header_encode',<N>    'header_length',<N>    ]<N><N><N>from base64 import b64encode<N>from binascii import b2a_base64, a2b_base64<N><N>CRLF = '\r\n'<N>NL = '\n'<N>EMPTYSTRING = ''<N><N>
# See also Charset.py<N>MISC_LEN = 7<N><N><N># Helpers<N>def header_length(bytearray):<N>    """Return the length of s when it is encoded with base64."""<N>    groups_of_3, leftover = divmod(len(bytearray), 3)<N>    # 4 bytes out for each 3 bytes (or nonzero fraction thereof) in.<N>    n = groups_of_3 * 4<N>    if leftover:<N>        n += 4<N>    return n<N><N>
from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import str<N>from future.builtins import next<N><N># Copyright (C) 2001-2007 Python Software Foundation<N># Author: Ben Gertzfield, Barry Warsaw<N># Contact: email-sig@python.org<N><N>
__all__ = [<N>    'Charset',<N>    'add_alias',<N>    'add_charset',<N>    'add_codec',<N>    ]<N><N>from functools import partial<N><N>from future.backports import email<N>from future.backports.email import errors<N>from future.backports.email.encoders import encode_7or8bit<N><N>
<N># Flags for types of header encodings<N>QP          = 1 # Quoted-Printable<N>BASE64      = 2 # Base64<N>SHORTEST    = 3 # the shorter of QP and base64, but only for headers<N><N># In "=?charset?q?hello_world?=", the =?, ?q?, and ?= add up to 7<N>RFC2047_CHROME_LEN = 7<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Encodings and related functions."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import str<N><N>
__all__ = [<N>    'encode_7or8bit',<N>    'encode_base64',<N>    'encode_noop',<N>    'encode_quopri',<N>    ]<N><N><N>try:<N>    from base64 import encodebytes as _bencode<N>except ImportError:<N>    # Py2 compatibility. TODO: test this!<N>    from base64 import encodestring as _bencode<N>from quopri import encodestring as _encodestring<N><N>
<N>def _qencode(s):<N>    enc = _encodestring(s, quotetabs=True)<N>    # Must encode spaces, which quopri.encodestring() doesn't do<N>    return enc.replace(' ', '=20')<N><N><N>def encode_base64(msg):<N>    """Encode the message's payload in Base64.<N><N>
    Also, add an appropriate Content-Transfer-Encoding header.<N>    """<N>    orig = msg.get_payload()<N>    encdata = str(_bencode(orig), 'ascii')<N>    msg.set_payload(encdata)<N>    msg['Content-Transfer-Encoding'] = 'base64'<N><N><N>def encode_quopri(msg):<N>    """Encode the message's payload in quoted-printable.<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""email package exception classes."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import super<N><N>
<N>class MessageError(Exception):<N>    """Base class for errors in the email package."""<N><N><N>class MessageParseError(MessageError):<N>    """Base class for message parsing errors."""<N><N><N>class HeaderParseError(MessageParseError):<N>    """Error while parsing headers."""<N><N>
<N>class BoundaryError(MessageParseError):<N>    """Couldn't find terminating boundary."""<N><N><N>class MultipartConversionError(MessageError, TypeError):<N>    """Conversion to a multipart is prohibited."""<N><N><N>class CharsetError(MessageError):<N>    """An illegal charset was given."""<N><N>
<N># These are parsing defects which the parser was able to work around.<N>class MessageDefect(ValueError):<N>    """Base class for a message defect."""<N><N>    def __init__(self, line=None):<N>        if line is not None:<N>            super().__init__(line)<N>        self.line = line<N><N>
class NoBoundaryInMultipartDefect(MessageDefect):<N>    """A message claimed to be a multipart but had no boundary parameter."""<N><N>class StartBoundaryNotFoundDefect(MessageDefect):<N>    """The claimed start boundary was never found."""<N><N>class CloseBoundaryNotFoundDefect(MessageDefect):<N>    """A start boundary was found, but not the corresponding close boundary."""<N><N>
class FirstHeaderLineIsContinuationDefect(MessageDefect):<N>    """A message had a continuation line as its first header line."""<N><N>class MisplacedEnvelopeHeaderDefect(MessageDefect):<N>    """A 'Unix-from' header was found in the middle of a header block."""<N><N>
class MissingHeaderBodySeparatorDefect(MessageDefect):<N>    """Found line with no leading whitespace and no colon before blank line."""<N># XXX: backward compatibility, just in case (it was never emitted).<N>MalformedHeaderDefect = MissingHeaderBodySeparatorDefect<N><N>
class MultipartInvariantViolationDefect(MessageDefect):<N>    """A message claimed to be a multipart but no subparts were found."""<N><N>class InvalidMultipartContentTransferEncodingDefect(MessageDefect):<N>    """An invalid content transfer encoding was set on the multipart itself."""<N><N>
class UndecodableBytesDefect(MessageDefect):<N>    """Header contained bytes that could not be decoded"""<N><N>class InvalidBase64PaddingDefect(MessageDefect):<N>    """base64 encoded sequence had an incorrect length"""<N><N>class InvalidBase64CharactersDefect(MessageDefect):<N>    """base64 encoded sequence had characters not in base64 alphabet"""<N><N>
# These errors are specific to header parsing.<N><N>class HeaderDefect(MessageDefect):<N>    """Base class for a header defect."""<N><N>    def __init__(self, *args, **kw):<N>        super().__init__(*args, **kw)<N><N>class InvalidHeaderDefect(HeaderDefect):<N>    """Header is not valid, message gives details."""<N><N>
class HeaderMissingRequiredValue(HeaderDefect):<N>    """A header that must have a value had none"""<N><N>class NonPrintableDefect(HeaderDefect):<N>    """ASCII characters outside the ascii-printable range found"""<N><N>    def __init__(self, non_printables):<N>        super().__init__(non_printables)<N>        self.non_printables = non_printables<N><N>
    def __str__(self):<N>        return ("the following ASCII non-printables found in header: "<N>            "{}".format(self.non_printables))<N><N>class ObsoleteHeaderDefect(HeaderDefect):<N>    """Header uses syntax declared obsolete by RFC 5322"""<N><N>
# Copyright (C) 2004-2006 Python Software Foundation<N># Authors: Baxter, Wouters and Warsaw<N># Contact: email-sig@python.org<N><N>"""FeedParser - An email feed parser.<N><N>The feed parser implements an interface for incrementally parsing an email<N>message, line by line.  This has advantages for certain applications, such as<N>those reading email messages off a socket.<N><N>
FeedParser.feed() is the primary interface for pushing new data into the<N>parser.  It returns when there's nothing more it can do with the available<N>data.  When you have no more data to push into the parser, call .close().<N>This completes the parsing and returns the root message object.<N><N>
The other advantage of this parser is that it will never raise a parsing<N>exception.  Instead, when it finds something unexpected, it adds a 'defect' to<N>the current message.  Defects are just instances that live on the message<N>object's .defects attribute.<N>"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import object, range, super<N>from future.utils import implements_iterator, PY3<N><N>
# Copyright (C) 2001-2010 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Classes to generate plain text from a message object tree."""<N>from __future__ import print_function<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import super<N>from future.builtins import str<N><N>
__all__ = ['Generator', 'DecodedGenerator', 'BytesGenerator']<N><N>import re<N>import sys<N>import time<N>import random<N>import warnings<N><N>from io import StringIO, BytesIO<N>from future.backports.email._policybase import compat32<N>from future.backports.email.header import Header<N>from future.backports.email.utils import _has_surrogates<N>import future.backports.email.charset as _charset<N><N>
UNDERSCORE = '_'<N>NL = '\n'  # XXX: no longer used by the code below.<N><N>fcre = re.compile(r'^From ', re.MULTILINE)<N><N><N>class Generator(object):<N>    """Generates output from a Message object tree.<N><N>    This basic generator writes the message to the given file object as plain<N>    text.<N>    """<N>    #<N>    # Public interface<N>    #<N><N>
    def __init__(self, outfp, mangle_from_=True, maxheaderlen=None, **_3to2kwargs):<N>        if 'policy' in _3to2kwargs: policy = _3to2kwargs['policy']; del _3to2kwargs['policy']<N>        else: policy = None<N>        """Create the generator for message flattening.<N><N>
        outfp is the output file-like object for writing the message to.  It<N>        must have a write() method.<N><N>        Optional mangle_from_ is a flag that, when True (the default), escapes<N>        From_ lines in the body of the message by putting a `>' in front of<N>        them.<N><N>
        Optional maxheaderlen specifies the longest length for a non-continued<N>        header.  When a header line is longer (in characters, with tabs<N>        expanded to 8 spaces) than maxheaderlen, the header will split as<N>        defined in the Header class.  Set maxheaderlen to zero to disable<N>        header wrapping.  The default is 78, as recommended (but not required)<N>        by RFC 2822.<N><N>
        The policy keyword specifies a policy object that controls a number of<N>        aspects of the generator's operation.  The default policy maintains<N>        backward compatibility.<N><N>        """<N>        self._fp = outfp<N>        self._mangle_from_ = mangle_from_<N>        self.maxheaderlen = maxheaderlen<N>        self.policy = policy<N><N>
    def write(self, s):<N>        # Just delegate to the file object<N>        self._fp.write(s)<N><N>    def flatten(self, msg, unixfrom=False, linesep=None):<N>        r"""Print the message object tree rooted at msg to the output file<N>        specified when the Generator instance was created.<N><N>
        unixfrom is a flag that forces the printing of a Unix From_ delimiter<N>        before the first object in the message tree.  If the original message<N>        has no From_ delimiter, a `standard' one is crafted.  By default, this<N>        is False to inhibit the printing of any From_ delimiter.<N><N>
# Copyright (C) 2002-2007 Python Software Foundation<N># Author: Ben Gertzfield, Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Header encoding and decoding functionality."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import bytes, range, str, super, zip<N><N>
__all__ = [<N>    'Header',<N>    'decode_header',<N>    'make_header',<N>    ]<N><N>import re<N>import binascii<N><N>from future.backports import email<N>from future.backports.email import base64mime<N>from future.backports.email.errors import HeaderParseError<N>import future.backports.email.charset as _charset<N><N>
# Helpers<N>from future.backports.email.quoprimime import _max_append, header_decode<N><N>Charset = _charset.Charset<N><N>NL = '\n'<N>SPACE = ' '<N>BSPACE = b' '<N>SPACE8 = ' ' * 8<N>EMPTYSTRING = ''<N>MAXLINELEN = 78<N>FWS = ' \t'<N><N>USASCII = Charset('us-ascii')<N>UTF8 = Charset('utf-8')<N><N>
"""Representing and manipulating email headers via custom objects.<N><N>This module provides an implementation of the HeaderRegistry API.<N>The implementation is designed to flexibly follow RFC5322 rules.<N><N>Eventually HeaderRegistry will be a public API, but it isn't yet,<N>and will probably change some before that happens.<N><N>
"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>from future.builtins import super<N>from future.builtins import str<N>from future.utils import text_to_native_str<N>from future.backports.email import utils<N>from future.backports.email import errors<N>from future.backports.email import _header_value_parser as parser<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Various types of useful iterators and generators."""<N>from __future__ import print_function<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = [<N>    'body_line_iterator',<N>    'typed_subpart_iterator',<N>    'walk',<N>    # Do not include _structure() since it's part of the debugging API.<N>    ]<N><N>import sys<N>from io import StringIO<N><N><N># This function will become a method of the Message class<N>def walk(self):<N>    """Walk over the message tree, yielding each subpart.<N><N>
    The walk is performed in depth-first order.  This method is a<N>    generator.<N>    """<N>    yield self<N>    if self.is_multipart():<N>        for subpart in self.get_payload():<N>            for subsubpart in subpart.walk():<N>                yield subsubpart<N><N>
<N># These two functions are imported into the Iterators.py interface module.<N>def body_line_iterator(msg, decode=False):<N>    """Iterate over the parts, returning string payloads line-by-line.<N><N>    Optional decode (default False) is passed through to .get_payload().<N>    """<N>    for subpart in msg.walk():<N>        payload = subpart.get_payload(decode=decode)<N>        if isinstance(payload, str):<N>            for line in StringIO(payload):<N>                yield line<N><N>
# -*- coding: utf-8 -*-<N># Copyright (C) 2001-2007 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Basic message object for the email package object model."""<N>from __future__ import absolute_import, division, unicode_literals<N>from future.builtins import list, range, str, zip<N><N>
__all__ = ['Message']<N><N>import re<N>import uu<N>import base64<N>import binascii<N>from io import BytesIO, StringIO<N><N># Intrapackage imports<N>from future.utils import as_native_str<N>from future.backports.email import utils<N>from future.backports.email import errors<N>from future.backports.email._policybase import compat32<N>from future.backports.email import charset as _charset<N>from future.backports.email._encoded_words import decode_b<N>Charset = _charset.Charset<N><N>
# Copyright (C) 2001-2007 Python Software Foundation<N># Author: Barry Warsaw, Thomas Wouters, Anthony Baxter<N># Contact: email-sig@python.org<N><N>"""A parser of RFC 2822 and MIME email messages."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['Parser', 'HeaderParser', 'BytesParser', 'BytesHeaderParser']<N><N>import warnings<N>from io import StringIO, TextIOWrapper<N><N>from future.backports.email.feedparser import FeedParser, BytesFeedParser<N>from future.backports.email.message import Message<N>from future.backports.email._policybase import compat32<N><N>
<N>class Parser(object):<N>    def __init__(self, _class=Message, **_3to2kwargs):<N>        """Parser of RFC 2822 and MIME email messages.<N><N>        Creates an in-memory object tree representing the email message, which<N>        can then be manipulated and turned over to a Generator to return the<N>        textual representation of the message.<N><N>
        The string must be formatted as a block of RFC 2822 headers and header<N>        continuation lines, optionally preceeded by a `Unix-from' header.  The<N>        header block is terminated either by the end of the string or by a<N>        blank line.<N><N>
        _class is the class to instantiate for new message objects when they<N>        must be created.  This class must have a constructor that can take<N>        zero arguments.  Default is Message.Message.<N><N>        The policy keyword specifies a policy object that controls a number of<N>        aspects of the parser's operation.  The default policy maintains<N>        backward compatibility.<N><N>
        """<N>        if 'policy' in _3to2kwargs: policy = _3to2kwargs['policy']; del _3to2kwargs['policy']<N>        else: policy = compat32<N>        self._class = _class<N>        self.policy = policy<N><N>    def parse(self, fp, headersonly=False):<N>        """Create a message structure from the data in a file.<N><N>
"""This will be the home for the policy that hooks in the new<N>code that adds all the email6 features.<N>"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import super<N><N>
from future.standard_library.email._policybase import (Policy, Compat32,<N>                                                  compat32, _extend_docstrings)<N>from future.standard_library.email.utils import _has_surrogates<N>from future.standard_library.email.headerregistry import HeaderRegistry as HeaderRegistry<N><N>
__all__ = [<N>    'Compat32',<N>    'compat32',<N>    'Policy',<N>    'EmailPolicy',<N>    'default',<N>    'strict',<N>    'SMTP',<N>    'HTTP',<N>    ]<N><N>@_extend_docstrings<N>class EmailPolicy(Policy):<N><N>    """+<N>    PROVISIONAL<N><N>    The API extensions enabled by this policy are currently provisional.<N>    Refer to the documentation for details.<N><N>
    This policy adds new header parsing and folding algorithms.  Instead of<N>    simple strings, headers are custom objects with custom attributes<N>    depending on the type of the field.  The folding algorithm fully<N>    implements RFCs 2047 and 5322.<N><N>
# Copyright (C) 2001-2010 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Miscellaneous utilities."""<N><N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future import utils<N>from future.builtins import bytes, int, str<N><N>
__all__ = [<N>    'collapse_rfc2231_value',<N>    'decode_params',<N>    'decode_rfc2231',<N>    'encode_rfc2231',<N>    'formataddr',<N>    'formatdate',<N>    'format_datetime',<N>    'getaddresses',<N>    'make_msgid',<N>    'mktime_tz',<N>    'parseaddr',<N>    'parsedate',<N>    'parsedate_tz',<N>    'parsedate_to_datetime',<N>    'unquote',<N>    ]<N><N>
import os<N>import re<N>if utils.PY2:<N>    re.ASCII = 0<N>import time<N>import base64<N>import random<N>import socket<N>from future.backports import datetime<N>from future.backports.urllib.parse import quote as url_quote, unquote as url_unquote<N>import warnings<N>from io import StringIO<N><N>
from future.backports.email._parseaddr import quote<N>from future.backports.email._parseaddr import AddressList as _AddressList<N>from future.backports.email._parseaddr import mktime_tz<N><N>from future.backports.email._parseaddr import parsedate, parsedate_tz, _parsedate_tz<N><N>
from quopri import decodestring as _qdecode<N><N># Intrapackage imports<N>from future.backports.email.encoders import _bencode, _qencode<N>from future.backports.email.charset import Charset<N><N>COMMASPACE = ', '<N>EMPTYSTRING = ''<N>UEMPTYSTRING = ''<N>CRLF = '\r\n'<N>TICK = "'"<N><N>
specialsre = re.compile(r'[][\\()<>@,:;".]')<N>escapesre = re.compile(r'[\\"]')<N><N># How to figure out if we are processing strings that come from a byte<N># source with undecodable characters.<N>_has_surrogates = re.compile(<N>    '([^\ud800-\udbff]|\A)[\udc00-\udfff]([^\udc00-\udfff]|\Z)').search<N><N>
# How to deal with a string containing bytes before handing it to the<N># application through the 'normal' interface.<N>def _sanitize(string):<N>    # Turn any escaped bytes into unicode 'unknown' char.<N>    original_bytes = string.encode('ascii', 'surrogateescape')<N>    return original_bytes.decode('ascii', 'replace')<N><N>
<N># Helpers<N><N>def formataddr(pair, charset='utf-8'):<N>    """The inverse of parseaddr(), this takes a 2-tuple of the form<N>    (realname, email_address) and returns the string value suitable<N>    for an RFC 2822 From, To or Cc header.<N><N>    If the first element of pair is false, then the second element is<N>    returned unmodified.<N><N>
""" Routines for manipulating RFC2047 encoded words.<N><N>This is currently a package-private API, but will be considered for promotion<N>to a public API if there is demand.<N><N>"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import bytes<N>from future.builtins import chr<N>from future.builtins import int<N>from future.builtins import str<N><N>
"""Header value parser implementing various email-related RFC parsing rules.<N><N>The parsing methods defined in this module implement various email related<N>parsing rules.  Principal among them is RFC 5322, which is the followon<N>to RFC 2822 and primarily a clarification of the former.  It also implements<N>RFC 2047 encoded word decoding.<N><N>
# Copyright (C) 2002-2007 Python Software Foundation<N># Contact: email-sig@python.org<N><N>"""Email address parsing code.<N><N>Lifted directly from rfc822.py.  This should eventually be rewritten.<N>"""<N><N>from __future__ import unicode_literals<N>from __future__ import print_function<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import int<N><N>
__all__ = [<N>    'mktime_tz',<N>    'parsedate',<N>    'parsedate_tz',<N>    'quote',<N>    ]<N><N>import time, calendar<N><N>SPACE = ' '<N>EMPTYSTRING = ''<N>COMMASPACE = ', '<N><N># Parse a date field<N>_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',<N>               'aug', 'sep', 'oct', 'nov', 'dec',<N>               'january', 'february', 'march', 'april', 'may', 'june', 'july',<N>               'august', 'september', 'october', 'november', 'december']<N><N>
_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']<N><N># The timezone table does not include the military time zones defined<N># in RFC822, other than Z.  According to RFC1123, the description in<N># RFC822 gets the signs wrong, so we can't rely on any such time<N># zones.  RFC1123 recommends that numeric timezone indicators be used<N># instead of timezone names.<N><N>
_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,<N>              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)<N>              'EST': -500, 'EDT': -400,  # Eastern<N>              'CST': -600, 'CDT': -500,  # Central<N>              'MST': -700, 'MDT': -600,  # Mountain<N>              'PST': -800, 'PDT': -700   # Pacific<N>              }<N><N>
<N>def parsedate_tz(data):<N>    """Convert a date string to a time tuple.<N><N>    Accounts for military timezones.<N>    """<N>    res = _parsedate_tz(data)<N>    if not res:<N>        return<N>    if res[9] is None:<N>        res[9] = 0<N>    return tuple(res)<N><N>
def _parsedate_tz(data):<N>    """Convert date to extended time tuple.<N><N>    The last (additional) element is the time zone offset in seconds, except if<N>    the timezone was specified as -0000.  In that case the last element is<N>    None.  This indicates a UTC timestamp that explicitly declaims knowledge of<N>    the source timezone, as opposed to a +0000 timestamp that indicates the<N>    source timezone really was UTC.<N><N>
"""Policy framework for the email package.<N><N>Allows fine grained feature control of how the package parses and emits data.<N>"""<N>from __future__ import unicode_literals<N>from __future__ import print_function<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import super<N>from future.builtins import str<N>from future.utils import with_metaclass<N><N>
import abc<N>from future.backports.email import header<N>from future.backports.email import charset as _charset<N>from future.backports.email.utils import _has_surrogates<N><N>__all__ = [<N>    'Policy',<N>    'Compat32',<N>    'compat32',<N>    ]<N><N>
# Copyright (C) 2001-2007 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""<N>Backport of the Python 3.3 email package for Python-Future.<N><N>A package for parsing, handling, and generating email messages.<N>"""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
# Install the surrogate escape handler here because this is used by many<N># modules in the email package.<N>from future.utils import surrogateescape<N>surrogateescape.register_surrogateescape()<N># (Should this be done globally by ``future``?)<N><N>
<N>__version__ = '5.1.0'<N><N>__all__ = [<N>    'base64mime',<N>    'charset',<N>    'encoders',<N>    'errors',<N>    'feedparser',<N>    'generator',<N>    'header',<N>    'iterators',<N>    'message',<N>    'message_from_file',<N>    'message_from_binary_file',<N>    'message_from_string',<N>    'message_from_bytes',<N>    'mime',<N>    'parser',<N>    'quoprimime',<N>    'utils',<N>    ]<N><N>
<N><N># Some convenience routines.  Don't import Parser and Message as side-effects<N># of importing email since those cascadingly import most of the rest of the<N># email package.<N>def message_from_string(s, *args, **kws):<N>    """Parse a string into a Message object model.<N><N>
    Optional _class and strict are passed to the Parser constructor.<N>    """<N>    from future.backports.email.parser import Parser<N>    return Parser(*args, **kws).parsestr(s)<N><N>def message_from_bytes(s, *args, **kws):<N>    """Parse a bytes string into a Message object model.<N><N>
    Optional _class and strict are passed to the Parser constructor.<N>    """<N>    from future.backports.email.parser import BytesParser<N>    return BytesParser(*args, **kws).parsebytes(s)<N><N>def message_from_file(fp, *args, **kws):<N>    """Read a file and parse its contents into a Message object model.<N><N>
    Optional _class and strict are passed to the Parser constructor.<N>    """<N>    from future.backports.email.parser import Parser<N>    return Parser(*args, **kws).parse(fp)<N><N>def message_from_binary_file(fp, *args, **kws):<N>    """Read a binary file and parse its contents into a Message object model.<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Keith Dart<N># Contact: email-sig@python.org<N><N>"""Class representing application/* type MIME documents."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
from future.backports.email import encoders<N>from future.backports.email.mime.nonmultipart import MIMENonMultipart<N><N>__all__ = ["MIMEApplication"]<N><N><N>class MIMEApplication(MIMENonMultipart):<N>    """Class for generating application/* MIME documents."""<N><N>
    def __init__(self, _data, _subtype='octet-stream',<N>                 _encoder=encoders.encode_base64, **_params):<N>        """Create an application/* type MIME document.<N><N>        _data is a string containing the raw application data.<N><N>        _subtype is the MIME content type subtype, defaulting to<N>        'octet-stream'.<N><N>
# Copyright (C) 2001-2007 Python Software Foundation<N># Author: Anthony Baxter<N># Contact: email-sig@python.org<N><N>"""Class representing audio/* type MIME documents."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['MIMEAudio']<N><N>import sndhdr<N><N>from io import BytesIO<N>from future.backports.email import encoders<N>from future.backports.email.mime.nonmultipart import MIMENonMultipart<N><N><N>_sndhdr_MIMEmap = {'au'  : 'basic',<N>                   'wav' :'x-wav',<N>                   'aiff':'x-aiff',<N>                   'aifc':'x-aiff',<N>                   }<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Base class for MIME specializations."""<N>from __future__ import absolute_import, division, unicode_literals<N>from future.backports.email import message<N><N>
__all__ = ['MIMEBase']<N><N><N>class MIMEBase(message.Message):<N>    """Base class for MIME specializations."""<N><N>    def __init__(self, _maintype, _subtype, **_params):<N>        """This constructor adds a Content-Type: and a MIME-Version: header.<N><N>
        The Content-Type: header is taken from the _maintype and _subtype<N>        arguments.  Additional parameters for this header are taken from the<N>        keyword arguments.<N>        """<N>        message.Message.__init__(self)<N>        ctype = '%s/%s' % (_maintype, _subtype)<N>        self.add_header('Content-Type', ctype, **_params)<N>        self['MIME-Version'] = '1.0'<N><N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Class representing image/* type MIME documents."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['MIMEImage']<N><N>import imghdr<N><N>from future.backports.email import encoders<N>from future.backports.email.mime.nonmultipart import MIMENonMultipart<N><N><N>class MIMEImage(MIMENonMultipart):<N>    """Class for generating image/* type MIME documents."""<N><N>
    def __init__(self, _imagedata, _subtype=None,<N>                 _encoder=encoders.encode_base64, **_params):<N>        """Create an image/* type MIME document.<N><N>        _imagedata is a string containing the raw image data.  If this data<N>        can be decoded by the standard Python `imghdr' module, then the<N>        subtype will be automatically included in the Content-Type header.<N>        Otherwise, you can specify the specific image subtype via the _subtype<N>        parameter.<N><N>
        _encoder is a function which will perform the actual encoding for<N>        transport of the image data.  It takes one argument, which is this<N>        Image instance.  It should use get_payload() and set_payload() to<N>        change the payload to the encoded form.  It should also add any<N>        Content-Transfer-Encoding or other headers to the message as<N>        necessary.  The default encoding is Base64.<N><N>
        Any additional keyword arguments are passed to the base class<N>        constructor, which turns them into parameters on the Content-Type<N>        header.<N>        """<N>        if _subtype is None:<N>            _subtype = imghdr.what(None, _imagedata)<N>        if _subtype is None:<N>            raise TypeError('Could not guess image MIME subtype')<N>        MIMENonMultipart.__init__(self, 'image', _subtype, **_params)<N>        self.set_payload(_imagedata)<N>        _encoder(self)<N><N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Class representing message/* MIME documents."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['MIMEMessage']<N><N>from future.backports.email import message<N>from future.backports.email.mime.nonmultipart import MIMENonMultipart<N><N><N>class MIMEMessage(MIMENonMultipart):<N>    """Class representing message/* MIME documents."""<N><N>
    def __init__(self, _msg, _subtype='rfc822'):<N>        """Create a message/* type MIME document.<N><N>        _msg is a message object and must be an instance of Message, or a<N>        derived class of Message, otherwise a TypeError is raised.<N><N>
# Copyright (C) 2002-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Base class for MIME multipart/* type messages."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['MIMEMultipart']<N><N>from future.backports.email.mime.base import MIMEBase<N><N><N>class MIMEMultipart(MIMEBase):<N>    """Base class for MIME multipart/* type messages."""<N><N>    def __init__(self, _subtype='mixed', boundary=None, _subparts=None,<N>                 **_params):<N>        """Creates a multipart/* type message.<N><N>
        By default, creates a multipart/mixed message, with proper<N>        Content-Type and MIME-Version headers.<N><N>        _subtype is the subtype of the multipart content type, defaulting to<N>        `mixed'.<N><N>        boundary is the multipart boundary string.  By default it is<N>        calculated as needed.<N><N>
        _subparts is a sequence of initial subparts for the payload.  It<N>        must be an iterable object, such as a list.  You can always<N>        attach new subparts to the message by using the attach() method.<N><N>        Additional parameters for the Content-Type header are taken from the<N>        keyword arguments (or passed into the _params argument).<N>        """<N>        MIMEBase.__init__(self, 'multipart', _subtype, **_params)<N><N>
        # Initialise _payload to an empty list as the Message superclass's<N>        # implementation of is_multipart assumes that _payload is a list for<N>        # multipart messages.<N>        self._payload = []<N><N>        if _subparts:<N>            for p in _subparts:<N>                self.attach(p)<N>        if boundary:<N>            self.set_boundary(boundary)<N><N><N>
# Copyright (C) 2002-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Base class for MIME type messages that are not multipart."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
# Copyright (C) 2001-2006 Python Software Foundation<N># Author: Barry Warsaw<N># Contact: email-sig@python.org<N><N>"""Class representing text/* type MIME documents."""<N>from __future__ import unicode_literals<N>from __future__ import division<N>from __future__ import absolute_import<N><N>
__all__ = ['MIMEText']<N><N>from future.backports.email.encoders import encode_7or8bit<N>from future.backports.email.mime.nonmultipart import MIMENonMultipart<N><N><N>class MIMEText(MIMENonMultipart):<N>    """Class for generating text/* type MIME documents."""<N><N>
    def __init__(self, _text, _subtype='plain', _charset=None):<N>        """Create a text/* type MIME document.<N><N>        _text is the string for this message object.<N><N>        _subtype is the MIME sub content type, defaulting to "plain".<N><N>
"""A parser for HTML and XHTML.<N><N>Backported for python-future from Python 3.3.<N>"""<N><N># This file is based on sgmllib.py, but the API is slightly different.<N><N># XXX There should be a way to distinguish between PCDATA (parsed<N># character data -- the normal case), RCDATA (replaceable character<N># data -- only char and entity references and end tags are special)<N># and CDATA (character data -- only end tags are special).<N><N>
from __future__ import (absolute_import, division,<N>                        print_function, unicode_literals)<N>from future.builtins import *<N>from future.backports import _markupbase<N>import re<N>import warnings<N><N># Regular expressions used for parsing<N><N>
"""<N>General functions for HTML manipulation, backported from Py3.<N><N>Note that this uses Python 2.7 code with the corresponding Python 3<N>module names and locations.<N>"""<N><N>from __future__ import unicode_literals<N><N><N>_escape_map = {ord('&'): '&amp;', ord('<'): '&lt;', ord('>'): '&gt;'}<N>_escape_map_full = {ord('&'): '&amp;', ord('<'): '&lt;', ord('>'): '&gt;',<N>                    ord('"'): '&quot;', ord('\''): '&#x27;'}<N><N>
"""HTTP/1.1 client library<N><N>A backport of the Python 3.3 http/client.py module for python-future.<N><N><intro stuff goes here><N><other stuff, too><N><N>HTTPConnection goes through a number of "states", which define when a client<N>may legally make another request or fetch the response for a particular<N>request. This diagram details these state transitions:<N><N>
r"""HTTP cookie handling for web clients.<N><N>This is a backport of the Py3.3 ``http.cookiejar`` module for<N>python-future.<N><N>This module has (now fairly distant) origins in Gisle Aas' Perl module<N>HTTP::Cookies, from the libwww-perl library.<N><N>
Docstrings, comments and debug strings in this code refer to the<N>attributes of the HTTP cookie system as cookie-attributes, to distinguish<N>them clearly from Python attributes.<N><N>Class diagram (note that BSDDBCookieJar and the MSIE* classes are not<N>distributed with the Python standard library, but are available from<N>http://wwwsearch.sf.net/):<N><N>
                        CookieJar____<N>                        /     \      \<N>            FileCookieJar      \      \<N>             /    |   \         \      \<N> MozillaCookieJar | LWPCookieJar \      \<N>                  |               |      \<N>                  |   ---MSIEBase |       \<N>                  |  /      |     |        \<N>                  | /   MSIEDBCookieJar BSDDBCookieJar<N>                  |/<N>               MSIECookieJar<N><N>
"""<N><N>from __future__ import unicode_literals<N>from __future__ import print_function<N>from __future__ import division<N>from __future__ import absolute_import<N>from future.builtins import filter, int, map, open, str<N>from future.utils import as_native_str, PY2<N><N>
__all__ = ['Cookie', 'CookieJar', 'CookiePolicy', 'DefaultCookiePolicy',<N>           'FileCookieJar', 'LWPCookieJar', 'LoadError', 'MozillaCookieJar']<N><N>import copy<N>import datetime<N>import re<N>if PY2:<N>    re.ASCII = 0<N>import time<N>from future.backports.urllib.parse import urlparse, urlsplit, quote<N>from future.backports.http.client import HTTP_PORT<N>try:<N>    import threading as _threading<N>except ImportError:<N>    import dummy_threading as _threading<N>from calendar import timegm<N><N>
debug = False   # set to True to enable debugging via the logging module<N>logger = None<N><N>def _debug(*args):<N>    if not debug:<N>        return<N>    global logger<N>    if not logger:<N>        import logging<N>        logger = logging.getLogger("http.cookiejar")<N>    return logger.debug(*args)<N><N>
"""HTTP server classes.<N><N>From Python 3.3<N><N>Note: BaseHTTPRequestHandler doesn't implement any HTTP request; see<N>SimpleHTTPRequestHandler for simple implementations of GET, HEAD and POST,<N>and CGIHTTPRequestHandler for CGI scripts.<N><N>It does, however, optionally implement HTTP/1.1 persistent connections,<N>as of version 0.3.<N><N>
Notes on CGIHTTPRequestHandler<N>------------------------------<N><N>This class implements GET and POST requests to cgi-bin scripts.<N><N>If the os.fork() function is not present (e.g. on Windows),<N>subprocess.Popen() is used as a fallback, with slightly altered semantics.<N><N>
In all cases, the implementation is intentionally naive -- all<N>requests are executed synchronously.<N><N>SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL<N>-- it may execute arbitrary Python code or external programs.<N><N>Note that status code 200 is sent prior to execution of a CGI script, so<N>scripts cannot send other status codes such as 302 (redirect).<N><N>
XXX To do:<N><N>- log requests even later (to capture byte count)<N>- log user-agent header and other interesting goodies<N>- send error log to separate file<N>"""<N><N>from __future__ import (absolute_import, division,<N>                        print_function, unicode_literals)<N>from future import utils<N>from future.builtins import *<N><N>
#!/usr/bin/env python3<N><N>"""<N>"PYSTONE" Benchmark Program<N><N>Version:        Python/1.1 (corresponds to C/1.1 plus 2 Pystone fixes)<N><N>Author:         Reinhold P. Weicker,  CACM Vol 27, No 10, 10/84 pg. 1013.<N><N>                Translated from ADA to C by Rick Richardson.<N>                Every method to preserve ADA-likeness has been used,<N>                at the expense of C-ness.<N><N>
                Translated from C to Python by Guido van Rossum.<N><N>Version History:<N><N>                Version 1.1 corrects two bugs in version 1.0:<N><N>                First, it leaked memory: in Proc1(), NextRecord ends<N>                up having a pointer to itself.  I have corrected this<N>                by zapping NextRecord.PtrComp at the end of Proc1().<N><N>
from __future__ import absolute_import, division, print_function, unicode_literals<N>from future.builtins import filter, str<N>from future import utils<N>import os<N>import sys<N>import ssl<N>import pprint<N>import socket<N>from future.backports.urllib import parse as urllib_parse<N>from future.backports.http.server import (HTTPServer as _HTTPServer,<N>    SimpleHTTPRequestHandler, BaseHTTPRequestHandler)<N>from future.backports.test import support<N>threading = support.import_module("threading")<N><N>
here = os.path.dirname(__file__)<N><N>HOST = support.HOST<N>CERTFILE = os.path.join(here, 'keycert.pem')<N><N># This one's based on HTTPServer, which is based on SocketServer<N><N>class HTTPSServer(_HTTPServer):<N><N>    def __init__(self, server_address, handler_class, context):<N>        _HTTPServer.__init__(self, server_address, handler_class)<N>        self.context = context<N><N>
# -*- coding: utf-8 -*-<N>"""Supporting definitions for the Python regression tests.<N><N>Backported for python-future from Python 3.3 test/support.py.<N>"""<N><N>from __future__ import (absolute_import, division,<N>                        print_function, unicode_literals)<N>from future import utils<N>from future.builtins import str, range, open, int, map, list<N><N>
import contextlib<N>import errno<N>import functools<N>import gc<N>import socket<N>import sys<N>import os<N>import platform<N>import shutil<N>import warnings<N>import unittest<N># For Python 2.6 compatibility:<N>if not hasattr(unittest, 'skip'):<N>    import unittest2 as unittest<N><N>
import importlib<N># import collections.abc    # not present on Py2.7<N>import re<N>import subprocess<N>import imp<N>import time<N>try:<N>    import sysconfig<N>except ImportError:<N>    # sysconfig is not available on Python 2.6. Try using distutils.sysconfig instead:<N>    from distutils import sysconfig<N>import fnmatch<N>import logging.handlers<N>import struct<N>import tempfile<N><N>
try:<N>    if utils.PY3:<N>        import _thread, threading<N>    else:<N>        import thread as _thread, threading<N>except ImportError:<N>    _thread = None<N>    threading = None<N>try:<N>    import multiprocessing.process<N>except ImportError:<N>    multiprocessing = None<N><N>
try:<N>    import zlib<N>except ImportError:<N>    zlib = None<N><N>try:<N>    import gzip<N>except ImportError:<N>    gzip = None<N><N>try:<N>    import bz2<N>except ImportError:<N>    bz2 = None<N><N>try:<N>    import lzma<N>except ImportError:<N>    lzma = None<N><N>
"""<N>test package backported for python-future.<N><N>Its primary purpose is to allow use of "import test.support" for running<N>the Python standard library unit tests using the new Python 3 stdlib<N>import location.<N><N>Python 3 renamed test.test_support to test.support.<N>"""<N>
"""<N>Ported using Python-Future from the Python 3.3 standard library.<N><N>Parse (absolute and relative) URLs.<N><N>urlparse module is based upon the following RFC specifications.<N><N>RFC 3986 (STD66): "Uniform Resource Identifiers" by T. Berners-Lee, R. Fielding<N>and L.  Masinter, January 2005.<N><N>
RFC 2732 : "Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter<N>and L.Masinter, December 1999.<N><N>RFC 2396:  "Uniform Resource Identifiers (URI)": Generic Syntax by T.<N>Berners-Lee, R. Fielding, and L. Masinter, August 1998.<N><N>
RFC 2368: "The mailto URL scheme", by P.Hoffman , L Masinter, J. Zawinski, July 1998.<N><N>RFC 1808: "Relative Uniform Resource Locators", by R. Fielding, UC Irvine, June<N>1995.<N><N>RFC 1738: "Uniform Resource Locators (URL)" by T. Berners-Lee, L. Masinter, M.<N>McCahill, December 1994<N><N>
"""<N>Ported using Python-Future from the Python 3.3 standard library.<N><N>An extensible library for opening URLs using a variety of protocols<N><N>The simplest way to use this module is to call the urlopen function,<N>which accepts a string containing a URL or a Request object (described<N>below).  It opens the URL and returns the results as file-like<N>object; the returned object has some extra methods described below.<N><N>
"""Response classes used by urllib.<N><N>The base class, addbase, defines a minimal file-like interface,<N>including read() and readline().  The typical response object is an<N>addinfourl instance, which defines an info() method that returns<N>headers and a geturl() method that returns the url.<N>"""<N>from __future__ import absolute_import, division, unicode_literals<N>from future.builtins import object<N><N>
from __future__ import absolute_import, division, unicode_literals<N>from future.builtins import str<N>""" robotparser.py<N><N>    Copyright (C) 2000  Bastian Kleineidam<N><N>    You can choose between two licenses when using this package:<N>    1) GNU GPLv2<N>    2) PSF license for Python 2.2<N><N>
    The robots.txt Exclusion Protocol is implemented as specified in<N>    http://info.webcrawler.com/mak/projects/robots/norobots-rfc.html<N>"""<N><N># Was: import urllib.parse, urllib.request<N>from future.backports import urllib<N>from future.backports.urllib import parse as _parse, request as _request<N>urllib.parse = _parse<N>urllib.request = _request<N><N>
<N>__all__ = ["RobotFileParser"]<N><N>class RobotFileParser(object):<N>    """ This class provides a set of methods to read, parse and answer<N>    questions about a single robots.txt file.<N><N>    """<N><N>    def __init__(self, url=''):<N>        self.entries = []<N>        self.default_entry = None<N>        self.disallow_all = False<N>        self.allow_all = False<N>        self.set_url(url)<N>        self.last_checked = 0<N><N>
    def mtime(self):<N>        """Returns the time the robots.txt file was last fetched.<N><N>        This is useful for long-running web spiders that need to<N>        check for new robots.txt files periodically.<N><N>        """<N>        return self.last_checked<N><N>
    def modified(self):<N>        """Sets the time the robots.txt file was last fetched to the<N>        current time.<N><N>        """<N>        import time<N>        self.last_checked = time.time()<N><N>    def set_url(self, url):<N>        """Sets the URL referring to a robots.txt file."""<N>        self.url = url<N>        self.host, self.path = urllib.parse.urlparse(url)[1:3]<N><N>
    def read(self):<N>        """Reads the robots.txt URL and feeds it to the parser."""<N>        try:<N>            f = urllib.request.urlopen(self.url)<N>        except urllib.error.HTTPError as err:<N>            if err.code in (401, 403):<N>                self.disallow_all = True<N>            elif err.code >= 400:<N>                self.allow_all = True<N>        else:<N>            raw = f.read()<N>            self.parse(raw.decode("utf-8").splitlines())<N><N>
    def _add_entry(self, entry):<N>        if "*" in entry.useragents:<N>            # the default entry is considered last<N>            if self.default_entry is None:<N>                # the first default entry wins<N>                self.default_entry = entry<N>        else:<N>            self.entries.append(entry)<N><N>
    def parse(self, lines):<N>        """Parse the input lines from a robots.txt file.<N><N>        We allow that a user-agent: line is not preceded by<N>        one or more blank lines.<N>        """<N>        # states:<N>        #   0: start state<N>        #   1: saw user-agent line<N>        #   2: saw an allow or disallow line<N>        state = 0<N>        entry = Entry()<N><N>
r"""<N>Ported using Python-Future from the Python 3.3 standard library.<N><N>XML-RPC Servers.<N><N>This module can be used to create simple XML-RPC servers<N>by creating a server and either installing functions, a<N>class instance, or by extending the SimpleXMLRPCServer<N>class.<N><N>
It can also be used to handle XML-RPC requests in a CGI<N>environment using CGIXMLRPCRequestHandler.<N><N>The Doc* classes can be used to create XML-RPC servers that<N>serve pydoc-style documentation in response to HTTP<N>GET requests. This documentation is dynamically generated<N>based on the functions and methods registered with the<N>server.<N><N>
A list of possible usage patterns follows:<N><N>1. Install functions:<N><N>server = SimpleXMLRPCServer(("localhost", 8000))<N>server.register_function(pow)<N>server.register_function(lambda x,y: x+y, 'add')<N>server.serve_forever()<N><N>2. Install an instance:<N><N>
"""<N>This disables builtin functions (and one exception class) which are<N>removed from Python 3.3.<N><N>This module is designed to be used like this::<N><N>    from future.builtins.disabled import *<N><N>This disables the following obsolete Py2 builtin functions::<N><N>
    apply, cmp, coerce, execfile, file, input, long,<N>    raw_input, reduce, reload, unicode, xrange<N><N>We don't hack __builtin__, which is very fragile because it contaminates<N>imported modules too. Instead, we just create new functions with<N>the same names as the obsolete builtins from Python 2 which raise<N>NameError exceptions when called.<N><N>
Note that both ``input()`` and ``raw_input()`` are among the disabled<N>functions (in this module). Although ``input()`` exists as a builtin in<N>Python 3, the Python 2 ``input()`` builtin is unsafe to use because it<N>can lead to shell injection. Therefore we shadow it by default upon ``from<N>future.builtins.disabled import *``, in case someone forgets to import our<N>replacement ``input()`` somehow and expects Python 3 semantics.<N><N>
See the ``future.builtins.misc`` module for a working version of<N>``input`` with Python 3 semantics.<N><N>(Note that callable() is not among the functions disabled; this was<N>reintroduced into Python 3.2.)<N><N>This exception class is also disabled:<N><N>
    StandardError<N><N>"""<N><N>from __future__ import division, absolute_import, print_function<N><N>from future import utils<N><N><N>OBSOLETE_BUILTINS = ['apply', 'chr', 'cmp', 'coerce', 'execfile', 'file',<N>                     'input', 'long', 'raw_input', 'reduce', 'reload',<N>                     'unicode', 'xrange', 'StandardError']<N><N>
<N>def disabled_function(name):<N>    '''<N>    Returns a function that cannot be called<N>    '''<N>    def disabled(*args, **kwargs):<N>        '''<N>        A function disabled by the ``future`` module. This function is<N>        no longer a builtin in Python 3.<N>        '''<N>        raise NameError('obsolete Python 2 builtin {0} is disabled'.format(name))<N>    return disabled<N><N>
"""<N>This module is designed to be used as follows::<N><N>    from future.builtins.iterators import *<N><N>And then, for example::<N><N>    for i in range(10**15):<N>        pass<N><N>    for (a, b) in zip(range(10**15), range(-10**15, 0)):<N>        pass<N><N>
'''<N>This module provides a newnext() function in Python 2 that mimics the<N>behaviour of ``next()`` in Python 3, falling back to Python 2's behaviour for<N>compatibility if this fails.<N><N>``newnext(iterator)`` calls the iterator's ``__next__()`` method if it exists. If this<N>doesn't exist, it falls back to calling a ``next()`` method.<N><N>
For example:<N><N>    >>> class Odds(object):<N>    ...     def __init__(self, start=1):<N>    ...         self.value = start - 2<N>    ...     def __next__(self):                 # note the Py3 interface<N>    ...         self.value += 2<N>    ...         return self.value<N>    ...     def __iter__(self):<N>    ...         return self<N>    ...<N>    >>> iterator = Odds()<N>    >>> next(iterator)<N>    1<N>    >>> next(iterator)<N>    3<N><N>
If you are defining your own custom iterator class as above, it is preferable<N>to explicitly decorate the class with the @implements_iterator decorator from<N>``future.utils`` as follows:<N><N>    >>> @implements_iterator<N>    ... class Odds(object):<N>    ...     # etc<N>    ...     pass<N><N>
This next() function is primarily for consuming iterators defined in Python 3<N>code elsewhere that we would like to run on Python 2 or 3.<N>'''<N><N>_builtin_next = next<N><N>_SENTINEL = object()<N><N>def newnext(iterator, default=_SENTINEL):<N>    """<N>    next(iterator[, default])<N><N>
"""<N>``python-future``: pure Python implementation of Python 3 round().<N>"""<N><N>from future.utils import PYPY, PY26, bind_method<N><N># Use the decimal module for simplicity of implementation (and<N># hopefully correctness).<N>from decimal import Decimal, ROUND_HALF_EVEN<N><N>
<N>def newround(number, ndigits=None):<N>    """<N>    See Python 3 documentation: uses Banker's Rounding.<N><N>    Delegates to the __round__ method if for some reason this exists.<N><N>    If not, rounds a number to a given precision in decimal digits (default<N>    0 digits). This returns an int when called with one argument,<N>    otherwise the same type as the number. ndigits may be negative.<N><N>
    See the test_round method in future/tests/test_builtins.py for<N>    examples.<N>    """<N>    return_int = False<N>    if ndigits is None:<N>        return_int = True<N>        ndigits = 0<N>    if hasattr(number, '__round__'):<N>        return number.__round__(ndigits)<N><N>
    if ndigits < 0:<N>        raise NotImplementedError('negative ndigits not supported yet')<N>    exponent = Decimal('10') ** (-ndigits)<N><N>    if PYPY:<N>        # Work around issue #24: round() breaks on PyPy with NumPy's types<N>        if 'numpy' in repr(type(number)):<N>            number = float(number)<N><N>
    if isinstance(number, Decimal):<N>        d = number<N>    else:<N>        if not PY26:<N>            d = Decimal.from_float(number).quantize(exponent,<N>                                                rounding=ROUND_HALF_EVEN)<N>        else:<N>            d = from_float_26(number).quantize(exponent, rounding=ROUND_HALF_EVEN)<N><N>
'''<N>This module provides a newsuper() function in Python 2 that mimics the<N>behaviour of super() in Python 3. It is designed to be used as follows:<N><N>    from __future__ import division, absolute_import, print_function<N>    from future.builtins import super<N><N>
And then, for example:<N><N>    class VerboseList(list):<N>        def append(self, item):<N>            print('Adding an item')<N>            super().append(item)        # new simpler super() function<N><N>Importing this module on Python 3 has no effect.<N><N>
This is based on (i.e. almost identical to) Ryan Kelly's magicsuper<N>module here:<N><N>    https://github.com/rfk/magicsuper.git<N><N>Excerpts from Ryan's docstring:<N><N>  "Of course, you can still explicitly pass in the arguments if you want<N>  to do something strange.  Sometimes you really do want that, e.g. to<N>  skip over some classes in the method resolution order.<N><N>
  "How does it work?  By inspecting the calling frame to determine the<N>  function object being executed and the object on which it's being<N>  called, and then walking the object's __mro__ chain to find out where<N>  that function was defined.  Yuck, but it seems to work..."<N>'''<N><N>
from __future__ import absolute_import<N>import sys<N>from types import FunctionType<N><N>from future.utils import PY3, PY26<N><N><N>_builtin_super = super<N><N>_SENTINEL = object()<N><N>def newsuper(typ=_SENTINEL, type_or_obj=_SENTINEL, framedepth=1):<N>    '''Like builtin super(), but capable of magic.<N><N>
    This acts just like the builtin super() function, but if called<N>    without any arguments it attempts to infer them at runtime.<N>    '''<N>    #  Infer the correct call if used without arguments.<N>    if typ is _SENTINEL:<N>        # We'll need to do some frame hacking.<N>        f = sys._getframe(framedepth)<N><N>
        try:<N>            # Get the function's first positional argument.<N>            type_or_obj = f.f_locals[f.f_code.co_varnames[0]]<N>        except (IndexError, KeyError,):<N>            raise RuntimeError('super() used in a function with no args')<N><N>
        try:<N>            # Get the MRO so we can crawl it.<N>            mro = type_or_obj.__mro__<N>        except (AttributeError, RuntimeError):  # see issue #160<N>            try:<N>                mro = type_or_obj.__class__.__mro__<N>            except AttributeError:<N>                raise RuntimeError('super() used with a non-newstyle class')<N><N>
import itertools<N><N>from future import utils<N>if utils.PY2:<N>    from __builtin__ import max as _builtin_max, min as _builtin_min<N>else:<N>    from builtins import max as _builtin_max, min as _builtin_min<N><N>_SENTINEL = object()<N><N><N>def newmin(*args, **kwargs):<N>    return new_min_max(_builtin_min, *args, **kwargs)<N><N>
<N>def newmax(*args, **kwargs):<N>    return new_min_max(_builtin_max, *args, **kwargs)<N><N><N>def new_min_max(_builtin_func, *args, **kwargs):<N>    """<N>    To support the argument "default" introduced in python 3.4 for min and max<N>    :param _builtin_func: builtin min or builtin max<N>    :param args:<N>    :param kwargs:<N>    :return: returns the min or max based on the arguments passed<N>    """<N><N>
    for key, _ in kwargs.items():<N>        if key not in set(['key', 'default']):<N>            raise TypeError('Illegal argument %s', key)<N><N>    if len(args) == 0:<N>        raise TypeError<N><N>    if len(args) != 1 and kwargs.get('default', _SENTINEL) is not _SENTINEL:<N>        raise TypeError<N><N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from builtins import *<N>else:<N>    __future_module__ = True<N>    from __builtin__ import *<N>    # Overwrite any old definitions with the equivalent future.builtins ones:<N>    from future.builtins import *<N>
from __future__ import absolute_import<N>import sys<N><N>from future.utils import PY2, PY26<N>__future_module__ = True<N><N>from collections import *<N><N>if PY2:<N>    from UserDict import UserDict<N>    from UserList import UserList<N>    from UserString import UserString<N><N>if PY26:<N>    from future.backports.misc import OrderedDict, Counter<N><N>if sys.version_info < (3, 3):<N>    from future.backports.misc import ChainMap, _count_elements<N>
from __future__ import absolute_import<N><N>from future.utils import PY2<N><N>if PY2:<N>    from ConfigParser import *<N>else:<N>    from configparser import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    import copyreg, sys<N>    # A "*" import uses Python 3's copyreg.__all__ which does not include<N>    # all public names in the API surface for copyreg, this avoids that<N>    # problem by just making our module _be_ a reference to the actual module.<N>    sys.modules['future.moves.copyreg'] = copyreg<N>else:<N>    __future_module__ = True<N>    from copy_reg import *<N>
from __future__ import absolute_import<N><N>from itertools import *<N>try:<N>    zip_longest = izip_longest<N>    filterfalse = ifilterfalse<N>except NameError:<N>    pass<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from pickle import *<N>else:<N>    __future_module__ = True<N>    try:<N>        from cPickle import *<N>    except ImportError:<N>        from pickle import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from queue import *<N>else:<N>    __future_module__ = True<N>    from Queue import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from reprlib import *<N>else:<N>    __future_module__ = True<N>    from repr import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from socketserver import *<N>else:<N>    __future_module__ = True<N>    from SocketServer import *<N>
from __future__ import absolute_import<N>from future.utils import PY2, PY26<N><N>from subprocess import *<N><N>if PY2:<N>    __future_module__ = True<N>    from commands import getoutput, getstatusoutput<N><N>if PY26:<N>    from future.backports.misc import check_output<N>
from __future__ import absolute_import<N><N>from future.utils import PY2<N><N>from sys import *<N><N>if PY2:<N>    from __builtin__ import intern<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from winreg import *<N>else:<N>    __future_module__ = True<N>    from _winreg import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from _dummy_thread import *<N>else:<N>    __future_module__ = True<N>    from dummy_thread import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from _markupbase import *<N>else:<N>    __future_module__ = True<N>    from markupbase import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from _thread import *<N>else:<N>    __future_module__ = True<N>    from thread import *<N>
# future.moves package<N>from __future__ import absolute_import<N>import sys<N>__future_module__ = True<N>from future.standard_library import import_top_level_modules<N><N>if sys.version_info[0] >= 3:<N>    import_top_level_modules()<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from dbm.dumb import *<N>else:<N>    __future_module__ = True<N>    from dumbdbm import *<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from dbm.gnu import *<N>else:<N>    __future_module__ = True<N>    from gdbm import *<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from dbm.ndbm import *<N>else:<N>    __future_module__ = True<N>    from dbm import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from dbm import *<N>else:<N>    __future_module__ = True<N>    from whichdb import *<N>    from anydbm import *<N><N># Py3.3's dbm/__init__.py imports ndbm but doesn't expose it via __all__.<N># In case some (badly written) code depends on dbm.ndbm after import dbm,<N># we simulate this:<N>if PY3:<N>    from dbm import ndbm<N>else:<N>    try:<N>        from future.moves.dbm import ndbm<N>    except ImportError:<N>        ndbm = None<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from html.entities import *<N>else:<N>    __future_module__ = True<N>    from htmlentitydefs import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N>__future_module__ = True<N><N>if PY3:<N>    from html.parser import *<N>else:<N>    from HTMLParser import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N>__future_module__ = True<N><N>if PY3:<N>    from html import *<N>else:<N>    # cgi.escape isn't good enough for the single Py3.3 html test to pass.<N>    # Define it inline here instead. From the Py3.4 stdlib. Note that the<N>    # html.escape() function from the Py3.3 stdlib is not suitable for use on<N>    # Py2.x.<N>    """<N>    General functions for HTML manipulation.<N>    """<N><N>
from future.utils import PY3<N><N>if PY3:<N>    from http.client import *<N>else:<N>    from httplib import *<N>    from httplib import HTTPMessage<N>    __future_module__ = True<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from http.cookiejar import *<N>else:<N>    __future_module__ = True<N>    from cookielib import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from http.cookies import *<N>else:<N>    __future_module__ = True<N>    from Cookie import *<N>    from Cookie import Morsel    # left out of __all__ on Py2.7!<N>
from __future__ import absolute_import<N>from future.standard_library import suspend_hooks<N>from future.utils import PY3<N><N>if PY3:<N>    from test.support import *<N>else:<N>    __future_module__ = True<N>    with suspend_hooks():<N>        from test.test_support import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if not PY3:<N>    __future_module__ = True<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.colorchooser import *<N>else:<N>    try:<N>        from tkColorChooser import *<N>    except ImportError:<N>        raise ImportError('The tkColorChooser module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.commondialog import *<N>else:<N>    try:<N>        from tkCommonDialog import *<N>    except ImportError:<N>        raise ImportError('The tkCommonDialog module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.constants import *<N>else:<N>    try:<N>        from Tkconstants import *<N>    except ImportError:<N>        raise ImportError('The Tkconstants module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.dialog import *<N>else:<N>    try:<N>        from Dialog import *<N>    except ImportError:<N>        raise ImportError('The Dialog module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.dnd import *<N>else:<N>    try:<N>        from Tkdnd import *<N>    except ImportError:<N>        raise ImportError('The Tkdnd module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.filedialog import *<N>else:<N>    try:<N>        from FileDialog import *<N>    except ImportError:<N>        raise ImportError('The FileDialog module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.font import *<N>else:<N>    try:<N>        from tkFont import *<N>    except ImportError:<N>        raise ImportError('The tkFont module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.messagebox import *<N>else:<N>    try:<N>        from tkMessageBox import *<N>    except ImportError:<N>        raise ImportError('The tkMessageBox module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.scrolledtext import *<N>else:<N>    try:<N>        from ScrolledText import *<N>    except ImportError:<N>        raise ImportError('The ScrolledText module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.simpledialog import *<N>else:<N>    try:<N>        from SimpleDialog import *<N>    except ImportError:<N>        raise ImportError('The SimpleDialog module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.tix import *<N>else:<N>    try:<N>        from Tix import *<N>    except ImportError:<N>        raise ImportError('The Tix module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N><N>from future.utils import PY3<N><N>if PY3:<N>    from tkinter.ttk import *<N>else:<N>    try:<N>        from ttk import *<N>    except ImportError:<N>        raise ImportError('The ttk module is missing. Does your Py2 '<N>                          'installation include tkinter?')<N>
from __future__ import absolute_import<N>from future.utils import PY3<N>__future_module__ = True<N><N>if not PY3:<N>    from Tkinter import *<N>    from Tkinter import (_cnfmerge, _default_root, _flatten,<N>                          _support_default_root, _test,<N>                         _tkinter, _setit)<N><N>
    try: # >= 2.7.4<N>        from Tkinter import (_join) <N>    except ImportError: <N>        pass<N><N>    try: # >= 2.7.4<N>        from Tkinter import (_stringify)<N>    except ImportError: <N>        pass<N><N>    try: # >= 2.7.9<N>        from Tkinter import (_splitdict)<N>    except ImportError:<N>        pass<N><N>
from __future__ import absolute_import<N>from future.standard_library import suspend_hooks<N><N>from future.utils import PY3<N><N>if PY3:<N>    from urllib.error import *<N>else:<N>    __future_module__ = True<N><N>    # We use this method to get at the original Py2 urllib before any renaming magic<N>    # ContentTooShortError = sys.py2_modules['urllib'].ContentTooShortError<N><N>    with suspend_hooks():<N>        from urllib import ContentTooShortError<N>        from urllib2 import URLError, HTTPError<N>
from __future__ import absolute_import<N>from future.standard_library import suspend_hooks<N><N>from future.utils import PY3<N><N>if PY3:<N>    from urllib.parse import *<N>else:<N>    __future_module__ = True<N>    from urlparse import (ParseResult, SplitResult, parse_qs, parse_qsl,<N>                          urldefrag, urljoin, urlparse, urlsplit,<N>                          urlunparse, urlunsplit)<N><N>
    # we use this method to get at the original py2 urllib before any renaming<N>    # quote = sys.py2_modules['urllib'].quote<N>    # quote_plus = sys.py2_modules['urllib'].quote_plus<N>    # unquote = sys.py2_modules['urllib'].unquote<N>    # unquote_plus = sys.py2_modules['urllib'].unquote_plus<N>    # urlencode = sys.py2_modules['urllib'].urlencode<N>    # splitquery = sys.py2_modules['urllib'].splitquery<N><N>
    with suspend_hooks():<N>        from urllib import (quote,<N>                            quote_plus,<N>                            unquote,<N>                            unquote_plus,<N>                            urlencode,<N>                            splitquery)<N><N><N>
from future import standard_library<N>from future.utils import PY3<N><N>if PY3:<N>    from urllib.response import *<N>else:<N>    __future_module__ = True<N>    with standard_library.suspend_hooks():<N>        from urllib import (addbase,<N>                            addclosehook,<N>                            addinfo,<N>                            addinfourl)<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from urllib.robotparser import *<N>else:<N>    __future_module__ = True<N>    from robotparser import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if not PY3:<N>    __future_module__ = True<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from xmlrpc.client import *<N>else:<N>    from xmlrpclib import *<N>
from __future__ import absolute_import<N>from future.utils import PY3<N><N>if PY3:<N>    from xmlrpc.server import *<N>else:<N>    from xmlrpclib import *<N>
"""<N>Python 3 reorganized the standard library (PEP 3108). This module exposes<N>several standard library modules to Python 2 under their new Python 3<N>names.<N><N>It is designed to be used as follows::<N><N>    from future import standard_library<N>    standard_library.install_aliases()<N><N>
And then these normal Py3 imports work on both Py3 and Py2::<N><N>    import builtins<N>    import copyreg<N>    import queue<N>    import reprlib<N>    import socketserver<N>    import winreg    # on Windows only<N>    import test.support<N>    import html, html.parser, html.entites<N>    import http, http.client, http.server<N>    import http.cookies, http.cookiejar<N>    import urllib.parse, urllib.request, urllib.response, urllib.error, urllib.robotparser<N>    import xmlrpc.client, xmlrpc.server<N><N>
    import _thread<N>    import _dummy_thread<N>    import _markupbase<N><N>    from itertools import filterfalse, zip_longest<N>    from sys import intern<N>    from collections import UserDict, UserList, UserString<N>    from collections import OrderedDict, Counter, ChainMap     # even on Py2.6<N>    from subprocess import getoutput, getstatusoutput<N>    from subprocess import check_output              # even on Py2.6<N><N>
(The renamed modules and functions are still available under their old<N>names on Python 2.)<N><N>This is a cleaner alternative to this idiom (see<N>http://docs.pythonsprints.com/python3_porting/py-porting.html)::<N><N>    try:<N>        import queue<N>    except ImportError:<N>        import Queue as queue<N><N>
<N>Limitations<N>-----------<N>We don't currently support these modules, but would like to::<N><N>    import dbm<N>    import dbm.dumb<N>    import dbm.gnu<N>    import collections.abc  # on Py33<N>    import pickle     # should (optionally) bring in cPickle on Python 2<N><N>
"""<N><N>from __future__ import absolute_import, division, print_function<N><N>import sys<N>import logging<N>import imp<N>import contextlib<N>import types<N>import copy<N>import os<N><N># Make a dedicated logger; leave the root logger to be configured<N># by the application.<N>flog = logging.getLogger('future_stdlib')<N>_formatter = logging.Formatter(logging.BASIC_FORMAT)<N>_handler = logging.StreamHandler()<N>_handler.setFormatter(_formatter)<N>flog.addHandler(_handler)<N>flog.setLevel(logging.WARN)<N><N>
from future.utils import PY2, PY3<N><N># The modules that are defined under the same names on Py3 but with<N># different contents in a significant way (e.g. submodules) are:<N>#   pickle (fast one)<N>#   dbm<N>#   urllib<N>#   test<N>#   email<N><N>REPLACED_MODULES = set(['test', 'urllib', 'pickle', 'dbm'])  # add email and dbm when we support it<N><N>
from __future__ import print_function, absolute_import<N>import os<N>import tempfile<N>import unittest<N>import sys<N>import re<N>import warnings<N>import io<N>from textwrap import dedent<N><N>from future.utils import bind_method, PY26, PY3, PY2, PY27<N>from future.moves.subprocess import check_output, STDOUT, CalledProcessError<N><N>
if PY26:<N>    import unittest2 as unittest<N><N><N>def reformat_code(code):<N>    """<N>    Removes any leading \n and dedents.<N>    """<N>    if code.startswith('\n'):<N>        code = code[1:]<N>    return dedent(code)<N><N><N>def order_future_lines(code):<N>    """<N>    Returns the code block with any ``__future__`` import lines sorted, and<N>    then any ``future`` import lines sorted, then any ``builtins`` import lines<N>    sorted.<N><N>
    This only sorts the lines within the expected blocks.<N><N>    See test_order_future_lines() for an example.<N>    """<N><N>    # We need .splitlines(keepends=True), which doesn't exist on Py2,<N>    # so we use this instead:<N>    lines = code.split('\n')<N><N>
    uufuture_line_numbers = [i for i, line in enumerate(lines)<N>                               if line.startswith('from __future__ import ')]<N><N>    future_line_numbers = [i for i, line in enumerate(lines)<N>                             if line.startswith('from future')<N>                             or line.startswith('from past')]<N><N>
    builtins_line_numbers = [i for i, line in enumerate(lines)<N>                             if line.startswith('from builtins')]<N><N>    assert code.lstrip() == code, ('internal usage error: '<N>            'dedent the code before calling order_future_lines()')<N><N>
    def mymax(numbers):<N>        return max(numbers) if len(numbers) > 0 else 0<N><N>    def mymin(numbers):<N>        return min(numbers) if len(numbers) > 0 else float('inf')<N><N>    assert mymax(uufuture_line_numbers) <= mymin(future_line_numbers), \<N>            'the __future__ and future imports are out of order'<N><N>
    # assert mymax(future_line_numbers) <= mymin(builtins_line_numbers), \<N>    #         'the future and builtins imports are out of order'<N><N>    uul = sorted([lines[i] for i in uufuture_line_numbers])<N>    sorted_uufuture_lines = dict(zip(uufuture_line_numbers, uul))<N><N>
    fl = sorted([lines[i] for i in future_line_numbers])<N>    sorted_future_lines = dict(zip(future_line_numbers, fl))<N><N>    bl = sorted([lines[i] for i in builtins_line_numbers])<N>    sorted_builtins_lines = dict(zip(builtins_line_numbers, bl))<N><N>
"""<N>Pure-Python implementation of a Python 3-like bytes object for Python 2.<N><N>Why do this? Without it, the Python 2 bytes object is a very, very<N>different beast to the Python 3 bytes object.<N>"""<N><N>from numbers import Integral<N>import string<N>import copy<N><N>
from future.utils import istext, isbytes, PY2, PY3, with_metaclass<N>from future.types import no, issubset<N>from future.types.newobject import newobject<N><N>if PY2:<N>    from collections import Iterable<N>else:<N>    from collections.abc import Iterable<N><N>
<N>_builtin_bytes = bytes<N><N>if PY3:<N>    # We'll probably never use newstr on Py3 anyway...<N>    unicode = str<N><N><N>class BaseNewBytes(type):<N>    def __instancecheck__(cls, instance):<N>        if cls == newbytes:<N>            return isinstance(instance, _builtin_bytes)<N>        else:<N>            return issubclass(instance.__class__, cls)<N><N>
<N>def _newchr(x):<N>    if isinstance(x, str):  # this happens on pypy<N>        return x.encode('ascii')<N>    else:<N>        return chr(x)<N><N><N>class newbytes(with_metaclass(BaseNewBytes, _builtin_bytes)):<N>    """<N>    A backport of the Python 3 bytes object to Py2<N>    """<N>    def __new__(cls, *args, **kwargs):<N>        """<N>        From the Py3 bytes docstring:<N><N>
        bytes(iterable_of_ints) -> bytes<N>        bytes(string, encoding[, errors]) -> bytes<N>        bytes(bytes_or_buffer) -> immutable copy of bytes_or_buffer<N>        bytes(int) -> bytes object of size given by the parameter initialized with null bytes<N>        bytes() -> empty bytes object<N><N>
        Construct an immutable array of bytes from:<N>          - an iterable yielding integers in range(256)<N>          - a text string encoded using the specified encoding<N>          - any object implementing the buffer API.<N>          - an integer<N>        """<N><N>
"""<N>A dict subclass for Python 2 that behaves like Python 3's dict<N><N>Example use:<N><N>>>> from builtins import dict<N>>>> d1 = dict()    # instead of {} for an empty dict<N>>>> d2 = dict(key1='value1', key2='value2')<N><N>The keys, values and items methods now return iterators on Python 2.x<N>(with set-like behaviour on Python 2.7).<N><N>
>>> for d in (d1, d2):<N>...     assert not isinstance(d.keys(), list)<N>...     assert not isinstance(d.values(), list)<N>...     assert not isinstance(d.items(), list)<N>"""<N><N>import sys<N><N>from future.utils import with_metaclass<N>from future.types.newobject import newobject<N><N>
<N>_builtin_dict = dict<N>ver = sys.version_info[:2]<N><N><N>class BaseNewDict(type):<N>    def __instancecheck__(cls, instance):<N>        if cls == newdict:<N>            return isinstance(instance, _builtin_dict)<N>        else:<N>            return issubclass(instance.__class__, cls)<N><N>
"""<N>Backport of Python 3's int, based on Py2's long.<N><N>They are very similar. The most notable difference is:<N><N>- representation: trailing L in Python 2 removed in Python 3<N>"""<N>from __future__ import division<N><N>import struct<N><N>from future.types.newbytes import newbytes<N>from future.types.newobject import newobject<N>from future.utils import PY3, isint, istext, isbytes, with_metaclass, native<N><N>
<N>if PY3:<N>    long = int<N>    from collections.abc import Iterable<N>else:<N>    from collections import Iterable<N><N><N>class BaseNewInt(type):<N>    def __instancecheck__(cls, instance):<N>        if cls == newint:<N>            # Special case for Py2 short or long int<N>            return isinstance(instance, (int, long))<N>        else:<N>            return issubclass(instance.__class__, cls)<N><N>
"""<N>A list subclass for Python 2 that behaves like Python 3's list.<N><N>The primary difference is that lists have a .copy() method in Py3.<N><N>Example use:<N><N>>>> from builtins import list<N>>>> l1 = list()    # instead of {} for an empty list<N>>>> l1.append('hello')<N>>>> l2 = l1.copy()<N><N>
"""<N><N>import sys<N>import copy<N><N>from future.utils import with_metaclass<N>from future.types.newobject import newobject<N><N><N>_builtin_list = list<N>ver = sys.version_info[:2]<N><N><N>class BaseNewList(type):<N>    def __instancecheck__(cls, instance):<N>        if cls == newlist:<N>            return isinstance(instance, _builtin_list)<N>        else:<N>            return issubclass(instance.__class__, cls)<N><N>
<N>class newlist(with_metaclass(BaseNewList, _builtin_list)):<N>    """<N>    A backport of the Python 3 list object to Py2<N>    """<N>    def copy(self):<N>        """<N>        L.copy() -> list -- a shallow copy of L<N>        """<N>        return copy.copy(self)<N><N>
    def clear(self):<N>        """L.clear() -> None -- remove all items from L"""<N>        for i in range(len(self)):<N>            self.pop()<N><N>    def __new__(cls, *args, **kwargs):<N>        """<N>        list() -> new empty list<N>        list(iterable) -> new list initialized from iterable's items<N>        """<N><N>
        if len(args) == 0:<N>            return super(newlist, cls).__new__(cls)<N>        elif type(args[0]) == newlist:<N>            value = args[0]<N>        else:<N>            value = args[0]<N>        return super(newlist, cls).__new__(cls, value)<N><N>
    def __add__(self, value):<N>        return newlist(super(newlist, self).__add__(value))<N><N>    def __radd__(self, left):<N>        " left + self "<N>        try:<N>            return newlist(left) + self<N>        except:<N>            return NotImplemented<N><N>
    def __getitem__(self, y):<N>        """<N>        x.__getitem__(y) <==> x[y]<N><N>        Warning: a bug in Python 2.x prevents indexing via a slice from<N>        returning a newlist object.<N>        """<N>        if isinstance(y, slice):<N>            return newlist(super(newlist, self).__getitem__(y))<N>        else:<N>            return super(newlist, self).__getitem__(y)<N><N>
"""<N>A pretty lame implementation of a memoryview object for Python 2.6.<N>"""<N>from numbers import Integral<N>import string<N><N>from future.utils import istext, isbytes, PY2, with_metaclass<N>from future.types import no, issubset<N><N>if PY2:<N>    from collections import Iterable<N>else:<N>    from collections.abc import Iterable<N><N>
# class BaseNewBytes(type):<N>#     def __instancecheck__(cls, instance):<N>#         return isinstance(instance, _builtin_bytes)<N><N><N>class newmemoryview(object):   # with_metaclass(BaseNewBytes, _builtin_bytes)):<N>    """<N>    A pretty lame backport of the Python 2.7 and Python 3.x<N>    memoryviewview object to Py2.6.<N>    """<N>    def __init__(self, obj):<N>        return obj<N><N>
"""<N>An object subclass for Python 2 that gives new-style classes written in the<N>style of Python 3 (with ``__next__`` and unicode-returning ``__str__`` methods)<N>the appropriate Python 2-style ``next`` and ``__unicode__`` methods for compatible.<N><N>
Example use::<N><N>    from builtins import object<N><N>    my_unicode_str = u'Unicode string: \u5b54\u5b50'<N><N>    class A(object):<N>        def __str__(self):<N>            return my_unicode_str<N><N>    a = A()<N>    print(str(a))<N><N>    # On Python 2, these relations hold:<N>    assert unicode(a) == my_unicode_string<N>    assert str(a) == my_unicode_string.encode('utf-8')<N><N>
<N>Another example::<N><N>    from builtins import object<N><N>    class Upper(object):<N>        def __init__(self, iterable):<N>            self._iter = iter(iterable)<N>        def __next__(self):                 # note the Py3 interface<N>            return next(self._iter).upper()<N>        def __iter__(self):<N>            return self<N><N>
"""<N>A substitute for the Python 3 open() function.<N><N>Note that io.open() is more complete but maybe slower. Even so, the<N>completeness may be a better default. TODO: compare these<N>"""<N><N>_builtin_open = open<N><N>class newopen(object):<N>    """Wrapper providing key part of Python 3 open() interface.<N><N>
    From IPython's py3compat.py module. License: BSD.<N>    """<N>    def __init__(self, fname, mode="r", encoding="utf-8"):<N>        self.f = _builtin_open(fname, mode)<N>        self.enc = encoding<N><N>    def write(self, s):<N>        return self.f.write(s.encode(self.enc))<N><N>
    def read(self, size=-1):<N>        return self.f.read(size).decode(self.enc)<N><N>    def close(self):<N>        return self.f.close()<N><N>    def __enter__(self):<N>        return self<N><N>    def __exit__(self, etype, value, traceback):<N>        self.f.close()<N><N><N>
"""<N>Nearly identical to xrange.py, by Dan Crosta, from<N><N>    https://github.com/dcrosta/xrange.git<N><N>This is included here in the ``future`` package rather than pointed to as<N>a dependency because there is no package for ``xrange`` on PyPI. It is<N>also tweaked to appear like a regular Python 3 ``range`` object rather<N>than a Python 2 xrange.<N><N>
From Dan Crosta's README:<N><N>    "A pure-Python implementation of Python 2.7's xrange built-in, with<N>    some features backported from the Python 3.x range built-in (which<N>    replaced xrange) in that version."<N><N>    Read more at<N>        https://late.am/post/2012/06/18/what-the-heck-is-an-xrange<N>"""<N>from __future__ import absolute_import<N><N>
from future.utils import PY2<N><N>if PY2:<N>    from collections import Sequence, Iterator<N>else:<N>    from collections.abc import Sequence, Iterator<N>from itertools import islice<N><N>from future.backports.misc import count   # with step parameter on Py2.6<N># For backward compatibility with python-future versions < 0.14.4:<N>_count = count<N><N>
"""<N>This module redefines ``str`` on Python 2.x to be a subclass of the Py2<N>``unicode`` type that behaves like the Python 3.x ``str``.<N><N>The main differences between ``newstr`` and Python 2.x's ``unicode`` type are<N>the stricter type-checking and absence of a `u''` prefix in the representation.<N><N>
It is designed to be used together with the ``unicode_literals`` import<N>as follows:<N><N>    >>> from __future__ import unicode_literals<N>    >>> from builtins import str, isinstance<N><N>On Python 3.x and normally on Python 2.x, these expressions hold<N><N>
    >>> str('blah') is 'blah'<N>    True<N>    >>> isinstance('blah', str)<N>    True<N><N>However, on Python 2.x, with this import:<N><N>    >>> from __future__ import unicode_literals<N><N>the same expressions are False:<N><N>    >>> str('blah') is 'blah'<N>    False<N>    >>> isinstance('blah', str)<N>    False<N><N>
This module is designed to be imported together with ``unicode_literals`` on<N>Python 2 to bring the meaning of ``str`` back into alignment with unprefixed<N>string literals (i.e. ``unicode`` subclasses).<N><N>Note that ``str()`` (and ``print()``) would then normally call the<N>``__unicode__`` method on objects in Python 2. To define string<N>representations of your objects portably across Py3 and Py2, use the<N>:func:`python_2_unicode_compatible` decorator in  :mod:`future.utils`.<N><N>
"""<N><N>from numbers import Number<N><N>from future.utils import PY3, istext, with_metaclass, isnewbytes<N>from future.types import no, issubset<N>from future.types.newobject import newobject<N><N><N>if PY3:<N>    # We'll probably never use newstr on Py3 anyway...<N>    unicode = str<N>    from collections.abc import Iterable<N>else:<N>    from collections import Iterable<N><N>
<N>class BaseNewStr(type):<N>    def __instancecheck__(cls, instance):<N>        if cls == newstr:<N>            return isinstance(instance, unicode)<N>        else:<N>            return issubclass(instance.__class__, cls)<N><N><N>class newstr(with_metaclass(BaseNewStr, unicode)):<N>    """<N>    A backport of the Python 3 str object to Py2<N>    """<N>    no_convert_msg = "Can't convert '{0}' object to str implicitly"<N><N>
"""<N>This module contains backports the data types that were significantly changed<N>in the transition from Python 2 to Python 3.<N><N>- an implementation of Python 3's bytes object (pure Python subclass of<N>  Python 2's builtin 8-bit str type)<N>- an implementation of Python 3's str object (pure Python subclass of<N>  Python 2's builtin unicode type)<N>- a backport of the range iterator from Py3 with slicing support<N><N>
It is used as follows::<N><N>    from __future__ import division, absolute_import, print_function<N>    from builtins import bytes, dict, int, range, str<N><N>to bring in the new semantics for these functions from Python 3. And<N>then, for example::<N><N>
    b = bytes(b'ABCD')<N>    assert list(b) == [65, 66, 67, 68]<N>    assert repr(b) == "b'ABCD'"<N>    assert [65, 66] in b<N><N>    # These raise TypeErrors:<N>    # b + u'EFGH'<N>    # b.split(u'B')<N>    # bytes(b',').join([u'Fred', u'Bill'])<N><N>
<N>    s = str(u'ABCD')<N><N>    # These raise TypeErrors:<N>    # s.join([b'Fred', b'Bill'])<N>    # s.startswith(b'A')<N>    # b'B' in s<N>    # s.find(b'A')<N>    # s.replace(u'A', b'a')<N><N>    # This raises an AttributeError:<N>    # s.decode('utf-8')<N><N>
    assert repr(s) == 'ABCD'      # consistent repr with Py3 (no u prefix)<N><N><N>    for i in range(10**11)[:10]:<N>        pass<N><N>and::<N><N>    class VerboseList(list):<N>        def append(self, item):<N>            print('Adding an item')<N>            super().append(item)        # new simpler super() function<N><N>
For more information:<N>---------------------<N><N>- future.types.newbytes<N>- future.types.newdict<N>- future.types.newint<N>- future.types.newobject<N>- future.types.newrange<N>- future.types.newstr<N><N><N>Notes<N>=====<N><N>range()<N>-------<N>``range`` is a custom class that backports the slicing behaviour from<N>Python 3 (based on the ``xrange`` module by Dan Crosta). See the<N>``newrange`` module docstring for more details.<N><N>
<N>super()<N>-------<N>``super()`` is based on Ryan Kelly's ``magicsuper`` module. See the<N>``newsuper`` module docstring for more details.<N><N><N>round()<N>-------<N>Python 3 modifies the behaviour of ``round()`` to use "Banker's Rounding".<N>See http://stackoverflow.com/a/10825998. See the ``newround`` module<N>docstring for more details.<N><N>
"""<N><N>from __future__ import absolute_import, division, print_function<N><N>import functools<N>from numbers import Integral<N><N>from future import utils<N><N><N># Some utility functions to enforce strict type-separation of unicode str and<N># bytes:<N>def disallow_types(argnums, disallowed_types):<N>    """<N>    A decorator that raises a TypeError if any of the given numbered<N>    arguments is of the corresponding given type (e.g. bytes or unicode<N>    string).<N><N>
    For example:<N><N>        @disallow_types([0, 1], [unicode, bytes])<N>        def f(a, b):<N>            pass<N><N>    raises a TypeError when f is called if a unicode object is passed as<N>    `a` or a bytes object is passed as `b`.<N><N>    This also skips over keyword arguments, so<N><N>
        @disallow_types([0, 1], [unicode, bytes])<N>        def g(a, b=None):<N>            pass<N><N>    doesn't raise an exception if g is called with only one argument a,<N>    e.g.:<N><N>        g(b'Byte string')<N><N>    Example use:<N><N>    >>> class newbytes(object):<N>    ...     @disallow_types([1], [unicode])<N>    ...     def __add__(self, other):<N>    ...          pass<N><N>
"""<N>This is Victor Stinner's pure-Python implementation of PEP 383: the "surrogateescape" error<N>handler of Python 3.<N><N>Source: misc/python/surrogateescape.py in https://bitbucket.org/haypo/misc<N>"""<N><N># This code is released under the Python license and the BSD 2-clause license<N><N>
import codecs<N>import sys<N><N>from future import utils<N><N><N>FS_ERRORS = 'surrogateescape'<N><N>#     # -- Python 2/3 compatibility -------------------------------------<N>#     FS_ERRORS = 'my_surrogateescape'<N><N>def u(text):<N>    if utils.PY3:<N>        return text<N>    else:<N>        return text.decode('unicode_escape')<N><N>
from .core import encode, decode, alabel, ulabel, IDNAError<N>import codecs<N>import re<N>from typing import Tuple, Optional<N><N>_unicode_dots_re = re.compile('[\u002e\u3002\uff0e\uff61]')<N><N>class Codec(codecs.Codec):<N><N>    def encode(self, data: str, errors: str = 'strict') -> Tuple[bytes, int]:<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return b"", 0<N><N>        return encode(data), len(data)<N><N>    def decode(self, data: bytes, errors: str = 'strict') -> Tuple[str, int]:<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return '', 0<N><N>        return decode(data), len(data)<N><N>class IncrementalEncoder(codecs.BufferedIncrementalEncoder):<N>    def _buffer_encode(self, data: str, errors: str, final: bool) -> Tuple[str, int]:  # type: ignore<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return "", 0<N><N>        labels = _unicode_dots_re.split(data)<N>        trailing_dot = ''<N>        if labels:<N>            if not labels[-1]:<N>                trailing_dot = '.'<N>                del labels[-1]<N>            elif not final:<N>                # Keep potentially unfinished label until the next call<N>                del labels[-1]<N>                if labels:<N>                    trailing_dot = '.'<N><N>
        result = []<N>        size = 0<N>        for label in labels:<N>            result.append(alabel(label))<N>            if size:<N>                size += 1<N>            size += len(label)<N><N>        # Join with U+002E<N>        result_str = '.'.join(result) + trailing_dot  # type: ignore<N>        size += len(trailing_dot)<N>        return result_str, size<N><N>
class IncrementalDecoder(codecs.BufferedIncrementalDecoder):<N>    def _buffer_decode(self, data: str, errors: str, final: bool) -> Tuple[str, int]:  # type: ignore<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return ('', 0)<N><N>        labels = _unicode_dots_re.split(data)<N>        trailing_dot = ''<N>        if labels:<N>            if not labels[-1]:<N>                trailing_dot = '.'<N>                del labels[-1]<N>            elif not final:<N>                # Keep potentially unfinished label until the next call<N>                del labels[-1]<N>                if labels:<N>                    trailing_dot = '.'<N><N>
        result = []<N>        size = 0<N>        for label in labels:<N>            result.append(ulabel(label))<N>            if size:<N>                size += 1<N>            size += len(label)<N><N>        result_str = '.'.join(result) + trailing_dot<N>        size += len(trailing_dot)<N>        return (result_str, size)<N><N>
from .core import *<N>from .codec import *<N>from typing import Any, Union<N><N>def ToASCII(label: str) -> bytes:<N>    return encode(label)<N><N>def ToUnicode(label: Union[bytes, bytearray]) -> str:<N>    return decode(label)<N><N>def nameprep(s: Any) -> None:<N>    raise NotImplementedError('IDNA 2008 does not utilise nameprep protocol')<N><N>
from . import idnadata<N>import bisect<N>import unicodedata<N>import re<N>from typing import Union, Optional<N>from .intranges import intranges_contain<N><N>_virama_combining_class = 9<N>_alabel_prefix = b'xn--'<N>_unicode_dots_re = re.compile('[\u002e\u3002\uff0e\uff61]')<N><N>
class IDNAError(UnicodeError):<N>    """ Base exception for all IDNA-encoding related problems """<N>    pass<N><N><N>class IDNABidiError(IDNAError):<N>    """ Exception when bidirectional requirements are not satisfied """<N>    pass<N><N><N>class InvalidCodepoint(IDNAError):<N>    """ Exception when a disallowed or unallocated codepoint is used """<N>    pass<N><N>
<N>class InvalidCodepointContext(IDNAError):<N>    """ Exception when the codepoint is not valid in the context it is used """<N>    pass<N><N><N>def _combining_class(cp: int) -> int:<N>    v = unicodedata.combining(chr(cp))<N>    if v == 0:<N>        if not unicodedata.name(chr(cp)):<N>            raise ValueError('Unknown character in unicodedata')<N>    return v<N><N>
def _is_script(cp: str, script: str) -> bool:<N>    return intranges_contain(ord(cp), idnadata.scripts[script])<N><N>def _punycode(s: str) -> bytes:<N>    return s.encode('punycode')<N><N>def _unot(s: int) -> str:<N>    return 'U+{:04X}'.format(s)<N><N>
<N>def valid_label_length(label: Union[bytes, str]) -> bool:<N>    if len(label) > 63:<N>        return False<N>    return True<N><N><N>def valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:<N>    if len(label) > (254 if trailing_dot else 253):<N>        return False<N>    return True<N><N>
"""<N>Given a list of integers, made up of (hopefully) a small number of long runs<N>of consecutive integers, compute a representation of the form<N>((start1, end1), (start2, end2) ...). Then answer the question "was x present<N>in the original list?" in time O(log(# runs)).<N>"""<N><N>
import bisect<N>from typing import List, Tuple<N><N>def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:<N>    """Represent a list of integers as a sequence of ranges:<N>    ((start_0, end_0), (start_1, end_1), ...), such that the original<N>    integers are exactly those x such that start_i <= x < end_i for some i.<N><N>
    Ranges are encoded as single integers (start << 32 | end), not as tuples.<N>    """<N><N>    sorted_list = sorted(list_)<N>    ranges = []<N>    last_write = -1<N>    for i in range(len(sorted_list)):<N>        if i+1 < len(sorted_list):<N>            if sorted_list[i] == sorted_list[i+1]-1:<N>                continue<N>        current_range = sorted_list[last_write+1:i+1]<N>        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))<N>        last_write = i<N><N>
from .package_data import __version__<N>from .core import (<N>    IDNABidiError,<N>    IDNAError,<N>    InvalidCodepoint,<N>    InvalidCodepointContext,<N>    alabel,<N>    check_bidi,<N>    check_hyphen_ok,<N>    check_initial_combiner,<N>    check_label,<N>    check_nfc,<N>    decode,<N>    encode,<N>    ulabel,<N>    uts46_remap,<N>    valid_contextj,<N>    valid_contexto,<N>    valid_label_length,<N>    valid_string_length,<N>)<N>from .intranges import intranges_contain<N><N>
__all__ = [<N>    "IDNABidiError",<N>    "IDNAError",<N>    "InvalidCodepoint",<N>    "InvalidCodepointContext",<N>    "alabel",<N>    "check_bidi",<N>    "check_hyphen_ok",<N>    "check_initial_combiner",<N>    "check_label",<N>    "check_nfc",<N>    "decode",<N>    "encode",<N>    "intranges_contain",<N>    "ulabel",<N>    "uts46_remap",<N>    "valid_contextj",<N>    "valid_contexto",<N>    "valid_label_length",<N>    "valid_string_length",<N>]<N><N><N>
"""<N>Utility functions from 2to3, 3to2 and python-modernize (and some home-grown<N>ones).<N><N>Licences:<N>2to3: PSF License v2<N>3to2: Apache Software License (from 3to2/setup.py)<N>python-modernize licence: BSD (from python-modernize/LICENSE)<N>"""<N><N>
from lib2to3.fixer_util import (FromImport, Newline, is_import,<N>                                find_root, does_tree_import, Comma)<N>from lib2to3.pytree import Leaf, Node<N>from lib2to3.pygram import python_symbols as syms, python_grammar<N>from lib2to3.pygram import token<N>from lib2to3.fixer_util import (Node, Call, Name, syms, Comma, Number)<N>import re<N><N>
"""<N>futurize: automatic conversion to clean 2/3 code using ``python-future``<N>======================================================================<N><N>Like Armin Ronacher's modernize.py, ``futurize`` attempts to produce clean<N>standard Python 3 code that runs on both Py2 and Py3.<N><N>
One pass<N>--------<N><N>Use it like this on Python 2 code:<N><N>  $ futurize --verbose mypython2script.py<N><N>This will attempt to port the code to standard Py3 code that also<N>provides Py2 compatibility with the help of the right imports from<N>``future``.<N><N>
To write changes to the files, use the -w flag.<N><N>Two stages<N>----------<N><N>The ``futurize`` script can also be called in two separate stages. First:<N><N>  $ futurize --stage1 mypython2script.py<N><N>This produces more modern Python 2 code that is not yet compatible with Python<N>3. The tests should still run and the diff should be uncontroversial to apply to<N>most Python projects that are willing to drop support for Python 2.5 and lower.<N><N>
After this, the recommended approach is to explicitly mark all strings that must<N>be byte-strings with a b'' prefix and all text (unicode) strings with a u''<N>prefix, and then invoke the second stage of Python 2 to 2/3 conversion with::<N><N>  $ futurize --stage2 mypython2script.py<N><N>
Stage 2 adds a dependency on ``future``. It converts most remaining Python<N>2-specific code to Python 3 code and adds appropriate imports from ``future``<N>to restore Py2 support.<N><N>The command above leaves all unadorned string literals as native strings<N>(byte-strings on Py2, unicode strings on Py3). If instead you would like all<N>unadorned string literals to be promoted to unicode, you can also pass this<N>flag:<N><N>
  $ futurize --stage2 --unicode-literals mypython2script.py<N><N>This adds the declaration ``from __future__ import unicode_literals`` to the<N>top of each file, which implicitly declares all unadorned string literals to be<N>unicode strings (``unicode`` on Py2).<N><N>
All imports<N>-----------<N><N>The --all-imports option forces adding all ``__future__`` imports,<N>``builtins`` imports, and standard library aliases, even if they don't<N>seem necessary for the current state of each module. (This can simplify<N>testing, and can reduce the need to think about Py2 compatibility when editing<N>the code further.)<N><N>
"""<N><N>from __future__ import (absolute_import, print_function, unicode_literals)<N>import future.utils<N>from future import __version__<N><N>import sys<N>import logging<N>import optparse<N>import os<N><N>from lib2to3.main import warn, StdoutRefactoringTool<N>from lib2to3 import refactor<N><N>
from libfuturize.fixes import (lib2to3_fix_names_stage1,<N>                               lib2to3_fix_names_stage2,<N>                               libfuturize_fix_names_stage1,<N>                               libfuturize_fix_names_stage2)<N><N>fixer_pkg = 'libfuturize.fixes'<N><N>
<N>def main(args=None):<N>    """Main program.<N><N>    Args:<N>        fixer_pkg: the name of a package where the fixers are located.<N>        args: optional; a list of command line arguments. If omitted,<N>              sys.argv[1:] is used.<N><N>
"""<N>Fixer for import statements, with a __future__ import line.<N><N>Based on lib2to3/fixes/fix_import.py, but extended slightly so it also<N>supports Cython modules.<N><N>If spam is being imported from the local directory, this import:<N>    from spam import eggs<N>becomes:<N>    from __future__ import absolute_import<N>    from .spam import eggs<N><N>
and this import:<N>    import spam<N>becomes:<N>    from __future__ import absolute_import<N>    from . import spam<N>"""<N><N>from os.path import dirname, join, exists, sep<N>from lib2to3.fixes.fix_import import FixImport<N>from lib2to3.fixer_util import FromImport, syms<N>from lib2to3.fixes.fix_import import traverse_imports<N><N>
from libfuturize.fixer_util import future_import<N><N><N>class FixAbsoluteImport(FixImport):<N>    run_order = 9<N><N>    def transform(self, node, results):<N>        """<N>        Copied from FixImport.transform(), but with this line added in<N>        any modules that had implicit relative imports changed:<N><N>
"""<N>Fixer for adding:<N><N>    from __future__ import absolute_import<N>    from __future__ import division<N>    from __future__ import print_function<N><N>This is "stage 1": hopefully uncontroversial changes.<N><N>Stage 2 adds ``unicode_literals``.<N>"""<N><N>
from lib2to3 import fixer_base<N>from libfuturize.fixer_util import future_import<N><N>class FixAddFutureImportsExceptUnicodeLiterals(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N><N>    run_order = 9<N><N>    def transform(self, node, results):<N>        # Reverse order:<N>        future_import(u"absolute_import", node)<N>        future_import(u"division", node)<N>        future_import(u"print_function", node)<N><N><N>
"""<N>Fixer that adds ``from past.builtins import basestring`` if there is a<N>reference to ``basestring``<N>"""<N><N>from lib2to3 import fixer_base<N><N>from libfuturize.fixer_util import touch_import_top<N><N><N>class FixBasestring(fixer_base.BaseFix):<N>    BM_compatible = True<N><N>    PATTERN = "'basestring'"<N><N>    def transform(self, node, results):<N>        touch_import_top(u'past.builtins', 'basestring', node)<N>
"""Optional fixer that changes all unprefixed string literals "..." to b"...".<N><N>br'abcd' is a SyntaxError on Python 2 but valid on Python 3.<N>ur'abcd' is a SyntaxError on Python 3 but valid on Python 2.<N><N>"""<N>from __future__ import unicode_literals<N><N>
import re<N>from lib2to3.pgen2 import token<N>from lib2to3 import fixer_base<N><N>_literal_re = re.compile(r"[^bBuUrR]?[\'\"]")<N><N>class FixBytes(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "STRING"<N><N>    def transform(self, node, results):<N>        if node.type == token.STRING:<N>            if _literal_re.match(node.value):<N>                new = node.clone()<N>                new.value = u'b' + new.value<N>                return new<N><N><N>
# coding: utf-8<N>"""<N>Fixer for the cmp() function on Py2, which was removed in Py3.<N><N>Adds this import line::<N><N>    from past.builtins import cmp<N><N>if cmp() is called in the code.<N>"""<N><N>from __future__ import unicode_literals<N>from lib2to3 import fixer_base<N><N>
from libfuturize.fixer_util import touch_import_top<N><N><N>expression = "name='cmp'"<N><N><N>class FixCmp(fixer_base.BaseFix):<N>    BM_compatible = True<N>    run_order = 9<N><N>    PATTERN = """<N>              power<<N>                 ({0}) trailer< '(' args=[any] ')' ><N>              rest=any* ><N>              """.format(expression)<N><N>
"""<N>UNFINISHED<N>For the ``future`` package.<N><N>Adds this import line:<N><N>    from __future__ import division<N><N>at the top so the code runs identically on Py3 and Py2.6/2.7<N>"""<N><N>from libpasteurize.fixes.fix_division import FixDivision<N>
"""<N>For the ``future`` package.<N><N>Adds this import line:<N><N>    from __future__ import division<N><N>at the top and changes any old-style divisions to be calls to<N>past.utils.old_div so the code runs as before on Py2.6/2.7 and has the same<N>behaviour on Py3.<N><N>
If "from __future__ import division" is already in effect, this fixer does<N>nothing.<N>"""<N><N>import re<N>from lib2to3.fixer_util import Leaf, Node, Comma<N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import (token, future_import, touch_import_top,<N>                                    wrap_in_fn_call)<N><N>
<N>def match_division(node):<N>    u"""<N>    __future__.division redefines the meaning of a single slash for division,<N>    so we match that and only that.<N>    """<N>    slash = token.SLASH<N>    return node.type == slash and not node.next_sibling.type == slash and \<N>                                  not node.prev_sibling.type == slash<N><N>
# coding: utf-8<N>"""<N>Fixer for the execfile() function on Py2, which was removed in Py3.<N><N>The Lib/lib2to3/fixes/fix_execfile.py module has some problems: see<N>python-future issue #37. This fixer merely imports execfile() from<N>past.builtins and leaves the code alone.<N><N>
Adds this import line::<N><N>    from past.builtins import execfile<N><N>for the function execfile() that was removed from Py3.<N>"""<N><N>from __future__ import unicode_literals<N>from lib2to3 import fixer_base<N><N>from libfuturize.fixer_util import touch_import_top<N><N>
<N>expression = "name='execfile'"<N><N><N>class FixExecfile(fixer_base.BaseFix):<N>    BM_compatible = True<N>    run_order = 9<N><N>    PATTERN = """<N>              power<<N>                 ({0}) trailer< '(' args=[any] ')' ><N>              rest=any* ><N>              """.format(expression)<N><N>
"""<N>For the ``future`` package.<N><N>Adds this import line::<N><N>    from builtins import XYZ<N><N>for each of the functions XYZ that is used in the module.<N><N>Adds these imports after any other imports (in an initial block of them).<N>"""<N><N>
from __future__ import unicode_literals<N><N>from lib2to3 import fixer_base<N>from lib2to3.pygram import python_symbols as syms<N>from lib2to3.fixer_util import Name, Call, in_special_context<N><N>from libfuturize.fixer_util import touch_import_top<N><N>
# All builtins are:<N>#     from future.builtins.iterators import (filter, map, zip)<N>#     from future.builtins.misc import (ascii, chr, hex, input, isinstance, oct, open, round, super)<N>#     from future.types import (bytes, dict, int, range, str)<N># We don't need isinstance any more.<N><N>
"""<N>For the ``future`` package.<N><N>Changes any imports needed to reflect the standard library reorganization. Also<N>Also adds these import lines:<N><N>    from future import standard_library<N>    standard_library.install_aliases()<N><N>after any __future__ imports but before any other imports.<N>"""<N><N>
from lib2to3.fixes.fix_imports import FixImports<N>from libfuturize.fixer_util import touch_import_top<N><N><N>class FixFutureStandardLibrary(FixImports):<N>    run_order = 8<N><N>    def transform(self, node, results):<N>        result = super(FixFutureStandardLibrary, self).transform(node, results)<N>        # TODO: add a blank line between any __future__ imports and this?<N>        touch_import_top(u'future', u'standard_library', node)<N>        return result<N><N><N>
"""<N>For the ``future`` package.<N><N>A special fixer that ensures that these lines have been added::<N><N>    from future import standard_library<N>    standard_library.install_hooks()<N><N>even if the only module imported was ``urllib``, in which case the regular fixer<N>wouldn't have added these lines.<N><N>
"""<N>Fixer for input.<N><N>Does a check for `from builtins import input` before running the lib2to3 fixer.<N>The fixer will not run when the input is already present.<N><N><N>this:<N>    a = input()<N>becomes:<N>    from builtins import input<N>    a = eval(input())<N><N>
and this:<N>    from builtins import input<N>    a = input()<N>becomes (no change):<N>    from builtins import input<N>    a = input()<N>"""<N><N>import lib2to3.fixes.fix_input<N>from lib2to3.fixer_util import does_tree_import<N><N><N>class FixInput(lib2to3.fixes.fix_input.FixInput):<N>    def transform(self, node, results):<N><N>
# coding: utf-8<N>"""Fixer for __metaclass__ = X -> (future.utils.with_metaclass(X)) methods.<N><N>   The various forms of classef (inherits nothing, inherits once, inherints<N>   many) don't parse the same in the CST so we look at ALL classes for<N>   a __metaclass__ and if we find one normalize the inherits to all be<N>   an arglist.<N><N>
   For one-liner classes ('class X: pass') there is no indent/dedent so<N>   we normalize those into having a suite.<N><N>   Moving the __metaclass__ into the classdef can also cause the class<N>   body to be empty so there is some special casing for that as well.<N><N>
"""<N>Based on fix_next.py by Collin Winter.<N><N>Replaces it.next() -> next(it), per PEP 3114.<N><N>Unlike fix_next.py, this fixer doesn't replace the name of a next method with __next__,<N>which would break Python 2 compatibility without further help from fixers in<N>stage 2.<N>"""<N><N>
# Local imports<N>from lib2to3.pgen2 import token<N>from lib2to3.pygram import python_symbols as syms<N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name, Call, find_binding<N><N>bind_warning = "Calls to builtin next() possibly shadowed by global binding"<N><N>
<N>class FixNextCall(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = """<N>    power< base=any+ trailer< '.' attr='next' > trailer< '(' ')' > ><N>    |<N>    power< head=any+ trailer< '.' attr='next' > not trailer< '(' ')' > ><N>    |<N>    global=global_stmt< 'global' any* 'next' any* ><N>    """<N><N>
    order = "pre" # Pre-order tree traversal<N><N>    def start_tree(self, tree, filename):<N>        super(FixNextCall, self).start_tree(tree, filename)<N><N>        n = find_binding('next', tree)<N>        if n:<N>            self.warning(n, bind_warning)<N>            self.shadowed_next = True<N>        else:<N>            self.shadowed_next = False<N><N>
"""<N>Fixer that adds ``from builtins import object`` if there is a line<N>like this:<N>    class Foo(object):<N>"""<N><N>from lib2to3 import fixer_base<N><N>from libfuturize.fixer_util import touch_import_top<N><N><N>class FixObject(fixer_base.BaseFix):<N><N>    PATTERN = u"classdef< 'class' NAME '(' name='object' ')' colon=':' any >"<N><N>    def transform(self, node, results):<N>        touch_import_top(u'builtins', 'object', node)<N>
"""<N>For the ``future`` package.<N><N>Adds this import line:<N><N>    from past.builtins import str as oldstr<N><N>at the top and wraps any unadorned string literals 'abc' or explicit byte-string<N>literals b'abc' in oldstr() calls so the code has the same behaviour on Py3 as<N>on Py2.6/2.7.<N>"""<N><N>
from __future__ import unicode_literals<N>import re<N>from lib2to3 import fixer_base<N>from lib2to3.pgen2 import token<N>from lib2to3.fixer_util import syms<N>from libfuturize.fixer_util import (future_import, touch_import_top,<N>                                    wrap_in_fn_call)<N><N>
"""<N>UNFINISHED<N><N>Fixer for turning multiple lines like these:<N><N>    from __future__ import division<N>    from __future__ import absolute_import<N>    from __future__ import print_function<N><N>into a single line like this:<N><N>    from __future__ import (absolute_import, division, print_function)<N><N>
This helps with testing of ``futurize``.<N>"""<N><N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import future_import<N><N>class FixOrderFutureImports(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N><N>    run_order = 10<N><N>
    # def match(self, node):<N>    #     """<N>    #     Match only once per file<N>    #     """<N>    #     if hasattr(node, 'type') and node.type == syms.file_input:<N>    #         return True<N>    #     return False<N><N>    def transform(self, node, results):<N>        # TODO    # write me<N>        pass<N><N><N>
# Copyright 2006 Google, Inc. All Rights Reserved.<N># Licensed to PSF under a Contributor Agreement.<N><N>"""Fixer for print.<N><N>Change:<N>    "print"          into "print()"<N>    "print ..."      into "print(...)"<N>    "print(...)"     not changed<N>    "print ... ,"    into "print(..., end=' ')"<N>    "print >>x, ..." into "print(..., file=x)"<N><N>
No changes are applied if print_function is imported from __future__<N><N>"""<N><N># Local imports<N>from lib2to3 import patcomp, pytree, fixer_base<N>from lib2to3.pgen2 import token<N>from lib2to3.fixer_util import Name, Call, Comma, String<N># from libmodernize import add_future<N><N>
parend_expr = patcomp.compile_pattern(<N>              """atom< '(' [arith_expr|atom|power|term|STRING|NAME] ')' >"""<N>              )<N><N><N>class FixPrint(fixer_base.BaseFix):<N><N>    BM_compatible = True<N><N>    PATTERN = """<N>              simple_stmt< any* bare='print' any* > | print_stmt<N>              """<N><N>
"""<N>For the ``future`` package.<N><N>Turns any print statements into functions and adds this import line:<N><N>    from __future__ import print_function<N><N>at the top to retain compatibility with Python 2.6+.<N>"""<N><N>from libfuturize.fixes.fix_print import FixPrint<N>from libfuturize.fixer_util import future_import<N><N>
class FixPrintWithImport(FixPrint):<N>    run_order = 7<N>    def transform(self, node, results):<N>        # Add the __future__ import first. (Otherwise any shebang or encoding<N>        # comment line attached as a prefix to the print statement will be<N>        # copied twice and appear twice.)<N>        future_import(u'print_function', node)<N>        n_stmt = super(FixPrintWithImport, self).transform(node, results)<N>        return n_stmt<N><N><N>
"""Fixer for 'raise E, V'<N><N>From Armin Ronacher's ``python-modernize``.<N><N>raise         -> raise<N>raise E       -> raise E<N>raise E, 5    -> raise E(5)<N>raise E, 5, T -> raise E(5).with_traceback(T)<N>raise E, None, T -> raise E.with_traceback(T)<N><N>
raise (((E, E'), E''), E'''), 5 -> raise E(5)<N>raise "foo", V, T               -> warns about string exceptions<N><N>raise E, (V1, V2) -> raise E(V1, V2)<N>raise E, (V1, V2), T -> raise E(V1, V2).with_traceback(T)<N><N><N>CAVEATS:<N>1) "raise E, V, T" cannot be translated safely in general. If V<N>   is not a tuple or a (number, string, None) literal, then:<N><N>
   raise E, V, T -> from future.utils import raise_<N>                    raise_(E, V, T)<N>"""<N># Author: Collin Winter, Armin Ronacher, Mark Huang<N><N># Local imports<N>from lib2to3 import pytree, fixer_base<N>from lib2to3.pgen2 import token<N>from lib2to3.fixer_util import Name, Call, is_tuple, Comma, Attr, ArgList<N><N>
from libfuturize.fixer_util import touch_import_top<N><N><N>class FixRaise(fixer_base.BaseFix):<N><N>    BM_compatible = True<N>    PATTERN = """<N>    raise_stmt< 'raise' exc=any [',' val=any [',' tb=any]] ><N>    """<N><N>    def transform(self, node, results):<N>        syms = self.syms<N><N>
"""<N>Fixer for removing any of these lines:<N><N>    from __future__ import with_statement<N>    from __future__ import nested_scopes<N>    from __future__ import generators<N><N>The reason is that __future__ imports like these are required to be the first<N>line of code (after docstrings) on Python 2.6+, which can get in the way.<N><N>
These imports are always enabled in Python 2.6+, which is the minimum sane<N>version to target for Py2/3 compatibility.<N>"""<N><N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import remove_future_import<N><N>class FixRemoveOldFutureImports(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N>    run_order = 1<N><N>
"""Fixer that changes unicode to str and unichr to chr, but -- unlike the<N>lib2to3 fix_unicode.py fixer, does not change u"..." into "...".<N><N>The reason is that Py3.3+ supports the u"..." string prefix, and, if<N>present, the prefix may provide useful information for disambiguating<N>between byte strings and unicode strings, which is often the hardest part<N>of the porting task.<N><N>
"""<N><N>from lib2to3.pgen2 import token<N>from lib2to3 import fixer_base<N><N>_mapping = {u"unichr" : u"chr", u"unicode" : u"str"}<N><N>class FixUnicodeKeepU(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "'unicode' | 'unichr'"<N><N>
"""<N>Adds this import:<N><N>    from __future__ import unicode_literals<N><N>"""<N><N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import future_import<N><N>class FixUnicodeLiteralsImport(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N><N>    run_order = 9<N><N>    def transform(self, node, results):<N>        future_import(u"unicode_literals", node)<N>
"""Fix UserDict.<N><N>Incomplete!<N><N>TODO: base this on fix_urllib perhaps?<N>"""<N><N><N># Local imports<N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name, attr_chain<N>from lib2to3.fixes.fix_imports import alternates, build_pattern, FixImports<N><N>
"""<N>For the ``future`` package.<N><N>Turns any xrange calls into range calls and adds this import line:<N><N>    from builtins import range<N><N>at the top.<N>"""<N><N>from lib2to3.fixes.fix_xrange import FixXrange<N><N>from libfuturize.fixer_util import touch_import_top<N><N><N>class FixXrangeWithImport(FixXrange):<N>    def transform(self, node, results):<N>        result = super(FixXrangeWithImport, self).transform(node, results)<N>        touch_import_top('builtins', 'range', node)<N>        return result<N>
"""<N>pasteurize: automatic conversion of Python 3 code to clean 2/3 code<N>===================================================================<N><N>``pasteurize`` attempts to convert existing Python 3 code into source-compatible<N>Python 2 and 3 code.<N><N>
Use it like this on Python 3 code:<N><N>  $ pasteurize --verbose mypython3script.py<N><N>This removes any Py3-only syntax (e.g. new metaclasses) and adds these<N>import lines:<N><N>    from __future__ import absolute_import<N>    from __future__ import division<N>    from __future__ import print_function<N>    from __future__ import unicode_literals<N>    from future import standard_library<N>    standard_library.install_hooks()<N>    from builtins import *<N><N>
To write changes to the files, use the -w flag.<N><N>It also adds any other wrappers needed for Py2/3 compatibility.<N><N>Note that separate stages are not available (or needed) when converting from<N>Python 3 with ``pasteurize`` as they are when converting from Python 2 with<N>``futurize``.<N><N>
The --all-imports option forces adding all ``__future__`` imports,<N>``builtins`` imports, and standard library aliases, even if they don't<N>seem necessary for the current state of each module. (This can simplify<N>testing, and can reduce the need to think about Py2 compatibility when editing<N>the code further.)<N><N>
"""<N><N>from __future__ import (absolute_import, print_function, unicode_literals)<N><N>import sys<N>import logging<N>import optparse<N>from lib2to3.main import main, warn, StdoutRefactoringTool<N>from lib2to3 import refactor<N><N>from future import __version__<N>from libpasteurize.fixes import fix_names<N><N>
u"""<N>Base classes for features that are backwards-incompatible.<N><N>Usage:<N>features = Features()<N>features.add(Feature("py3k_feature", "power< 'py3k' any* >", "2.7"))<N>PATTERN = features.PATTERN<N>"""<N><N>pattern_unformatted = u"%s=%s" # name=pattern, for dict lookups<N>message_unformatted = u"""<N>%s is only supported in Python %s and above."""<N><N>
class Feature(object):<N>    u"""<N>    A feature has a name, a pattern, and a minimum version of Python 2.x<N>    required to use the feature (or 3.x if there is no backwards-compatible<N>    version of 2.x)<N>    """<N>    def __init__(self, name, PATTERN, version):<N>        self.name = name<N>        self._pattern = PATTERN<N>        self.version = version<N><N>
    def message_text(self):<N>        u"""<N>        Format the above text with the name and minimum version required.<N>        """<N>        return message_unformatted % (self.name, self.version)<N><N>class Features(set):<N>    u"""<N>    A set of features that generates a pattern for the features it contains.<N>    This set will act like a mapping in that we map names to patterns.<N>    """<N>    mapping = {}<N><N>
"""<N>For the ``future`` package.<N><N>Adds this import line::<N><N>    from builtins import (ascii, bytes, chr, dict, filter, hex, input,<N>                          int, list, map, next, object, oct, open, pow,<N>                          range, round, str, super, zip)<N><N>
to a module, irrespective of whether each definition is used.<N><N>Adds these imports after any other imports (in an initial block of them).<N>"""<N><N>from __future__ import unicode_literals<N><N>from lib2to3 import fixer_base<N><N>from libfuturize.fixer_util import touch_import_top<N><N>
<N>class FixAddAllFutureBuiltins(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N>    run_order = 1<N><N>    def transform(self, node, results):<N>        # import_str = """(ascii, bytes, chr, dict, filter, hex, input,<N>        #                      int, list, map, next, object, oct, open, pow,<N>        #                      range, round, str, super, zip)"""<N>        touch_import_top(u'builtins', '*', node)<N><N>
        # builtins = """ascii bytes chr dict filter hex input<N>        #                      int list map next object oct open pow<N>        #                      range round str super zip"""<N>        # for builtin in sorted(builtins.split(), reverse=True):<N>        #     touch_import_top(u'builtins', builtin, node)<N><N><N>
"""<N>Fixer for adding:<N><N>    from __future__ import absolute_import<N>    from __future__ import division<N>    from __future__ import print_function<N>    from __future__ import unicode_literals<N><N>This is done when converting from Py3 to both Py3/Py2.<N>"""<N><N>
from lib2to3 import fixer_base<N>from libfuturize.fixer_util import future_import<N><N>class FixAddAllFutureImports(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N>    run_order = 1<N><N>    def transform(self, node, results):<N>        future_import(u"absolute_import", node)<N>        future_import(u"division", node)<N>        future_import(u"print_function", node)<N>        future_import(u"unicode_literals", node)<N><N><N>
"""<N>For the ``future`` package.<N><N>Adds this import line:<N><N>    from future import standard_library<N><N>after any __future__ imports but before any other imports. Doesn't actually<N>change the imports to Py3 style.<N>"""<N><N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import touch_import_top<N><N>
class FixAddFutureStandardLibraryImport(fixer_base.BaseFix):<N>    BM_compatible = True<N>    PATTERN = "file_input"<N>    run_order = 8<N><N>    def transform(self, node, results):<N>        # TODO: add a blank line between any __future__ imports and this?<N>        touch_import_top(u'future', u'standard_library', node)<N>        # TODO: also add standard_library.install_hooks()<N><N><N>
u"""<N>Fixer to remove function annotations<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.pgen2 import token<N>from lib2to3.fixer_util import syms<N><N>warning_text = u"Removing function annotations completely."<N><N>def param_without_annotations(node):<N>    return node.children[0]<N><N>
class FixAnnotations(fixer_base.BaseFix):<N><N>    warned = False<N><N>    def warn_once(self, node, reason):<N>        if not self.warned:<N>            self.warned = True<N>            self.warning(node, reason=reason)<N><N>    PATTERN = u"""<N>              funcdef< 'def' any parameters< '(' [params=any] ')' > ['->' ret=any] ':' any* ><N>              """<N><N>
u"""<N>Warn about features that are not present in Python 2.5, giving a message that<N>points to the earliest version of Python 2.x (or 3.x, if none) that supports it<N>"""<N><N>from .feature_base import Feature, Features<N>from lib2to3 import fixer_base<N><N>
u"""<N>Fixer for getfullargspec -> getargspec<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name<N><N>warn_msg = u"some of the values returned by getfullargspec are not valid in Python 2 and have no equivalent."<N><N>class FixFullargspec(fixer_base.BaseFix):<N><N>    PATTERN = u"'getfullargspec'"<N><N>    def transform(self, node, results):<N>        self.warning(node, warn_msg)<N>        return Name(u"getargspec", prefix=node.prefix)<N>
"""<N>Adds this import line:<N><N>    from builtins import XYZ<N><N>for each of the functions XYZ that is used in the module.<N>"""<N><N>from __future__ import unicode_literals<N><N>from lib2to3 import fixer_base<N>from lib2to3.pygram import python_symbols as syms<N>from lib2to3.fixer_util import Name, Call, in_special_context<N><N>
from libfuturize.fixer_util import touch_import_top<N><N># All builtins are:<N>#     from future.builtins.iterators import (filter, map, zip)<N>#     from future.builtins.misc import (ascii, chr, hex, input, isinstance, oct, open, round, super)<N>#     from future.types import (bytes, dict, int, range, str)<N># We don't need isinstance any more.<N><N>
replaced_builtins = '''filter map zip<N>                       ascii chr hex input next oct open round super<N>                       bytes dict int range str'''.split()<N><N>expression = '|'.join(["name='{0}'".format(name) for name in replaced_builtins])<N><N>
<N>class FixFutureBuiltins(fixer_base.BaseFix):<N>    BM_compatible = True<N>    run_order = 9<N><N>    # Currently we only match uses as a function. This doesn't match e.g.:<N>    #     if isinstance(s, str):<N>    #         ...<N>    PATTERN = """<N>              power<<N>                 ({0}) trailer< '(' args=[any] ')' ><N>              rest=any* ><N>              """.format(expression)<N><N>
u"""<N>Fixer for os.getcwd() -> os.getcwdu().<N>Also warns about "from os import getcwd", suggesting the above form.<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name<N><N>class FixGetcwd(fixer_base.BaseFix):<N><N>    PATTERN = u"""<N>              power< 'os' trailer< dot='.' name='getcwd' > any* ><N>              |<N>              import_from< 'from' 'os' 'import' bad='getcwd' ><N>              """<N><N>
    def transform(self, node, results):<N>        if u"name" in results:<N>            name = results[u"name"]<N>            name.replace(Name(u"getcwdu", prefix=name.prefix))<N>        elif u"bad" in results:<N>            # Can't convert to getcwdu and then expect to catch every use.<N>            self.cannot_convert(node, u"import os, use os.getcwd() instead.")<N>            return<N>        else:<N>            raise ValueError(u"For some reason, the pattern matcher failed.")<N><N><N>
u"""<N>Fixer for standard library imports renamed in Python 3<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name, is_probably_builtin, Newline, does_tree_import<N>from lib2to3.pygram import python_symbols as syms<N>from lib2to3.pgen2 import token<N>from lib2to3.pytree import Node, Leaf<N><N>
u"""<N>Fixer for Python 3 function parameter syntax<N>This fixer is rather sensitive to incorrect py3k syntax.<N>"""<N><N># Note: "relevant" parameters are parameters following the first STAR in the list.<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import token, String, Newline, Comma, Name<N>from libfuturize.fixer_util import indentation, suitify, DoubleStar<N><N>
u"""<N>Fixer for memoryview(s) -> buffer(s).<N>Explicit because some memoryview methods are invalid on buffer objects.<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name<N><N><N>class FixMemoryview(fixer_base.BaseFix):<N><N>
    explicit = True # User must specify that they want this.<N><N>    PATTERN = u"""<N>              power< name='memoryview' trailer< '(' [any] ')' ><N>              rest=any* ><N>              """<N><N>    def transform(self, node, results):<N>        name = results[u"name"]<N>        name.replace(Name(u"buffer", prefix=name.prefix))<N><N><N>
u"""<N>Fixer for (metaclass=X) -> __metaclass__ = X<N>Some semantics (see PEP 3115) may be altered in the translation."""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name, syms, Node, Leaf, Newline, find_root<N>from lib2to3.pygram import token<N>from libfuturize.fixer_util import indentation, suitify<N># from ..fixer_util import Name, syms, Node, Leaf, Newline, find_root, indentation, suitify<N><N>
u"""<N>Fixer for "class Foo: ..." -> "class Foo(object): ..."<N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import LParen, RParen, Name<N><N>from libfuturize.fixer_util import touch_import_top<N><N><N>def insert_object(node, idx):<N>    node.insert_child(idx, RParen())<N>    node.insert_child(idx, Name(u"object"))<N>    node.insert_child(idx, LParen())<N><N>
u"""<N>Fixer for:<N>it.__next__() -> it.next().<N>next(it) -> it.next().<N>"""<N><N>from lib2to3.pgen2 import token<N>from lib2to3.pygram import python_symbols as syms<N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Name, Call, find_binding, Attr<N><N>
u"""<N>Fixer for print: from __future__ import print_function.<N>"""<N><N>from lib2to3 import fixer_base<N>from libfuturize.fixer_util import future_import<N><N>class FixPrintfunction(fixer_base.BaseFix):<N><N>    # explicit = True<N><N>    PATTERN = u"""<N>              power< 'print' trailer < '(' any* ')' > any* ><N>              """<N><N>    def transform(self, node, results):<N>        future_import(u"print_function", node)<N>
u"""Fixer for 'raise E(V).with_traceback(T)' -> 'raise E, V, T'"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Comma, Node, Leaf, token, syms<N><N>class FixRaise(fixer_base.BaseFix):<N><N>    PATTERN = u"""<N>    raise_stmt< 'raise' (power< name=any [trailer< '(' val=any* ')' >]<N>        [trailer< '.' 'with_traceback' > trailer< '(' trc=any ')' >] > | any) ['from' chain=any] >"""<N><N>
u"""Fixer for<N>              raise E(V).with_traceback(T)<N>    to:<N>              from future.utils import raise_<N>              ...<N>              raise_(E, V, T)<N><N>TODO: FIXME!!<N><N>"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.fixer_util import Comma, Node, Leaf, token, syms<N><N>
u"""Fixer for 'g.throw(E(V).with_traceback(T))' -> 'g.throw(E, V, T)'"""<N><N>from lib2to3 import fixer_base<N>from lib2to3.pytree import Node, Leaf<N>from lib2to3.pgen2 import token<N>from lib2to3.fixer_util import Comma<N><N>class FixThrow(fixer_base.BaseFix):<N><N>
u"""<N>Fixer for:<N>(a,)* *b (,c)* [,] = s<N>for (a,)* *b (,c)* [,] in d: ...<N>"""<N><N>from lib2to3 import fixer_base<N>from itertools import count<N>from lib2to3.fixer_util import (Assign, Comma, Call, Newline, Name,<N>                                Number, token, syms, Node, Leaf)<N>from libfuturize.fixer_util import indentation, suitify, commatize<N># from libfuturize.fixer_util import Assign, Comma, Call, Newline, Name, Number, indentation, suitify, commatize, token, syms, Node, Leaf<N><N>
from __future__ import absolute_import<N>import sys<N>from . import ws2_32<N>from . import oleaut32<N><N>"""<N>A small module for keeping a database of ordinal to symbol<N>mappings for DLLs which frequently get linked without symbolic<N>infoz.<N>"""<N><N>
ords = {<N>    b"ws2_32.dll": ws2_32.ord_names,<N>    b"wsock32.dll": ws2_32.ord_names,<N>    b"oleaut32.dll": oleaut32.ord_names,<N>}<N><N>PY3 = sys.version_info > (3,)<N><N>if PY3:<N><N>    def formatOrdString(ord_val):<N>        return "ord{}".format(ord_val).encode()<N><N>
from __future__ import unicode_literals<N><N>import inspect<N><N>from future.utils import PY2, PY3, exec_<N><N>if PY2:<N>    from collections import Mapping<N>else:<N>    from collections.abc import Mapping<N><N>if PY3:<N>    import builtins<N>    from collections.abc import Mapping<N><N>
    def apply(f, *args, **kw):<N>        return f(*args, **kw)<N><N>    from past.builtins import str as oldstr<N><N>    def chr(i):<N>        """<N>        Return a byte-string of one character with ordinal i; 0 <= i <= 256<N>        """<N>        return oldstr(bytes((i,)))<N><N>
    def cmp(x, y):<N>        """<N>        cmp(x, y) -> integer<N><N>        Return negative if x<y, zero if x==y, positive if x>y.<N>        """<N>        return (x > y) - (x < y)<N><N>    from sys import intern<N><N>    def oct(number):<N>        """oct(number) -> string<N><N>
"""<N>This module is designed to be used as follows::<N><N>    from past.builtins.noniterators import filter, map, range, reduce, zip<N><N>And then, for example::<N><N>    assert isinstance(range(5), list)<N><N>The list-producing functions this brings in are::<N><N>
- ``filter``<N>- ``map``<N>- ``range``<N>- ``reduce``<N>- ``zip``<N><N>"""<N><N>from __future__ import division, absolute_import, print_function<N><N>from itertools import chain, starmap<N>import itertools       # since zip_longest doesn't exist on Py2<N>from past.types import basestring<N>from past.utils import PY3<N><N>
<N>def flatmap(f, items):<N>    return chain.from_iterable(map(f, items))<N><N><N>if PY3:<N>    import builtins<N><N>    # list-producing versions of the major Python iterating functions<N>    def oldfilter(*args):<N>        """<N>        filter(function or None, sequence) -> list, tuple, or string<N><N>
"""<N>A resurrection of some old functions from Python 2 for use in Python 3. These<N>should be used sparingly, to help with porting efforts, since code using them<N>is no longer standard Python 3 code.<N><N>This module provides the following:<N><N>1. Implementations of these builtin functions which have no equivalent on Py3:<N><N>
- apply<N>- chr<N>- cmp<N>- execfile<N><N>2. Aliases:<N><N>- intern <- sys.intern<N>- raw_input <- input<N>- reduce <- functools.reduce<N>- reload <- imp.reload<N>- unichr <- chr<N>- unicode <- str<N>- xrange <- range<N><N>3. List-producing versions of the corresponding Python 3 iterator-producing functions:<N><N>
# -*- coding: utf-8 -*-<N>"""<N>past.translation<N>==================<N><N>The ``past.translation`` package provides an import hook for Python 3 which<N>transparently runs ``futurize`` fixers over Python 2 code on import to convert<N>print statements into functions, etc.<N><N>
It is intended to assist users in migrating to Python 3.x even if some<N>dependencies still only support Python 2.x.<N><N>Usage<N>-----<N><N>Once your Py2 package is installed in the usual module search path, the import<N>hook is invoked as follows:<N><N>
    >>> from past.translation import autotranslate<N>    >>> autotranslate('mypackagename')<N><N>Or:<N><N>    >>> autotranslate(['mypackage1', 'mypackage2'])<N><N>You can unregister the hook using::<N><N>    >>> from past.translation import remove_hooks<N>    >>> remove_hooks()<N><N>
Author: Ed Schofield.<N>Inspired by and based on ``uprefix`` by Vinay M. Sajip.<N>"""<N><N>import imp<N>import logging<N>import marshal<N>import os<N>import sys<N>import copy<N>from lib2to3.pgen2.parse import ParseError<N>from lib2to3.refactor import RefactoringTool<N><N>
from libfuturize import fixes<N><N><N>logger = logging.getLogger(__name__)<N>logger.setLevel(logging.DEBUG)<N><N>myfixes = (list(fixes.libfuturize_fix_names_stage1) +<N>           list(fixes.lib2to3_fix_names_stage1) +<N>           list(fixes.libfuturize_fix_names_stage2) +<N>           list(fixes.lib2to3_fix_names_stage2))<N><N>
"""<N>An implementation of the basestring type for Python 3<N><N>Example use:<N><N>>>> s = b'abc'<N>>>> assert isinstance(s, basestring)<N>>>> from past.types import str as oldstr<N>>>> s2 = oldstr(b'abc')<N>>>> assert isinstance(s2, basestring)<N><N>
"""<N><N>import sys<N><N>from past.utils import with_metaclass, PY2<N><N>if PY2:<N>    str = unicode<N><N>ver = sys.version_info[:2]<N><N><N>class BaseBaseString(type):<N>    def __instancecheck__(cls, instance):<N>        return isinstance(instance, (bytes, str))<N><N>
    def __subclasshook__(cls, thing):<N>        # TODO: What should go here?<N>        raise NotImplemented<N><N><N>class basestring(with_metaclass(BaseBaseString)):<N>    """<N>    A minimal backport of the Python 2 basestring type to Py3<N>    """<N><N>
"""<N>A dict subclass for Python 3 that behaves like Python 2's dict<N><N>Example use:<N><N>>>> from past.builtins import dict<N>>>> d1 = dict()    # instead of {} for an empty dict<N>>>> d2 = dict(key1='value1', key2='value2')<N><N>The keys, values and items methods now return lists on Python 3.x and there are<N>methods for iterkeys, itervalues, iteritems, and viewkeys etc.<N><N>
>>> for d in (d1, d2):<N>...     assert isinstance(d.keys(), list)<N>...     assert isinstance(d.values(), list)<N>...     assert isinstance(d.items(), list)<N>"""<N><N>import sys<N><N>from past.utils import with_metaclass<N><N><N>_builtin_dict = dict<N>ver = sys.version_info[:2]<N><N>
<N>class BaseOldDict(type):<N>    def __instancecheck__(cls, instance):<N>        return isinstance(instance, _builtin_dict)<N><N><N>class olddict(with_metaclass(BaseOldDict, _builtin_dict)):<N>    """<N>    A backport of the Python 3 dict object to Py2<N>    """<N>    iterkeys = _builtin_dict.keys<N>    viewkeys = _builtin_dict.keys<N><N>
    def keys(self):<N>        return list(super(olddict, self).keys())<N><N>    itervalues = _builtin_dict.values<N>    viewvalues = _builtin_dict.values<N><N>    def values(self):<N>        return list(super(olddict, self).values())<N><N>    iteritems = _builtin_dict.items<N>    viewitems = _builtin_dict.items<N><N>
"""<N>Pure-Python implementation of a Python 2-like str object for Python 3.<N>"""<N><N>from numbers import Integral<N><N>from past.utils import PY2, with_metaclass<N><N>if PY2:<N>    from collections import Iterable<N>else:<N>    from collections.abc import Iterable<N><N>
"""<N>Forward-ports of types from Python 2 for use with Python 3:<N><N>- ``basestring``: equivalent to ``(str, bytes)`` in ``isinstance`` checks<N>- ``dict``: with list-producing .keys() etc. methods<N>- ``str``: bytes-like, but iterating over them doesn't product integers<N>- ``long``: alias of Py3 int with ``L`` suffix in the ``repr``<N>- ``unicode``: alias of Py3 str with ``u`` prefix in the ``repr``<N><N>
"""<N>Various non-built-in utility functions and definitions for Py2<N>compatibility in Py3.<N><N>For example:<N><N>    >>> # The old_div() function behaves like Python 2's / operator<N>    >>> # without "from __future__ import division"<N>    >>> from past.utils import old_div<N>    >>> old_div(3, 2)    # like 3/2 in Py2<N>    0<N>    >>> old_div(3, 2.0)  # like 3/2.0 in Py2<N>    1.5<N>"""<N><N>
import sys<N>import numbers<N><N>PY3 = sys.version_info[0] >= 3<N>PY2 = sys.version_info[0] == 2<N>PYPY = hasattr(sys, 'pypy_translation_info')<N><N><N>def with_metaclass(meta, *bases):<N>    """<N>    Function from jinja2/_compat.py. License: BSD.<N><N>
from typing import List, Optional<N><N>__version__ = "22.0.4"<N><N><N>def main(args: Optional[List[str]] = None) -> int:<N>    """This is an internal API only meant for use by pip's own console scripts.<N><N>    For additional details, see https://github.com/pypa/pip/issues/7498.<N>    """<N>    from pip._internal.utils.entrypoints import _wrapper<N><N>    return _wrapper(args)<N>
import os<N>import sys<N>import warnings<N><N># Remove '' and current working directory from the first entry<N># of sys.path, if present to avoid using current directory<N># in pip commands check, freeze, install, list and show,<N># when invoked as python -m pip <command><N>if sys.path[0] in ("", os.getcwd()):<N>    sys.path.pop(0)<N><N>
# If we are running from a wheel, add the wheel to sys.path<N># This allows the usage python pip-*.whl/pip install pip-*.whl<N>if __package__ == "":<N>    # __file__ is pip-*.whl/pip/__main__.py<N>    # first dirname call strips of '/__main__.py', second strips off '/pip'<N>    # Resulting path is the name of the wheel itself<N>    # Add that to sys.path so we can import pip<N>    path = os.path.dirname(os.path.dirname(__file__))<N>    sys.path.insert(0, path)<N><N>
if __name__ == "__main__":<N>    # Work around the error reported in #9540, pending a proper fix.<N>    # Note: It is essential the warning filter is set *before* importing<N>    #       pip, as the deprecation happens at import time, not runtime.<N>    warnings.filterwarnings(<N>        "ignore", category=DeprecationWarning, module=".*packaging\\.version"<N>    )<N>    from pip._internal.cli.main import main as _main<N><N>
"""Build Environment used for isolation during sdist building<N>"""<N><N>import contextlib<N>import logging<N>import os<N>import pathlib<N>import sys<N>import textwrap<N>import zipfile<N>from collections import OrderedDict<N>from sysconfig import get_paths<N>from types import TracebackType<N>from typing import TYPE_CHECKING, Iterable, Iterator, List, Optional, Set, Tuple, Type<N><N>
"""Cache Management<N>"""<N><N>import hashlib<N>import json<N>import logging<N>import os<N>from typing import Any, Dict, List, Optional, Set<N><N>from pip._vendor.packaging.tags import Tag, interpreter_name, interpreter_version<N>from pip._vendor.packaging.utils import canonicalize_name<N><N>
from pip._internal.exceptions import InvalidWheelFilename<N>from pip._internal.models.format_control import FormatControl<N>from pip._internal.models.link import Link<N>from pip._internal.models.wheel import Wheel<N>from pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds<N>from pip._internal.utils.urls import path_to_url<N><N>
logger = logging.getLogger(__name__)<N><N><N>def _hash_dict(d: Dict[str, str]) -> str:<N>    """Return a stable sha224 of a dictionary."""<N>    s = json.dumps(d, sort_keys=True, separators=(",", ":"), ensure_ascii=True)<N>    return hashlib.sha224(s.encode("ascii")).hexdigest()<N><N>
<N>class Cache:<N>    """An abstract class - provides cache directories for data from links<N><N><N>    :param cache_dir: The root of the cache.<N>    :param format_control: An object of FormatControl class to limit<N>        binaries being read from the cache.<N>    :param allowed_formats: which formats of files the cache should store.<N>        ('binary' and 'source' are the only allowed values)<N>    """<N><N>
    def __init__(<N>        self, cache_dir: str, format_control: FormatControl, allowed_formats: Set[str]<N>    ) -> None:<N>        super().__init__()<N>        assert not cache_dir or os.path.isabs(cache_dir)<N>        self.cache_dir = cache_dir or None<N>        self.format_control = format_control<N>        self.allowed_formats = allowed_formats<N><N>
        _valid_formats = {"source", "binary"}<N>        assert self.allowed_formats.union(_valid_formats) == _valid_formats<N><N>    def _get_cache_path_parts(self, link: Link) -> List[str]:<N>        """Get parts of part that must be os.path.joined with cache_dir"""<N><N>
        # We want to generate an url to use as our cache key, we don't want to<N>        # just re-use the URL because it might have other items in the fragment<N>        # and we don't care about those.<N>        key_parts = {"url": link.url_without_fragment}<N>        if link.hash_name is not None and link.hash is not None:<N>            key_parts[link.hash_name] = link.hash<N>        if link.subdirectory_fragment:<N>            key_parts["subdirectory"] = link.subdirectory_fragment<N><N>
        # Include interpreter name, major and minor version in cache key<N>        # to cope with ill-behaved sdists that build a different wheel<N>        # depending on the python version their setup.py is being run on,<N>        # and don't encode the difference in compatibility tags.<N>        # https://github.com/pypa/pip/issues/7296<N>        key_parts["interpreter_name"] = interpreter_name()<N>        key_parts["interpreter_version"] = interpreter_version()<N><N>
        # Encode our key url with sha224, we'll use this because it has similar<N>        # security properties to sha256, but with a shorter total output (and<N>        # thus less secure). However the differences don't make a lot of<N>        # difference for our use case here.<N>        hashed = _hash_dict(key_parts)<N><N>
        # We want to nest the directories some to prevent having a ton of top<N>        # level directories where we might run out of sub directories on some<N>        # FS.<N>        parts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]<N><N>        return parts<N><N>
    def _get_candidates(self, link: Link, canonical_package_name: str) -> List[Any]:<N>        can_not_cache = not self.cache_dir or not canonical_package_name or not link<N>        if can_not_cache:<N>            return []<N><N>        formats = self.format_control.get_allowed_formats(canonical_package_name)<N>        if not self.allowed_formats.intersection(formats):<N>            return []<N><N>
        candidates = []<N>        path = self.get_path_for_link(link)<N>        if os.path.isdir(path):<N>            for candidate in os.listdir(path):<N>                candidates.append((candidate, path))<N>        return candidates<N><N>    def get_path_for_link(self, link: Link) -> str:<N>        """Return a directory to store cached items in for link."""<N>        raise NotImplementedError()<N><N>
    def get(<N>        self,<N>        link: Link,<N>        package_name: Optional[str],<N>        supported_tags: List[Tag],<N>    ) -> Link:<N>        """Returns a link to a cached item if it exists, otherwise returns the<N>        passed link.<N>        """<N>        raise NotImplementedError()<N><N>
<N>class SimpleWheelCache(Cache):<N>    """A cache of wheels for future installs."""<N><N>    def __init__(self, cache_dir: str, format_control: FormatControl) -> None:<N>        super().__init__(cache_dir, format_control, {"binary"})<N><N>    def get_path_for_link(self, link: Link) -> str:<N>        """Return a directory to store cached wheels for link<N><N>
"""Configuration management setup<N><N>Some terminology:<N>- name<N>  As written in config files.<N>- value<N>  Value associated with a name<N>- key<N>  Name combined with it's section (section.name)<N>- variant<N>  A single word describing where the configuration key-value pair came from<N>"""<N><N>
import configparser<N>import locale<N>import os<N>import sys<N>from typing import Any, Dict, Iterable, List, NewType, Optional, Tuple<N><N>from pip._internal.exceptions import (<N>    ConfigurationError,<N>    ConfigurationFileCouldNotBeLoaded,<N>)<N>from pip._internal.utils import appdirs<N>from pip._internal.utils.compat import WINDOWS<N>from pip._internal.utils.logging import getLogger<N>from pip._internal.utils.misc import ensure_dir, enum<N><N>
"""Exceptions used throughout package.<N><N>This module MUST NOT try to import from anything within `pip._internal` to<N>operate. This is expected to be importable from any/all files within the<N>subpackage and, thus, should not depend on them.<N>"""<N><N>
import configparser<N>import re<N>from itertools import chain, groupby, repeat<N>from typing import TYPE_CHECKING, Dict, List, Optional, Union<N><N>from pip._vendor.requests.models import Request, Response<N>from pip._vendor.rich.console import Console, ConsoleOptions, RenderResult<N>from pip._vendor.rich.markup import escape<N>from pip._vendor.rich.text import Text<N><N>
if TYPE_CHECKING:<N>    from hashlib import _Hash<N>    from typing import Literal<N><N>    from pip._internal.metadata import BaseDistribution<N>    from pip._internal.req.req_install import InstallRequirement<N><N><N>#<N># Scaffolding<N>#<N>def _is_kebab_case(s: str) -> bool:<N>    return re.match(r"^[a-z]+(-[a-z]+)*$", s) is not None<N><N>
<N>def _prefix_with_indent(<N>    s: Union[Text, str],<N>    console: Console,<N>    *,<N>    prefix: str,<N>    indent: str,<N>) -> Text:<N>    if isinstance(s, Text):<N>        text = s<N>    else:<N>        text = console.render_str(s)<N><N>    return console.render_str(prefix, overflow="ignore") + console.render_str(<N>        f"\n{indent}", overflow="ignore"<N>    ).join(text.split(allow_blank=True))<N><N>
<N>class PipError(Exception):<N>    """The base pip error."""<N><N><N>class DiagnosticPipError(PipError):<N>    """An error, that presents diagnostic information to the user.<N><N>    This contains a bunch of logic, to enable pretty presentation of our error<N>    messages. Each error gets a unique reference. Each error can also include<N>    additional context, a hint and/or a note -- which are presented with the<N>    main error message in a consistent style.<N><N>
from typing import List, Optional<N><N><N>def main(args: Optional[List[str]] = None) -> int:<N>    """This is preserved for old console scripts that may still be referencing<N>    it.<N><N>    For additional details, see https://github.com/pypa/pip/issues/7498.<N>    """<N>    from pip._internal.utils.entrypoints import _wrapper<N><N>    return _wrapper(args)<N>
import os<N>from collections import namedtuple<N>from typing import Any, List, Optional<N><N>from pip._vendor import tomli<N>from pip._vendor.packaging.requirements import InvalidRequirement, Requirement<N><N>from pip._internal.exceptions import (<N>    InstallationError,<N>    InvalidPyProjectBuildRequires,<N>    MissingPyProjectBuildRequires,<N>)<N><N>
<N>def _is_list_of_str(obj: Any) -> bool:<N>    return isinstance(obj, list) and all(isinstance(item, str) for item in obj)<N><N><N>def make_pyproject_path(unpacked_source_directory: str) -> str:<N>    return os.path.join(unpacked_source_directory, "pyproject.toml")<N><N>
<N>BuildSystemDetails = namedtuple(<N>    "BuildSystemDetails", ["requires", "backend", "check", "backend_path"]<N>)<N><N><N>def load_pyproject_toml(<N>    use_pep517: Optional[bool], pyproject_toml: str, setup_py: str, req_name: str<N>) -> Optional[BuildSystemDetails]:<N>    """Load the pyproject.toml file.<N><N>
    Parameters:<N>        use_pep517 - Has the user requested PEP 517 processing? None<N>                     means the user hasn't explicitly specified.<N>        pyproject_toml - Location of the project's pyproject.toml file<N>        setup_py - Location of the project's setup.py file<N>        req_name - The name of the requirement we're processing (for<N>                   error reporting)<N><N>
"""Orchestrator for building wheels from InstallRequirements.<N>"""<N><N>import logging<N>import os.path<N>import re<N>import shutil<N>from typing import Any, Callable, Iterable, List, Optional, Tuple<N><N>from pip._vendor.packaging.utils import canonicalize_name, canonicalize_version<N>from pip._vendor.packaging.version import InvalidVersion, Version<N><N>
from typing import List, Optional<N><N>import pip._internal.utils.inject_securetransport  # noqa<N>from pip._internal.utils import _log<N><N># init_logging() must be called before any call to logging.getLogger()<N># which happens at import of most modules.<N>_log.init_logging()<N><N>
<N>def main(args: (Optional[List[str]]) = None) -> int:<N>    """This is preserved for old console scripts that may still be referencing<N>    it.<N><N>    For additional details, see https://github.com/pypa/pip/issues/7498.<N>    """<N>    from pip._internal.utils.entrypoints import _wrapper<N><N>
"""Logic that powers autocompletion installed by ``pip completion``.<N>"""<N><N>import optparse<N>import os<N>import sys<N>from itertools import chain<N>from typing import Any, Iterable, List, Optional<N><N>from pip._internal.cli.main_parser import create_main_parser<N>from pip._internal.commands import commands_dict, create_command<N>from pip._internal.metadata import get_default_environment<N><N>
<N>def autocomplete() -> None:<N>    """Entry Point for completion of main and subcommand options."""<N>    # Don't complete if user hasn't sourced bash_completion file.<N>    if "PIP_AUTO_COMPLETE" not in os.environ:<N>        return<N>    cwords = os.environ["COMP_WORDS"].split()[1:]<N>    cword = int(os.environ["COMP_CWORD"])<N>    try:<N>        current = cwords[cword - 1]<N>    except IndexError:<N>        current = ""<N><N>
"""Base Command class, and related routines"""<N><N>import functools<N>import logging<N>import logging.config<N>import optparse<N>import os<N>import sys<N>import traceback<N>from optparse import Values<N>from typing import Any, Callable, List, Optional, Tuple<N><N>
"""<N>shared options and groups<N><N>The principle here is to define options once, but *not* instantiate them<N>globally. One reason being that options with action='append' can carry state<N>between parses. pip parses general options twice internally, and shouldn't<N>pass on state. To be consistent, all options will follow this design.<N>"""<N><N>
# The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import logging<N>import os<N>import textwrap<N>from functools import partial<N>from optparse import SUPPRESS_HELP, Option, OptionGroup, OptionParser, Values<N>from textwrap import dedent<N>from typing import Any, Callable, Dict, Optional, Tuple<N><N>
from contextlib import ExitStack, contextmanager<N>from typing import ContextManager, Iterator, TypeVar<N><N>_T = TypeVar("_T", covariant=True)<N><N><N>class CommandContextMixIn:<N>    def __init__(self) -> None:<N>        super().__init__()<N>        self._in_main_context = False<N>        self._main_context = ExitStack()<N><N>
    @contextmanager<N>    def main_context(self) -> Iterator[None]:<N>        assert not self._in_main_context<N><N>        self._in_main_context = True<N>        try:<N>            with self._main_context:<N>                yield<N>        finally:<N>            self._in_main_context = False<N><N>
"""Primary application entrypoint.<N>"""<N>import locale<N>import logging<N>import os<N>import sys<N>from typing import List, Optional<N><N>from pip._internal.cli.autocompletion import autocomplete<N>from pip._internal.cli.main_parser import parse_command<N>from pip._internal.commands import create_command<N>from pip._internal.exceptions import PipError<N>from pip._internal.utils import deprecation<N><N>
logger = logging.getLogger(__name__)<N><N><N># Do not import and use main() directly! Using it directly is actively<N># discouraged by pip's maintainers. The name, location and behavior of<N># this function is subject to change, so calling it directly is not<N># portable across different pip versions.<N><N>
# In addition, running pip in-process is unsupported and unsafe. This is<N># elaborated in detail at<N># https://pip.pypa.io/en/stable/user_guide/#using-pip-from-your-program.<N># That document also provides suggestions that should work for nearly<N># all users that are considering importing and using main() directly.<N><N>
# However, we know that certain users will still want to invoke pip<N># in-process. If you understand and accept the implications of using pip<N># in an unsupported manner, the best approach is to use runpy to avoid<N># depending on the exact location of this entry point.<N><N>
# The following example shows how to use runpy to invoke pip in that<N># case:<N>#<N>#     sys.argv = ["pip", your, args, here]<N>#     runpy.run_module("pip", run_name="__main__")<N>#<N># Note that this will exit the process after running, unlike a direct<N># call to main. As it is not safe to do any processing after calling<N># main, this should not be an issue in practice.<N><N>
<N>def main(args: Optional[List[str]] = None) -> int:<N>    if args is None:<N>        args = sys.argv[1:]<N><N>    # Configure our deprecation warnings to be sent through loggers<N>    deprecation.install_warning_logger()<N><N>    autocomplete()<N><N>
"""A single place for constructing and exposing the main parser<N>"""<N><N>import os<N>import sys<N>from typing import List, Tuple<N><N>from pip._internal.cli import cmdoptions<N>from pip._internal.cli.parser import ConfigOptionParser, UpdatingDefaultsHelpFormatter<N>from pip._internal.commands import commands_dict, get_similar_commands<N>from pip._internal.exceptions import CommandError<N>from pip._internal.utils.misc import get_pip_version, get_prog<N><N>
__all__ = ["create_main_parser", "parse_command"]<N><N><N>def create_main_parser() -> ConfigOptionParser:<N>    """Creates and returns the main parser for pip's CLI"""<N><N>    parser = ConfigOptionParser(<N>        usage="\n%prog <command> [options]",<N>        add_help_option=False,<N>        formatter=UpdatingDefaultsHelpFormatter(),<N>        name="global",<N>        prog=get_prog(),<N>    )<N>    parser.disable_interspersed_args()<N><N>
    parser.version = get_pip_version()<N><N>    # add the general options<N>    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)<N>    parser.add_option_group(gen_opts)<N><N>    # so the help formatter knows<N>    parser.main = True  # type: ignore<N><N>
    # create command listing for description<N>    description = [""] + [<N>        f"{name:27} {command_info.summary}"<N>        for name, command_info in commands_dict.items()<N>    ]<N>    parser.description = "\n".join(description)<N><N>    return parser<N><N>
"""Base option parser setup"""<N><N>import logging<N>import optparse<N>import shutil<N>import sys<N>import textwrap<N>from contextlib import suppress<N>from typing import Any, Dict, Iterator, List, Tuple<N><N>from pip._internal.cli.status_codes import UNKNOWN_ERROR<N>from pip._internal.configuration import Configuration, ConfigurationError<N>from pip._internal.utils.misc import redact_auth_from_url, strtobool<N><N>
logger = logging.getLogger(__name__)<N><N><N>class PrettyHelpFormatter(optparse.IndentedHelpFormatter):<N>    """A prettier/less verbose help formatter for optparse."""<N><N>    def __init__(self, *args: Any, **kwargs: Any) -> None:<N>        # help position must be aligned with __init__.parseopts.description<N>        kwargs["max_help_position"] = 30<N>        kwargs["indent_increment"] = 1<N>        kwargs["width"] = shutil.get_terminal_size()[0] - 2<N>        super().__init__(*args, **kwargs)<N><N>
    def format_option_strings(self, option: optparse.Option) -> str:<N>        return self._format_option_strings(option)<N><N>    def _format_option_strings(<N>        self, option: optparse.Option, mvarfmt: str = " <{}>", optsep: str = ", "<N>    ) -> str:<N>        """<N>        Return a comma-separated list of option strings and metavars.<N><N>
        :param option:  tuple of (short opt, long opt), e.g: ('-f', '--format')<N>        :param mvarfmt: metavar format string<N>        :param optsep:  separator<N>        """<N>        opts = []<N><N>        if option._short_opts:<N>            opts.append(option._short_opts[0])<N>        if option._long_opts:<N>            opts.append(option._long_opts[0])<N>        if len(opts) > 1:<N>            opts.insert(1, optsep)<N><N>
        if option.takes_value():<N>            assert option.dest is not None<N>            metavar = option.metavar or option.dest.lower()<N>            opts.append(mvarfmt.format(metavar.lower()))<N><N>        return "".join(opts)<N><N>    def format_heading(self, heading: str) -> str:<N>        if heading == "Options":<N>            return ""<N>        return heading + ":\n"<N><N>
    def format_usage(self, usage: str) -> str:<N>        """<N>        Ensure there is only one newline between usage and the first heading<N>        if there is no description.<N>        """<N>        msg = "\nUsage: {}\n".format(self.indent_lines(textwrap.dedent(usage), "  "))<N>        return msg<N><N>
"""Contains the Command base classes that depend on PipSession.<N><N>The classes in this module are in a separate module so the commands not<N>needing download / PackageFinder capability don't unnecessarily import the<N>PackageFinder machinery and all its vendored dependencies, etc.<N>"""<N><N>
import contextlib<N>import itertools<N>import logging<N>import sys<N>import time<N>from typing import IO, Iterator<N><N>from pip._vendor.progress import HIDE_CURSOR, SHOW_CURSOR<N><N>from pip._internal.utils.compat import WINDOWS<N>from pip._internal.utils.logging import get_indentation<N><N>
SUCCESS = 0<N>ERROR = 1<N>UNKNOWN_ERROR = 2<N>VIRTUALENV_NOT_FOUND = 3<N>PREVIOUS_BUILD_DIR_ERROR = 4<N>NO_MATCHES_FOUND = 23<N>
"""Subpackage containing all of pip's command line interface related code<N>"""<N><N># This file intentionally does not import submodules<N>
import os<N>import textwrap<N>from optparse import Values<N>from typing import Any, List<N><N>import pip._internal.utils.filesystem as filesystem<N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import ERROR, SUCCESS<N>from pip._internal.exceptions import CommandError, PipError<N>from pip._internal.utils.logging import getLogger<N><N>
logger = getLogger(__name__)<N><N><N>class CacheCommand(Command):<N>    """<N>    Inspect and manage pip's wheel cache.<N><N>    Subcommands:<N><N>    - dir: Show the cache directory.<N>    - info: Show information about the cache.<N>    - list: List filenames of packages stored in the cache.<N>    - remove: Remove one or more package from the cache.<N>    - purge: Remove all items from the cache.<N><N>
    ``<pattern>`` can be a glob expression or a package name.<N>    """<N><N>    ignore_require_venv = True<N>    usage = """<N>        %prog dir<N>        %prog info<N>        %prog list [<pattern>] [--format=[human, abspath]]<N>        %prog remove <pattern><N>        %prog purge<N>    """<N><N>
    def add_options(self) -> None:<N><N>        self.cmd_opts.add_option(<N>            "--format",<N>            action="store",<N>            dest="list_format",<N>            default="human",<N>            choices=("human", "abspath"),<N>            help="Select the output format among: human (default) or abspath",<N>        )<N><N>
        self.parser.insert_option_group(0, self.cmd_opts)<N><N>    def run(self, options: Values, args: List[str]) -> int:<N>        handlers = {<N>            "dir": self.get_cache_dir,<N>            "info": self.get_cache_info,<N>            "list": self.list_cache_items,<N>            "remove": self.remove_cache_items,<N>            "purge": self.purge_cache,<N>        }<N><N>
        if not options.cache_dir:<N>            logger.error("pip cache commands can not function since cache is disabled.")<N>            return ERROR<N><N>        # Determine action<N>        if not args or args[0] not in handlers:<N>            logger.error(<N>                "Need an action (%s) to perform.",<N>                ", ".join(sorted(handlers)),<N>            )<N>            return ERROR<N><N>
        action = args[0]<N><N>        # Error handling happens here, not in the action-handlers.<N>        try:<N>            handlers[action](options, args[1:])<N>        except PipError as e:<N>            logger.error(e.args[0])<N>            return ERROR<N><N>
        return SUCCESS<N><N>    def get_cache_dir(self, options: Values, args: List[Any]) -> None:<N>        if args:<N>            raise CommandError("Too many arguments")<N><N>        logger.info(options.cache_dir)<N><N>    def get_cache_info(self, options: Values, args: List[Any]) -> None:<N>        if args:<N>            raise CommandError("Too many arguments")<N><N>
        num_http_files = len(self._find_http_files(options))<N>        num_packages = len(self._find_wheels(options, "*"))<N><N>        http_cache_location = self._cache_dir(options, "http")<N>        wheels_cache_location = self._cache_dir(options, "wheels")<N>        http_cache_size = filesystem.format_directory_size(http_cache_location)<N>        wheels_cache_size = filesystem.format_directory_size(wheels_cache_location)<N><N>
import logging<N>from optparse import Values<N>from typing import List<N><N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import ERROR, SUCCESS<N>from pip._internal.operations.check import (<N>    check_package_set,<N>    create_package_set_from_installed,<N>)<N>from pip._internal.utils.misc import write_output<N><N>
logger = logging.getLogger(__name__)<N><N><N>class CheckCommand(Command):<N>    """Verify installed packages have compatible dependencies."""<N><N>    usage = """<N>      %prog [options]"""<N><N>    def run(self, options: Values, args: List[str]) -> int:<N><N>
import sys<N>import textwrap<N>from optparse import Values<N>from typing import List<N><N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import SUCCESS<N>from pip._internal.utils.misc import get_prog<N><N>BASE_COMPLETION = """<N># pip {shell} completion start{script}# pip {shell} completion end<N>"""<N><N>
import locale<N>import logging<N>import os<N>import sys<N>from optparse import Values<N>from types import ModuleType<N>from typing import Any, Dict, List, Optional<N><N>import pip._vendor<N>from pip._vendor.certifi import where<N>from pip._vendor.packaging.version import parse as parse_version<N><N>
from pip import __file__ as pip_location<N>from pip._internal.cli import cmdoptions<N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.cmdoptions import make_target_python<N>from pip._internal.cli.status_codes import SUCCESS<N>from pip._internal.configuration import Configuration<N>from pip._internal.metadata import get_environment<N>from pip._internal.utils.logging import indent_log<N>from pip._internal.utils.misc import get_pip_version<N><N>
logger = logging.getLogger(__name__)<N><N><N>def show_value(name: str, value: Any) -> None:<N>    logger.info("%s: %s", name, value)<N><N><N>def show_sys_implementation() -> None:<N>    logger.info("sys.implementation:")<N>    implementation_name = sys.implementation.name<N>    with indent_log():<N>        show_value("name", implementation_name)<N><N>
<N>def create_vendor_txt_map() -> Dict[str, str]:<N>    vendor_txt_path = os.path.join(<N>        os.path.dirname(pip_location), "_vendor", "vendor.txt"<N>    )<N><N>    with open(vendor_txt_path) as f:<N>        # Purge non version specifying lines.<N>        # Also, remove any space prefix or suffixes (including comments).<N>        lines = [<N>            line.strip().split(" ", 1)[0] for line in f.readlines() if "==" in line<N>        ]<N><N>
    # Transform into "module" -> version dict.<N>    return dict(line.split("==", 1) for line in lines)  # type: ignore<N><N><N>def get_module_from_module_name(module_name: str) -> ModuleType:<N>    # Module name can be uppercase in vendor.txt for some reason...<N>    module_name = module_name.lower()<N>    # PATCH: setuptools is actually only pkg_resources.<N>    if module_name == "setuptools":<N>        module_name = "pkg_resources"<N><N>
    __import__(f"pip._vendor.{module_name}", globals(), locals(), level=0)<N>    return getattr(pip._vendor, module_name)<N><N><N>def get_vendor_version_from_module(module_name: str) -> Optional[str]:<N>    module = get_module_from_module_name(module_name)<N>    version = getattr(module, "__version__", None)<N><N>
    if not version:<N>        # Try to find version in debundled module info.<N>        env = get_environment([os.path.dirname(module.__file__)])<N>        dist = env.get_distribution(module_name)<N>        if dist:<N>            version = str(dist.version)<N><N>
import sys<N>from optparse import Values<N>from typing import List<N><N>from pip._internal.cli import cmdoptions<N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import SUCCESS<N>from pip._internal.operations.freeze import freeze<N>from pip._internal.utils.compat import stdlib_pkgs<N><N>
DEV_PKGS = {"pip", "setuptools", "distribute", "wheel"}<N><N><N>class FreezeCommand(Command):<N>    """<N>    Output installed packages in requirements format.<N><N>    packages are listed in a case-insensitive sorted order.<N>    """<N><N>    usage = """<N>      %prog [options]"""<N>    log_streams = ("ext://sys.stderr", "ext://sys.stderr")<N><N>
import hashlib<N>import logging<N>import sys<N>from optparse import Values<N>from typing import List<N><N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import ERROR, SUCCESS<N>from pip._internal.utils.hashes import FAVORITE_HASH, STRONG_HASHES<N>from pip._internal.utils.misc import read_chunks, write_output<N><N>
logger = logging.getLogger(__name__)<N><N><N>class HashCommand(Command):<N>    """<N>    Compute a hash of a local package archive.<N><N>    These can be used with --hash in a requirements file to do repeatable<N>    installs.<N>    """<N><N>    usage = "%prog [options] <file> ..."<N>    ignore_require_venv = True<N><N>
    def add_options(self) -> None:<N>        self.cmd_opts.add_option(<N>            "-a",<N>            "--algorithm",<N>            dest="algorithm",<N>            choices=STRONG_HASHES,<N>            action="store",<N>            default=FAVORITE_HASH,<N>            help="The hash algorithm to use: one of {}".format(<N>                ", ".join(STRONG_HASHES)<N>            ),<N>        )<N>        self.parser.insert_option_group(0, self.cmd_opts)<N><N>
    def run(self, options: Values, args: List[str]) -> int:<N>        if not args:<N>            self.parser.print_usage(sys.stderr)<N>            return ERROR<N><N>        algorithm = options.algorithm<N>        for path in args:<N>            write_output(<N>                "%s:\n--hash=%s:%s", path, algorithm, _hash_of_file(path, algorithm)<N>            )<N>        return SUCCESS<N><N>
<N>def _hash_of_file(path: str, algorithm: str) -> str:<N>    """Return the hash digest of a file."""<N>    with open(path, "rb") as archive:<N>        hash = hashlib.new(algorithm)<N>        for chunk in read_chunks(archive):<N>            hash.update(chunk)<N>    return hash.hexdigest()<N><N><N>
from optparse import Values<N>from typing import List<N><N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import SUCCESS<N>from pip._internal.exceptions import CommandError<N><N><N>class HelpCommand(Command):<N>    """Show help for commands"""<N><N>
    usage = """<N>      %prog <command>"""<N>    ignore_require_venv = True<N><N>    def run(self, options: Values, args: List[str]) -> int:<N>        from pip._internal.commands import (<N>            commands_dict,<N>            create_command,<N>            get_similar_commands,<N>        )<N><N>
        try:<N>            # 'pip help' with no args is handled by pip.__init__.parseopt()<N>            cmd_name = args[0]  # the command we need help for<N>        except IndexError:<N>            return SUCCESS<N><N>        if cmd_name not in commands_dict:<N>            guess = get_similar_commands(cmd_name)<N><N>
            msg = [f'unknown command "{cmd_name}"']<N>            if guess:<N>                msg.append(f'maybe you meant "{guess}"')<N><N>            raise CommandError(" - ".join(msg))<N><N>        command = create_command(cmd_name)<N>        command.parser.print_help()<N><N>
import logging<N>import shutil<N>import sys<N>import textwrap<N>import xmlrpc.client<N>from collections import OrderedDict<N>from optparse import Values<N>from typing import TYPE_CHECKING, Dict, List, Optional<N><N>from pip._vendor.packaging.version import parse as parse_version<N><N>
import logging<N>from optparse import Values<N>from typing import Iterator, List, NamedTuple, Optional<N><N>from pip._vendor.packaging.utils import canonicalize_name<N><N>from pip._internal.cli.base_command import Command<N>from pip._internal.cli.status_codes import ERROR, SUCCESS<N>from pip._internal.metadata import BaseDistribution, get_default_environment<N>from pip._internal.utils.misc import write_output<N><N>
logger = logging.getLogger(__name__)<N><N><N>class ShowCommand(Command):<N>    """<N>    Show information about one or more installed packages.<N><N>    The output is in RFC-compliant mail header format.<N>    """<N><N>    usage = """<N>      %prog [options] <package> ..."""<N>    ignore_require_venv = True<N><N>
    def add_options(self) -> None:<N>        self.cmd_opts.add_option(<N>            "-f",<N>            "--files",<N>            dest="files",<N>            action="store_true",<N>            default=False,<N>            help="Show the full list of installed files for each package.",<N>        )<N><N>
        self.parser.insert_option_group(0, self.cmd_opts)<N><N>    def run(self, options: Values, args: List[str]) -> int:<N>        if not args:<N>            logger.warning("ERROR: Please provide a package name or names.")<N>            return ERROR<N>        query = args<N><N>
"""<N>Package containing all pip commands<N>"""<N><N>import importlib<N>from collections import namedtuple<N>from typing import Any, Dict, Optional<N><N>from pip._internal.cli.base_command import Command<N><N>CommandInfo = namedtuple("CommandInfo", "module_path, class_name, summary")<N><N>
import abc<N><N>from pip._internal.index.package_finder import PackageFinder<N>from pip._internal.metadata.base import BaseDistribution<N>from pip._internal.req import InstallRequirement<N><N><N>class AbstractDistribution(metaclass=abc.ABCMeta):<N>    """A base class for handling installable artifacts.<N><N>
    The requirements for anything installable are as follows:<N><N>     - we must be able to determine the requirement name<N>       (or we can't correctly handle the non-upgrade case).<N><N>     - for packages with setup requirements, we must also be able<N>       to determine their requirements without installing additional<N>       packages (for the same reason as run-time dependencies)<N><N>
     - we must be able to create a Distribution object exposing the<N>       above metadata.<N>    """<N><N>    def __init__(self, req: InstallRequirement) -> None:<N>        super().__init__()<N>        self.req = req<N><N>    @abc.abstractmethod<N>    def get_metadata_distribution(self) -> BaseDistribution:<N>        raise NotImplementedError()<N><N>
from pip._internal.distributions.base import AbstractDistribution<N>from pip._internal.index.package_finder import PackageFinder<N>from pip._internal.metadata import BaseDistribution<N><N><N>class InstalledDistribution(AbstractDistribution):<N>    """Represents an installed package.<N><N>
    This does not need any preparation as the required information has already<N>    been computed.<N>    """<N><N>    def get_metadata_distribution(self) -> BaseDistribution:<N>        assert self.req.satisfied_by is not None, "not actually installed"<N>        return self.req.satisfied_by<N><N>
import logging<N>from typing import Iterable, Set, Tuple<N><N>from pip._internal.build_env import BuildEnvironment<N>from pip._internal.distributions.base import AbstractDistribution<N>from pip._internal.exceptions import InstallationError<N>from pip._internal.index.package_finder import PackageFinder<N>from pip._internal.metadata import BaseDistribution<N>from pip._internal.utils.subprocess import runner_with_spinner_message<N><N>
logger = logging.getLogger(__name__)<N><N><N>class SourceDistribution(AbstractDistribution):<N>    """Represents a source distribution.<N><N>    The preparation step for these needs metadata for the packages to be<N>    generated, either using PEP 517 or using the legacy `setup.py egg_info`.<N>    """<N><N>
    def get_metadata_distribution(self) -> BaseDistribution:<N>        return self.req.get_dist()<N><N>    def prepare_distribution_metadata(<N>        self, finder: PackageFinder, build_isolation: bool<N>    ) -> None:<N>        # Load pyproject.toml, to determine whether PEP 517 is to be used<N>        self.req.load_pyproject_toml()<N><N>
from pip._vendor.packaging.utils import canonicalize_name<N><N>from pip._internal.distributions.base import AbstractDistribution<N>from pip._internal.index.package_finder import PackageFinder<N>from pip._internal.metadata import (<N>    BaseDistribution,<N>    FilesystemWheel,<N>    get_wheel_distribution,<N>)<N><N>
from pip._internal.distributions.base import AbstractDistribution<N>from pip._internal.distributions.sdist import SourceDistribution<N>from pip._internal.distributions.wheel import WheelDistribution<N>from pip._internal.req.req_install import InstallRequirement<N><N>
<N>def make_distribution_for_install_requirement(<N>    install_req: InstallRequirement,<N>) -> AbstractDistribution:<N>    """Returns a Distribution for the given InstallRequirement"""<N>    # Editable requirements will always be source distributions. They use the<N>    # legacy logic until we create a modern standard for them.<N>    if install_req.editable:<N>        return SourceDistribution(install_req)<N><N>
"""Routines related to PyPI, indexes"""<N><N># The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import functools<N>import itertools<N>import logging<N>import re<N>from typing import FrozenSet, Iterable, List, Optional, Set, Tuple, Union<N><N>
from pip._vendor.packaging import specifiers<N>from pip._vendor.packaging.tags import Tag<N>from pip._vendor.packaging.utils import canonicalize_name<N>from pip._vendor.packaging.version import _BaseVersion<N>from pip._vendor.packaging.version import parse as parse_version<N><N>
import logging<N>import mimetypes<N>import os<N>import pathlib<N>from typing import Callable, Iterable, Optional, Tuple<N><N>from pip._internal.models.candidate import InstallationCandidate<N>from pip._internal.models.link import Link<N>from pip._internal.utils.urls import path_to_url, url_to_path<N>from pip._internal.vcs import is_url<N><N>
logger = logging.getLogger(__name__)<N><N>FoundCandidates = Iterable[InstallationCandidate]<N>FoundLinks = Iterable[Link]<N>CandidatesFromPage = Callable[[Link], Iterable[InstallationCandidate]]<N>PageValidator = Callable[[Link], bool]<N><N><N>class LinkSource:<N>    @property<N>    def link(self) -> Optional[Link]:<N>        """Returns the underlying link, if there's one."""<N>        raise NotImplementedError()<N><N>
    def page_candidates(self) -> FoundCandidates:<N>        """Candidates found by parsing an archive listing HTML file."""<N>        raise NotImplementedError()<N><N>    def file_links(self) -> FoundLinks:<N>        """Links found by specifying archives directly."""<N>        raise NotImplementedError()<N><N>
<N>def _is_html_file(file_url: str) -> bool:<N>    return mimetypes.guess_type(file_url, strict=False)[0] == "text/html"<N><N><N>class _FlatDirectorySource(LinkSource):<N>    """Link source specified by ``--find-links=<path-to-dir>``.<N><N>    This looks the content of the directory, and returns:<N><N>
    * ``page_candidates``: Links listed on each HTML file in the directory.<N>    * ``file_candidates``: Archives in the directory.<N>    """<N><N>    def __init__(<N>        self,<N>        candidates_from_page: CandidatesFromPage,<N>        path: str,<N>    ) -> None:<N>        self._candidates_from_page = candidates_from_page<N>        self._path = pathlib.Path(os.path.realpath(path))<N><N>
    @property<N>    def link(self) -> Optional[Link]:<N>        return None<N><N>    def page_candidates(self) -> FoundCandidates:<N>        for path in self._path.iterdir():<N>            url = path_to_url(str(path))<N>            if not _is_html_file(url):<N>                continue<N>            yield from self._candidates_from_page(Link(url))<N><N>
    def file_links(self) -> FoundLinks:<N>        for path in self._path.iterdir():<N>            url = path_to_url(str(path))<N>            if _is_html_file(url):<N>                continue<N>            yield Link(url)<N><N><N>class _LocalFileSource(LinkSource):<N>    """``--find-links=<path-or-url>`` or ``--[extra-]index-url=<path-or-url>``.<N><N>
    If a URL is supplied, it must be a ``file:`` URL. If a path is supplied to<N>    the option, it is converted to a URL first. This returns:<N><N>    * ``page_candidates``: Links listed on an HTML file.<N>    * ``file_candidates``: The non-HTML file.<N>    """<N><N>
    def __init__(<N>        self,<N>        candidates_from_page: CandidatesFromPage,<N>        link: Link,<N>    ) -> None:<N>        self._candidates_from_page = candidates_from_page<N>        self._link = link<N><N>    @property<N>    def link(self) -> Optional[Link]:<N>        return self._link<N><N>
    def page_candidates(self) -> FoundCandidates:<N>        if not _is_html_file(self._link.url):<N>            return<N>        yield from self._candidates_from_page(self._link)<N><N>    def file_links(self) -> FoundLinks:<N>        if _is_html_file(self._link.url):<N>            return<N>        yield self._link<N><N>
<N>class _RemoteFileSource(LinkSource):<N>    """``--find-links=<url>`` or ``--[extra-]index-url=<url>``.<N><N>    This returns:<N><N>    * ``page_candidates``: Links listed on an HTML file.<N>    * ``file_candidates``: The non-HTML file.<N>    """<N><N>
    def __init__(<N>        self,<N>        candidates_from_page: CandidatesFromPage,<N>        page_validator: PageValidator,<N>        link: Link,<N>    ) -> None:<N>        self._candidates_from_page = candidates_from_page<N>        self._page_validator = page_validator<N>        self._link = link<N><N>
    @property<N>    def link(self) -> Optional[Link]:<N>        return self._link<N><N>    def page_candidates(self) -> FoundCandidates:<N>        if not self._page_validator(self._link):<N>            return<N>        yield from self._candidates_from_page(self._link)<N><N>
    def file_links(self) -> FoundLinks:<N>        yield self._link<N><N><N>class _IndexDirectorySource(LinkSource):<N>    """``--[extra-]index-url=<path-to-directory>``.<N><N>    This is treated like a remote URL; ``candidates_from_page`` contains logic<N>    for this by appending ``index.html`` to the link.<N>    """<N><N>
    def __init__(<N>        self,<N>        candidates_from_page: CandidatesFromPage,<N>        link: Link,<N>    ) -> None:<N>        self._candidates_from_page = candidates_from_page<N>        self._link = link<N><N>    @property<N>    def link(self) -> Optional[Link]:<N>        return self._link<N><N>
    def page_candidates(self) -> FoundCandidates:<N>        yield from self._candidates_from_page(self._link)<N><N>    def file_links(self) -> FoundLinks:<N>        return ()<N><N><N>def build_source(<N>    location: str,<N>    *,<N>    candidates_from_page: CandidatesFromPage,<N>    page_validator: PageValidator,<N>    expand_dir: bool,<N>    cache_link_parsing: bool,<N>) -> Tuple[Optional[str], Optional[LinkSource]]:<N><N>
    path: Optional[str] = None<N>    url: Optional[str] = None<N>    if os.path.exists(location):  # Is a local path.<N>        url = path_to_url(location)<N>        path = location<N>    elif location.startswith("file:"):  # A file: URL.<N>        url = location<N>        path = url_to_path(location)<N>    elif is_url(location):<N>        url = location<N><N>
import functools<N>import os<N>import site<N>import sys<N>import sysconfig<N>import typing<N><N>from pip._internal.utils import appdirs<N>from pip._internal.utils.virtualenv import running_under_virtualenv<N><N># Application Directories<N>USER_CACHE_DIR = appdirs.user_cache_dir("pip")<N><N>
# FIXME doesn't account for venv linked to global site-packages<N>site_packages: typing.Optional[str] = sysconfig.get_path("purelib")<N><N><N>def get_major_minor_version() -> str:<N>    """<N>    Return the major-minor version of the current Python as a string, e.g.<N>    "3.7" or "3.10".<N>    """<N>    return "{}.{}".format(*sys.version_info)<N><N>
<N>def get_src_prefix() -> str:<N>    if running_under_virtualenv():<N>        src_prefix = os.path.join(sys.prefix, "src")<N>    else:<N>        # FIXME: keep src in cwd for now (it is not a temporary folder)<N>        try:<N>            src_prefix = os.path.join(os.getcwd(), "src")<N>        except OSError:<N>            # In case the current working directory has been renamed or deleted<N>            sys.exit("The folder you are executing pip from can no longer be found.")<N><N>
    # under macOS + virtualenv sys.prefix is not properly resolved<N>    # it is something like /path/to/python/bin/..<N>    return os.path.abspath(src_prefix)<N><N><N>try:<N>    # Use getusersitepackages if this is present, as it ensures that the<N>    # value is initialised properly.<N>    user_site: typing.Optional[str] = site.getusersitepackages()<N>except AttributeError:<N>    user_site = site.USER_SITE<N><N>
import distutils.util  # FIXME: For change_root.<N>import logging<N>import os<N>import sys<N>import sysconfig<N>import typing<N><N>from pip._internal.exceptions import InvalidSchemeCombination, UserInstallationInvalid<N>from pip._internal.models.scheme import SCHEME_KEYS, Scheme<N>from pip._internal.utils.virtualenv import running_under_virtualenv<N><N>
import functools<N>import logging<N>import os<N>import pathlib<N>import sys<N>import sysconfig<N>from typing import Any, Dict, Iterator, List, Optional, Tuple<N><N>from pip._internal.models.scheme import SCHEME_KEYS, Scheme<N>from pip._internal.utils.compat import WINDOWS<N>from pip._internal.utils.deprecation import deprecated<N>from pip._internal.utils.virtualenv import running_under_virtualenv<N><N>
from . import _distutils, _sysconfig<N>from .base import (<N>    USER_CACHE_DIR,<N>    get_major_minor_version,<N>    get_src_prefix,<N>    is_osx_framework,<N>    site_packages,<N>    user_site,<N>)<N><N>__all__ = [<N>    "USER_CACHE_DIR",<N>    "get_bin_prefix",<N>    "get_bin_user",<N>    "get_major_minor_version",<N>    "get_platlib",<N>    "get_prefixed_libs",<N>    "get_purelib",<N>    "get_scheme",<N>    "get_src_prefix",<N>    "site_packages",<N>    "user_site",<N>]<N><N>
<N>logger = logging.getLogger(__name__)<N><N><N>_PLATLIBDIR: str = getattr(sys, "platlibdir", "lib")<N><N>_USE_SYSCONFIG_DEFAULT = sys.version_info >= (3, 10)<N><N><N>def _should_use_sysconfig() -> bool:<N>    """This function determines the value of _USE_SYSCONFIG.<N><N>
    By default, pip uses sysconfig on Python 3.10+.<N>    But Python distributors can override this decision by setting:<N>        sysconfig._PIP_USE_SYSCONFIG = True / False<N>    Rationale in https://github.com/pypa/pip/issues/10647<N><N>    This is a function for testability, but should be constant during any one<N>    run.<N>    """<N>    return bool(getattr(sysconfig, "_PIP_USE_SYSCONFIG", _USE_SYSCONFIG_DEFAULT))<N><N>
<N>_USE_SYSCONFIG = _should_use_sysconfig()<N><N># Be noisy about incompatibilities if this platforms "should" be using<N># sysconfig, but is explicitly opting out and using distutils instead.<N>if _USE_SYSCONFIG_DEFAULT and not _USE_SYSCONFIG:<N>    _MISMATCH_LEVEL = logging.WARNING<N>else:<N>    _MISMATCH_LEVEL = logging.DEBUG<N><N>
<N>def _looks_like_bpo_44860() -> bool:<N>    """The resolution to bpo-44860 will change this incorrect platlib.<N><N>    See <https://bugs.python.org/issue44860>.<N>    """<N>    from distutils.command.install import INSTALL_SCHEMES  # type: ignore<N><N>
import csv<N>import email.message<N>import json<N>import logging<N>import pathlib<N>import re<N>import zipfile<N>from typing import (<N>    IO,<N>    TYPE_CHECKING,<N>    Collection,<N>    Container,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Tuple,<N>    Union,<N>)<N><N>
from pip._vendor.packaging.requirements import Requirement<N>from pip._vendor.packaging.specifiers import InvalidSpecifier, SpecifierSet<N>from pip._vendor.packaging.utils import NormalizedName<N>from pip._vendor.packaging.version import LegacyVersion, Version<N><N>
import email.message<N>import email.parser<N>import logging<N>import os<N>import pathlib<N>import zipfile<N>from typing import Collection, Iterable, Iterator, List, Mapping, NamedTuple, Optional<N><N>from pip._vendor import pkg_resources<N>from pip._vendor.packaging.requirements import Requirement<N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N>from pip._vendor.packaging.version import parse as parse_version<N><N>
from pip._internal.exceptions import InvalidWheel, NoneMetadataError, UnsupportedWheel<N>from pip._internal.utils.misc import display_path<N>from pip._internal.utils.wheel import parse_wheel, read_wheel_metadata_file<N><N>from .base import (<N>    BaseDistribution,<N>    BaseEntryPoint,<N>    BaseEnvironment,<N>    DistributionVersion,<N>    InfoPath,<N>    Wheel,<N>)<N><N>
logger = logging.getLogger(__name__)<N><N><N>class EntryPoint(NamedTuple):<N>    name: str<N>    value: str<N>    group: str<N><N><N>class WheelMetadata:<N>    """IMetadataProvider that reads metadata files from a dictionary.<N><N>    This also maps metadata decoding exceptions to our internal exception type.<N>    """<N><N>
    def __init__(self, metadata: Mapping[str, bytes], wheel_name: str) -> None:<N>        self._metadata = metadata<N>        self._wheel_name = wheel_name<N><N>    def has_metadata(self, name: str) -> bool:<N>        return name in self._metadata<N><N>
    def get_metadata(self, name: str) -> str:<N>        try:<N>            return self._metadata[name].decode()<N>        except UnicodeDecodeError as e:<N>            # Augment the default error with the origin of the file.<N>            raise UnsupportedWheel(<N>                f"Error decoding metadata for {self._wheel_name}: {e} in {name} file"<N>            )<N><N>
    def get_metadata_lines(self, name: str) -> Iterable[str]:<N>        return pkg_resources.yield_lines(self.get_metadata(name))<N><N>    def metadata_isdir(self, name: str) -> bool:<N>        return False<N><N>    def metadata_listdir(self, name: str) -> List[str]:<N>        return []<N><N>
    def run_script(self, script_name: str, namespace: str) -> None:<N>        pass<N><N><N>class Distribution(BaseDistribution):<N>    def __init__(self, dist: pkg_resources.Distribution) -> None:<N>        self._dist = dist<N><N>    @classmethod<N>    def from_directory(cls, directory: str) -> "Distribution":<N>        dist_dir = directory.rstrip(os.sep)<N><N>
from typing import List, Optional<N><N>from .base import BaseDistribution, BaseEnvironment, FilesystemWheel, MemoryWheel, Wheel<N><N>__all__ = [<N>    "BaseDistribution",<N>    "BaseEnvironment",<N>    "FilesystemWheel",<N>    "MemoryWheel",<N>    "Wheel",<N>    "get_default_environment",<N>    "get_environment",<N>    "get_wheel_distribution",<N>]<N><N>
<N>def get_default_environment() -> BaseEnvironment:<N>    """Get the default representation for the current environment.<N><N>    This returns an Environment instance from the chosen backend. The default<N>    Environment instance should be built from ``sys.path`` and may use caching<N>    to share instance state accorss calls.<N>    """<N>    from .pkg_resources import Environment<N><N>
    return Environment.default()<N><N><N>def get_environment(paths: Optional[List[str]]) -> BaseEnvironment:<N>    """Get a representation of the environment specified by ``paths``.<N><N>    This returns an Environment instance from the chosen backend based on the<N>    given import paths. The backend must build a fresh instance representing<N>    the state of installed distributions when this function is called.<N>    """<N>    from .pkg_resources import Environment<N><N>
    return Environment.from_paths(paths)<N><N><N>def get_directory_distribution(directory: str) -> BaseDistribution:<N>    """Get the distribution metadata representation in the specified directory.<N><N>    This returns a Distribution instance from the chosen backend based on<N>    the given on-disk ``.dist-info`` directory.<N>    """<N>    from .pkg_resources import Distribution<N><N>
    return Distribution.from_directory(directory)<N><N><N>def get_wheel_distribution(wheel: Wheel, canonical_name: str) -> BaseDistribution:<N>    """Get the representation of the specified wheel's distribution metadata.<N><N>    This returns a Distribution instance from the chosen backend based on<N>    the given wheel's ``.dist-info`` directory.<N><N>
from pip._vendor.packaging.version import parse as parse_version<N><N>from pip._internal.models.link import Link<N>from pip._internal.utils.models import KeyBasedCompareMixin<N><N><N>class InstallationCandidate(KeyBasedCompareMixin):<N>    """Represents a potential "candidate" for installation."""<N><N>
    __slots__ = ["name", "version", "link"]<N><N>    def __init__(self, name: str, version: str, link: Link) -> None:<N>        self.name = name<N>        self.version = parse_version(version)<N>        self.link = link<N><N>        super().__init__(<N>            key=(self.name, self.version, self.link),<N>            defining_class=InstallationCandidate,<N>        )<N><N>
    def __repr__(self) -> str:<N>        return "<InstallationCandidate({!r}, {!r}, {!r})>".format(<N>            self.name,<N>            self.version,<N>            self.link,<N>        )<N><N>    def __str__(self) -> str:<N>        return "{!r} candidate (version {} at {})".format(<N>            self.name,<N>            self.version,<N>            self.link,<N>        )<N><N><N>
""" PEP 610 """<N>import json<N>import re<N>import urllib.parse<N>from typing import Any, Dict, Iterable, Optional, Type, TypeVar, Union<N><N>__all__ = [<N>    "DirectUrl",<N>    "DirectUrlValidationError",<N>    "DirInfo",<N>    "ArchiveInfo",<N>    "VcsInfo",<N>]<N><N>
from typing import FrozenSet, Optional, Set<N><N>from pip._vendor.packaging.utils import canonicalize_name<N><N>from pip._internal.exceptions import CommandError<N><N><N>class FormatControl:<N>    """Helper for managing formats from which a package can be installed."""<N><N>
    __slots__ = ["no_binary", "only_binary"]<N><N>    def __init__(<N>        self,<N>        no_binary: Optional[Set[str]] = None,<N>        only_binary: Optional[Set[str]] = None,<N>    ) -> None:<N>        if no_binary is None:<N>            no_binary = set()<N>        if only_binary is None:<N>            only_binary = set()<N><N>
        self.no_binary = no_binary<N>        self.only_binary = only_binary<N><N>    def __eq__(self, other: object) -> bool:<N>        if not isinstance(other, self.__class__):<N>            return NotImplemented<N><N>        if self.__slots__ != other.__slots__:<N>            return False<N><N>
"""<N>For types associated with installation schemes.<N><N>For a general overview of available schemes and their context, see<N>https://docs.python.org/3/install/index.html#alternate-installation.<N>"""<N><N><N>SCHEME_KEYS = ["platlib", "purelib", "headers", "scripts", "data"]<N><N>
import itertools<N>import logging<N>import os<N>import posixpath<N>import urllib.parse<N>from typing import List<N><N>from pip._vendor.packaging.utils import canonicalize_name<N><N>from pip._internal.models.index import PyPI<N>from pip._internal.utils.compat import has_tls<N>from pip._internal.utils.misc import normalize_path, redact_auth_from_url<N><N>
from typing import Optional<N><N>from pip._internal.models.format_control import FormatControl<N><N><N>class SelectionPreferences:<N>    """<N>    Encapsulates the candidate selection preferences for downloading<N>    and installing files.<N>    """<N><N>
import sys<N>from typing import List, Optional, Tuple<N><N>from pip._vendor.packaging.tags import Tag<N><N>from pip._internal.utils.compatibility_tags import get_supported, version_info_to_nodot<N>from pip._internal.utils.misc import normalize_version_info<N><N>
<N>class TargetPython:<N><N>    """<N>    Encapsulates the properties of a Python interpreter one is targeting<N>    for a package install, download, etc.<N>    """<N><N>    __slots__ = [<N>        "_given_py_version_info",<N>        "abis",<N>        "implementation",<N>        "platforms",<N>        "py_version",<N>        "py_version_info",<N>        "_valid_tags",<N>    ]<N><N>
"""Represents a wheel file and provides access to the various parts of the<N>name that have meaning.<N>"""<N>import re<N>from typing import Dict, Iterable, List<N><N>from pip._vendor.packaging.tags import Tag<N><N>from pip._internal.exceptions import InvalidWheelFilename<N><N>
<N>class Wheel:<N>    """A wheel file"""<N><N>    wheel_file_re = re.compile(<N>        r"""^(?P<namever>(?P<name>.+?)-(?P<ver>.*?))<N>        ((-(?P<build>\d[^-]*?))?-(?P<pyver>.+?)-(?P<abi>.+?)-(?P<plat>.+?)<N>        \.whl|\.dist-info)$""",<N>        re.VERBOSE,<N>    )<N><N>
"""Network Authentication Helpers<N><N>Contains interface (MultiDomainBasicAuth) and associated glue code for<N>providing credentials in the context of network requests.<N>"""<N><N>import urllib.parse<N>from typing import Any, Dict, List, Optional, Tuple<N><N>
from pip._vendor.requests.auth import AuthBase, HTTPBasicAuth<N>from pip._vendor.requests.models import Request, Response<N>from pip._vendor.requests.utils import get_netrc_auth<N><N>from pip._internal.utils.logging import getLogger<N>from pip._internal.utils.misc import (<N>    ask,<N>    ask_input,<N>    ask_password,<N>    remove_auth_from_url,<N>    split_auth_netloc_from_url,<N>)<N>from pip._internal.vcs.versioncontrol import AuthInfo<N><N>
logger = getLogger(__name__)<N><N>Credentials = Tuple[str, str, str]<N><N>try:<N>    import keyring<N>except ImportError:<N>    keyring = None  # type: ignore[assignment]<N>except Exception as exc:<N>    logger.warning(<N>        "Keyring is skipped due to an exception: %s",<N>        str(exc),<N>    )<N>    keyring = None  # type: ignore[assignment]<N><N>
"""HTTP cache implementation.<N>"""<N><N>import os<N>from contextlib import contextmanager<N>from typing import Iterator, Optional<N><N>from pip._vendor.cachecontrol.cache import BaseCache<N>from pip._vendor.cachecontrol.caches import FileCache<N>from pip._vendor.requests.models import Response<N><N>
from pip._internal.utils.filesystem import adjacent_tmp_file, replace<N>from pip._internal.utils.misc import ensure_dir<N><N><N>def is_from_cache(response: Response) -> bool:<N>    return getattr(response, "from_cache", False)<N><N><N>@contextmanager<N>def suppressed_cache_errors() -> Iterator[None]:<N>    """If we can't access the cache then we can just skip caching and process<N>    requests as if caching wasn't enabled.<N>    """<N>    try:<N>        yield<N>    except OSError:<N>        pass<N><N>
<N>class SafeFileCache(BaseCache):<N>    """<N>    A file based cache which is safe to use even when the target directory may<N>    not be accessible or writable.<N>    """<N><N>    def __init__(self, directory: str) -> None:<N>        assert directory is not None, "Cache directory must not be None."<N>        super().__init__()<N>        self.directory = directory<N><N>
    def _get_cache_path(self, name: str) -> str:<N>        # From cachecontrol.caches.file_cache.FileCache._fn, brought into our<N>        # class for backwards-compatibility and to avoid using a non-public<N>        # method.<N>        hashed = FileCache.encode(name)<N>        parts = list(hashed[:5]) + [hashed]<N>        return os.path.join(self.directory, *parts)<N><N>
    def get(self, key: str) -> Optional[bytes]:<N>        path = self._get_cache_path(key)<N>        with suppressed_cache_errors():<N>            with open(path, "rb") as f:<N>                return f.read()<N><N>    def set(self, key: str, value: bytes, expires: Optional[int] = None) -> None:<N>        path = self._get_cache_path(key)<N>        with suppressed_cache_errors():<N>            ensure_dir(os.path.dirname(path))<N><N>
            with adjacent_tmp_file(path) as f:<N>                f.write(value)<N><N>            replace(f.name, path)<N><N>    def delete(self, key: str) -> None:<N>        path = self._get_cache_path(key)<N>        with suppressed_cache_errors():<N>            os.remove(path)<N><N><N>
"""Lazy ZIP over HTTP"""<N><N>__all__ = ["HTTPRangeRequestUnsupported", "dist_from_wheel_url"]<N><N>from bisect import bisect_left, bisect_right<N>from contextlib import contextmanager<N>from tempfile import NamedTemporaryFile<N>from typing import Any, Dict, Iterator, List, Optional, Tuple<N>from zipfile import BadZipfile, ZipFile<N><N>
from pip._vendor.packaging.utils import canonicalize_name<N>from pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response<N><N>from pip._internal.metadata import BaseDistribution, MemoryWheel, get_wheel_distribution<N>from pip._internal.network.session import PipSession<N>from pip._internal.network.utils import HEADERS, raise_for_status, response_chunks<N><N>
"""PipSession and supporting code, containing all pip-specific<N>network request configuration and behavior.<N>"""<N><N>import email.utils<N>import io<N>import ipaddress<N>import json<N>import logging<N>import mimetypes<N>import os<N>import platform<N>import shutil<N>import subprocess<N>import sys<N>import urllib.parse<N>import warnings<N>from typing import Any, Dict, Iterator, List, Mapping, Optional, Sequence, Tuple, Union<N><N>
from pip._vendor import requests, urllib3<N>from pip._vendor.cachecontrol import CacheControlAdapter<N>from pip._vendor.requests.adapters import BaseAdapter, HTTPAdapter<N>from pip._vendor.requests.models import PreparedRequest, Response<N>from pip._vendor.requests.structures import CaseInsensitiveDict<N>from pip._vendor.urllib3.connectionpool import ConnectionPool<N>from pip._vendor.urllib3.exceptions import InsecureRequestWarning<N><N>
from pip import __version__<N>from pip._internal.metadata import get_default_environment<N>from pip._internal.models.link import Link<N>from pip._internal.network.auth import MultiDomainBasicAuth<N>from pip._internal.network.cache import SafeFileCache<N><N>
# Import ssl from compat so the initial import occurs in only one place.<N>from pip._internal.utils.compat import has_tls<N>from pip._internal.utils.glibc import libc_ver<N>from pip._internal.utils.misc import build_url_from_netloc, parse_netloc<N>from pip._internal.utils.urls import url_to_path<N><N>
"""xmlrpclib.Transport implementation<N>"""<N><N>import logging<N>import urllib.parse<N>import xmlrpc.client<N>from typing import TYPE_CHECKING, Tuple<N><N>from pip._internal.exceptions import NetworkConnectionError<N>from pip._internal.network.session import PipSession<N>from pip._internal.network.utils import raise_for_status<N><N>
if TYPE_CHECKING:<N>    from xmlrpc.client import _HostType, _Marshallable<N><N>logger = logging.getLogger(__name__)<N><N><N>class PipXmlrpcTransport(xmlrpc.client.Transport):<N>    """Provide a `xmlrpclib.Transport` implementation via a `PipSession`<N>    object.<N>    """<N><N>
    def __init__(<N>        self, index_url: str, session: PipSession, use_datetime: bool = False<N>    ) -> None:<N>        super().__init__(use_datetime)<N>        index_parts = urllib.parse.urlparse(index_url)<N>        self._scheme = index_parts.scheme<N>        self._session = session<N><N>
"""Validation of dependencies of packages<N>"""<N><N>import logging<N>from typing import Callable, Dict, List, NamedTuple, Optional, Set, Tuple<N><N>from pip._vendor.packaging.requirements import Requirement<N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N><N>
from pip._internal.distributions import make_distribution_for_install_requirement<N>from pip._internal.metadata import get_default_environment<N>from pip._internal.metadata.base import DistributionVersion<N>from pip._internal.req.req_install import InstallRequirement<N><N>
logger = logging.getLogger(__name__)<N><N><N>class PackageDetails(NamedTuple):<N>    version: DistributionVersion<N>    dependencies: List[Requirement]<N><N><N># Shorthands<N>PackageSet = Dict[NormalizedName, PackageDetails]<N>Missing = Tuple[NormalizedName, Requirement]<N>Conflicting = Tuple[NormalizedName, DistributionVersion, Requirement]<N><N>
import collections<N>import logging<N>import os<N>from typing import Container, Dict, Iterable, Iterator, List, NamedTuple, Optional, Set<N><N>from pip._vendor.packaging.utils import canonicalize_name<N>from pip._vendor.packaging.version import Version<N><N>
from pip._internal.exceptions import BadCommand, InstallationError<N>from pip._internal.metadata import BaseDistribution, get_environment<N>from pip._internal.req.constructors import (<N>    install_req_from_editable,<N>    install_req_from_line,<N>)<N>from pip._internal.req.req_file import COMMENT_RE<N>from pip._internal.utils.direct_url_helpers import direct_url_as_pep440_direct_reference<N><N>
logger = logging.getLogger(__name__)<N><N><N>class _EditableInfo(NamedTuple):<N>    requirement: str<N>    comments: List[str]<N><N><N>def freeze(<N>    requirement: Optional[List[str]] = None,<N>    local_only: bool = False,<N>    user_only: bool = False,<N>    paths: Optional[List[str]] = None,<N>    isolated: bool = False,<N>    exclude_editable: bool = False,<N>    skip: Container[str] = (),<N>) -> Iterator[str]:<N>    installations: Dict[str, FrozenRequirement] = {}<N><N>
    dists = get_environment(paths).iter_installed_distributions(<N>        local_only=local_only,<N>        skip=(),<N>        user_only=user_only,<N>    )<N>    for dist in dists:<N>        req = FrozenRequirement.from_dist(dist)<N>        if exclude_editable and req.editable:<N>            continue<N>        installations[req.canonical_name] = req<N><N>
"""Prepares a distribution for installation<N>"""<N><N># The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import logging<N>import mimetypes<N>import os<N>import shutil<N>from typing import Dict, Iterable, List, Optional<N><N>
"""Metadata generation logic for source distributions.<N>"""<N><N>import os<N><N>from pip._vendor.pep517.wrappers import Pep517HookCaller<N><N>from pip._internal.build_env import BuildEnvironment<N>from pip._internal.exceptions import (<N>    InstallationSubprocessError,<N>    MetadataGenerationFailed,<N>)<N>from pip._internal.utils.subprocess import runner_with_spinner_message<N>from pip._internal.utils.temp_dir import TempDirectory<N><N>
<N>def generate_metadata(<N>    build_env: BuildEnvironment, backend: Pep517HookCaller, details: str<N>) -> str:<N>    """Generate metadata using mechanisms described in PEP 517.<N><N>    Returns the generated metadata directory.<N>    """<N>    metadata_tmpdir = TempDirectory(kind="modern-metadata", globally_managed=True)<N><N>
"""Metadata generation logic for source distributions.<N>"""<N><N>import os<N><N>from pip._vendor.pep517.wrappers import Pep517HookCaller<N><N>from pip._internal.build_env import BuildEnvironment<N>from pip._internal.exceptions import (<N>    InstallationSubprocessError,<N>    MetadataGenerationFailed,<N>)<N>from pip._internal.utils.subprocess import runner_with_spinner_message<N>from pip._internal.utils.temp_dir import TempDirectory<N><N>
<N>def generate_editable_metadata(<N>    build_env: BuildEnvironment, backend: Pep517HookCaller, details: str<N>) -> str:<N>    """Generate metadata using mechanisms described in PEP 660.<N><N>    Returns the generated metadata directory.<N>    """<N>    metadata_tmpdir = TempDirectory(kind="modern-metadata", globally_managed=True)<N><N>
import logging<N>import os<N>from typing import Optional<N><N>from pip._vendor.pep517.wrappers import Pep517HookCaller<N><N>from pip._internal.utils.subprocess import runner_with_spinner_message<N><N>logger = logging.getLogger(__name__)<N><N><N>def build_wheel_pep517(<N>    name: str,<N>    backend: Pep517HookCaller,<N>    metadata_directory: str,<N>    tempd: str,<N>) -> Optional[str]:<N>    """Build one InstallRequirement using the PEP 517 build process.<N><N>
import logging<N>import os<N>from typing import Optional<N><N>from pip._vendor.pep517.wrappers import HookMissing, Pep517HookCaller<N><N>from pip._internal.utils.subprocess import runner_with_spinner_message<N><N>logger = logging.getLogger(__name__)<N><N>
<N>def build_wheel_editable(<N>    name: str,<N>    backend: Pep517HookCaller,<N>    metadata_directory: str,<N>    tempd: str,<N>) -> Optional[str]:<N>    """Build one InstallRequirement using the PEP 660 build process.<N><N>    Returns path to wheel if successfully built. Otherwise, returns None.<N>    """<N>    assert metadata_directory is not None<N>    try:<N>        logger.debug("Destination directory: %s", tempd)<N><N>
import logging<N>import os.path<N>from typing import List, Optional<N><N>from pip._internal.cli.spinners import open_spinner<N>from pip._internal.utils.setuptools_build import make_setuptools_bdist_wheel_args<N>from pip._internal.utils.subprocess import call_subprocess, format_command_args<N><N>
logger = logging.getLogger(__name__)<N><N><N>def format_command_result(<N>    command_args: List[str],<N>    command_output: str,<N>) -> str:<N>    """Format command information for logging."""<N>    command_desc = format_command_args(command_args)<N>    text = f"Command arguments: {command_desc}\n"<N><N>
    if not command_output:<N>        text += "Command output: None"<N>    elif logger.getEffectiveLevel() > logging.DEBUG:<N>        text += "Command output: [use --verbose to show]"<N>    else:<N>        if not command_output.endswith("\n"):<N>            command_output += "\n"<N>        text += f"Command output:\n{command_output}"<N><N>
"""Legacy editable installation process, i.e. `setup.py develop`.<N>"""<N>import logging<N>from typing import List, Optional, Sequence<N><N>from pip._internal.build_env import BuildEnvironment<N>from pip._internal.utils.logging import indent_log<N>from pip._internal.utils.setuptools_build import make_setuptools_develop_args<N>from pip._internal.utils.subprocess import call_subprocess<N><N>
"""Backing implementation for InstallRequirement's various constructors<N><N>The idea here is that these formed a major chunk of InstallRequirement's size<N>so, moving them and support code dedicated to them outside of that class<N>helps creates for better understandability for the rest of the code.<N><N>
These are meant to be used elsewhere within pip to create instances of<N>InstallRequirement.<N>"""<N><N>import logging<N>import os<N>import re<N>from typing import Any, Dict, Optional, Set, Tuple, Union<N><N>from pip._vendor.packaging.markers import Marker<N>from pip._vendor.packaging.requirements import InvalidRequirement, Requirement<N>from pip._vendor.packaging.specifiers import Specifier<N><N>
"""<N>Requirements file parsing<N>"""<N><N>import optparse<N>import os<N>import re<N>import shlex<N>import urllib.parse<N>from optparse import Values<N>from typing import (<N>    TYPE_CHECKING,<N>    Any,<N>    Callable,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Tuple,<N>)<N><N>
from pip._internal.cli import cmdoptions<N>from pip._internal.exceptions import InstallationError, RequirementsFileParseError<N>from pip._internal.models.search_scope import SearchScope<N>from pip._internal.network.session import PipSession<N>from pip._internal.network.utils import raise_for_status<N>from pip._internal.utils.encoding import auto_decode<N>from pip._internal.utils.urls import get_url_scheme<N><N>
if TYPE_CHECKING:<N>    # NoReturn introduced in 3.6.2; imported only for type checking to maintain<N>    # pip compatibility with older patch versions of Python 3.6<N>    from typing import NoReturn<N><N>    from pip._internal.index.package_finder import PackageFinder<N><N>
# The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import functools<N>import logging<N>import os<N>import shutil<N>import sys<N>import uuid<N>import zipfile<N>from typing import Any, Collection, Dict, Iterable, List, Optional, Sequence, Union<N><N>
from pip._vendor.packaging.markers import Marker<N>from pip._vendor.packaging.requirements import Requirement<N>from pip._vendor.packaging.specifiers import SpecifierSet<N>from pip._vendor.packaging.utils import canonicalize_name<N>from pip._vendor.packaging.version import Version<N>from pip._vendor.packaging.version import parse as parse_version<N>from pip._vendor.pep517.wrappers import Pep517HookCaller<N><N>
import logging<N>from collections import OrderedDict<N>from typing import Dict, Iterable, List, Optional, Tuple<N><N>from pip._vendor.packaging.utils import canonicalize_name<N><N>from pip._internal.exceptions import InstallationError<N>from pip._internal.models.wheel import Wheel<N>from pip._internal.req.req_install import InstallRequirement<N>from pip._internal.utils import compatibility_tags<N><N>
logger = logging.getLogger(__name__)<N><N><N>class RequirementSet:<N>    def __init__(self, check_supported_wheels: bool = True) -> None:<N>        """Create a RequirementSet."""<N><N>        self.requirements: Dict[str, InstallRequirement] = OrderedDict()<N>        self.check_supported_wheels = check_supported_wheels<N><N>
        self.unnamed_requirements: List[InstallRequirement] = []<N><N>    def __str__(self) -> str:<N>        requirements = sorted(<N>            (req for req in self.requirements.values() if not req.comes_from),<N>            key=lambda req: canonicalize_name(req.name or ""),<N>        )<N>        return " ".join(str(req.req) for req in requirements)<N><N>
    def __repr__(self) -> str:<N>        requirements = sorted(<N>            self.requirements.values(),<N>            key=lambda req: canonicalize_name(req.name or ""),<N>        )<N><N>        format_string = "<{classname} object; {count} requirement(s): {reqs}>"<N>        return format_string.format(<N>            classname=self.__class__.__name__,<N>            count=len(requirements),<N>            reqs=", ".join(str(req.req) for req in requirements),<N>        )<N><N>
    def add_unnamed_requirement(self, install_req: InstallRequirement) -> None:<N>        assert not install_req.name<N>        self.unnamed_requirements.append(install_req)<N><N>    def add_named_requirement(self, install_req: InstallRequirement) -> None:<N>        assert install_req.name<N><N>
        project_name = canonicalize_name(install_req.name)<N>        self.requirements[project_name] = install_req<N><N>    def add_requirement(<N>        self,<N>        install_req: InstallRequirement,<N>        parent_req_name: Optional[str] = None,<N>        extras_requested: Optional[Iterable[str]] = None,<N>    ) -> Tuple[List[InstallRequirement], Optional[InstallRequirement]]:<N>        """Add install_req as a requirement to install.<N><N>
import contextlib<N>import hashlib<N>import logging<N>import os<N>from types import TracebackType<N>from typing import Dict, Iterator, Optional, Set, Type, Union<N><N>from pip._internal.models.link import Link<N>from pip._internal.req.req_install import InstallRequirement<N>from pip._internal.utils.temp_dir import TempDirectory<N><N>
import collections<N>import logging<N>from typing import Iterator, List, Optional, Sequence, Tuple<N><N>from pip._internal.utils.logging import indent_log<N><N>from .req_file import parse_requirements<N>from .req_install import InstallRequirement<N>from .req_set import RequirementSet<N><N>
__all__ = [<N>    "RequirementSet",<N>    "InstallRequirement",<N>    "parse_requirements",<N>    "install_given_reqs",<N>]<N><N>logger = logging.getLogger(__name__)<N><N><N>class InstallationResult:<N>    def __init__(self, name: str) -> None:<N>        self.name = name<N><N>
    def __repr__(self) -> str:<N>        return f"InstallationResult(name={self.name!r})"<N><N><N>def _validate_requirements(<N>    requirements: List[InstallRequirement],<N>) -> Iterator[Tuple[str, InstallRequirement]]:<N>    for req in requirements:<N>        assert req.name, f"invalid to-be-installed requirement: {req}"<N>        yield req.name, req<N><N>
<N>def install_given_reqs(<N>    requirements: List[InstallRequirement],<N>    install_options: List[str],<N>    global_options: Sequence[str],<N>    root: Optional[str],<N>    home: Optional[str],<N>    prefix: Optional[str],<N>    warn_script_location: bool,<N>    use_user_site: bool,<N>    pycompile: bool,<N>) -> List[InstallationResult]:<N>    """<N>    Install everything in the given list.<N><N>
    (to be called after having downloaded and unpacked the packages)<N>    """<N>    to_install = collections.OrderedDict(_validate_requirements(requirements))<N><N>    if to_install:<N>        logger.info(<N>            "Installing collected packages: %s",<N>            ", ".join(to_install.keys()),<N>        )<N><N>
    installed = []<N><N>    with indent_log():<N>        for req_name, requirement in to_install.items():<N>            if requirement.should_reinstall:<N>                logger.info("Attempting uninstall: %s", req_name)<N>                with indent_log():<N>                    uninstalled_pathset = requirement.uninstall(auto_confirm=True)<N>            else:<N>                uninstalled_pathset = None<N><N>
from typing import Callable, List, Optional<N><N>from pip._internal.req.req_install import InstallRequirement<N>from pip._internal.req.req_set import RequirementSet<N><N>InstallRequirementProvider = Callable[<N>    [str, Optional[InstallRequirement]], InstallRequirement<N>]<N><N>
<N>class BaseResolver:<N>    def resolve(<N>        self, root_reqs: List[InstallRequirement], check_supported_wheels: bool<N>    ) -> RequirementSet:<N>        raise NotImplementedError()<N><N>    def get_installation_order(<N>        self, req_set: RequirementSet<N>    ) -> List[InstallRequirement]:<N>        raise NotImplementedError()<N><N><N>
"""Dependency Resolution<N><N>The dependency resolution in pip is performed as follows:<N><N>for top-level requirements:<N>    a. only one spec allowed per project, regardless of conflicts or not.<N>       otherwise a "double requirement" exception is raised<N>    b. they override sub-dependency requirements.<N>for sub-dependencies<N>    a. "first found, wins" (where the order is breadth first)<N>"""<N><N>
# The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import logging<N>import sys<N>from collections import defaultdict<N>from itertools import chain<N>from typing import DefaultDict, Iterable, List, Optional, Set, Tuple<N><N>
from typing import FrozenSet, Iterable, Optional, Tuple, Union<N><N>from pip._vendor.packaging.specifiers import SpecifierSet<N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N>from pip._vendor.packaging.version import LegacyVersion, Version<N><N>
from pip._internal.models.link import Link, links_equivalent<N>from pip._internal.req.req_install import InstallRequirement<N>from pip._internal.utils.hashes import Hashes<N><N>CandidateLookup = Tuple[Optional["Candidate"], Optional[InstallRequirement]]<N>CandidateVersion = Union[LegacyVersion, Version]<N><N>
<N>def format_name(project: str, extras: FrozenSet[str]) -> str:<N>    if not extras:<N>        return project<N>    canonical_extras = sorted(canonicalize_name(e) for e in extras)<N>    return "{}[{}]".format(project, ",".join(canonical_extras))<N><N>
<N>class Constraint:<N>    def __init__(<N>        self, specifier: SpecifierSet, hashes: Hashes, links: FrozenSet[Link]<N>    ) -> None:<N>        self.specifier = specifier<N>        self.hashes = hashes<N>        self.links = links<N><N>    @classmethod<N>    def empty(cls) -> "Constraint":<N>        return Constraint(SpecifierSet(), Hashes(), frozenset())<N><N>
    @classmethod<N>    def from_ireq(cls, ireq: InstallRequirement) -> "Constraint":<N>        links = frozenset([ireq.link]) if ireq.link else frozenset()<N>        return Constraint(ireq.specifier, ireq.hashes(trust_internet=False), links)<N><N>    def __bool__(self) -> bool:<N>        return bool(self.specifier) or bool(self.hashes) or bool(self.links)<N><N>
    def __and__(self, other: InstallRequirement) -> "Constraint":<N>        if not isinstance(other, InstallRequirement):<N>            return NotImplemented<N>        specifier = self.specifier & other.specifier<N>        hashes = self.hashes & other.hashes(trust_internet=False)<N>        links = self.links<N>        if other.link:<N>            links = links.union([other.link])<N>        return Constraint(specifier, hashes, links)<N><N>
import logging<N>import sys<N>from typing import TYPE_CHECKING, Any, FrozenSet, Iterable, Optional, Tuple, Union, cast<N><N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N>from pip._vendor.packaging.version import Version<N><N>
import contextlib<N>import functools<N>import logging<N>from typing import (<N>    TYPE_CHECKING,<N>    Dict,<N>    FrozenSet,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Mapping,<N>    NamedTuple,<N>    Optional,<N>    Sequence,<N>    Set,<N>    Tuple,<N>    TypeVar,<N>    cast,<N>)<N><N>
from pip._vendor.packaging.requirements import InvalidRequirement<N>from pip._vendor.packaging.specifiers import SpecifierSet<N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N>from pip._vendor.resolvelib import ResolutionImpossible<N><N>
"""Utilities to lazily create and visit candidates found.<N><N>Creating and visiting a candidate is a *very* costly operation. It involves<N>fetching, extracting, potentially building modules from source, and verifying<N>distribution metadata. It is therefore crucial for performance to keep<N>everything here lazy all the way down, so we only touch candidates that we<N>absolutely need, and not "download the world" when we only need one version of<N>something.<N>"""<N><N>
import functools<N>from collections.abc import Sequence<N>from typing import TYPE_CHECKING, Any, Callable, Iterator, Optional, Set, Tuple<N><N>from pip._vendor.packaging.version import _BaseVersion<N><N>from .base import Candidate<N><N>IndexCandidateInfo = Tuple[_BaseVersion, Callable[[], Optional[Candidate]]]<N><N>
import collections<N>import math<N>from typing import (<N>    TYPE_CHECKING,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    Mapping,<N>    Sequence,<N>    TypeVar,<N>    Union,<N>)<N><N>from pip._vendor.resolvelib.providers import AbstractProvider<N><N>
from .base import Candidate, Constraint, Requirement<N>from .candidates import REQUIRES_PYTHON_IDENTIFIER<N>from .factory import Factory<N><N>if TYPE_CHECKING:<N>    from pip._vendor.resolvelib.providers import Preference<N>    from pip._vendor.resolvelib.resolvers import RequirementInformation<N><N>
from collections import defaultdict<N>from logging import getLogger<N>from typing import Any, DefaultDict<N><N>from pip._vendor.resolvelib.reporters import BaseReporter<N><N>from .base import Candidate, Requirement<N><N>logger = getLogger(__name__)<N><N>
from pip._vendor.packaging.specifiers import SpecifierSet<N>from pip._vendor.packaging.utils import NormalizedName, canonicalize_name<N><N>from pip._internal.req.req_install import InstallRequirement<N><N>from .base import Candidate, CandidateLookup, Requirement, format_name<N><N>
<N>class ExplicitRequirement(Requirement):<N>    def __init__(self, candidate: Candidate) -> None:<N>        self.candidate = candidate<N><N>    def __str__(self) -> str:<N>        return str(self.candidate)<N><N>    def __repr__(self) -> str:<N>        return "{class_name}({candidate!r})".format(<N>            class_name=self.__class__.__name__,<N>            candidate=self.candidate,<N>        )<N><N>
    @property<N>    def project_name(self) -> NormalizedName:<N>        # No need to canonicalize - the candidate did this<N>        return self.candidate.project_name<N><N>    @property<N>    def name(self) -> str:<N>        # No need to canonicalize - the candidate did this<N>        return self.candidate.name<N><N>
    def format_for_error(self) -> str:<N>        return self.candidate.format_for_error()<N><N>    def get_candidate_lookup(self) -> CandidateLookup:<N>        return self.candidate, None<N><N>    def is_satisfied_by(self, candidate: Candidate) -> bool:<N>        return candidate == self.candidate<N><N>
<N>class SpecifierRequirement(Requirement):<N>    def __init__(self, ireq: InstallRequirement) -> None:<N>        assert ireq.link is None, "This is a link, not a specifier"<N>        self._ireq = ireq<N>        self._extras = frozenset(ireq.extras)<N><N>
    def __str__(self) -> str:<N>        return str(self._ireq.req)<N><N>    def __repr__(self) -> str:<N>        return "{class_name}({requirement!r})".format(<N>            class_name=self.__class__.__name__,<N>            requirement=str(self._ireq.req),<N>        )<N><N>
    @property<N>    def project_name(self) -> NormalizedName:<N>        assert self._ireq.req, "Specifier-backed ireq is always PEP 508"<N>        return canonicalize_name(self._ireq.req.name)<N><N>    @property<N>    def name(self) -> str:<N>        return format_name(self.project_name, self._extras)<N><N>
    def format_for_error(self) -> str:<N><N>        # Convert comma-separated specifiers into "A, B, ..., F and G"<N>        # This makes the specifier a bit more "human readable", without<N>        # risking a change in meaning. (Hopefully! Not all edge cases have<N>        # been checked)<N>        parts = [s.strip() for s in str(self).split(",")]<N>        if len(parts) == 0:<N>            return ""<N>        elif len(parts) == 1:<N>            return parts[0]<N><N>
import functools<N>import logging<N>import os<N>from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, cast<N><N>from pip._vendor.packaging.utils import canonicalize_name<N>from pip._vendor.resolvelib import BaseReporter, ResolutionImpossible<N>from pip._vendor.resolvelib import Resolver as RLResolver<N>from pip._vendor.resolvelib.structs import DirectedGraph<N><N>
"""<N>This code wraps the vendored appdirs module to so the return values are<N>compatible for the current pip code base.<N><N>The intention is to rewrite current usages gradually, keeping the tests pass,<N>and eventually drop this after all usages are changed.<N>"""<N><N>
import os<N>import sys<N>from typing import List<N><N>from pip._vendor import platformdirs as _appdirs<N><N><N>def user_cache_dir(appname: str) -> str:<N>    return _appdirs.user_cache_dir(appname, appauthor=False)<N><N><N>def _macos_user_config_dir(appname: str, roaming: bool = True) -> str:<N>    # Use ~/Application Support/pip, if the directory exists.<N>    path = _appdirs.user_data_dir(appname, appauthor=False, roaming=roaming)<N>    if os.path.isdir(path):<N>        return path<N><N>
    # Use a Linux-like ~/.config/pip, by default.<N>    linux_like_path = "~/.config/"<N>    if appname:<N>        linux_like_path = os.path.join(linux_like_path, appname)<N><N>    return os.path.expanduser(linux_like_path)<N><N><N>def user_config_dir(appname: str, roaming: bool = True) -> str:<N>    if sys.platform == "darwin":<N>        return _macos_user_config_dir(appname, roaming)<N><N>
    return _appdirs.user_config_dir(appname, appauthor=False, roaming=roaming)<N><N><N># for the discussion regarding site_config_dir locations<N># see <https://github.com/pypa/pip/issues/1733><N>def site_config_dirs(appname: str) -> List[str]:<N>    if sys.platform == "darwin":<N>        return [_appdirs.site_data_dir(appname, appauthor=False, multipath=True)]<N><N>
"""Stuff that differs in different Python versions and platform<N>distributions."""<N><N>import logging<N>import os<N>import sys<N><N>__all__ = ["get_path_uid", "stdlib_pkgs", "WINDOWS"]<N><N><N>logger = logging.getLogger(__name__)<N><N><N>def has_tls() -> bool:<N>    try:<N>        import _ssl  # noqa: F401  # ignore unused<N><N>
        return True<N>    except ImportError:<N>        pass<N><N>    from pip._vendor.urllib3.util import IS_PYOPENSSL<N><N>    return IS_PYOPENSSL<N><N><N>def get_path_uid(path: str) -> int:<N>    """<N>    Return path's uid.<N><N>    Does not follow symlinks:<N>        https://github.com/pypa/pip/pull/935#discussion_r5307003<N><N>
"""Generate and work with PEP 425 Compatibility Tags.<N>"""<N><N>import re<N>from typing import List, Optional, Tuple<N><N>from pip._vendor.packaging.tags import (<N>    PythonVersion,<N>    Tag,<N>    compatible_tags,<N>    cpython_tags,<N>    generic_tags,<N>    interpreter_name,<N>    interpreter_version,<N>    mac_platforms,<N>)<N><N>
"""For when pip wants to check the date or time.<N>"""<N><N>import datetime<N><N><N>def today_is_later_than(year: int, month: int, day: int) -> bool:<N>    today = datetime.date.today()<N>    given = datetime.date(year, month, day)<N><N>    return today > given<N>
"""<N>A module that implements tooling to enable easy warnings about deprecations.<N>"""<N><N>import logging<N>import warnings<N>from typing import Any, Optional, TextIO, Type, Union<N><N>from pip._vendor.packaging.version import parse<N><N>from pip import __version__ as current_version  # NOTE: tests patch this name.<N><N>
from typing import Optional<N><N>from pip._internal.models.direct_url import ArchiveInfo, DirectUrl, DirInfo, VcsInfo<N>from pip._internal.models.link import Link<N>from pip._internal.utils.urls import path_to_url<N>from pip._internal.vcs import vcs<N><N>
# The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import os<N>import re<N>import sys<N>from typing import Optional<N><N>from pip._internal.locations import site_packages, user_site<N>from pip._internal.utils.virtualenv import (<N>    running_under_virtualenv,<N>    virtualenv_no_global,<N>)<N><N>
__all__ = [<N>    "egg_link_path_from_sys_path",<N>    "egg_link_path_from_location",<N>]<N><N><N>def _egg_link_name(raw_name: str) -> str:<N>    """<N>    Convert a Name metadata value to a .egg-link name, by applying<N>    the same substitution as pkg_resources's safe_name function.<N>    Note: we cannot use canonicalize_name because it has a different logic.<N>    """<N>    return re.sub("[^A-Za-z0-9.]+", "-", raw_name) + ".egg-link"<N><N>
<N>def egg_link_path_from_sys_path(raw_name: str) -> Optional[str]:<N>    """<N>    Look for a .egg-link file for project name, by walking sys.path.<N>    """<N>    egg_link_name = _egg_link_name(raw_name)<N>    for path_item in sys.path:<N>        egg_link = os.path.join(path_item, egg_link_name)<N>        if os.path.isfile(egg_link):<N>            return egg_link<N>    return None<N><N>
<N>def egg_link_path_from_location(raw_name: str) -> Optional[str]:<N>    """<N>    Return the path for the .egg-link file if it exists, otherwise, None.<N><N>    There's 3 scenarios:<N>    1) not in a virtualenv<N>       try to find in site.USER_SITE, then site_packages<N>    2) in a no-global virtualenv<N>       try to find in site_packages<N>    3) in a yes-global virtualenv<N>       try to find in site_packages, then site.USER_SITE<N>       (don't look in global location)<N><N>
    For #1 and #3, there could be odd cases, where there's an egg-link in 2<N>    locations.<N><N>    This method will just return the first one found.<N>    """<N>    sites = []<N>    if running_under_virtualenv():<N>        sites.append(site_packages)<N>        if not virtualenv_no_global() and user_site:<N>            sites.append(user_site)<N>    else:<N>        if user_site:<N>            sites.append(user_site)<N>        sites.append(site_packages)<N><N>
import codecs<N>import locale<N>import re<N>import sys<N>from typing import List, Tuple<N><N>BOMS: List[Tuple[bytes, str]] = [<N>    (codecs.BOM_UTF8, "utf-8"),<N>    (codecs.BOM_UTF16, "utf-16"),<N>    (codecs.BOM_UTF16_BE, "utf-16-be"),<N>    (codecs.BOM_UTF16_LE, "utf-16-le"),<N>    (codecs.BOM_UTF32, "utf-32"),<N>    (codecs.BOM_UTF32_BE, "utf-32-be"),<N>    (codecs.BOM_UTF32_LE, "utf-32-le"),<N>]<N><N>
import sys<N>from typing import List, Optional<N><N>from pip._internal.cli.main import main<N><N><N>def _wrapper(args: Optional[List[str]] = None) -> int:<N>    """Central wrapper for all old entrypoints.<N><N>    Historically pip has had several entrypoints defined. Because of issues<N>    arising from PATH, sys.path, multiple Pythons, their interactions, and most<N>    of them having a pip installed, users suffer every time an entrypoint gets<N>    moved.<N><N>
import fnmatch<N>import os<N>import os.path<N>import random<N>import shutil<N>import stat<N>import sys<N>from contextlib import contextmanager<N>from tempfile import NamedTemporaryFile<N>from typing import Any, BinaryIO, Iterator, List, Union, cast<N><N>
from pip._vendor.tenacity import retry, stop_after_delay, wait_fixed<N><N>from pip._internal.utils.compat import get_path_uid<N>from pip._internal.utils.misc import format_size<N><N><N>def check_path_owner(path: str) -> bool:<N>    # If we don't have a way to check the effective uid of this process, then<N>    # we'll just assume that we own the directory.<N>    if sys.platform == "win32" or not hasattr(os, "geteuid"):<N>        return True<N><N>
# The following comment should be removed at some point in the future.<N># mypy: strict-optional=False<N><N>import os<N>import sys<N>from typing import Optional, Tuple<N><N><N>def glibc_version_string() -> Optional[str]:<N>    "Returns glibc version string, or None if not using glibc."<N>    return glibc_version_string_confstr() or glibc_version_string_ctypes()<N><N>
import hashlib<N>from typing import TYPE_CHECKING, BinaryIO, Dict, Iterator, List<N><N>from pip._internal.exceptions import HashMismatch, HashMissing, InstallationError<N>from pip._internal.utils.misc import read_chunks<N><N>if TYPE_CHECKING:<N>    from hashlib import _Hash<N><N>
    # NoReturn introduced in 3.6.2; imported only for type checking to maintain<N>    # pip compatibility with older patch versions of Python 3.6<N>    from typing import NoReturn<N><N><N># The recommended hash algo of the moment. Change this whenever the state of<N># the art changes; it won't hurt backward compatibility.<N>FAVORITE_HASH = "sha256"<N><N>
<N># Names of hashlib algorithms allowed by the --hash option and ``pip hash``<N># Currently, those are the ones at least as collision-resistant as sha256.<N>STRONG_HASHES = ["sha256", "sha384", "sha512"]<N><N><N>class Hashes:<N>    """A wrapper that builds multiple hashes at once and checks them against<N>    known-good values<N><N>
    """<N><N>    def __init__(self, hashes: Dict[str, List[str]] = None) -> None:<N>        """<N>        :param hashes: A dict of algorithm names pointing to lists of allowed<N>            hex digests<N>        """<N>        allowed = {}<N>        if hashes is not None:<N>            for alg, keys in hashes.items():<N>                # Make sure values are always sorted (to ease equality checks)<N>                allowed[alg] = sorted(keys)<N>        self._allowed = allowed<N><N>
    def __and__(self, other: "Hashes") -> "Hashes":<N>        if not isinstance(other, Hashes):<N>            return NotImplemented<N><N>        # If either of the Hashes object is entirely empty (i.e. no hash<N>        # specified at all), all hashes from the other object are allowed.<N>        if not other:<N>            return self<N>        if not self:<N>            return other<N><N>
        # Otherwise only hashes that present in both objects are allowed.<N>        new = {}<N>        for alg, values in other._allowed.items():<N>            if alg not in self._allowed:<N>                continue<N>            new[alg] = [v for v in values if v in self._allowed[alg]]<N>        return Hashes(new)<N><N>
    @property<N>    def digest_count(self) -> int:<N>        return sum(len(digests) for digests in self._allowed.values())<N><N>    def is_hash_allowed(self, hash_name: str, hex_digest: str) -> bool:<N>        """Return whether the given hex digest is allowed."""<N>        return hex_digest in self._allowed.get(hash_name, [])<N><N>
    def check_against_chunks(self, chunks: Iterator[bytes]) -> None:<N>        """Check good hashes against ones built from iterable of chunks of<N>        data.<N><N>        Raise HashMismatch if none match.<N><N>        """<N>        gots = {}<N>        for hash_name in self._allowed.keys():<N>            try:<N>                gots[hash_name] = hashlib.new(hash_name)<N>            except (ValueError, TypeError):<N>                raise InstallationError(f"Unknown hash name: {hash_name}")<N><N>
        for chunk in chunks:<N>            for hash in gots.values():<N>                hash.update(chunk)<N><N>        for hash_name, got in gots.items():<N>            if got.hexdigest() in self._allowed[hash_name]:<N>                return<N>        self._raise(gots)<N><N>
    def _raise(self, gots: Dict[str, "_Hash"]) -> "NoReturn":<N>        raise HashMismatch(self._allowed, gots)<N><N>    def check_against_file(self, file: BinaryIO) -> None:<N>        """Check good hashes against a file-like object<N><N>        Raise HashMismatch if none match.<N><N>
        """<N>        return self.check_against_chunks(read_chunks(file))<N><N>    def check_against_path(self, path: str) -> None:<N>        with open(path, "rb") as file:<N>            return self.check_against_file(file)<N><N>    def __bool__(self) -> bool:<N>        """Return whether I know any known-good hashes."""<N>        return bool(self._allowed)<N><N>
    def __eq__(self, other: object) -> bool:<N>        if not isinstance(other, Hashes):<N>            return NotImplemented<N>        return self._allowed == other._allowed<N><N>    def __hash__(self) -> int:<N>        return hash(<N>            ",".join(<N>                sorted(<N>                    ":".join((alg, digest))<N>                    for alg, digest_list in self._allowed.items()<N>                    for digest in digest_list<N>                )<N>            )<N>        )<N><N>
<N>class MissingHashes(Hashes):<N>    """A workalike for Hashes used when we're missing a hash for a requirement<N><N>    It computes the actual hash of the requirement and raises a HashMissing<N>    exception showing it to the user.<N><N>    """<N><N>
    def __init__(self) -> None:<N>        """Don't offer the ``hashes`` kwarg."""<N>        # Pass our favorite hash in to generate a "gotten hash". With the<N>        # empty list, it will never match, so an error will always raise.<N>        super().__init__(hashes={FAVORITE_HASH: []})<N><N>
"""A helper module that injects SecureTransport, on import.<N><N>The import should be done as early as possible, to ensure all requests and<N>sessions (or whatever) are created after injecting SecureTransport.<N><N>Note that we only do the injection on macOS, when the linked OpenSSL is too<N>old to handle TLSv1.2.<N>"""<N><N>
import sys<N><N><N>def inject_securetransport() -> None:<N>    # Only relevant on macOS<N>    if sys.platform != "darwin":<N>        return<N><N>    try:<N>        import ssl<N>    except ImportError:<N>        return<N><N>    # Checks for OpenSSL 1.0.1<N>    if ssl.OPENSSL_VERSION_NUMBER >= 0x1000100F:<N>        return<N><N>
import contextlib<N>import errno<N>import logging<N>import logging.handlers<N>import os<N>import sys<N>import threading<N>from dataclasses import dataclass<N>from logging import Filter<N>from typing import IO, Any, ClassVar, Iterator, List, Optional, TextIO, Type<N><N>
from pip._vendor.rich.console import (<N>    Console,<N>    ConsoleOptions,<N>    ConsoleRenderable,<N>    RenderResult,<N>)<N>from pip._vendor.rich.highlighter import NullHighlighter<N>from pip._vendor.rich.logging import RichHandler<N>from pip._vendor.rich.segment import Segment<N>from pip._vendor.rich.style import Style<N><N>
from pip._internal.exceptions import DiagnosticPipError<N>from pip._internal.utils._log import VERBOSE, getLogger<N>from pip._internal.utils.compat import WINDOWS<N>from pip._internal.utils.deprecation import DEPRECATION_MSG_PREFIX<N>from pip._internal.utils.misc import ensure_dir<N><N>
_log_state = threading.local()<N>subprocess_logger = getLogger("pip.subprocessor")<N><N><N>class BrokenStdoutLoggingError(Exception):<N>    """<N>    Raised if BrokenPipeError occurs for the stdout stream while logging.<N>    """<N><N><N>def _is_broken_pipe_error(exc_class: Type[BaseException], exc: BaseException) -> bool:<N>    if exc_class is BrokenPipeError:<N>        return True<N><N>
    # On Windows, a broken pipe can show up as EINVAL rather than EPIPE:<N>    # https://bugs.python.org/issue19612<N>    # https://bugs.python.org/issue30418<N>    if not WINDOWS:<N>        return False<N><N>    return isinstance(exc, OSError) and exc.errno in (errno.EINVAL, errno.EPIPE)<N><N>
<N>@contextlib.contextmanager<N>def indent_log(num: int = 2) -> Iterator[None]:<N>    """<N>    A context manager which will cause the log output to be indented for any<N>    log messages emitted inside it.<N>    """<N>    # For thread-safety<N>    _log_state.indentation = get_indentation()<N>    _log_state.indentation += num<N>    try:<N>        yield<N>    finally:<N>        _log_state.indentation -= num<N><N>
<N>def get_indentation() -> int:<N>    return getattr(_log_state, "indentation", 0)<N><N><N>class IndentingFormatter(logging.Formatter):<N>    default_time_format = "%Y-%m-%dT%H:%M:%S"<N><N>    def __init__(<N>        self,<N>        *args: Any,<N>        add_timestamp: bool = False,<N>        **kwargs: Any,<N>    ) -> None:<N>        """<N>        A logging.Formatter that obeys the indent_log() context manager.<N><N>
"""Utilities for defining models<N>"""<N><N>import operator<N>from typing import Any, Callable, Type<N><N><N>class KeyBasedCompareMixin:<N>    """Provides comparison capabilities that is based on a key"""<N><N>    __slots__ = ["_compare_key", "_defining_class"]<N><N>
    def __init__(self, key: Any, defining_class: Type["KeyBasedCompareMixin"]) -> None:<N>        self._compare_key = key<N>        self._defining_class = defining_class<N><N>    def __hash__(self) -> int:<N>        return hash(self._compare_key)<N><N>
    def __lt__(self, other: Any) -> bool:<N>        return self._compare(other, operator.__lt__)<N><N>    def __le__(self, other: Any) -> bool:<N>        return self._compare(other, operator.__le__)<N><N>    def __gt__(self, other: Any) -> bool:<N>        return self._compare(other, operator.__gt__)<N><N>
    def __ge__(self, other: Any) -> bool:<N>        return self._compare(other, operator.__ge__)<N><N>    def __eq__(self, other: Any) -> bool:<N>        return self._compare(other, operator.__eq__)<N><N>    def _compare(self, other: Any, method: Callable[[Any, Any], bool]) -> bool:<N>        if not isinstance(other, self._defining_class):<N>            return NotImplemented<N><N>
import functools<N>import logging<N>import re<N>from typing import NewType, Optional, Tuple, cast<N><N>from pip._vendor.packaging import specifiers, version<N>from pip._vendor.packaging.requirements import Requirement<N><N>NormalizedExtra = NewType("NormalizedExtra", str)<N><N>
logger = logging.getLogger(__name__)<N><N><N>def check_requires_python(<N>    requires_python: Optional[str], version_info: Tuple[int, ...]<N>) -> bool:<N>    """<N>    Check if the given Python version matches a "Requires-Python" specifier.<N><N>    :param version_info: A 3-tuple of ints representing a Python<N>        major-minor-micro version to check (e.g. `sys.version_info[:3]`).<N><N>
    :return: `True` if the given Python version satisfies the requirement.<N>        Otherwise, return `False`.<N><N>    :raises InvalidSpecifier: If `requires_python` has an invalid format.<N>    """<N>    if requires_python is None:<N>        # The package provides no information<N>        return True<N>    requires_python_specifier = specifiers.SpecifierSet(requires_python)<N><N>
import logging<N>import os<N>import shlex<N>import subprocess<N>from typing import (<N>    TYPE_CHECKING,<N>    Any,<N>    Callable,<N>    Iterable,<N>    List,<N>    Mapping,<N>    Optional,<N>    Union,<N>)<N><N>from pip._vendor.rich.markup import escape<N><N>
from pip._internal.cli.spinners import SpinnerInterface, open_spinner<N>from pip._internal.exceptions import InstallationSubprocessError<N>from pip._internal.utils.logging import VERBOSE, subprocess_logger<N>from pip._internal.utils.misc import HiddenText<N><N>
import errno<N>import itertools<N>import logging<N>import os.path<N>import tempfile<N>from contextlib import ExitStack, contextmanager<N>from typing import Any, Dict, Iterator, Optional, TypeVar, Union<N><N>from pip._internal.utils.misc import enum, rmtree<N><N>
logger = logging.getLogger(__name__)<N><N>_T = TypeVar("_T", bound="TempDirectory")<N><N><N># Kinds of temporary directories. Only needed for ones that are<N># globally-managed.<N>tempdir_kinds = enum(<N>    BUILD_ENV="build-env",<N>    EPHEM_WHEEL_CACHE="ephem-wheel-cache",<N>    REQ_BUILD="req-build",<N>)<N><N>
<N>_tempdir_manager: Optional[ExitStack] = None<N><N><N>@contextmanager<N>def global_tempdir_manager() -> Iterator[None]:<N>    global _tempdir_manager<N>    with ExitStack() as stack:<N>        old_tempdir_manager, _tempdir_manager = _tempdir_manager, stack<N>        try:<N>            yield<N>        finally:<N>            _tempdir_manager = old_tempdir_manager<N><N>
<N>class TempDirectoryTypeRegistry:<N>    """Manages temp directory behavior"""<N><N>    def __init__(self) -> None:<N>        self._should_delete: Dict[str, bool] = {}<N><N>    def set_delete(self, kind: str, value: bool) -> None:<N>        """Indicate whether a TempDirectory of the given kind should be<N>        auto-deleted.<N>        """<N>        self._should_delete[kind] = value<N><N>
    def get_delete(self, kind: str) -> bool:<N>        """Get configured auto-delete flag for a given TempDirectory type,<N>        default True.<N>        """<N>        return self._should_delete.get(kind, True)<N><N><N>_tempdir_registry: Optional[TempDirectoryTypeRegistry] = None<N><N>
<N>@contextmanager<N>def tempdir_registry() -> Iterator[TempDirectoryTypeRegistry]:<N>    """Provides a scoped global tempdir registry that can be used to dictate<N>    whether directories should be deleted.<N>    """<N>    global _tempdir_registry<N>    old_tempdir_registry = _tempdir_registry<N>    _tempdir_registry = TempDirectoryTypeRegistry()<N>    try:<N>        yield _tempdir_registry<N>    finally:<N>        _tempdir_registry = old_tempdir_registry<N><N>
<N>class _Default:<N>    pass<N><N><N>_default = _Default()<N><N><N>class TempDirectory:<N>    """Helper class that owns and cleans up a temporary directory.<N><N>    This class can be used as a context manager or as an OO representation of a<N>    temporary directory.<N><N>
    Attributes:<N>        path<N>            Location to the created temporary directory<N>        delete<N>            Whether the directory should be deleted when exiting<N>            (when used as a contextmanager)<N><N>    Methods:<N>        cleanup()<N>            Deletes the temporary directory<N><N>
    When used as a context manager, if the delete attribute is True, on<N>    exiting the context the temporary directory is deleted.<N>    """<N><N>    def __init__(<N>        self,<N>        path: Optional[str] = None,<N>        delete: Union[bool, None, _Default] = _default,<N>        kind: str = "temp",<N>        globally_managed: bool = False,<N>    ):<N>        super().__init__()<N><N>
        if delete is _default:<N>            if path is not None:<N>                # If we were given an explicit directory, resolve delete option<N>                # now.<N>                delete = False<N>            else:<N>                # Otherwise, we wait until cleanup and see what<N>                # tempdir_registry says.<N>                delete = None<N><N>
        # The only time we specify path is in for editables where it<N>        # is the value of the --src option.<N>        if path is None:<N>            path = self._create(kind)<N><N>        self._path = path<N>        self._deleted = False<N>        self.delete = delete<N>        self.kind = kind<N><N>
        if globally_managed:<N>            assert _tempdir_manager is not None<N>            _tempdir_manager.enter_context(self)<N><N>    @property<N>    def path(self) -> str:<N>        assert not self._deleted, f"Attempted to access deleted path: {self._path}"<N>        return self._path<N><N>
    def __repr__(self) -> str:<N>        return f"<{self.__class__.__name__} {self.path!r}>"<N><N>    def __enter__(self: _T) -> _T:<N>        return self<N><N>    def __exit__(self, exc: Any, value: Any, tb: Any) -> None:<N>        if self.delete is not None:<N>            delete = self.delete<N>        elif _tempdir_registry:<N>            delete = _tempdir_registry.get_delete(self.kind)<N>        else:<N>            delete = True<N><N>
"""Utilities related archives.<N>"""<N><N>import logging<N>import os<N>import shutil<N>import stat<N>import tarfile<N>import zipfile<N>from typing import Iterable, List, Optional<N>from zipfile import ZipInfo<N><N>from pip._internal.exceptions import InstallationError<N>from pip._internal.utils.filetypes import (<N>    BZ2_EXTENSIONS,<N>    TAR_EXTENSIONS,<N>    XZ_EXTENSIONS,<N>    ZIP_EXTENSIONS,<N>)<N>from pip._internal.utils.misc import ensure_dir<N><N>
logger = logging.getLogger(__name__)<N><N><N>SUPPORTED_EXTENSIONS = ZIP_EXTENSIONS + TAR_EXTENSIONS<N><N>try:<N>    import bz2  # noqa<N><N>    SUPPORTED_EXTENSIONS += BZ2_EXTENSIONS<N>except ImportError:<N>    logger.debug("bz2 module is not available")<N><N>
try:<N>    # Only for Python 3.3+<N>    import lzma  # noqa<N><N>    SUPPORTED_EXTENSIONS += XZ_EXTENSIONS<N>except ImportError:<N>    logger.debug("lzma module is not available")<N><N><N>def current_umask() -> int:<N>    """Get the current umask which involves having to set it temporarily."""<N>    mask = os.umask(0)<N>    os.umask(mask)<N>    return mask<N><N>
<N>def split_leading_dir(path: str) -> List[str]:<N>    path = path.lstrip("/").lstrip("\\")<N>    if "/" in path and (<N>        ("\\" in path and path.find("/") < path.find("\\")) or "\\" not in path<N>    ):<N>        return path.split("/", 1)<N>    elif "\\" in path:<N>        return path.split("\\", 1)<N>    else:<N>        return [path, ""]<N><N>
<N>def has_leading_dir(paths: Iterable[str]) -> bool:<N>    """Returns true if all the paths have the same leading path name<N>    (i.e., everything is in one subdirectory in an archive)"""<N>    common_prefix = None<N>    for path in paths:<N>        prefix, rest = split_leading_dir(path)<N>        if not prefix:<N>            return False<N>        elif common_prefix is None:<N>            common_prefix = prefix<N>        elif prefix != common_prefix:<N>            return False<N>    return True<N><N>
<N>def is_within_directory(directory: str, target: str) -> bool:<N>    """<N>    Return true if the absolute path of target is within the directory<N>    """<N>    abs_directory = os.path.abspath(directory)<N>    abs_target = os.path.abspath(target)<N><N>
    prefix = os.path.commonprefix([abs_directory, abs_target])<N>    return prefix == abs_directory<N><N><N>def set_extracted_file_to_default_mode_plus_executable(path: str) -> None:<N>    """<N>    Make file present at path have execute for user/group/world<N>    (chmod +x) is no-op on windows per python docs<N>    """<N>    os.chmod(path, (0o777 & ~current_umask() | 0o111))<N><N>
<N>def zip_item_is_executable(info: ZipInfo) -> bool:<N>    mode = info.external_attr >> 16<N>    # if mode and regular file and any execute permissions for<N>    # user/group/world?<N>    return bool(mode and stat.S_ISREG(mode) and mode & 0o111)<N><N>
import os<N>import string<N>import urllib.parse<N>import urllib.request<N>from typing import Optional<N><N>from .compat import WINDOWS<N><N><N>def get_url_scheme(url: str) -> Optional[str]:<N>    if ":" not in url:<N>        return None<N>    return url.split(":", 1)[0].lower()<N><N>
<N>def path_to_url(path: str) -> str:<N>    """<N>    Convert a path to a file: URL.  The path will be made absolute and have<N>    quoted path parts.<N>    """<N>    path = os.path.normpath(os.path.abspath(path))<N>    url = urllib.parse.urljoin("file:", urllib.request.pathname2url(path))<N>    return url<N><N>
<N>def url_to_path(url: str) -> str:<N>    """<N>    Convert a file: URL to a path.<N>    """<N>    assert url.startswith(<N>        "file:"<N>    ), f"You can only turn file: urls into filenames (not {url!r})"<N><N>    _, netloc, path, _, _ = urllib.parse.urlsplit(url)<N><N>
    if not netloc or netloc == "localhost":<N>        # According to RFC 8089, same as empty authority.<N>        netloc = ""<N>    elif WINDOWS:<N>        # If we have a UNC path, prepend UNC share notation.<N>        netloc = "\\\\" + netloc<N>    else:<N>        raise ValueError(<N>            f"non-local file URIs are not supported on this platform: {url!r}"<N>        )<N><N>
import logging<N>import os<N>import re<N>import site<N>import sys<N>from typing import List, Optional<N><N>logger = logging.getLogger(__name__)<N>_INCLUDE_SYSTEM_SITE_PACKAGES_REGEX = re.compile(<N>    r"include-system-site-packages\s*=\s*(?P<value>true|false)"<N>)<N><N>
<N>def _running_under_venv() -> bool:<N>    """Checks if sys.base_prefix and sys.prefix match.<N><N>    This handles PEP 405 compliant virtual environments.<N>    """<N>    return sys.prefix != getattr(sys, "base_prefix", sys.prefix)<N><N><N>def _running_under_regular_virtualenv() -> bool:<N>    """Checks if sys.real_prefix is set.<N><N>
    This handles virtual environments created with pypa's virtualenv.<N>    """<N>    # pypa/virtualenv case<N>    return hasattr(sys, "real_prefix")<N><N><N>def running_under_virtualenv() -> bool:<N>    """Return True if we're running inside a virtualenv, False otherwise."""<N>    return _running_under_venv() or _running_under_regular_virtualenv()<N><N>
"""Support functions for working with wheel files.<N>"""<N><N>import logging<N>from email.message import Message<N>from email.parser import Parser<N>from typing import Tuple<N>from zipfile import BadZipFile, ZipFile<N><N>from pip._vendor.packaging.utils import canonicalize_name<N><N>
from pip._internal.exceptions import UnsupportedWheel<N><N>VERSION_COMPATIBLE = (1, 0)<N><N><N>logger = logging.getLogger(__name__)<N><N><N>def parse_wheel(wheel_zip: ZipFile, name: str) -> Tuple[str, Message]:<N>    """Extract information from the provided wheel, ensuring it meets basic<N>    standards.<N><N>
    Returns the name of the .dist-info directory and the parsed WHEEL metadata.<N>    """<N>    try:<N>        info_dir = wheel_dist_info_dir(wheel_zip, name)<N>        metadata = wheel_metadata(wheel_zip, info_dir)<N>        version = wheel_version(metadata)<N>    except UnsupportedWheel as e:<N>        raise UnsupportedWheel("{} has an invalid wheel, {}".format(name, str(e)))<N><N>
    check_compatibility(version, name)<N><N>    return info_dir, metadata<N><N><N>def wheel_dist_info_dir(source: ZipFile, name: str) -> str:<N>    """Returns the name of the contained .dist-info directory.<N><N>    Raises AssertionError or UnsupportedWheel if not found, >1 found, or<N>    it doesn't match the provided name.<N>    """<N>    # Zip file path separators must be /<N>    subdirs = {p.split("/", 1)[0] for p in source.namelist()}<N><N>
    info_dirs = [s for s in subdirs if s.endswith(".dist-info")]<N><N>    if not info_dirs:<N>        raise UnsupportedWheel(".dist-info directory not found")<N><N>    if len(info_dirs) > 1:<N>        raise UnsupportedWheel(<N>            "multiple .dist-info directories found: {}".format(", ".join(info_dirs))<N>        )<N><N>
    info_dir = info_dirs[0]<N><N>    info_dir_name = canonicalize_name(info_dir)<N>    canonical_name = canonicalize_name(name)<N>    if not info_dir_name.startswith(canonical_name):<N>        raise UnsupportedWheel(<N>            ".dist-info directory {!r} does not start with {!r}".format(<N>                info_dir, canonical_name<N>            )<N>        )<N><N>
    return info_dir<N><N><N>def read_wheel_metadata_file(source: ZipFile, path: str) -> bytes:<N>    try:<N>        return source.read(path)<N>        # BadZipFile for general corruption, KeyError for missing entry,<N>        # and RuntimeError for password-protected files<N>    except (BadZipFile, KeyError, RuntimeError) as e:<N>        raise UnsupportedWheel(f"could not read {path!r} file: {e!r}")<N><N>
<N>def wheel_metadata(source: ZipFile, dist_info_dir: str) -> Message:<N>    """Return the WHEEL metadata of an extracted wheel, if possible.<N>    Otherwise, raise UnsupportedWheel.<N>    """<N>    path = f"{dist_info_dir}/WHEEL"<N>    # Zip file path separators must be /<N>    wheel_contents = read_wheel_metadata_file(source, path)<N><N>
    try:<N>        wheel_text = wheel_contents.decode()<N>    except UnicodeDecodeError as e:<N>        raise UnsupportedWheel(f"error decoding {path!r}: {e!r}")<N><N>    # FeedParser (used by Parser) does not raise any exceptions. The returned<N>    # message may have .defects populated, but for backwards-compatibility we<N>    # currently ignore them.<N>    return Parser().parsestr(wheel_text)<N><N>
<N>def wheel_version(wheel_data: Message) -> Tuple[int, ...]:<N>    """Given WHEEL metadata, return the parsed Wheel-Version.<N>    Otherwise, raise UnsupportedWheel.<N>    """<N>    version_text = wheel_data["Wheel-Version"]<N>    if version_text is None:<N>        raise UnsupportedWheel("WHEEL is missing Wheel-Version")<N><N>
    version = version_text.strip()<N><N>    try:<N>        return tuple(map(int, version.split(".")))<N>    except ValueError:<N>        raise UnsupportedWheel(f"invalid Wheel-Version: {version!r}")<N><N><N>def check_compatibility(version: Tuple[int, ...], name: str) -> None:<N>    """Raises errors or warns if called with an incompatible Wheel-Version.<N><N>
    pip should refuse to install a Wheel-Version that's a major series<N>    ahead of what it's compatible with (e.g 2.0 > 1.1); and warn when<N>    installing a version only minor version ahead (e.g 1.2 > 1.1).<N><N>    version: a 2-tuple representing a Wheel-Version (Major, Minor)<N>    name: name of wheel or package to raise exception about<N><N>
    :raises UnsupportedWheel: when an incompatible Wheel-Version is given<N>    """<N>    if version[0] > VERSION_COMPATIBLE[0]:<N>        raise UnsupportedWheel(<N>            "{}'s Wheel-Version ({}) is not compatible with this version "<N>            "of pip".format(name, ".".join(map(str, version)))<N>        )<N>    elif version > VERSION_COMPATIBLE:<N>        logger.warning(<N>            "Installing from a newer Wheel-Version (%s)",<N>            ".".join(map(str, version)),<N>        )<N><N><N>
"""Customize logging<N><N>Defines custom logger class for the `logger.verbose(...)` method.<N><N>init_logging() must be called before any other modules that call logging.getLogger.<N>"""<N><N>import logging<N>from typing import Any, cast<N><N># custom log level for `--verbose` output<N># between DEBUG and INFO<N>VERBOSE = 15<N><N>
<N>class VerboseLogger(logging.Logger):<N>    """Custom Logger, defining a verbose log-level<N><N>    VERBOSE is between INFO and DEBUG.<N>    """<N><N>    def verbose(self, msg: str, *args: Any, **kwargs: Any) -> None:<N>        return self.log(VERBOSE, msg, *args, **kwargs)<N><N>
<N>def getLogger(name: str) -> VerboseLogger:<N>    """logging.getLogger, but ensures our VerboseLogger class is returned"""<N>    return cast(VerboseLogger, logging.getLogger(name))<N><N><N>def init_logging() -> None:<N>    """Register our VerboseLogger and VERBOSE log level.<N><N>
import logging<N>from typing import List, Optional, Tuple<N><N>from pip._internal.utils.misc import HiddenText, display_path<N>from pip._internal.utils.subprocess import make_command<N>from pip._internal.utils.urls import path_to_url<N>from pip._internal.vcs.versioncontrol import (<N>    AuthInfo,<N>    RemoteNotFoundError,<N>    RevOptions,<N>    VersionControl,<N>    vcs,<N>)<N><N>
logger = logging.getLogger(__name__)<N><N><N>class Bazaar(VersionControl):<N>    name = "bzr"<N>    dirname = ".bzr"<N>    repo_name = "branch"<N>    schemes = (<N>        "bzr+http",<N>        "bzr+https",<N>        "bzr+ssh",<N>        "bzr+sftp",<N>        "bzr+ftp",<N>        "bzr+lp",<N>        "bzr+file",<N>    )<N><N>
import configparser<N>import logging<N>import os<N>from typing import List, Optional, Tuple<N><N>from pip._internal.exceptions import BadCommand, InstallationError<N>from pip._internal.utils.misc import HiddenText, display_path<N>from pip._internal.utils.subprocess import make_command<N>from pip._internal.utils.urls import path_to_url<N>from pip._internal.vcs.versioncontrol import (<N>    RevOptions,<N>    VersionControl,<N>    find_path_to_project_root_from_repo_root,<N>    vcs,<N>)<N><N>
logger = logging.getLogger(__name__)<N><N><N>class Mercurial(VersionControl):<N>    name = "hg"<N>    dirname = ".hg"<N>    repo_name = "clone"<N>    schemes = (<N>        "hg+file",<N>        "hg+http",<N>        "hg+https",<N>        "hg+ssh",<N>        "hg+static-http",<N>    )<N><N>
import logging<N>import os<N>import re<N>from typing import List, Optional, Tuple<N><N>from pip._internal.utils.misc import (<N>    HiddenText,<N>    display_path,<N>    is_console_interactive,<N>    is_installable_dir,<N>    split_auth_from_netloc,<N>)<N>from pip._internal.utils.subprocess import CommandArgs, make_command<N>from pip._internal.vcs.versioncontrol import (<N>    AuthInfo,<N>    RemoteNotFoundError,<N>    RevOptions,<N>    VersionControl,<N>    vcs,<N>)<N><N>
logger = logging.getLogger(__name__)<N><N>_svn_xml_url_re = re.compile('url="([^"]+)"')<N>_svn_rev_re = re.compile(r'committed-rev="(\d+)"')<N>_svn_info_xml_rev_re = re.compile(r'\s*revision="(\d+)"')<N>_svn_info_xml_url_re = re.compile(r"<url>(.*)</url>")<N><N>
<N>class Subversion(VersionControl):<N>    name = "svn"<N>    dirname = ".svn"<N>    repo_name = "checkout"<N>    schemes = ("svn+ssh", "svn+http", "svn+https", "svn+svn", "svn+file")<N><N>    @classmethod<N>    def should_add_vcs_url_prefix(cls, remote_url: str) -> bool:<N>        return True<N><N>
    @staticmethod<N>    def get_base_rev_args(rev: str) -> List[str]:<N>        return ["-r", rev]<N><N>    @classmethod<N>    def get_revision(cls, location: str) -> str:<N>        """<N>        Return the maximum revision for all files under a given location<N>        """<N>        # Note: taken from setuptools.command.egg_info<N>        revision = 0<N><N>
        for base, dirs, _ in os.walk(location):<N>            if cls.dirname not in dirs:<N>                dirs[:] = []<N>                continue  # no sense walking uncontrolled subdirs<N>            dirs.remove(cls.dirname)<N>            entries_fn = os.path.join(base, cls.dirname, "entries")<N>            if not os.path.exists(entries_fn):<N>                # FIXME: should we warn?<N>                continue<N><N>
            dirurl, localrev = cls._get_svn_url_rev(base)<N><N>            if base == location:<N>                assert dirurl is not None<N>                base = dirurl + "/"  # save the root url<N>            elif not dirurl or not dirurl.startswith(base):<N>                dirs[:] = []<N>                continue  # not part of the same svn tree, skip it<N>            revision = max(revision, localrev)<N>        return str(revision)<N><N>
"""Handles all VCS (version control) support"""<N><N>import logging<N>import os<N>import shutil<N>import sys<N>import urllib.parse<N>from typing import (<N>    TYPE_CHECKING,<N>    Any,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Mapping,<N>    Optional,<N>    Tuple,<N>    Type,<N>    Union,<N>)<N><N>
import abc<N>import collections<N>import collections.abc<N>import operator<N>import sys<N>import typing<N><N># After PEP 560, internal typing API was substantially reworked.<N># This is especially important for Protocol class which uses internal APIs<N># quite extensively.<N>PEP_560 = sys.version_info[:3] >= (3, 7, 0)<N><N>
if PEP_560:<N>    GenericMeta = type<N>else:<N>    # 3.6<N>    from typing import GenericMeta, _type_vars  # noqa<N><N># The two functions below are copies of typing internal helpers.<N># They are needed by _ProtocolMeta<N><N><N>def _no_slots_copy(dct):<N>    dict_copy = dict(dct)<N>    if '__slots__' in dict_copy:<N>        for slot in dict_copy['__slots__']:<N>            dict_copy.pop(slot, None)<N>    return dict_copy<N><N>
<N>def _check_generic(cls, parameters):<N>    if not cls.__parameters__:<N>        raise TypeError(f"{cls} is not a generic class")<N>    alen = len(parameters)<N>    elen = len(cls.__parameters__)<N>    if alen != elen:<N>        raise TypeError(f"Too {'many' if alen > elen else 'few'} arguments for {cls};"<N>                        f" actual {alen}, expected {elen}")<N><N>
<N># Please keep __all__ alphabetized within each category.<N>__all__ = [<N>    # Super-special typing primitives.<N>    'ClassVar',<N>    'Concatenate',<N>    'Final',<N>    'ParamSpec',<N>    'Self',<N>    'Type',<N><N>    # ABCs (from collections.abc).<N>    'Awaitable',<N>    'AsyncIterator',<N>    'AsyncIterable',<N>    'Coroutine',<N>    'AsyncGenerator',<N>    'AsyncContextManager',<N>    'ChainMap',<N><N>
    # Concrete collection types.<N>    'ContextManager',<N>    'Counter',<N>    'Deque',<N>    'DefaultDict',<N>    'OrderedDict',<N>    'TypedDict',<N><N>    # Structural checks, a.k.a. protocols.<N>    'SupportsIndex',<N><N>    # One-off things.<N>    'Annotated',<N>    'final',<N>    'IntVar',<N>    'Literal',<N>    'NewType',<N>    'overload',<N>    'Protocol',<N>    'runtime',<N>    'runtime_checkable',<N>    'Text',<N>    'TypeAlias',<N>    'TypeGuard',<N>    'TYPE_CHECKING',<N>]<N><N>
if PEP_560:<N>    __all__.extend(["get_args", "get_origin", "get_type_hints"])<N><N># 3.6.2+<N>if hasattr(typing, 'NoReturn'):<N>    NoReturn = typing.NoReturn<N># 3.6.0-3.6.1<N>else:<N>    class _NoReturn(typing._FinalTypingBase, _root=True):<N>        """Special type indicating functions that never return.<N>        Example::<N><N>
          from typing import NoReturn<N><N>          def stop() -> NoReturn:<N>              raise Exception('no way')<N><N>        This type is invalid in other positions, e.g., ``List[NoReturn]``<N>        will fail in static type checkers.<N>        """<N>        __slots__ = ()<N><N>
        def __instancecheck__(self, obj):<N>            raise TypeError("NoReturn cannot be used with isinstance().")<N><N>        def __subclasscheck__(self, cls):<N>            raise TypeError("NoReturn cannot be used with issubclass().")<N><N>    NoReturn = _NoReturn(_root=True)<N><N>
# Some unconstrained type variables.  These are used by the container types.<N># (These are not for export.)<N>T = typing.TypeVar('T')  # Any type.<N>KT = typing.TypeVar('KT')  # Key type.<N>VT = typing.TypeVar('VT')  # Value type.<N>T_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.<N>T_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.<N><N>
ClassVar = typing.ClassVar<N><N># On older versions of typing there is an internal class named "Final".<N># 3.8+<N>if hasattr(typing, 'Final') and sys.version_info[:2] >= (3, 7):<N>    Final = typing.Final<N># 3.7<N>elif sys.version_info[:2] >= (3, 7):<N>    class _FinalForm(typing._SpecialForm, _root=True):<N><N>
        def __repr__(self):<N>            return 'typing_extensions.' + self._name<N><N>        def __getitem__(self, parameters):<N>            item = typing._type_check(parameters,<N>                                      f'{self._name} accepts only single type')<N>            return typing._GenericAlias(self, (item,))<N><N>
    Final = _FinalForm('Final',<N>                       doc="""A special typing construct to indicate that a name<N>                       cannot be re-assigned or overridden in a subclass.<N>                       For example:<N><N>                           MAX_SIZE: Final = 9000<N>                           MAX_SIZE += 1  # Error reported by type checker<N><N>
"""<N>pip._vendor is for vendoring dependencies of pip to prevent needing pip to<N>depend on something external.<N><N>Files inside of pip._vendor should be considered immutable and should only be<N>updated to versions from upstream.<N>"""<N>from __future__ import absolute_import<N><N>
import glob<N>import os.path<N>import sys<N><N># Downstream redistributors which have debundled our dependencies should also<N># patch this value to be true. This will trigger the additional patching<N># to cause things like "six" to be available as pip.<N>DEBUNDLED = False<N><N>
# By default, look in this directory for a bunch of .whl files which we will<N># add to the beginning of sys.path before attempting to import anything. This<N># is done to support downstream re-distributors like Debian and Fedora who<N># wish to create their own Wheels for our dependencies to aid in debundling.<N>WHEEL_DIR = os.path.abspath(os.path.dirname(__file__))<N><N>
<N># Define a small helper function to alias our vendored modules to the real ones<N># if the vendored ones do not exist. This idea of this was taken from<N># https://github.com/kennethreitz/requests/pull/2567.<N>def vendored(modulename):<N>    vendored_name = "{0}.{1}".format(__name__, modulename)<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>import types<N>import functools<N>import zlib<N><N>from pip._vendor.requests.adapters import HTTPAdapter<N><N>from .controller import CacheController, PERMANENT_REDIRECT_STATUSES<N>from .cache import DictCache<N>from .filewrapper import CallbackFileWrapper<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>"""<N>The cache object API for implementing caches. The default is a thread<N>safe in-memory dictionary.<N>"""<N>from threading import Lock<N><N><N>class BaseCache(object):<N><N>
    def get(self, key):<N>        raise NotImplementedError()<N><N>    def set(self, key, value, expires=None):<N>        raise NotImplementedError()<N><N>    def delete(self, key):<N>        raise NotImplementedError()<N><N>    def close(self):<N>        pass<N><N>
<N>class DictCache(BaseCache):<N><N>    def __init__(self, init_dict=None):<N>        self.lock = Lock()<N>        self.data = init_dict or {}<N><N>    def get(self, key):<N>        return self.data.get(key, None)<N><N>    def set(self, key, value, expires=None):<N>        with self.lock:<N>            self.data.update({key: value})<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>try:<N>    from urllib.parse import urljoin<N>except ImportError:<N>    from urlparse import urljoin<N><N><N>try:<N>    import cPickle as pickle<N>except ImportError:<N>    import pickle<N><N>
# Handle the case where the requests module has been patched to not have<N># urllib3 bundled as part of its source.<N>try:<N>    from pip._vendor.requests.packages.urllib3.response import HTTPResponse<N>except ImportError:<N>    from pip._vendor.urllib3.response import HTTPResponse<N><N>
try:<N>    from pip._vendor.requests.packages.urllib3.util import is_fp_closed<N>except ImportError:<N>    from pip._vendor.urllib3.util import is_fp_closed<N><N># Replicate some six behaviour<N>try:<N>    text_type = unicode<N>except NameError:<N>    text_type = str<N><N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>"""<N>The httplib2 algorithms ported for use with requests.<N>"""<N>import logging<N>import re<N>import calendar<N>import time<N>from email.utils import parsedate_tz<N><N>
from pip._vendor.requests.structures import CaseInsensitiveDict<N><N>from .cache import DictCache<N>from .serialize import Serializer<N><N><N>logger = logging.getLogger(__name__)<N><N>URI = re.compile(r"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\?([^#]*))?(#(.*))?")<N><N>
PERMANENT_REDIRECT_STATUSES = (301, 308)<N><N><N>def parse_uri(uri):<N>    """Parses a URI using the regex given in Appendix B of RFC 3986.<N><N>        (scheme, authority, path, query, fragment) = parse_uri(uri)<N>    """<N>    groups = URI.match(uri).groups()<N>    return (groups[1], groups[3], groups[4], groups[6], groups[8])<N><N>
<N>class CacheController(object):<N>    """An interface to see if request should cached or not.<N>    """<N><N>    def __init__(<N>        self, cache=None, cache_etags=True, serializer=None, status_codes=None<N>    ):<N>        self.cache = DictCache() if cache is None else cache<N>        self.cache_etags = cache_etags<N>        self.serializer = serializer or Serializer()<N>        self.cacheable_status_codes = status_codes or (200, 203, 300, 301, 308)<N><N>
    @classmethod<N>    def _urlnorm(cls, uri):<N>        """Normalize the URL to create a safe key for the cache"""<N>        (scheme, authority, path, query, fragment) = parse_uri(uri)<N>        if not scheme or not authority:<N>            raise Exception("Only absolute URIs are allowed. uri = %s" % uri)<N><N>
        scheme = scheme.lower()<N>        authority = authority.lower()<N><N>        if not path:<N>            path = "/"<N><N>        # Could do syntax based normalization of the URI before<N>        # computing the digest. See Section 6.2.2 of Std 66.<N>        request_uri = query and "?".join([path, query]) or path<N>        defrag_uri = scheme + "://" + authority + request_uri<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>from tempfile import NamedTemporaryFile<N>import mmap<N><N><N>class CallbackFileWrapper(object):<N>    """<N>    Small wrapper around a fp object which will tee everything read into a<N>    buffer, and when that file is closed it will execute a callback with the<N>    contents of that buffer.<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>import calendar<N>import time<N><N>from email.utils import formatdate, parsedate, parsedate_tz<N><N>from datetime import datetime, timedelta<N><N>TIME_FMT = "%a, %d %b %Y %H:%M:%S GMT"<N><N>
<N>def expire_after(delta, date=None):<N>    date = date or datetime.utcnow()<N>    return date + delta<N><N><N>def datetime_to_header(dt):<N>    return formatdate(calendar.timegm(dt.timetuple()))<N><N><N>class BaseHeuristic(object):<N><N>    def warning(self, response):<N>        """<N>        Return a valid 1xx warning header value describing the cache<N>        adjustments.<N><N>
        The response is provided too allow warnings like 113<N>        http://tools.ietf.org/html/rfc7234#section-5.5.4 where we need<N>        to explicitly say response is over 24 hours old.<N>        """<N>        return '110 - "Response is Stale"'<N><N>
    def update_headers(self, response):<N>        """Update the response headers with any new headers.<N><N>        NOTE: This SHOULD always include some Warning header to<N>              signify that the response was cached by the client, not<N>              by way of the provided headers.<N>        """<N>        return {}<N><N>
    def apply(self, response):<N>        updated_headers = self.update_headers(response)<N><N>        if updated_headers:<N>            response.headers.update(updated_headers)<N>            warning_header_value = self.warning(response)<N>            if warning_header_value is not None:<N>                response.headers.update({"Warning": warning_header_value})<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>import base64<N>import io<N>import json<N>import zlib<N><N>from pip._vendor import msgpack<N>from pip._vendor.requests.structures import CaseInsensitiveDict<N><N>
from .compat import HTTPResponse, pickle, text_type<N><N><N>def _b64_decode_bytes(b):<N>    return base64.b64decode(b.encode("ascii"))<N><N><N>def _b64_decode_str(s):<N>    return _b64_decode_bytes(s).decode("utf8")<N><N><N>_default_body_read = object()<N><N>
<N>class Serializer(object):<N>    def dumps(self, request, response, body=None):<N>        response_headers = CaseInsensitiveDict(response.headers)<N><N>        if body is None:<N>            # When a body isn't passed in, we'll read the response. We<N>            # also update the response with a new file handler to be<N>            # sure it acts as though it was never read.<N>            body = response.read(decode_content=False)<N>            response._fp = io.BytesIO(body)<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>from .adapter import CacheControlAdapter<N>from .cache import DictCache<N><N><N>def CacheControl(<N>    sess,<N>    cache=None,<N>    cache_etags=True,<N>    serializer=None,<N>    heuristic=None,<N>    controller_class=None,<N>    adapter_class=None,<N>    cacheable_methods=None,<N>):<N><N>
    cache = DictCache() if cache is None else cache<N>    adapter_class = adapter_class or CacheControlAdapter<N>    adapter = adapter_class(<N>        cache,<N>        cache_etags=cache_etags,<N>        serializer=serializer,<N>        heuristic=heuristic,<N>        controller_class=controller_class,<N>        cacheable_methods=cacheable_methods,<N>    )<N>    sess.mount("http://", adapter)<N>    sess.mount("https://", adapter)<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>import logging<N><N>from pip._vendor import requests<N><N>from pip._vendor.cachecontrol.adapter import CacheControlAdapter<N>from pip._vendor.cachecontrol.cache import DictCache<N>from pip._vendor.cachecontrol.controller import logger<N><N>
from argparse import ArgumentParser<N><N><N>def setup_logging():<N>    logger.setLevel(logging.DEBUG)<N>    handler = logging.StreamHandler()<N>    logger.addHandler(handler)<N><N><N>def get_session():<N>    adapter = CacheControlAdapter(<N>        DictCache(), cache_etags=True, serializer=None, heuristic=None<N>    )<N>    sess = requests.Session()<N>    sess.mount("http://", adapter)<N>    sess.mount("https://", adapter)<N><N>
    sess.cache_controller = adapter.controller<N>    return sess<N><N><N>def get_args():<N>    parser = ArgumentParser()<N>    parser.add_argument("url", help="The URL to try and cache")<N>    return parser.parse_args()<N><N><N>def main(args=None):<N>    args = get_args()<N>    sess = get_session()<N><N>
    # Make a request to get a response<N>    resp = sess.get(args.url)<N><N>    # Turn on logging<N>    setup_logging()<N><N>    # try setting the cache<N>    sess.cache_controller.cache_response(resp.request, resp.raw)<N><N>    # Now try to get it<N>    if sess.cache_controller.cached_request(resp.request):<N>        print("Cached!")<N>    else:<N>        print("Not cached :(")<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>"""CacheControl import Interface.<N><N>Make it easy to import from cachecontrol without long namespaces.<N>"""<N>__author__ = "Eric Larson"<N>__email__ = "eric@ionrock.org"<N>__version__ = "0.12.10"<N><N>from .wrapper import CacheControl<N>from .adapter import CacheControlAdapter<N>from .controller import CacheController<N><N>import logging<N>logging.getLogger(__name__).addHandler(logging.NullHandler())<N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>import hashlib<N>import os<N>from textwrap import dedent<N><N>from ..cache import BaseCache<N>from ..controller import CacheController<N><N>try:<N>    FileNotFoundError<N>except NameError:<N>    # py2.X<N>    FileNotFoundError = (IOError, OSError)<N><N>
<N>def _secure_open_write(filename, fmode):<N>    # We only want to write to this file, so open it in write only mode<N>    flags = os.O_WRONLY<N><N>    # os.O_CREAT | os.O_EXCL will fail if the file already exists, so we only<N>    #  will open *new* files.<N>    # We specify this because we want to ensure that the mode we pass is the<N>    # mode of the file.<N>    flags |= os.O_CREAT | os.O_EXCL<N><N>
    # Do not follow symlinks to prevent someone from making a symlink that<N>    # we follow and insecurely open a cache file.<N>    if hasattr(os, "O_NOFOLLOW"):<N>        flags |= os.O_NOFOLLOW<N><N>    # On Windows we'll mark this file as binary<N>    if hasattr(os, "O_BINARY"):<N>        flags |= os.O_BINARY<N><N>
    # Before we open our file, we want to delete any existing file that is<N>    # there<N>    try:<N>        os.remove(filename)<N>    except (IOError, OSError):<N>        # The file must not exist already, so we can just skip ahead to opening<N>        pass<N><N>
    # Open our file, the use of os.O_CREAT | os.O_EXCL will ensure that if a<N>    # race condition happens between the os.remove and this line, that an<N>    # error will be raised. Because we utilize a lockfile this should only<N>    # happen if someone is attempting to attack us.<N>    fd = os.open(filename, flags, fmode)<N>    try:<N>        return os.fdopen(fd, "wb")<N><N>
    except:<N>        # An error occurred wrapping our FD in a file object<N>        os.close(fd)<N>        raise<N><N><N>class FileCache(BaseCache):<N><N>    def __init__(<N>        self,<N>        directory,<N>        forever=False,<N>        filemode=0o0600,<N>        dirmode=0o0700,<N>        use_dir_lock=None,<N>        lock_class=None,<N>    ):<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>from __future__ import division<N><N>from datetime import datetime<N>from pip._vendor.cachecontrol.cache import BaseCache<N><N><N>class RedisCache(BaseCache):<N><N>
    def __init__(self, conn):<N>        self.conn = conn<N><N>    def get(self, key):<N>        return self.conn.get(key)<N><N>    def set(self, key, value, expires=None):<N>        if not expires:<N>            self.conn.set(key, value)<N>        else:<N>            expires = expires - datetime.utcnow()<N>            self.conn.setex(key, int(expires.total_seconds()), value)<N><N>
    def delete(self, key):<N>        self.conn.delete(key)<N><N>    def clear(self):<N>        """Helper for clearing all the keys in a database. Use with<N>        caution!"""<N>        for key in self.conn.keys():<N>            self.conn.delete(key)<N><N>
# SPDX-FileCopyrightText: 2015 Eric Larson<N>#<N># SPDX-License-Identifier: Apache-2.0<N><N>from .file_cache import FileCache  # noqa<N>from .redis_cache import RedisCache  # noqa<N>
import argparse<N><N>from pip._vendor.certifi import contents, where<N><N>parser = argparse.ArgumentParser()<N>parser.add_argument("-c", "--contents", action="store_true")<N>args = parser.parse_args()<N><N>if args.contents:<N>    print(contents())<N>else:<N>    print(where())<N>
"""<N>All of the Enums that are used throughout the chardet package.<N><N>:author: Dan Blanchard (dan.blanchard@gmail.com)<N>"""<N><N><N>class InputState(object):<N>    """<N>    This enum represents the different states a universal detector can be in.<N>    """<N>    PURE_ASCII = 0<N>    ESC_ASCII = 1<N>    HIGH_BYTE = 2<N><N>
<N>class LanguageFilter(object):<N>    """<N>    This enum represents the different language filters we can apply to a<N>    ``UniversalDetector``.<N>    """<N>    CHINESE_SIMPLIFIED = 0x01<N>    CHINESE_TRADITIONAL = 0x02<N>    JAPANESE = 0x04<N>    KOREAN = 0x08<N>    NON_CJK = 0x10<N>    ALL = 0x1F<N>    CHINESE = CHINESE_SIMPLIFIED | CHINESE_TRADITIONAL<N>    CJK = CHINESE | JAPANESE | KOREAN<N><N>
<N>class ProbingState(object):<N>    """<N>    This enum represents the different states a prober can be in.<N>    """<N>    DETECTING = 0<N>    FOUND_IT = 1<N>    NOT_ME = 2<N><N><N>class MachineState(object):<N>    """<N>    This enum represents the different states a state machine can be in.<N>    """<N>    START = 0<N>    ERROR = 1<N>    ITS_ME = 2<N><N>
<N>class SequenceLikelihood(object):<N>    """<N>    This enum represents the likelihood of a character following the previous one.<N>    """<N>    NEGATIVE = 0<N>    UNLIKELY = 1<N>    LIKELY = 2<N>    POSITIVE = 3<N><N>    @classmethod<N>    def get_num_categories(cls):<N>        """:returns: The number of likelihood categories in the enum."""<N>        return 4<N><N>
<N>class CharacterCategory(object):<N>    """<N>    This enum represents the different categories language models for<N>    ``SingleByteCharsetProber`` put characters into.<N><N>    Anything less than CONTROL is considered a letter.<N>    """<N>    UNDEFINED = 255<N>    LINE_BREAK = 254<N>    SYMBOL = 253<N>    DIGIT = 252<N>    CONTROL = 251<N><N><N>
"""<N>This module exists only to simplify retrieving the version number of chardet<N>from within setup.py and from chardet subpackages.<N><N>:author: Dan Blanchard (dan.blanchard@gmail.com)<N>"""<N><N>__version__ = "4.0.0"<N>VERSION = __version__.split('.')<N>
"""<N>Script which takes one or more file paths and reports on their detected<N>encodings<N><N>Example::<N><N>    % chardetect somefile someotherfile<N>    somefile: windows-1252 with confidence 0.5<N>    someotherfile: ascii with confidence 1.0<N><N>
If no paths are provided, it takes its input from stdin.<N><N>"""<N><N>from __future__ import absolute_import, print_function, unicode_literals<N><N>import argparse<N>import sys<N><N>from pip._vendor.chardet import __version__<N>from pip._vendor.chardet.compat import PY2<N>from pip._vendor.chardet.universaldetector import UniversalDetector<N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N>'''<N>This module generates ANSI character codes to printing colors to terminals.<N>See: http://en.wikipedia.org/wiki/ANSI_escape_code<N>'''<N><N>CSI = '\033['<N>OSC = '\033]'<N>BEL = '\a'<N><N>
<N>def code_to_chars(code):<N>    return CSI + str(code) + 'm'<N><N>def set_title(title):<N>    return OSC + '2;' + title + BEL<N><N>def clear_screen(mode=2):<N>    return CSI + str(mode) + 'J'<N><N>def clear_line(mode=2):<N>    return CSI + str(mode) + 'K'<N><N>
<N>class AnsiCodes(object):<N>    def __init__(self):<N>        # the subclasses declare class attributes which are numbers.<N>        # Upon instantiation we define instance attributes, which are the same<N>        # as the class attributes but wrapped with the ANSI escape sequence<N>        for name in dir(self):<N>            if not name.startswith('_'):<N>                value = getattr(self, name)<N>                setattr(self, name, code_to_chars(value))<N><N>
<N>class AnsiCursor(object):<N>    def UP(self, n=1):<N>        return CSI + str(n) + 'A'<N>    def DOWN(self, n=1):<N>        return CSI + str(n) + 'B'<N>    def FORWARD(self, n=1):<N>        return CSI + str(n) + 'C'<N>    def BACK(self, n=1):<N>        return CSI + str(n) + 'D'<N>    def POS(self, x=1, y=1):<N>        return CSI + str(y) + ';' + str(x) + 'H'<N><N>
<N>class AnsiFore(AnsiCodes):<N>    BLACK           = 30<N>    RED             = 31<N>    GREEN           = 32<N>    YELLOW          = 33<N>    BLUE            = 34<N>    MAGENTA         = 35<N>    CYAN            = 36<N>    WHITE           = 37<N>    RESET           = 39<N><N>
    # These are fairly well supported, but not part of the standard.<N>    LIGHTBLACK_EX   = 90<N>    LIGHTRED_EX     = 91<N>    LIGHTGREEN_EX   = 92<N>    LIGHTYELLOW_EX  = 93<N>    LIGHTBLUE_EX    = 94<N>    LIGHTMAGENTA_EX = 95<N>    LIGHTCYAN_EX    = 96<N>    LIGHTWHITE_EX   = 97<N><N>
<N>class AnsiBack(AnsiCodes):<N>    BLACK           = 40<N>    RED             = 41<N>    GREEN           = 42<N>    YELLOW          = 43<N>    BLUE            = 44<N>    MAGENTA         = 45<N>    CYAN            = 46<N>    WHITE           = 47<N>    RESET           = 49<N><N>
    # These are fairly well supported, but not part of the standard.<N>    LIGHTBLACK_EX   = 100<N>    LIGHTRED_EX     = 101<N>    LIGHTGREEN_EX   = 102<N>    LIGHTYELLOW_EX  = 103<N>    LIGHTBLUE_EX    = 104<N>    LIGHTMAGENTA_EX = 105<N>    LIGHTCYAN_EX    = 106<N>    LIGHTWHITE_EX   = 107<N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N>import re<N>import sys<N>import os<N><N>from .ansi import AnsiFore, AnsiBack, AnsiStyle, Style, BEL<N>from .winterm import WinTerm, WinColor, WinStyle<N>from .win32 import windll, winapi_test<N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N>import atexit<N>import contextlib<N>import sys<N><N>from .ansitowin32 import AnsiToWin32<N><N><N>orig_stdout = None<N>orig_stderr = None<N><N>wrapped_stdout = None<N>wrapped_stderr = None<N><N>
atexit_done = False<N><N><N>def reset_all():<N>    if AnsiToWin32 is not None:    # Issue #74: objects might become None at exit<N>        AnsiToWin32(orig_stdout).reset_all()<N><N><N>def init(autoreset=False, convert=None, strip=None, wrap=True):<N><N>
    if not wrap and any([autoreset, convert, strip]):<N>        raise ValueError('wrap=False conflicts with any other arg=True')<N><N>    global wrapped_stdout, wrapped_stderr<N>    global orig_stdout, orig_stderr<N><N>    orig_stdout = sys.stdout<N>    orig_stderr = sys.stderr<N><N>
    if sys.stdout is None:<N>        wrapped_stdout = None<N>    else:<N>        sys.stdout = wrapped_stdout = \<N>            wrap_stream(orig_stdout, convert, strip, autoreset, wrap)<N>    if sys.stderr is None:<N>        wrapped_stderr = None<N>    else:<N>        sys.stderr = wrapped_stderr = \<N>            wrap_stream(orig_stderr, convert, strip, autoreset, wrap)<N><N>
    global atexit_done<N>    if not atexit_done:<N>        atexit.register(reset_all)<N>        atexit_done = True<N><N><N>def deinit():<N>    if orig_stdout is not None:<N>        sys.stdout = orig_stdout<N>    if orig_stderr is not None:<N>        sys.stderr = orig_stderr<N><N>
<N>@contextlib.contextmanager<N>def colorama_text(*args, **kwargs):<N>    init(*args, **kwargs)<N>    try:<N>        yield<N>    finally:<N>        deinit()<N><N><N>def reinit():<N>    if wrapped_stdout is not None:<N>        sys.stdout = wrapped_stdout<N>    if wrapped_stderr is not None:<N>        sys.stderr = wrapped_stderr<N><N>
<N>def wrap_stream(stream, convert, strip, autoreset, wrap):<N>    if wrap:<N>        wrapper = AnsiToWin32(stream,<N>            convert=convert, strip=strip, autoreset=autoreset)<N>        if wrapper.should_wrap():<N>            stream = wrapper.stream<N>    return stream<N><N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N><N># from winbase.h<N>STDOUT = -11<N>STDERR = -12<N><N>try:<N>    import ctypes<N>    from ctypes import LibraryLoader<N>    windll = LibraryLoader(ctypes.WinDLL)<N>    from ctypes import wintypes<N>except (AttributeError, ImportError):<N>    windll = None<N>    SetConsoleTextAttribute = lambda *_: None<N>    winapi_test = lambda *_: None<N>else:<N>    from ctypes import byref, Structure, c_char, POINTER<N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N>from . import win32<N><N><N># from wincon.h<N>class WinColor(object):<N>    BLACK   = 0<N>    BLUE    = 1<N>    GREEN   = 2<N>    CYAN    = 3<N>    RED     = 4<N>    MAGENTA = 5<N>    YELLOW  = 6<N>    GREY    = 7<N><N>
# from wincon.h<N>class WinStyle(object):<N>    NORMAL              = 0x00 # dim text, dim background<N>    BRIGHT              = 0x08 # bright text, dim background<N>    BRIGHT_BACKGROUND   = 0x80 # dim text, bright background<N><N>class WinTerm(object):<N><N>
# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.<N>from .initialise import init, deinit, reinit, colorama_text<N>from .ansi import Fore, Back, Style, Cursor<N>from .ansitowin32 import AnsiToWin32<N><N>__version__ = '0.4.4'<N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2013-2017 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>from __future__ import absolute_import<N><N>import os<N>import re<N>import sys<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2017 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""PEP 376 implementation."""<N><N>from __future__ import unicode_literals<N><N>import base64<N>import codecs<N>import contextlib<N>import hashlib<N>import logging<N>import os<N>import posixpath<N>import sys<N>import zipimport<N><N>
from . import DistlibException, resources<N>from .compat import StringIO<N>from .version import get_scheme, UnsupportedVersionError<N>from .metadata import (Metadata, METADATA_FILENAME, WHEEL_METADATA_FILENAME,<N>                       LEGACY_METADATA_FILENAME)<N>from .util import (parse_requirement, cached_property, parse_name_and_version,<N>                   read_exports, write_exports, CSVReader, CSVWriter)<N><N>
<N>__all__ = ['Distribution', 'BaseInstalledDistribution',<N>           'InstalledDistribution', 'EggInfoDistribution',<N>           'DistributionPath']<N><N><N>logger = logging.getLogger(__name__)<N><N>EXPORTS_FILENAME = 'pydist-exports.json'<N>COMMANDS_FILENAME = 'pydist-commands.json'<N><N>
DIST_FILES = ('INSTALLER', METADATA_FILENAME, 'RECORD', 'REQUESTED',<N>              'RESOURCES', EXPORTS_FILENAME, 'SHARED')<N><N>DISTINFO_EXT = '.dist-info'<N><N><N>class _Cache(object):<N>    """<N>    A simple cache mapping names and .dist-info paths to distributions<N>    """<N>    def __init__(self):<N>        """<N>        Initialise an instance. There is normally one for each DistributionPath.<N>        """<N>        self.name = {}<N>        self.path = {}<N>        self.generated = False<N><N>
    def clear(self):<N>        """<N>        Clear the cache, setting it to its initial state.<N>        """<N>        self.name.clear()<N>        self.path.clear()<N>        self.generated = False<N><N>    def add(self, dist):<N>        """<N>        Add a distribution to the cache.<N>        :param dist: The distribution to add.<N>        """<N>        if dist.path not in self.path:<N>            self.path[dist.path] = dist<N>            self.name.setdefault(dist.key, []).append(dist)<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2013 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>import hashlib<N>import logging<N>import os<N>import shutil<N>import subprocess<N>import tempfile<N>try:<N>    from threading import Thread<N>except ImportError:<N>    from dummy_threading import Thread<N><N>
from . import DistlibException<N>from .compat import (HTTPBasicAuthHandler, Request, HTTPPasswordMgr,<N>                     urlparse, build_opener, string_types)<N>from .util import zip_dir, ServerProxy<N><N>logger = logging.getLogger(__name__)<N><N>
DEFAULT_INDEX = 'https://pypi.org/pypi'<N>DEFAULT_REALM = 'pypi'<N><N>class PackageIndex(object):<N>    """<N>    This class represents a package index compatible with PyPI, the Python<N>    Package Index.<N>    """<N><N>    boundary = b'----------ThIs_Is_tHe_distlib_index_bouNdaRY_$'<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2015 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N><N>import gzip<N>from io import BytesIO<N>import json<N>import logging<N>import os<N>import posixpath<N>import re<N>try:<N>    import threading<N>except ImportError:  # pragma: no cover<N>    import dummy_threading as threading<N>import zlib<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2013 Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""<N>Class representing the list of files in a distribution.<N><N>Equivalent to distutils.filelist, but fixes some problems.<N>"""<N>import fnmatch<N>import logging<N>import os<N>import re<N>import sys<N><N>
from . import DistlibException<N>from .compat import fsdecode<N>from .util import convert_path<N><N><N>__all__ = ['Manifest']<N><N>logger = logging.getLogger(__name__)<N><N># a \ followed by some spaces + EOL<N>_COLLAPSE_PATTERN = re.compile('\\\\w*\n', re.M)<N>_COMMENTED_LINE = re.compile('#.*?(?=\n)|\n(?=$)', re.M | re.S)<N><N>
#<N># Due to the different results returned by fnmatch.translate, we need<N># to do slightly different processing for Python 2.7 and 3.2 ... this needed<N># to be brought in for Python 3.6 onwards.<N>#<N>_PYTHON_VERSION = sys.version_info[:2]<N><N>class Manifest(object):<N>    """A list of files built by on exploring the filesystem and filtered by<N>    applying various patterns to what we find there.<N>    """<N><N>
    def __init__(self, base=None):<N>        """<N>        Initialise an instance.<N><N>        :param base: The base directory to explore under.<N>        """<N>        self.base = os.path.abspath(os.path.normpath(base or os.getcwd()))<N>        self.prefix = self.base + os.sep<N>        self.allfiles = None<N>        self.files = set()<N><N>
    #<N>    # Public API<N>    #<N><N>    def findall(self):<N>        """Find all files under the base and set ``allfiles`` to the absolute<N>        pathnames of files found.<N>        """<N>        from stat import S_ISREG, S_ISDIR, S_ISLNK<N><N>        self.allfiles = allfiles = []<N>        root = self.base<N>        stack = [root]<N>        pop = stack.pop<N>        push = stack.append<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2017 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""<N>Parser for the environment markers micro-language defined in PEP 508.<N>"""<N><N>
# Note: In PEP 345, the micro-language was Python compatible, so the ast<N># module could be used to parse it. However, PEP 508 introduced operators such<N># as ~= and === which aren't in Python, necessitating a different approach.<N><N>import os<N>import re<N>import sys<N>import platform<N><N>
from .compat import string_types<N>from .util import in_venv, parse_marker<N>from .version import NormalizedVersion as NV<N><N>__all__ = ['interpret']<N><N>_VERSION_PATTERN = re.compile(r'((\d+(\.\d+)*\w*)|\'(\d+(\.\d+)*\w*)\'|\"(\d+(\.\d+)*\w*)\")')<N><N>
def _is_literal(o):<N>    if not isinstance(o, string_types) or not o:<N>        return False<N>    return o[0] in '\'"'<N><N>def _get_versions(s):<N>    result = []<N>    for m in _VERSION_PATTERN.finditer(s):<N>        result.append(NV(m.groups()[0]))<N>    return set(result)<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""Implementation of the Metadata for Python packages PEPs.<N><N>Supports all metadata formats (1.0, 1.1, 1.2, 1.3/2.1 and withdrawn 2.0).<N>"""<N>from __future__ import unicode_literals<N><N>
import codecs<N>from email import message_from_file<N>import json<N>import logging<N>import re<N><N><N>from . import DistlibException, __version__<N>from .compat import StringIO, string_types, text_type<N>from .markers import interpret<N>from .util import extract_by_key, get_extras<N>from .version import get_scheme, PEP440_VERSION_RE<N><N>
logger = logging.getLogger(__name__)<N><N><N>class MetadataMissingError(DistlibException):<N>    """A required metadata is missing"""<N><N><N>class MetadataConflictError(DistlibException):<N>    """Attempt to read or write metadata fields that are conflictual."""<N><N>
<N>class MetadataUnrecognizedVersionError(DistlibException):<N>    """Unknown metadata version number."""<N><N><N>class MetadataInvalidError(DistlibException):<N>    """A metadata value is invalid"""<N><N># public API of this module<N>__all__ = ['Metadata', 'PKG_INFO_ENCODING', 'PKG_INFO_PREFERRED_VERSION']<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2013-2017 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>from __future__ import unicode_literals<N><N>import bisect<N>import io<N>import logging<N>import os<N>import pkgutil<N>import sys<N>import types<N>import zipimport<N><N>
from . import DistlibException<N>from .util import cached_property, get_cache_base, Cache<N><N>logger = logging.getLogger(__name__)<N><N><N>cache = None    # created when needed<N><N><N>class ResourceCache(Cache):<N>    def __init__(self, base=None):<N>        if base is None:<N>            # Use native string to avoid issues on 2.x: see Python #20140.<N>            base = os.path.join(get_cache_base(), str('resource-cache'))<N>        super(ResourceCache, self).__init__(base)<N><N>
    def is_stale(self, resource, path):<N>        """<N>        Is the cache stale for the given resource?<N><N>        :param resource: The :class:`Resource` being cached.<N>        :param path: The path of the resource in the cache.<N>        :return: True if the cache is stale.<N>        """<N>        # Cache invalidation is a hard problem :-)<N>        return True<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2013-2015 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>from io import BytesIO<N>import logging<N>import os<N>import re<N>import struct<N>import sys<N><N>
from .compat import sysconfig, detect_encoding, ZipFile<N>from .resources import finder<N>from .util import (FileOperator, get_export_entry, convert_path,<N>                   get_executable, get_platform, in_venv)<N><N>logger = logging.getLogger(__name__)<N><N>
_DEFAULT_MANIFEST = '''<N><?xml version="1.0" encoding="UTF-8" standalone="yes"?><N><assembly xmlns="urn:schemas-microsoft-com:asm.v1" manifestVersion="1.0"><N> <assemblyIdentity version="1.0.0.0"<N> processorArchitecture="X86"<N> name="%s"<N> type="win32"/><N><N>
 <!-- Identify the application security requirements. --><N> <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3"><N> <security><N> <requestedPrivileges><N> <requestedExecutionLevel level="asInvoker" uiAccess="false"/><N> </requestedPrivileges><N> </security><N> </trustInfo><N></assembly>'''.strip()<N><N>
# check if Python is called on the first line with this expression<N>FIRST_LINE_RE = re.compile(b'^#!.*pythonw?[0-9.]*([ \t].*)?$')<N>SCRIPT_TEMPLATE = r'''# -*- coding: utf-8 -*-<N>import re<N>import sys<N>from %(module)s import %(import_name)s<N>if __name__ == '__main__':<N>    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])<N>    sys.exit(%(func)s())<N>'''<N><N>
#<N># Copyright (C) 2012-2021 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>import codecs<N>from collections import deque<N>import contextlib<N>import csv<N>from glob import iglob as std_iglob<N>import io<N>import json<N>import logging<N>import os<N>import py_compile<N>import re<N>import socket<N>try:<N>    import ssl<N>except ImportError:  # pragma: no cover<N>    ssl = None<N>import subprocess<N>import sys<N>import tarfile<N>import tempfile<N>import textwrap<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2017 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""<N>Implementation of a flexible versioning scheme providing support for PEP-440,<N>setuptools-compatible and semantic versioning.<N>"""<N><N>
import logging<N>import re<N><N>from .compat import string_types<N>from .util import parse_requirement<N><N>__all__ = ['NormalizedVersion', 'NormalizedMatcher',<N>           'LegacyVersion', 'LegacyMatcher',<N>           'SemanticVersion', 'SemanticMatcher',<N>           'UnsupportedVersionError', 'get_scheme']<N><N>
logger = logging.getLogger(__name__)<N><N><N>class UnsupportedVersionError(ValueError):<N>    """This is an unsupported version."""<N>    pass<N><N><N>class Version(object):<N>    def __init__(self, s):<N>        self._string = s = s.strip()<N>        self._parts = parts = self.parse(s)<N>        assert isinstance(parts, tuple)<N>        assert len(parts) > 0<N><N>
    def parse(self, s):<N>        raise NotImplementedError('please implement in a subclass')<N><N>    def _check_compatible(self, other):<N>        if type(self) != type(other):<N>            raise TypeError('cannot compare %r and %r' % (self, other))<N><N>
    def __eq__(self, other):<N>        self._check_compatible(other)<N>        return self._parts == other._parts<N><N>    def __ne__(self, other):<N>        return not self.__eq__(other)<N><N>    def __lt__(self, other):<N>        self._check_compatible(other)<N>        return self._parts < other._parts<N><N>
    def __gt__(self, other):<N>        return not (self.__lt__(other) or self.__eq__(other))<N><N>    def __le__(self, other):<N>        return self.__lt__(other) or self.__eq__(other)<N><N>    def __ge__(self, other):<N>        return self.__gt__(other) or self.__eq__(other)<N><N>
    # See http://docs.python.org/reference/datamodel#object.__hash__<N>    def __hash__(self):<N>        return hash(self._parts)<N><N>    def __repr__(self):<N>        return "%s('%s')" % (self.__class__.__name__, self._string)<N><N>    def __str__(self):<N>        return self._string<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2013-2020 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>from __future__ import unicode_literals<N><N>import base64<N>import codecs<N>import datetime<N>from email import message_from_file<N>import hashlib<N>import imp<N>import json<N>import logging<N>import os<N>import posixpath<N>import re<N>import shutil<N>import sys<N>import tempfile<N>import zipfile<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012-2019 Vinay Sajip.<N># Licensed to the Python Software Foundation under a contributor agreement.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>import logging<N><N>__version__ = '0.3.3'<N><N>class DistlibException(Exception):<N>    pass<N><N>
try:<N>    from logging import NullHandler<N>except ImportError: # pragma: no cover<N>    class NullHandler(logging.Handler):<N>        def handle(self, record): pass<N>        def emit(self, record): pass<N>        def createLock(self): self.lock = None<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""Backports for individual classes and functions."""<N><N>import os<N>import sys<N><N>__all__ = ['cache_from_source', 'callable', 'fsencode']<N><N>
<N>try:<N>    from imp import cache_from_source<N>except ImportError:<N>    def cache_from_source(py_file, debug=__debug__):<N>        ext = debug and 'c' or 'o'<N>        return py_file + ext<N><N><N>try:<N>    callable = callable<N>except NameError:<N>    from collections import Callable<N><N>
    def callable(obj):<N>        return isinstance(obj, Callable)<N><N><N>try:<N>    fsencode = os.fsencode<N>except AttributeError:<N>    def fsencode(filename):<N>        if isinstance(filename, bytes):<N>            return filename<N>        elif isinstance(filename, str):<N>            return filename.encode(sys.getfilesystemencoding())<N>        else:<N>            raise TypeError("expect bytes or str, not %s" %<N>                            type(filename).__name__)<N><N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""Utility functions for copying and archiving files and directory trees.<N><N>XXX The functions here don't copy the resource fork or other metadata on Mac.<N><N>
"""<N><N>import os<N>import sys<N>import stat<N>from os.path import abspath<N>import fnmatch<N>try:<N>    from collections.abc import Callable<N>except ImportError:<N>    from collections import Callable<N>import errno<N>from . import tarfile<N><N>try:<N>    import bz2<N>    _BZ2_SUPPORTED = True<N>except ImportError:<N>    _BZ2_SUPPORTED = False<N><N>
# -*- coding: utf-8 -*-<N>#<N># Copyright (C) 2012 The Python Software Foundation.<N># See LICENSE.txt and CONTRIBUTORS.txt.<N>#<N>"""Access to Python's configuration information."""<N><N>import codecs<N>import os<N>import re<N>import sys<N>from os.path import pardir, realpath<N>try:<N>    import configparser<N>except ImportError:<N>    import ConfigParser as configparser<N><N>
<N>__all__ = [<N>    'get_config_h_filename',<N>    'get_config_var',<N>    'get_config_vars',<N>    'get_makefile_filename',<N>    'get_path',<N>    'get_path_names',<N>    'get_paths',<N>    'get_platform',<N>    'get_python_version',<N>    'get_scheme_names',<N>    'parse_config_h',<N>]<N><N>
<N>def _safe_realpath(path):<N>    try:<N>        return realpath(path)<N>    except OSError:<N>        return path<N><N><N>if sys.executable:<N>    _PROJECT_BASE = os.path.dirname(_safe_realpath(sys.executable))<N>else:<N>    # sys.executable can be empty if argv[0] has been changed and Python is<N>    # unable to retrieve the real program name<N>    _PROJECT_BASE = _safe_realpath(os.getcwd())<N><N>
if os.name == "nt" and "pcbuild" in _PROJECT_BASE[-8:].lower():<N>    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir))<N># PC/VS7.1<N>if os.name == "nt" and "\\pc\\v" in _PROJECT_BASE[-10:].lower():<N>    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir, pardir))<N># PC/AMD64<N>if os.name == "nt" and "\\pcbuild\\amd64" in _PROJECT_BASE[-14:].lower():<N>    _PROJECT_BASE = _safe_realpath(os.path.join(_PROJECT_BASE, pardir, pardir))<N><N>
<N>def is_python_build():<N>    for fn in ("Setup.dist", "Setup.local"):<N>        if os.path.isfile(os.path.join(_PROJECT_BASE, "Modules", fn)):<N>            return True<N>    return False<N><N>_PYTHON_BUILD = is_python_build()<N><N>_cfg_read = False<N><N>
"""Modules copied from Python 3 standard libraries, for internal use only.<N><N>Individual classes and functions are found in d2._backport.misc.  Intended<N>usage is to always import things missing from 3.1 from that module: the<N>built-in/stdlib objects will be used if found.<N>"""<N>
from __future__ import absolute_import, division, unicode_literals<N>from pip._vendor.six import with_metaclass, viewkeys<N><N>import types<N><N>from . import _inputstream<N>from . import _tokenizer<N><N>from . import treebuilders<N>from .treebuilders.base import Marker<N><N>
from . import _utils<N>from .constants import (<N>    spaceCharacters, asciiUpper2Lower,<N>    specialElements, headingElements, cdataElements, rcdataElements,<N>    tokenTypes, tagTokenTypes,<N>    namespaces,<N>    htmlIntegrationPointElements, mathmlTextIntegrationPointElements,<N>    adjustForeignAttributes as adjustForeignAttributesMap,<N>    adjustMathMLAttributes, adjustSVGAttributes,<N>    E,<N>    _ReparseException<N>)<N><N>
<N>def parse(doc, treebuilder="etree", namespaceHTMLElements=True, **kwargs):<N>    """Parse an HTML document as a string or file-like object into a tree<N><N>    :arg doc: the document to parse as a string or file-like object<N><N>    :arg treebuilder: the treebuilder to use when parsing<N><N>
    :arg namespaceHTMLElements: whether or not to namespace HTML elements<N><N>    :returns: parsed tree<N><N>    Example:<N><N>    >>> from html5lib.html5parser import parse<N>    >>> parse('<html><body><p>This is a doc</p></body></html>')<N>    <Element u'{http://www.w3.org/1999/xhtml}html' at 0x7feac4909db0><N><N>
    """<N>    tb = treebuilders.getTreeBuilder(treebuilder)<N>    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)<N>    return p.parse(doc, **kwargs)<N><N><N>def parseFragment(doc, container="div", treebuilder="etree", namespaceHTMLElements=True, **kwargs):<N>    """Parse an HTML fragment as a string or file-like object into a tree<N><N>
    :arg doc: the fragment to parse as a string or file-like object<N><N>    :arg container: the container context to parse the fragment in<N><N>    :arg treebuilder: the treebuilder to use when parsing<N><N>    :arg namespaceHTMLElements: whether or not to namespace HTML elements<N><N>
    :returns: parsed tree<N><N>    Example:<N><N>    >>> from html5lib.html5libparser import parseFragment<N>    >>> parseFragment('<b>this is a fragment</b>')<N>    <Element u'DOCUMENT_FRAGMENT' at 0x7feac484b090><N><N>    """<N>    tb = treebuilders.getTreeBuilder(treebuilder)<N>    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)<N>    return p.parseFragment(doc, container=container, **kwargs)<N><N>
<N>def method_decorator_metaclass(function):<N>    class Decorated(type):<N>        def __new__(meta, classname, bases, classDict):<N>            for attributeName, attribute in classDict.items():<N>                if isinstance(attribute, types.FunctionType):<N>                    attribute = function(attribute)<N><N>
                classDict[attributeName] = attribute<N>            return type.__new__(meta, classname, bases, classDict)<N>    return Decorated<N><N><N>class HTMLParser(object):<N>    """HTML parser<N><N>    Generates a tree structure from a stream of (possibly malformed) HTML.<N><N>
    """<N><N>    def __init__(self, tree=None, strict=False, namespaceHTMLElements=True, debug=False):<N>        """<N>        :arg tree: a treebuilder class controlling the type of tree that will be<N>            returned. Built in treebuilders can be accessed through<N>            html5lib.treebuilders.getTreeBuilder(treeType)<N><N>
        :arg strict: raise an exception when a parse error is encountered<N><N>        :arg namespaceHTMLElements: whether or not to namespace HTML elements<N><N>        :arg debug: whether or not to enable debug mode which logs things<N><N>        Example:<N><N>
        >>> from html5lib.html5parser import HTMLParser<N>        >>> parser = HTMLParser()                     # generates parser with etree builder<N>        >>> parser = HTMLParser('lxml', strict=True)  # generates parser with lxml builder which is strict<N><N>
        """<N><N>        # Raise an exception on the first error encountered<N>        self.strict = strict<N><N>        if tree is None:<N>            tree = treebuilders.getTreeBuilder("etree")<N>        self.tree = tree(namespaceHTMLElements)<N>        self.errors = []<N><N>
        self.phases = {name: cls(self, self.tree) for name, cls in<N>                       getPhases(debug).items()}<N><N>    def _parse(self, stream, innerHTML=False, container="div", scripting=False, **kwargs):<N><N>        self.innerHTMLMode = innerHTML<N>        self.container = container<N>        self.scripting = scripting<N>        self.tokenizer = _tokenizer.HTMLTokenizer(stream, parser=self, **kwargs)<N>        self.reset()<N><N>
        try:<N>            self.mainLoop()<N>        except _ReparseException:<N>            self.reset()<N>            self.mainLoop()<N><N>    def reset(self):<N>        self.tree.reset()<N>        self.firstStartTag = False<N>        self.errors = []<N>        self.log = []  # only used with debug mode<N>        # "quirks" / "limited quirks" / "no quirks"<N>        self.compatMode = "no quirks"<N><N>
from __future__ import absolute_import, division, unicode_literals<N>from pip._vendor.six import text_type<N><N>import re<N><N>from codecs import register_error, xmlcharrefreplace_errors<N><N>from .constants import voidElements, booleanAttributes, spaceCharacters<N>from .constants import rcdataElements, entities, xmlEntities<N>from . import treewalkers, _utils<N>from xml.sax.saxutils import escape<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from pip._vendor.six import text_type<N>from pip._vendor.six.moves import http_client, urllib<N><N>import codecs<N>import re<N>from io import BytesIO, StringIO<N><N>from pip._vendor import webencodings<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from pip._vendor.six import unichr as chr<N><N>from collections import deque, OrderedDict<N>from sys import version_info<N><N>from .constants import spaceCharacters<N>from .constants import entities<N>from .constants import asciiLetters, asciiUpper2Lower<N>from .constants import digits, hexDigits, EOF<N>from .constants import tokenTypes, tagTokenTypes<N>from .constants import replacementCharacters<N><N>
from ._inputstream import HTMLInputStream<N><N>from ._trie import Trie<N><N>entitiesTrie = Trie(entities)<N><N>if version_info >= (3, 7):<N>    attributeMap = dict<N>else:<N>    attributeMap = OrderedDict<N><N><N>class HTMLTokenizer(object):<N>    """ This class takes care of tokenizing HTML.<N><N>
    * self.currentToken<N>      Holds the token that is currently being processed.<N><N>    * self.state<N>      Holds a reference to the method to be invoked... XXX<N><N>    * self.stream<N>      Points to HTMLInputStream object.<N>    """<N><N>    def __init__(self, stream, parser=None, **kwargs):<N><N>
        self.stream = HTMLInputStream(stream, **kwargs)<N>        self.parser = parser<N><N>        # Setup the initial tokenizer state<N>        self.escapeFlag = False<N>        self.lastFourChars = []<N>        self.state = self.dataState<N>        self.escape = False<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from types import ModuleType<N><N>try:<N>    from collections.abc import Mapping<N>except ImportError:<N>    from collections import Mapping<N><N>from pip._vendor.six import text_type, PY3<N><N>
if PY3:<N>    import xml.etree.ElementTree as default_etree<N>else:<N>    try:<N>        import xml.etree.cElementTree as default_etree<N>    except ImportError:<N>        import xml.etree.ElementTree as default_etree<N><N><N>__all__ = ["default_etree", "MethodDispatcher", "isSurrogatePair",<N>           "surrogatePairToCodepoint", "moduleFactoryFactory",<N>           "supports_lone_surrogates"]<N><N>
"""<N>HTML parsing library based on the `WHATWG HTML specification<N><https://whatwg.org/html>`_. The parser is designed to be compatible with<N>existing HTML found in the wild and implements well-defined error recovery that<N>is largely compatible with modern desktop web browsers.<N><N>
Example usage::<N><N>    from pip._vendor import html5lib<N>    with open("my_document.html", "rb") as f:<N>        tree = html5lib.parse(f)<N><N>For convenience, this module re-exports the following names:<N><N>* :func:`~.html5parser.parse`<N>* :func:`~.html5parser.parseFragment`<N>* :class:`~.html5parser.HTMLParser`<N>* :func:`~.treebuilders.getTreeBuilder`<N>* :func:`~.treewalkers.getTreeWalker`<N>* :func:`~.serializer.serialize`<N>"""<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from .html5parser import HTMLParser, parse, parseFragment<N>from .treebuilders import getTreeBuilder<N>from .treewalkers import getTreeWalker<N>from .serializer import serialize<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from . import base<N><N>from collections import OrderedDict<N><N><N>def _attr_key(attr):<N>    """Return an appropriate key for an attribute for sorting<N><N>    Attributes have a namespace that can be either ``None`` or a string. We<N>    can't compare the two because they're different types, so we convert<N>    ``None`` to an empty string first.<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N><N>class Filter(object):<N>    def __init__(self, source):<N>        self.source = source<N><N>    def __iter__(self):<N>        return iter(self.source)<N><N>    def __getattr__(self, name):<N>        return getattr(self.source, name)<N>
from __future__ import absolute_import, division, unicode_literals<N><N>from . import base<N><N><N>class Filter(base.Filter):<N>    """Injects ``<meta charset=ENCODING>`` tag into head of document"""<N>    def __init__(self, source, encoding):<N>        """Creates a Filter<N><N>
        :arg source: the source token stream<N><N>        :arg encoding: the encoding to set<N><N>        """<N>        base.Filter.__init__(self, source)<N>        self.encoding = encoding<N><N>    def __iter__(self):<N>        state = "pre_head"<N>        meta_found = (self.encoding is None)<N>        pending = []<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from pip._vendor.six import text_type<N><N>from . import base<N>from ..constants import namespaces, voidElements<N><N>from ..constants import spaceCharacters<N>spaceCharacters = "".join(spaceCharacters)<N><N>
<N>class Filter(base.Filter):<N>    """Lints the token stream for errors<N><N>    If it finds any errors, it'll raise an ``AssertionError``.<N><N>    """<N>    def __init__(self, source, require_matching_tags=True):<N>        """Creates a Filter<N><N>
        :arg source: the source token stream<N><N>        :arg require_matching_tags: whether or not to require matching tags<N><N>        """<N>        super(Filter, self).__init__(source)<N>        self.require_matching_tags = require_matching_tags<N><N>
"""Deprecated from html5lib 1.1.<N><N>See `here <https://github.com/html5lib/html5lib-python/issues/443>`_ for<N>information about its deprecation; `Bleach <https://github.com/mozilla/bleach>`_<N>is recommended as a replacement. Please let us know in the aforementioned issue<N>if Bleach is unsuitable for your needs.<N><N>
"""<N>from __future__ import absolute_import, division, unicode_literals<N><N>import re<N>import warnings<N>from xml.sax.saxutils import escape, unescape<N><N>from pip._vendor.six.moves import urllib_parse as urlparse<N><N>from . import base<N>from ..constants import namespaces, prefixes<N><N>
__all__ = ["Filter"]<N><N><N>_deprecation_msg = (<N>    "html5lib's sanitizer is deprecated; see " +<N>    "https://github.com/html5lib/html5lib-python/issues/443 and please let " +<N>    "us know if Bleach is unsuitable for your needs"<N>)<N><N>warnings.warn(_deprecation_msg, DeprecationWarning)<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>import re<N><N>from . import base<N>from ..constants import rcdataElements, spaceCharacters<N>spaceCharacters = "".join(spaceCharacters)<N><N>SPACES_REGEX = re.compile("[%s]+" % spaceCharacters)<N><N>
<N>class Filter(base.Filter):<N>    """Collapses whitespace except in pre, textarea, and script elements"""<N>    spacePreserveElements = frozenset(["pre", "textarea"] + list(rcdataElements))<N><N>    def __iter__(self):<N>        preserve = 0<N>        for token in base.Filter.__iter__(self):<N>            type = token["type"]<N>            if type == "StartTag" \<N>                    and (preserve or token["name"] in self.spacePreserveElements):<N>                preserve += 1<N><N>
            elif type == "EndTag" and preserve:<N>                preserve -= 1<N><N>            elif not preserve and type == "SpaceCharacters" and token["data"]:<N>                # Test on token["data"] above to not introduce spaces where there were not<N>                token["data"] = " "<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from genshi.core import QName, Attrs<N>from genshi.core import START, END, TEXT, COMMENT, DOCTYPE<N><N><N>def to_genshi(walker):<N>    """Convert a tree to a genshi tree<N><N>    :arg walker: the treewalker to use to walk the tree to convert it<N><N>
    :returns: generator of genshi nodes<N><N>    """<N>    text = []<N>    for token in walker:<N>        type = token["type"]<N>        if type in ("Characters", "SpaceCharacters"):<N>            text.append(token["data"])<N>        elif text:<N>            yield TEXT, "".join(text), (None, -1, -1)<N>            text = []<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from xml.sax.xmlreader import AttributesNSImpl<N><N>from ..constants import adjustForeignAttributes, unadjustForeignAttributes<N><N>prefix_mapping = {}<N>for prefix, localName, namespace in adjustForeignAttributes.values():<N>    if prefix is not None:<N>        prefix_mapping[prefix] = namespace<N><N>
<N>def to_sax(walker, handler):<N>    """Call SAX-like content handler based on treewalker walker<N><N>    :arg walker: the treewalker to use to walk the tree to convert it<N><N>    :arg handler: SAX handler to use<N><N>    """<N>    handler.startDocument()<N>    for prefix, namespace in prefix_mapping.items():<N>        handler.startPrefixMapping(prefix, namespace)<N><N>
"""Tree adapters let you convert from one tree structure to another<N><N>Example:<N><N>.. code-block:: python<N><N>   from pip._vendor import html5lib<N>   from pip._vendor.html5lib.treeadapters import genshi<N><N>   doc = '<html><body>Hi!</body></html>'<N>   treebuilder = html5lib.getTreeBuilder('etree')<N>   parser = html5lib.HTMLParser(tree=treebuilder)<N>   tree = parser.parse(doc)<N>   TreeWalker = html5lib.getTreeWalker('etree')<N><N>
   genshi_tree = genshi.to_genshi(TreeWalker(tree))<N><N>"""<N>from __future__ import absolute_import, division, unicode_literals<N><N>from . import sax<N><N>__all__ = ["sax"]<N><N>try:<N>    from . import genshi  # noqa<N>except ImportError:<N>    pass<N>else:<N>    __all__.append("genshi")<N><N><N>
from __future__ import absolute_import, division, unicode_literals<N>from pip._vendor.six import text_type<N><N>from ..constants import scopingElements, tableInsertModeElements, namespaces<N><N># The scope markers are inserted when entering object elements,<N># marquees, table cells, and table captions, and are used to prevent formatting<N># from "leaking" into tables, object elements, and marquees.<N>Marker = None<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N><N>try:<N>    from collections.abc import MutableMapping<N>except ImportError:  # Python 2.7<N>    from collections import MutableMapping<N>from xml.dom import minidom, Node<N>import weakref<N><N>
from . import base<N>from .. import constants<N>from ..constants import namespaces<N>from .._utils import moduleFactoryFactory<N><N><N>def getDomBuilder(DomImplementation):<N>    Dom = DomImplementation<N><N>    class AttrList(MutableMapping):<N>        def __init__(self, element):<N>            self.element = element<N><N>
        def __iter__(self):<N>            return iter(self.element.attributes.keys())<N><N>        def __setitem__(self, name, value):<N>            if isinstance(name, tuple):<N>                raise NotImplementedError<N>            else:<N>                attr = self.element.ownerDocument.createAttribute(name)<N>                attr.value = value<N>                self.element.attributes[name] = attr<N><N>
        def __len__(self):<N>            return len(self.element.attributes)<N><N>        def items(self):<N>            return list(self.element.attributes.items())<N><N>        def values(self):<N>            return list(self.element.attributes.values())<N><N>
        def __getitem__(self, name):<N>            if isinstance(name, tuple):<N>                raise NotImplementedError<N>            else:<N>                return self.element.attributes[name].value<N><N>        def __delitem__(self, name):<N>            if isinstance(name, tuple):<N>                raise NotImplementedError<N>            else:<N>                del self.element.attributes[name]<N><N>
    class NodeBuilder(base.Node):<N>        def __init__(self, element):<N>            base.Node.__init__(self, element.nodeName)<N>            self.element = element<N><N>        namespace = property(lambda self: hasattr(self.element, "namespaceURI") and<N>                             self.element.namespaceURI or None)<N><N>
        def appendChild(self, node):<N>            node.parent = self<N>            self.element.appendChild(node.element)<N><N>        def insertText(self, data, insertBefore=None):<N>            text = self.element.ownerDocument.createTextNode(data)<N>            if insertBefore:<N>                self.element.insertBefore(text, insertBefore.element)<N>            else:<N>                self.element.appendChild(text)<N><N>
        def insertBefore(self, node, refNode):<N>            self.element.insertBefore(node.element, refNode.element)<N>            node.parent = self<N><N>        def removeChild(self, node):<N>            if node.element.parentNode == self.element:<N>                self.element.removeChild(node.element)<N>            node.parent = None<N><N>
        def reparentChildren(self, newParent):<N>            while self.element.hasChildNodes():<N>                child = self.element.firstChild<N>                self.element.removeChild(child)<N>                newParent.element.appendChild(child)<N>            self.childNodes = []<N><N>
from __future__ import absolute_import, division, unicode_literals<N># pylint:disable=protected-access<N><N>from pip._vendor.six import text_type<N><N>import re<N><N>from copy import copy<N><N>from . import base<N>from .. import _ihatexml<N>from .. import constants<N>from ..constants import namespaces<N>from .._utils import moduleFactoryFactory<N><N>
"""Module for supporting the lxml.etree library. The idea here is to use as much<N>of the native library as possible, without using fragile hacks like custom element<N>names that break between releases. The downside of this is that we cannot represent<N>all possible trees; specifically the following are known to cause problems:<N><N>
Text or comments as siblings of the root element<N>Docypes with no name<N><N>When any of these things occur, we emit a DataLossWarning<N>"""<N><N>from __future__ import absolute_import, division, unicode_literals<N># pylint:disable=protected-access<N><N>
import warnings<N>import re<N>import sys<N><N>try:<N>    from collections.abc import MutableMapping<N>except ImportError:<N>    from collections import MutableMapping<N><N>from . import base<N>from ..constants import DataLossWarning<N>from .. import constants<N>from . import etree as etree_builders<N>from .. import _ihatexml<N><N>
import lxml.etree as etree<N>from pip._vendor.six import PY3, binary_type<N><N><N>fullTree = True<N>tag_regexp = re.compile("{([^}]*)}(.*)")<N><N>comment_type = etree.Comment("asd").tag<N><N><N>class DocumentType(object):<N>    def __init__(self, name, publicId, systemId):<N>        self.name = name<N>        self.publicId = publicId<N>        self.systemId = systemId<N><N>
<N>class Document(object):<N>    def __init__(self):<N>        self._elementTree = None<N>        self._childNodes = []<N><N>    def appendChild(self, element):<N>        last = self._elementTree.getroot()<N>        for last in self._elementTree.getroot().itersiblings():<N>            pass<N><N>
        last.addnext(element._element)<N><N>    def _getChildNodes(self):<N>        return self._childNodes<N><N>    childNodes = property(_getChildNodes)<N><N><N>def testSerializer(element):<N>    rv = []<N>    infosetFilter = _ihatexml.InfosetFilter(preventDoubleDashComments=True)<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from xml.dom import Node<N>from ..constants import namespaces, voidElements, spaceCharacters<N><N>__all__ = ["DOCUMENT", "DOCTYPE", "TEXT", "ELEMENT", "COMMENT", "ENTITY", "UNKNOWN",<N>           "TreeWalker", "NonRecursiveTreeWalker"]<N><N>
DOCUMENT = Node.DOCUMENT_NODE<N>DOCTYPE = Node.DOCUMENT_TYPE_NODE<N>TEXT = Node.TEXT_NODE<N>ELEMENT = Node.ELEMENT_NODE<N>COMMENT = Node.COMMENT_NODE<N>ENTITY = Node.ENTITY_NODE<N>UNKNOWN = "<#UNKNOWN#>"<N><N>spaceCharacters = "".join(spaceCharacters)<N><N>
<N>class TreeWalker(object):<N>    """Walks a tree yielding tokens<N><N>    Tokens are dicts that all have a ``type`` field specifying the type of the<N>    token.<N><N>    """<N>    def __init__(self, tree):<N>        """Creates a TreeWalker<N><N>        :arg tree: the tree to walk<N><N>
        """<N>        self.tree = tree<N><N>    def __iter__(self):<N>        raise NotImplementedError<N><N>    def error(self, msg):<N>        """Generates an error token with the given message<N><N>        :arg msg: the error message<N><N>        :returns: SerializeError token<N><N>
        """<N>        return {"type": "SerializeError", "data": msg}<N><N>    def emptyTag(self, namespace, name, attrs, hasChildren=False):<N>        """Generates an EmptyTag token<N><N>        :arg namespace: the namespace of the token--can be ``None``<N><N>
        :arg name: the name of the element<N><N>        :arg attrs: the attributes of the element as a dict<N><N>        :arg hasChildren: whether or not to yield a SerializationError because<N>            this tag shouldn't have children<N><N>        :returns: EmptyTag token<N><N>
        """<N>        yield {"type": "EmptyTag", "name": name,<N>               "namespace": namespace,<N>               "data": attrs}<N>        if hasChildren:<N>            yield self.error("Void element has children")<N><N>    def startTag(self, namespace, name, attrs):<N>        """Generates a StartTag token<N><N>
        :arg namespace: the namespace of the token--can be ``None``<N><N>        :arg name: the name of the element<N><N>        :arg attrs: the attributes of the element as a dict<N><N>        :returns: StartTag token<N><N>        """<N>        return {"type": "StartTag",<N>                "name": name,<N>                "namespace": namespace,<N>                "data": attrs}<N><N>
    def endTag(self, namespace, name):<N>        """Generates an EndTag token<N><N>        :arg namespace: the namespace of the token--can be ``None``<N><N>        :arg name: the name of the element<N><N>        :returns: EndTag token<N><N>        """<N>        return {"type": "EndTag",<N>                "name": name,<N>                "namespace": namespace}<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from xml.dom import Node<N><N>from . import base<N><N><N>class TreeWalker(base.NonRecursiveTreeWalker):<N>    def getNodeDetails(self, node):<N>        if node.nodeType == Node.DOCUMENT_TYPE_NODE:<N>            return base.DOCTYPE, node.name, node.publicId, node.systemId<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from collections import OrderedDict<N>import re<N><N>from pip._vendor.six import string_types<N><N>from . import base<N>from .._utils import moduleFactoryFactory<N><N>tag_regexp = re.compile("{([^}]*)}(.*)")<N><N>
<N>def getETreeBuilder(ElementTreeImplementation):<N>    ElementTree = ElementTreeImplementation<N>    ElementTreeCommentType = ElementTree.Comment("asd").tag<N><N>    class TreeWalker(base.NonRecursiveTreeWalker):  # pylint:disable=unused-variable<N>        """Given the particular ElementTree representation, this implementation,<N>        to avoid using recursion, returns "nodes" as tuples with the following<N>        content:<N><N>
from __future__ import absolute_import, division, unicode_literals<N>from pip._vendor.six import text_type<N><N>from collections import OrderedDict<N><N>from lxml import etree<N>from ..treebuilders.etree import tag_regexp<N><N>from . import base<N><N>
from .. import _ihatexml<N><N><N>def ensure_str(s):<N>    if s is None:<N>        return None<N>    elif isinstance(s, text_type):<N>        return s<N>    else:<N>        return s.decode("ascii", "strict")<N><N><N>class Root(object):<N>    def __init__(self, et):<N>        self.elementtree = et<N>        self.children = []<N><N>
        try:<N>            if et.docinfo.internalDTD:<N>                self.children.append(Doctype(self,<N>                                             ensure_str(et.docinfo.root_name),<N>                                             ensure_str(et.docinfo.public_id),<N>                                             ensure_str(et.docinfo.system_url)))<N>        except AttributeError:<N>            pass<N><N>
        try:<N>            node = et.getroot()<N>        except AttributeError:<N>            node = et<N><N>        while node.getprevious() is not None:<N>            node = node.getprevious()<N>        while node is not None:<N>            self.children.append(node)<N>            node = node.getnext()<N><N>
        self.text = None<N>        self.tail = None<N><N>    def __getitem__(self, key):<N>        return self.children[key]<N><N>    def getnext(self):<N>        return None<N><N>    def __len__(self):<N>        return 1<N><N><N>class Doctype(object):<N>    def __init__(self, root_node, name, public_id, system_id):<N>        self.root_node = root_node<N>        self.name = name<N>        self.public_id = public_id<N>        self.system_id = system_id<N><N>
        self.text = None<N>        self.tail = None<N><N>    def getnext(self):<N>        return self.root_node.children[1]<N><N><N>class FragmentRoot(Root):<N>    def __init__(self, children):<N>        self.children = [FragmentWrapper(self, child) for child in children]<N>        self.text = self.tail = None<N><N>
    def getnext(self):<N>        return None<N><N><N>class FragmentWrapper(object):<N>    def __init__(self, fragment_root, obj):<N>        self.root_node = fragment_root<N>        self.obj = obj<N>        if hasattr(self.obj, 'text'):<N>            self.text = ensure_str(self.obj.text)<N>        else:<N>            self.text = None<N>        if hasattr(self.obj, 'tail'):<N>            self.tail = ensure_str(self.obj.tail)<N>        else:<N>            self.tail = None<N><N>
    def __getattr__(self, name):<N>        return getattr(self.obj, name)<N><N>    def getnext(self):<N>        siblings = self.root_node.children<N>        idx = siblings.index(self)<N>        if idx < len(siblings) - 1:<N>            return siblings[idx + 1]<N>        else:<N>            return None<N><N>
    def __getitem__(self, key):<N>        return self.obj[key]<N><N>    def __bool__(self):<N>        return bool(self.obj)<N><N>    def getparent(self):<N>        return None<N><N>    def __str__(self):<N>        return str(self.obj)<N><N>    def __unicode__(self):<N>        return str(self.obj)<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from genshi.core import QName<N>from genshi.core import START, END, XML_NAMESPACE, DOCTYPE, TEXT<N>from genshi.core import START_NS, END_NS, START_CDATA, END_CDATA, PI, COMMENT<N><N>
from . import base<N><N>from ..constants import voidElements, namespaces<N><N><N>class TreeWalker(base.TreeWalker):<N>    def __iter__(self):<N>        # Buffer the events so we can pass in the following one<N>        previous = None<N>        for event in self.tree:<N>            if previous is not None:<N>                for token in self.tokens(previous, event):<N>                    yield token<N>            previous = event<N><N>
"""A collection of modules for iterating through different kinds of<N>tree, generating tokens identical to those produced by the tokenizer<N>module.<N><N>To create a tree walker for a new type of tree, you need to<N>implement a tree walker object (called TreeWalker by convention) that<N>implements a 'serialize' method which takes a tree as sole argument and<N>returns an iterator which generates tokens.<N>"""<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from .. import constants<N>from .._utils import default_etree<N><N>__all__ = ["getTreeWalker", "pprint"]<N><N>treeWalkerCache = {}<N><N><N>def getTreeWalker(treeType, implementation=None, **kwargs):<N>    """Get a TreeWalker class for various types of tree with built-in support<N><N>
    :arg str treeType: the name of the tree type required (case-insensitive).<N>        Supported values are:<N><N>        * "dom": The xml.dom.minidom DOM implementation<N>        * "etree": A generic walker for tree implementations exposing an<N>          elementtree-like interface (known to work with ElementTree,<N>          cElementTree and lxml.etree).<N>        * "lxml": Optimized walker for lxml.etree<N>        * "genshi": a Genshi stream<N><N>
    :arg implementation: A module implementing the tree type e.g.<N>        xml.etree.ElementTree or cElementTree (Currently applies to the "etree"<N>        tree type only).<N><N>    :arg kwargs: keyword arguments passed to the etree walker--for other<N>        walkers, this has no effect<N><N>
from __future__ import absolute_import, division, unicode_literals<N>from pip._vendor.six import text_type<N><N>from bisect import bisect_left<N><N>from ._base import Trie as ABCTrie<N><N><N>class Trie(ABCTrie):<N>    def __init__(self, data):<N>        if not all(isinstance(x, text_type) for x in data.keys()):<N>            raise TypeError("All keys must be strings")<N><N>
        self._data = data<N>        self._keys = sorted(data.keys())<N>        self._cachestr = ""<N>        self._cachepoints = (0, len(data))<N><N>    def __contains__(self, key):<N>        return key in self._data<N><N>    def __len__(self):<N>        return len(self._data)<N><N>
    def __iter__(self):<N>        return iter(self._data)<N><N>    def __getitem__(self, key):<N>        return self._data[key]<N><N>    def keys(self, prefix=None):<N>        if prefix is None or prefix == "" or not self._keys:<N>            return set(self._keys)<N><N>
        if prefix.startswith(self._cachestr):<N>            lo, hi = self._cachepoints<N>            start = i = bisect_left(self._keys, prefix, lo, hi)<N>        else:<N>            start = i = bisect_left(self._keys, prefix)<N><N>        keys = set()<N>        if start == len(self._keys):<N>            return keys<N><N>
        while self._keys[i].startswith(prefix):<N>            keys.add(self._keys[i])<N>            i += 1<N><N>        self._cachestr = prefix<N>        self._cachepoints = (start, i)<N><N>        return keys<N><N>    def has_keys_with_prefix(self, prefix):<N>        if prefix in self._data:<N>            return True<N><N>
        if prefix.startswith(self._cachestr):<N>            lo, hi = self._cachepoints<N>            i = bisect_left(self._keys, prefix, lo, hi)<N>        else:<N>            i = bisect_left(self._keys, prefix)<N><N>        if i == len(self._keys):<N>            return False<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>try:<N>    from collections.abc import Mapping<N>except ImportError:  # Python 2.7<N>    from collections import Mapping<N><N><N>class Trie(Mapping):<N>    """Abstract base class for tries"""<N><N>
    def keys(self, prefix=None):<N>        # pylint:disable=arguments-differ<N>        keys = super(Trie, self).keys()<N><N>        if prefix is None:<N>            return set(keys)<N><N>        return {x for x in keys if x.startswith(prefix)}<N><N>    def has_keys_with_prefix(self, prefix):<N>        for key in self.keys():<N>            if key.startswith(prefix):<N>                return True<N><N>
        return False<N><N>    def longest_prefix(self, prefix):<N>        if prefix in self:<N>            return prefix<N><N>        for i in range(1, len(prefix) + 1):<N>            if prefix[:-i] in self:<N>                return prefix[:-i]<N><N>
from __future__ import absolute_import, division, unicode_literals<N><N>from .py import Trie<N><N>__all__ = ["Trie"]<N>
from .core import encode, decode, alabel, ulabel, IDNAError<N>import codecs<N>import re<N>from typing import Tuple, Optional<N><N>_unicode_dots_re = re.compile('[\u002e\u3002\uff0e\uff61]')<N><N>class Codec(codecs.Codec):<N><N>    def encode(self, data: str, errors: str = 'strict') -> Tuple[bytes, int]:<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return b"", 0<N><N>        return encode(data), len(data)<N><N>    def decode(self, data: bytes, errors: str = 'strict') -> Tuple[str, int]:<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return '', 0<N><N>        return decode(data), len(data)<N><N>class IncrementalEncoder(codecs.BufferedIncrementalEncoder):<N>    def _buffer_encode(self, data: str, errors: str, final: bool) -> Tuple[str, int]:  # type: ignore<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return "", 0<N><N>        labels = _unicode_dots_re.split(data)<N>        trailing_dot = ''<N>        if labels:<N>            if not labels[-1]:<N>                trailing_dot = '.'<N>                del labels[-1]<N>            elif not final:<N>                # Keep potentially unfinished label until the next call<N>                del labels[-1]<N>                if labels:<N>                    trailing_dot = '.'<N><N>
        result = []<N>        size = 0<N>        for label in labels:<N>            result.append(alabel(label))<N>            if size:<N>                size += 1<N>            size += len(label)<N><N>        # Join with U+002E<N>        result_str = '.'.join(result) + trailing_dot  # type: ignore<N>        size += len(trailing_dot)<N>        return result_str, size<N><N>
class IncrementalDecoder(codecs.BufferedIncrementalDecoder):<N>    def _buffer_decode(self, data: str, errors: str, final: bool) -> Tuple[str, int]:  # type: ignore<N>        if errors != 'strict':<N>            raise IDNAError('Unsupported error handling \"{}\"'.format(errors))<N><N>
        if not data:<N>            return ('', 0)<N><N>        labels = _unicode_dots_re.split(data)<N>        trailing_dot = ''<N>        if labels:<N>            if not labels[-1]:<N>                trailing_dot = '.'<N>                del labels[-1]<N>            elif not final:<N>                # Keep potentially unfinished label until the next call<N>                del labels[-1]<N>                if labels:<N>                    trailing_dot = '.'<N><N>
        result = []<N>        size = 0<N>        for label in labels:<N>            result.append(ulabel(label))<N>            if size:<N>                size += 1<N>            size += len(label)<N><N>        result_str = '.'.join(result) + trailing_dot<N>        size += len(trailing_dot)<N>        return (result_str, size)<N><N>
from .core import *<N>from .codec import *<N>from typing import Any, Union<N><N>def ToASCII(label: str) -> bytes:<N>    return encode(label)<N><N>def ToUnicode(label: Union[bytes, bytearray]) -> str:<N>    return decode(label)<N><N>def nameprep(s: Any) -> None:<N>    raise NotImplementedError('IDNA 2008 does not utilise nameprep protocol')<N><N>
from . import idnadata<N>import bisect<N>import unicodedata<N>import re<N>from typing import Union, Optional<N>from .intranges import intranges_contain<N><N>_virama_combining_class = 9<N>_alabel_prefix = b'xn--'<N>_unicode_dots_re = re.compile('[\u002e\u3002\uff0e\uff61]')<N><N>
class IDNAError(UnicodeError):<N>    """ Base exception for all IDNA-encoding related problems """<N>    pass<N><N><N>class IDNABidiError(IDNAError):<N>    """ Exception when bidirectional requirements are not satisfied """<N>    pass<N><N><N>class InvalidCodepoint(IDNAError):<N>    """ Exception when a disallowed or unallocated codepoint is used """<N>    pass<N><N>
<N>class InvalidCodepointContext(IDNAError):<N>    """ Exception when the codepoint is not valid in the context it is used """<N>    pass<N><N><N>def _combining_class(cp: int) -> int:<N>    v = unicodedata.combining(chr(cp))<N>    if v == 0:<N>        if not unicodedata.name(chr(cp)):<N>            raise ValueError('Unknown character in unicodedata')<N>    return v<N><N>
def _is_script(cp: str, script: str) -> bool:<N>    return intranges_contain(ord(cp), idnadata.scripts[script])<N><N>def _punycode(s: str) -> bytes:<N>    return s.encode('punycode')<N><N>def _unot(s: int) -> str:<N>    return 'U+{:04X}'.format(s)<N><N>
<N>def valid_label_length(label: Union[bytes, str]) -> bool:<N>    if len(label) > 63:<N>        return False<N>    return True<N><N><N>def valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:<N>    if len(label) > (254 if trailing_dot else 253):<N>        return False<N>    return True<N><N>
"""<N>Given a list of integers, made up of (hopefully) a small number of long runs<N>of consecutive integers, compute a representation of the form<N>((start1, end1), (start2, end2) ...). Then answer the question "was x present<N>in the original list?" in time O(log(# runs)).<N>"""<N><N>
import bisect<N>from typing import List, Tuple<N><N>def intranges_from_list(list_: List[int]) -> Tuple[int, ...]:<N>    """Represent a list of integers as a sequence of ranges:<N>    ((start_0, end_0), (start_1, end_1), ...), such that the original<N>    integers are exactly those x such that start_i <= x < end_i for some i.<N><N>
    Ranges are encoded as single integers (start << 32 | end), not as tuples.<N>    """<N><N>    sorted_list = sorted(list_)<N>    ranges = []<N>    last_write = -1<N>    for i in range(len(sorted_list)):<N>        if i+1 < len(sorted_list):<N>            if sorted_list[i] == sorted_list[i+1]-1:<N>                continue<N>        current_range = sorted_list[last_write+1:i+1]<N>        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))<N>        last_write = i<N><N>
from .package_data import __version__<N>from .core import (<N>    IDNABidiError,<N>    IDNAError,<N>    InvalidCodepoint,<N>    InvalidCodepointContext,<N>    alabel,<N>    check_bidi,<N>    check_hyphen_ok,<N>    check_initial_combiner,<N>    check_label,<N>    check_nfc,<N>    decode,<N>    encode,<N>    ulabel,<N>    uts46_remap,<N>    valid_contextj,<N>    valid_contexto,<N>    valid_label_length,<N>    valid_string_length,<N>)<N>from .intranges import intranges_contain<N><N>
__all__ = [<N>    "IDNABidiError",<N>    "IDNAError",<N>    "InvalidCodepoint",<N>    "InvalidCodepointContext",<N>    "alabel",<N>    "check_bidi",<N>    "check_hyphen_ok",<N>    "check_initial_combiner",<N>    "check_label",<N>    "check_nfc",<N>    "decode",<N>    "encode",<N>    "intranges_contain",<N>    "ulabel",<N>    "uts46_remap",<N>    "valid_contextj",<N>    "valid_contexto",<N>    "valid_label_length",<N>    "valid_string_length",<N>]<N><N><N>
class UnpackException(Exception):<N>    """Base class for some exceptions raised while unpacking.<N><N>    NOTE: unpack may raise exception other than subclass of<N>    UnpackException.  If you want to catch all error, catch<N>    Exception instead.<N>    """<N><N>
<N>class BufferFull(UnpackException):<N>    pass<N><N><N>class OutOfData(UnpackException):<N>    pass<N><N><N>class FormatError(ValueError, UnpackException):<N>    """Invalid msgpack format"""<N><N><N>class StackError(ValueError, UnpackException):<N>    """Too nested"""<N><N>
<N># Deprecated.  Use ValueError instead<N>UnpackValueError = ValueError<N><N><N>class ExtraData(UnpackValueError):<N>    """ExtraData is raised when there is trailing data.<N><N>    This exception is raised while only one-shot (not streaming)<N>    unpack.<N>    """<N><N>
    def __init__(self, unpacked, extra):<N>        self.unpacked = unpacked<N>        self.extra = extra<N><N>    def __str__(self):<N>        return "unpack(b) received extra data."<N><N><N># Deprecated.  Use Exception instead to catch all exception during packing.<N>PackException = Exception<N>PackValueError = ValueError<N>PackOverflowError = OverflowError<N><N><N>
# coding: utf-8<N>from collections import namedtuple<N>import datetime<N>import sys<N>import struct<N><N><N>PY2 = sys.version_info[0] == 2<N><N>if PY2:<N>    int_types = (int, long)<N>    _utc = None<N>else:<N>    int_types = int<N>    try:<N>        _utc = datetime.timezone.utc<N>    except AttributeError:<N>        _utc = datetime.timezone(datetime.timedelta(0))<N><N>
<N>class ExtType(namedtuple("ExtType", "code data")):<N>    """ExtType represents ext type in msgpack."""<N><N>    def __new__(cls, code, data):<N>        if not isinstance(code, int):<N>            raise TypeError("code must be int")<N>        if not isinstance(data, bytes):<N>            raise TypeError("data must be bytes")<N>        if not 0 <= code <= 127:<N>            raise ValueError("code must be 0~127")<N>        return super(ExtType, cls).__new__(cls, code, data)<N><N>
<N>class Timestamp(object):<N>    """Timestamp represents the Timestamp extension type in msgpack.<N><N>    When built with Cython, msgpack uses C methods to pack and unpack `Timestamp`. When using pure-Python<N>    msgpack, :func:`to_bytes` and :func:`from_bytes` are used to pack and unpack `Timestamp`.<N><N>
    This class is immutable: Do not override seconds and nanoseconds.<N>    """<N><N>    __slots__ = ["seconds", "nanoseconds"]<N><N>    def __init__(self, seconds, nanoseconds=0):<N>        """Initialize a Timestamp object.<N><N>        :param int seconds:<N>            Number of seconds since the UNIX epoch (00:00:00 UTC Jan 1 1970, minus leap seconds).<N>            May be negative.<N><N>
"""Fallback pure Python implementation of msgpack"""<N>from datetime import datetime as _DateTime<N>import sys<N>import struct<N><N><N>PY2 = sys.version_info[0] == 2<N>if PY2:<N>    int_types = (int, long)<N><N>    def dict_iteritems(d):<N>        return d.iteritems()<N><N>
<N>else:<N>    int_types = int<N>    unicode = str<N>    xrange = range<N><N>    def dict_iteritems(d):<N>        return d.items()<N><N><N>if sys.version_info < (3, 5):<N>    # Ugly hack...<N>    RecursionError = RuntimeError<N><N>    def _is_recursionerror(e):<N>        return (<N>            len(e.args) == 1<N>            and isinstance(e.args[0], str)<N>            and e.args[0].startswith("maximum recursion depth exceeded")<N>        )<N><N>
<N>else:<N><N>    def _is_recursionerror(e):<N>        return True<N><N><N>if hasattr(sys, "pypy_version_info"):<N>    # StringIO is slow on PyPy, StringIO is faster.  However: PyPy's own<N>    # StringBuilder is fastest.<N>    from __pypy__ import newlist_hint<N><N>
    try:<N>        from __pypy__.builders import BytesBuilder as StringBuilder<N>    except ImportError:<N>        from __pypy__.builders import StringBuilder<N>    USING_STRINGBUILDER = True<N><N>    class StringIO(object):<N>        def __init__(self, s=b""):<N>            if s:<N>                self.builder = StringBuilder(len(s))<N>                self.builder.append(s)<N>            else:<N>                self.builder = StringBuilder()<N><N>
        def write(self, s):<N>            if isinstance(s, memoryview):<N>                s = s.tobytes()<N>            elif isinstance(s, bytearray):<N>                s = bytes(s)<N>            self.builder.append(s)<N><N>        def getvalue(self):<N>            return self.builder.build()<N><N>
<N>else:<N>    USING_STRINGBUILDER = False<N>    from io import BytesIO as StringIO<N><N>    newlist_hint = lambda size: []<N><N><N>from .exceptions import BufferFull, OutOfData, ExtraData, FormatError, StackError<N><N>from .ext import ExtType, Timestamp<N><N>
<N>EX_SKIP = 0<N>EX_CONSTRUCT = 1<N>EX_READ_ARRAY_HEADER = 2<N>EX_READ_MAP_HEADER = 3<N><N>TYPE_IMMEDIATE = 0<N>TYPE_ARRAY = 1<N>TYPE_MAP = 2<N>TYPE_RAW = 3<N>TYPE_BIN = 4<N>TYPE_EXT = 5<N><N>DEFAULT_RECURSE_LIMIT = 511<N><N><N>def _check_type_strict(obj, t, type=type, tuple=tuple):<N>    if type(t) is tuple:<N>        return type(obj) in t<N>    else:<N>        return type(obj) is t<N><N>
<N>def _get_data_from_buffer(obj):<N>    view = memoryview(obj)<N>    if view.itemsize != 1:<N>        raise ValueError("cannot unpack from multi-byte object")<N>    return view<N><N><N>def unpackb(packed, **kwargs):<N>    """<N>    Unpack an object from `packed`.<N><N>
    Raises ``ExtraData`` when *packed* contains extra bytes.<N>    Raises ``ValueError`` when *packed* is incomplete.<N>    Raises ``FormatError`` when *packed* is not valid msgpack.<N>    Raises ``StackError`` when *packed* contains too nested.<N>    Other exceptions can be raised during unpacking.<N><N>
# coding: utf-8<N>from ._version import version<N>from .exceptions import *<N>from .ext import ExtType, Timestamp<N><N>import os<N>import sys<N><N><N>if os.environ.get("MSGPACK_PUREPYTHON") or sys.version_info[0] == 2:<N>    from .fallback import Packer, unpackb, Unpacker<N>else:<N>    try:<N>        from ._cmsgpack import Packer, unpackb, Unpacker<N>    except ImportError:<N>        from .fallback import Packer, unpackb, Unpacker<N><N>
<N>def pack(o, stream, **kwargs):<N>    """<N>    Pack object `o` and write it to `stream`<N><N>    See :class:`Packer` for options.<N>    """<N>    packer = Packer(**kwargs)<N>    stream.write(packer.pack(o))<N><N><N>def packb(o, **kwargs):<N>    """<N>    Pack object `o` and return packed bytes<N><N>
    See :class:`Packer` for options.<N>    """<N>    return Packer(**kwargs).pack(o)<N><N><N>def unpack(stream, **kwargs):<N>    """<N>    Unpack an object from `stream`.<N><N>    Raises `ExtraData` when `stream` contains extra bytes.<N>    See :class:`Unpacker` for options.<N>    """<N>    data = stream.read()<N>    return unpackb(data, **kwargs)<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import operator<N>import os<N>import platform<N>import sys<N>from typing import Any, Callable, Dict, List, Optional, Tuple, Union<N><N>
from pip._vendor.pyparsing import (  # noqa: N817<N>    Forward,<N>    Group,<N>    Literal as L,<N>    ParseException,<N>    ParseResults,<N>    QuotedString,<N>    ZeroOrMore,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .specifiers import InvalidSpecifier, Specifier<N><N>
__all__ = [<N>    "InvalidMarker",<N>    "UndefinedComparison",<N>    "UndefinedEnvironmentName",<N>    "Marker",<N>    "default_environment",<N>]<N><N>Operator = Callable[[str, str], bool]<N><N><N>class InvalidMarker(ValueError):<N>    """<N>    An invalid marker was found, users should refer to PEP 508.<N>    """<N><N>
<N>class UndefinedComparison(ValueError):<N>    """<N>    An invalid operation was attempted on a value that doesn't support it.<N>    """<N><N><N>class UndefinedEnvironmentName(ValueError):<N>    """<N>    A name was attempted to be used that does not exist inside of the<N>    environment.<N>    """<N><N>
<N>class Node:<N>    def __init__(self, value: Any) -> None:<N>        self.value = value<N><N>    def __str__(self) -> str:<N>        return str(self.value)<N><N>    def __repr__(self) -> str:<N>        return f"<{self.__class__.__name__}('{self}')>"<N><N>
    def serialize(self) -> str:<N>        raise NotImplementedError<N><N><N>class Variable(Node):<N>    def serialize(self) -> str:<N>        return str(self)<N><N><N>class Value(Node):<N>    def serialize(self) -> str:<N>        return f'"{self}"'<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>import string<N>import urllib.parse<N>from typing import List, Optional as TOptional, Set<N><N>
from pip._vendor.pyparsing import (  # noqa<N>    Combine,<N>    Literal as L,<N>    Optional,<N>    ParseException,<N>    Regex,<N>    Word,<N>    ZeroOrMore,<N>    originalTextFor,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .markers import MARKER_EXPR, Marker<N>from .specifiers import LegacySpecifier, Specifier, SpecifierSet<N><N>
<N>class InvalidRequirement(ValueError):<N>    """<N>    An invalid requirement was found, users should refer to PEP 508.<N>    """<N><N><N>ALPHANUM = Word(string.ascii_letters + string.digits)<N><N>LBRACKET = L("[").suppress()<N>RBRACKET = L("]").suppress()<N>LPAREN = L("(").suppress()<N>RPAREN = L(")").suppress()<N>COMMA = L(",").suppress()<N>SEMICOLON = L(";").suppress()<N>AT = L("@").suppress()<N><N>
PUNCTUATION = Word("-_.")<N>IDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM)<N>IDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END))<N><N>NAME = IDENTIFIER("name")<N>EXTRA = IDENTIFIER<N><N>URI = Regex(r"[^ ]+")("url")<N>URL = AT + URI<N><N>
EXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA)<N>EXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)("extras")<N><N>VERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE)<N>VERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE)<N><N>
VERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY<N>VERSION_MANY = Combine(<N>    VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=",", adjacent=False<N>)("_raw_spec")<N>_VERSION_SPEC = Optional((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY)<N>_VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or "")<N><N>
VERSION_SPEC = originalTextFor(_VERSION_SPEC)("specifier")<N>VERSION_SPEC.setParseAction(lambda s, l, t: t[1])<N><N>MARKER_EXPR = originalTextFor(MARKER_EXPR())("marker")<N>MARKER_EXPR.setParseAction(<N>    lambda s, l, t: Marker(s[t._original_start : t._original_end])<N>)<N>MARKER_SEPARATOR = SEMICOLON<N>MARKER = MARKER_SEPARATOR + MARKER_EXPR<N><N>
VERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER)<N>URL_AND_MARKER = URL + Optional(MARKER)<N><N>NAMED_REQUIREMENT = NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER)<N><N>REQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd<N># pyparsing isn't thread safe during initialization, so we do it eagerly, see<N># issue #104<N>REQUIREMENT.parseString("x[]")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import abc<N>import functools<N>import itertools<N>import re<N>import warnings<N>from typing import (<N>    Callable,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Pattern,<N>    Set,<N>    Tuple,<N>    TypeVar,<N>    Union,<N>)<N><N>
from .utils import canonicalize_version<N>from .version import LegacyVersion, Version, parse<N><N>ParsedVersion = Union[Version, LegacyVersion]<N>UnparsedVersion = Union[Version, LegacyVersion, str]<N>VersionTypeVar = TypeVar("VersionTypeVar", bound=UnparsedVersion)<N>CallableOperator = Callable[[ParsedVersion, str], bool]<N><N>
<N>class InvalidSpecifier(ValueError):<N>    """<N>    An invalid specifier was found, users should refer to PEP 440.<N>    """<N><N><N>class BaseSpecifier(metaclass=abc.ABCMeta):<N>    @abc.abstractmethod<N>    def __str__(self) -> str:<N>        """<N>        Returns the str representation of this Specifier like object. This<N>        should be representative of the Specifier itself.<N>        """<N><N>
    @abc.abstractmethod<N>    def __hash__(self) -> int:<N>        """<N>        Returns a hash value for this Specifier like object.<N>        """<N><N>    @abc.abstractmethod<N>    def __eq__(self, other: object) -> bool:<N>        """<N>        Returns a boolean representing whether or not the two Specifier like<N>        objects are equal.<N>        """<N><N>
    @abc.abstractproperty<N>    def prereleases(self) -> Optional[bool]:<N>        """<N>        Returns whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>    @prereleases.setter<N>    def prereleases(self, value: bool) -> None:<N>        """<N>        Sets whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import logging<N>import platform<N>import sys<N>import sysconfig<N>from importlib.machinery import EXTENSION_SUFFIXES<N>from typing import (<N>    Dict,<N>    FrozenSet,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Sequence,<N>    Tuple,<N>    Union,<N>    cast,<N>)<N><N>
from . import _manylinux, _musllinux<N><N>logger = logging.getLogger(__name__)<N><N>PythonVersion = Sequence[int]<N>MacVersion = Tuple[int, int]<N><N>INTERPRETER_SHORT_NAMES: Dict[str, str] = {<N>    "python": "py",  # Generic.<N>    "cpython": "cp",<N>    "pypy": "pp",<N>    "ironpython": "ip",<N>    "jython": "jy",<N>}<N><N>
<N>_32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32<N><N><N>class Tag:<N>    """<N>    A representation of the tag triple for a wheel.<N><N>    Instances are considered immutable and thus are hashable. Equality checking<N>    is also supported.<N>    """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>from typing import FrozenSet, NewType, Tuple, Union, cast<N><N>
from .tags import Tag, parse_tag<N>from .version import InvalidVersion, Version<N><N>BuildTag = Union[Tuple[()], Tuple[int, str]]<N>NormalizedName = NewType("NormalizedName", str)<N><N><N>class InvalidWheelFilename(ValueError):<N>    """<N>    An invalid wheel filename was found, users should refer to PEP 427.<N>    """<N><N>
<N>class InvalidSdistFilename(ValueError):<N>    """<N>    An invalid sdist filename was found, users should refer to the packaging user guide.<N>    """<N><N><N>_canonicalize_regex = re.compile(r"[-_.]+")<N># PEP 427: The build number must start with a digit.<N>_build_tag_regex = re.compile(r"(\d+)(.*)")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import collections<N>import itertools<N>import re<N>import warnings<N>from typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union<N><N>
"""PEP 656 support.<N><N>This module implements logic to detect if the currently running Python is<N>linked against musl, and what musl version is used.<N>"""<N><N>import contextlib<N>import functools<N>import operator<N>import os<N>import re<N>import struct<N>import subprocess<N>import sys<N>from typing import IO, Iterator, NamedTuple, Optional, Tuple<N><N>
<N>def _read_unpacked(f: IO[bytes], fmt: str) -> Tuple[int, ...]:<N>    return struct.unpack(fmt, f.read(struct.calcsize(fmt)))<N><N><N>def _parse_ld_musl_from_elf(f: IO[bytes]) -> Optional[str]:<N>    """Detect musl libc location by parsing the Python executable.<N><N>
    Based on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca<N>    ELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html<N>    """<N>    f.seek(0)<N>    try:<N>        ident = _read_unpacked(f, "16B")<N>    except struct.error:<N>        return None<N>    if ident[:4] != tuple(b"\x7fELF"):  # Invalid magic, not ELF.<N>        return None<N>    f.seek(struct.calcsize("HHI"), 1)  # Skip file type, machine, and version.<N><N>
    try:<N>        # e_fmt: Format for program header.<N>        # p_fmt: Format for section header.<N>        # p_idx: Indexes to find p_type, p_offset, and p_filesz.<N>        e_fmt, p_fmt, p_idx = {<N>            1: ("IIIIHHH", "IIIIIIII", (0, 1, 4)),  # 32-bit.<N>            2: ("QQQIHHH", "IIQQQQQQ", (0, 2, 5)),  # 64-bit.<N>        }[ident[4]]<N>    except KeyError:<N>        return None<N>    else:<N>        p_get = operator.itemgetter(*p_idx)<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N><N>class InfinityType:<N>    def __repr__(self) -> str:<N>        return "Infinity"<N><N>
    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return False<N><N>    def __le__(self, other: object) -> bool:<N>        return False<N><N>    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>
    def __gt__(self, other: object) -> bool:<N>        return True<N><N>    def __ge__(self, other: object) -> bool:<N>        return True<N><N>    def __neg__(self: object) -> "NegativeInfinityType":<N>        return NegativeInfinity<N><N><N>Infinity = InfinityType()<N><N>
<N>class NegativeInfinityType:<N>    def __repr__(self) -> str:<N>        return "-Infinity"<N><N>    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return True<N><N>    def __le__(self, other: object) -> bool:<N>        return True<N><N>
    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>    def __gt__(self, other: object) -> bool:<N>        return False<N><N>    def __ge__(self, other: object) -> bool:<N>        return False<N><N>    def __neg__(self: object) -> InfinityType:<N>        return Infinity<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N><N>
__title__ = "packaging"<N>__summary__ = "Core utilities for Python packages"<N>__uri__ = "https://github.com/pypa/packaging"<N><N>__version__ = "21.3"<N><N>__author__ = "Donald Stufft and individual contributors"<N>__email__ = "donald@stufft.io"<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>from .__about__ import (<N>    __author__,<N>    __copyright__,<N>    __email__,<N>    __license__,<N>    __summary__,<N>    __title__,<N>    __uri__,<N>    __version__,<N>)<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N>
"""Build a project using PEP 517 hooks.<N>"""<N>import argparse<N>import io<N>import logging<N>import os<N>import shutil<N><N>from .envbuild import BuildEnvironment<N>from .wrappers import Pep517HookCaller<N>from .dirtools import tempdir, mkdir_p<N>from .compat import FileNotFoundError, toml_load<N><N>
log = logging.getLogger(__name__)<N><N><N>def validate_system(system):<N>    """<N>    Ensure build system has the requisite fields.<N>    """<N>    required = {'requires', 'build-backend'}<N>    if not (required <= set(system)):<N>        message = "Missing required fields: {missing}".format(<N>            missing=required-set(system),<N>        )<N>        raise ValueError(message)<N><N>
<N>def load_system(source_dir):<N>    """<N>    Load the build system from a source dir (pyproject.toml).<N>    """<N>    pyproject = os.path.join(source_dir, 'pyproject.toml')<N>    with io.open(pyproject, 'rb') as f:<N>        pyproject_data = toml_load(f)<N>    return pyproject_data['build-system']<N><N>
"""Check a project and backend by attempting to build using PEP 517 hooks.<N>"""<N>import argparse<N>import io<N>import logging<N>import os<N>from os.path import isfile, join as pjoin<N>import shutil<N>from subprocess import CalledProcessError<N>import sys<N>import tarfile<N>from tempfile import mkdtemp<N>import zipfile<N><N>
"""Python 2/3 compatibility"""<N>import io<N>import json<N>import sys<N><N><N># Handle reading and writing JSON in UTF-8, on Python 3 and 2.<N><N>if sys.version_info[0] >= 3:<N>    # Python 3<N>    def write_json(obj, path, **kwargs):<N>        with open(path, 'w', encoding='utf-8') as f:<N>            json.dump(obj, f, **kwargs)<N><N>
    def read_json(path):<N>        with open(path, 'r', encoding='utf-8') as f:<N>            return json.load(f)<N><N>else:<N>    # Python 2<N>    def write_json(obj, path, **kwargs):<N>        with open(path, 'wb') as f:<N>            json.dump(obj, f, encoding='utf-8', **kwargs)<N><N>
    def read_json(path):<N>        with open(path, 'rb') as f:<N>            return json.load(f)<N><N><N># FileNotFoundError<N><N>try:<N>    FileNotFoundError = FileNotFoundError<N>except NameError:<N>    FileNotFoundError = IOError<N><N><N>if sys.version_info < (3, 6):<N>    from toml import load as _toml_load  # noqa: F401<N><N>
    def toml_load(f):<N>        w = io.TextIOWrapper(f, encoding="utf8", newline="")<N>        try:<N>            return _toml_load(w)<N>        finally:<N>            w.detach()<N><N>    from toml import TomlDecodeError as TOMLDecodeError  # noqa: F401<N>else:<N>    from pip._vendor.tomli import load as toml_load  # noqa: F401<N>    from pip._vendor.tomli import TOMLDecodeError  # noqa: F401<N><N><N>
import os<N>import io<N>import contextlib<N>import tempfile<N>import shutil<N>import errno<N>import zipfile<N><N><N>@contextlib.contextmanager<N>def tempdir():<N>    """Create a temporary directory in a context manager."""<N>    td = tempfile.mkdtemp()<N>    try:<N>        yield td<N>    finally:<N>        shutil.rmtree(td)<N><N>
<N>def mkdir_p(*args, **kwargs):<N>    """Like `mkdir`, but does not raise an exception if the<N>    directory already exists.<N>    """<N>    try:<N>        return os.mkdir(*args, **kwargs)<N>    except OSError as exc:<N>        if exc.errno != errno.EEXIST:<N>            raise<N><N>
"""Build wheels/sdists by installing build deps to a temporary environment.<N>"""<N><N>import io<N>import os<N>import logging<N>import shutil<N>from subprocess import check_call<N>import sys<N>from sysconfig import get_paths<N>from tempfile import mkdtemp<N><N>
from .compat import toml_load<N>from .wrappers import Pep517HookCaller, LoggerWrapper<N><N>log = logging.getLogger(__name__)<N><N><N>def _load_pyproject(source_dir):<N>    with io.open(<N>            os.path.join(source_dir, 'pyproject.toml'),<N>            'rb',<N>            ) as f:<N>        pyproject_data = toml_load(f)<N>    buildsys = pyproject_data['build-system']<N>    return (<N>        buildsys['requires'],<N>        buildsys['build-backend'],<N>        buildsys.get('backend-path'),<N>    )<N><N>
"""Build metadata for a project using PEP 517 hooks.<N>"""<N>import argparse<N>import logging<N>import os<N>import shutil<N>import functools<N><N>try:<N>    import importlib.metadata as imp_meta<N>except ImportError:<N>    import importlib_metadata as imp_meta<N><N>
try:<N>    from zipfile import Path<N>except ImportError:<N>    from zipp import Path<N><N>from .envbuild import BuildEnvironment<N>from .wrappers import Pep517HookCaller, quiet_subprocess_runner<N>from .dirtools import tempdir, mkdir_p, dir_to_zipfile<N>from .build import validate_system, load_system, compat_system<N><N>
log = logging.getLogger(__name__)<N><N><N>def _prep_meta(hooks, env, dest):<N>    reqs = hooks.get_requires_for_build_wheel({})<N>    log.info('Got build requires: %s', reqs)<N><N>    env.pip_install(reqs)<N>    log.info('Installed dynamic build dependencies')<N><N>
    with tempdir() as td:<N>        log.info('Trying to build metadata in %s', td)<N>        filename = hooks.prepare_metadata_for_build_wheel(td, {})<N>        source = os.path.join(td, filename)<N>        shutil.move(source, os.path.join(dest, os.path.basename(filename)))<N><N>
<N>def build(source_dir='.', dest=None, system=None):<N>    system = system or load_system(source_dir)<N>    dest = os.path.join(source_dir, dest or 'dist')<N>    mkdir_p(dest)<N>    validate_system(system)<N>    hooks = Pep517HookCaller(<N>        source_dir, system['build-backend'], system.get('backend-path')<N>    )<N><N>
    with hooks.subprocess_runner(quiet_subprocess_runner):<N>        with BuildEnvironment() as env:<N>            env.pip_install(system['requires'])<N>            _prep_meta(hooks, env, dest)<N><N><N>def build_as_zip(builder=build):<N>    with tempdir() as out_dir:<N>        builder(dest=out_dir)<N>        return dir_to_zipfile(out_dir)<N><N>
<N>def load(root):<N>    """<N>    Given a source directory (root) of a package,<N>    return an importlib.metadata.Distribution object<N>    with metadata build from that package.<N>    """<N>    root = os.path.expanduser(root)<N>    system = compat_system(root)<N>    builder = functools.partial(build, source_dir=root, system=system)<N>    path = Path(build_as_zip(builder))<N>    return imp_meta.PathDistribution(path)<N><N>
<N>parser = argparse.ArgumentParser()<N>parser.add_argument(<N>    'source_dir',<N>    help="A directory containing pyproject.toml",<N>)<N>parser.add_argument(<N>    '--out-dir', '-o',<N>    help="Destination in which to save the builds relative to source dir",<N>)<N><N>
import threading<N>from contextlib import contextmanager<N>import os<N>from os.path import abspath, join as pjoin<N>import shutil<N>from subprocess import check_call, check_output, STDOUT<N>import sys<N>from tempfile import mkdtemp<N><N>from . import compat<N>from .in_process import _in_proc_script_path<N><N>
__all__ = [<N>    'BackendUnavailable',<N>    'BackendInvalid',<N>    'HookMissing',<N>    'UnsupportedOperation',<N>    'default_subprocess_runner',<N>    'quiet_subprocess_runner',<N>    'Pep517HookCaller',<N>]<N><N><N>@contextmanager<N>def tempdir():<N>    td = mkdtemp()<N>    try:<N>        yield td<N>    finally:<N>        shutil.rmtree(td)<N><N>
<N>class BackendUnavailable(Exception):<N>    """Will be raised if the backend cannot be imported in the hook process."""<N>    def __init__(self, traceback):<N>        self.traceback = traceback<N><N><N>class BackendInvalid(Exception):<N>    """Will be raised if the backend is invalid."""<N>    def __init__(self, backend_name, backend_path, message):<N>        self.backend_name = backend_name<N>        self.backend_path = backend_path<N>        self.message = message<N><N>
<N>class HookMissing(Exception):<N>    """Will be raised on missing hooks."""<N>    def __init__(self, hook_name):<N>        super(HookMissing, self).__init__(hook_name)<N>        self.hook_name = hook_name<N><N><N>class UnsupportedOperation(Exception):<N>    """May be raised by build_sdist if the backend indicates that it can't."""<N>    def __init__(self, traceback):<N>        self.traceback = traceback<N><N>
<N>def default_subprocess_runner(cmd, cwd=None, extra_environ=None):<N>    """The default method of calling the wrapper subprocess."""<N>    env = os.environ.copy()<N>    if extra_environ:<N>        env.update(extra_environ)<N><N>    check_call(cmd, cwd=cwd, env=env)<N><N>
<N>def quiet_subprocess_runner(cmd, cwd=None, extra_environ=None):<N>    """A method of calling the wrapper subprocess while suppressing output."""<N>    env = os.environ.copy()<N>    if extra_environ:<N>        env.update(extra_environ)<N><N>    check_output(cmd, cwd=cwd, env=env, stderr=STDOUT)<N><N>
<N>def norm_and_check(source_tree, requested):<N>    """Normalise and check a backend path.<N><N>    Ensure that the requested backend path is specified as a relative path,<N>    and resolves to a location under the given source tree.<N><N>    Return an absolute version of the requested path.<N>    """<N>    if os.path.isabs(requested):<N>        raise ValueError("paths must be relative")<N><N>
"""Wrappers to build Python packages using PEP 517 hooks<N>"""<N><N>__version__ = '0.12.0'<N><N>from .wrappers import *  # noqa: F401, F403<N>
"""This is invoked in a subprocess to call the build backend hooks.<N><N>It expects:<N>- Command line args: hook_name, control_dir<N>- Environment variables:<N>      PEP517_BUILD_BACKEND=entry.point:spec<N>      PEP517_BACKEND_PATH=paths (separated with os.pathsep)<N>- control_dir/input.json:<N>  - {"kwargs": {...}}<N><N>
Results:<N>- control_dir/output.json<N>  - {"return_val": ...}<N>"""<N>from glob import glob<N>from importlib import import_module<N>import json<N>import os<N>import os.path<N>from os.path import join as pjoin<N>import re<N>import shutil<N>import sys<N>import traceback<N><N>
# This file is run as a script, and `import compat` is not zip-safe, so we<N># include write_json() and read_json() from compat.py.<N>#<N># Handle reading and writing JSON in UTF-8, on Python 3 and 2.<N><N>if sys.version_info[0] >= 3:<N>    # Python 3<N>    def write_json(obj, path, **kwargs):<N>        with open(path, 'w', encoding='utf-8') as f:<N>            json.dump(obj, f, **kwargs)<N><N>
    def read_json(path):<N>        with open(path, 'r', encoding='utf-8') as f:<N>            return json.load(f)<N><N>else:<N>    # Python 2<N>    def write_json(obj, path, **kwargs):<N>        with open(path, 'wb') as f:<N>            json.dump(obj, f, encoding='utf-8', **kwargs)<N><N>
    def read_json(path):<N>        with open(path, 'rb') as f:<N>            return json.load(f)<N><N><N>class BackendUnavailable(Exception):<N>    """Raised if we cannot import the backend"""<N>    def __init__(self, traceback):<N>        self.traceback = traceback<N><N>
<N>class BackendInvalid(Exception):<N>    """Raised if the backend is invalid"""<N>    def __init__(self, message):<N>        self.message = message<N><N><N>class HookMissing(Exception):<N>    """Raised if a hook is missing and we are not executing the fallback"""<N>    def __init__(self, hook_name=None):<N>        super(HookMissing, self).__init__(hook_name)<N>        self.hook_name = hook_name<N><N>
<N>def contained_in(filename, directory):<N>    """Test if a file is located within the given directory."""<N>    filename = os.path.normcase(os.path.abspath(filename))<N>    directory = os.path.normcase(os.path.abspath(directory))<N>    return os.path.commonprefix([filename, directory]) == directory<N><N>
<N>def _build_backend():<N>    """Find and load the build backend"""<N>    # Add in-tree backend directories to the front of sys.path.<N>    backend_path = os.environ.get('PEP517_BACKEND_PATH')<N>    if backend_path:<N>        extra_pathitems = backend_path.split(os.pathsep)<N>        sys.path[:0] = extra_pathitems<N><N>
    ep = os.environ['PEP517_BUILD_BACKEND']<N>    mod_path, _, obj_path = ep.partition(':')<N>    try:<N>        obj = import_module(mod_path)<N>    except ImportError:<N>        raise BackendUnavailable(traceback.format_exc())<N><N>    if backend_path:<N>        if not any(<N>            contained_in(obj.__file__, path)<N>            for path in extra_pathitems<N>        ):<N>            raise BackendInvalid("Backend was not loaded from backend-path")<N><N>
    if obj_path:<N>        for path_part in obj_path.split('.'):<N>            obj = getattr(obj, path_part)<N>    return obj<N><N><N>def _supported_features():<N>    """Return the list of options features supported by the backend.<N><N>    Returns a list of strings.<N>    The only possible value is 'build_editable'.<N>    """<N>    backend = _build_backend()<N>    features = []<N>    if hasattr(backend, "build_editable"):<N>        features.append("build_editable")<N>    return features<N><N>
<N>def get_requires_for_build_wheel(config_settings):<N>    """Invoke the optional get_requires_for_build_wheel hook<N><N>    Returns [] if the hook is not defined.<N>    """<N>    backend = _build_backend()<N>    try:<N>        hook = backend.get_requires_for_build_wheel<N>    except AttributeError:<N>        return []<N>    else:<N>        return hook(config_settings)<N><N>
<N>def get_requires_for_build_editable(config_settings):<N>    """Invoke the optional get_requires_for_build_editable hook<N><N>    Returns [] if the hook is not defined.<N>    """<N>    backend = _build_backend()<N>    try:<N>        hook = backend.get_requires_for_build_editable<N>    except AttributeError:<N>        return []<N>    else:<N>        return hook(config_settings)<N><N>
"""This is a subpackage because the directory is on sys.path for _in_process.py<N><N>The subpackage should stay as empty as possible to avoid shadowing modules that<N>the backend might import.<N>"""<N>from os.path import dirname, abspath, join as pjoin<N>from contextlib import contextmanager<N><N>
try:<N>    import importlib.resources as resources<N><N>    def _in_proc_script_path():<N>        return resources.path(__package__, '_in_process.py')<N>except ImportError:<N>    @contextmanager<N>    def _in_proc_script_path():<N>        yield pjoin(dirname(abspath(__file__)), '_in_process.py')<N><N><N>
import os<N>import errno<N>import sys<N><N>from pip._vendor import six<N><N><N>def _makedirs_31(path, exist_ok=False):<N>    try:<N>        os.makedirs(path)<N>    except OSError as exc:<N>        if not exist_ok or exc.errno != errno.EEXIST:<N>            raise<N><N>
<N># rely on compatibility behavior until mode considerations<N>#  and exists_ok considerations are disentangled.<N># See https://github.com/pypa/setuptools/pull/1083#issuecomment-315168663<N>needs_makedirs = (<N>    six.PY2 or<N>    (3, 4) <= sys.version_info < (3, 4, 1)<N>)<N>makedirs = _makedirs_31 if needs_makedirs else os.makedirs<N><N><N>
# coding: utf-8<N>"""<N>Package resource API<N>--------------------<N><N>A resource is a logical file contained within a package, or a logical<N>subdirectory thereof.  The package resource API expects resource names<N>to have their path parts separated with ``/``, *not* whatever the local<N>path separator is.  Do not use os.path operations to manipulate resource<N>names being passed into the API.<N><N>
The package resource API is designed to work with normal filesystem packages,<N>.egg files, and unpacked .egg files.  It can also work in a limited way with<N>.zip files and with custom PEP 302 loaders that support the ``get_data()``<N>method.<N>"""<N><N>
from __future__ import absolute_import<N><N>import sys<N>import os<N>import io<N>import time<N>import re<N>import types<N>import zipfile<N>import zipimport<N>import warnings<N>import stat<N>import functools<N>import pkgutil<N>import operator<N>import platform<N>import collections<N>import plistlib<N>import email.parser<N>import errno<N>import tempfile<N>import textwrap<N>import itertools<N>import inspect<N>import ntpath<N>import posixpath<N>from pkgutil import get_importer<N><N>
try:<N>    import _imp<N>except ImportError:<N>    # Python 3.2 compatibility<N>    import imp as _imp<N><N>try:<N>    FileExistsError<N>except NameError:<N>    FileExistsError = OSError<N><N>from pip._vendor import six<N>from pip._vendor.six.moves import urllib, map, filter<N><N>
# capture these to bypass sandboxing<N>from os import utime<N>try:<N>    from os import mkdir, rename, unlink<N>    WRITE_SUPPORT = True<N>except ImportError:<N>    # no write support, probably under GAE<N>    WRITE_SUPPORT = False<N><N>from os import open as os_open<N>from os.path import isdir, split<N><N>
from __future__ import annotations<N><N>import os<N>import re<N>import sys<N>from functools import lru_cache<N><N>from .api import PlatformDirsABC<N><N><N>class Android(PlatformDirsABC):<N>    """<N>    Follows the guidance `from here <https://android.stackexchange.com/a/216132>`_. Makes use of the<N>    `appname <platformdirs.api.PlatformDirsABC.appname>` and<N>    `version <platformdirs.api.PlatformDirsABC.version>`.<N>    """<N><N>
    @property<N>    def user_data_dir(self) -> str:<N>        """:return: data directory tied to the user, e.g. ``/data/user/<userid>/<packagename>/files/<AppName>``"""<N>        return self._append_app_name_and_version(_android_folder(), "files")<N><N>
    @property<N>    def site_data_dir(self) -> str:<N>        """:return: data directory shared by users, same as `user_data_dir`"""<N>        return self.user_data_dir<N><N>    @property<N>    def user_config_dir(self) -> str:<N>        """<N>        :return: config directory tied to the user, e.g. ``/data/user/<userid>/<packagename>/shared_prefs/<AppName>``<N>        """<N>        return self._append_app_name_and_version(_android_folder(), "shared_prefs")<N><N>
    @property<N>    def site_config_dir(self) -> str:<N>        """:return: config directory shared by the users, same as `user_config_dir`"""<N>        return self.user_config_dir<N><N>    @property<N>    def user_cache_dir(self) -> str:<N>        """:return: cache directory tied to the user, e.g. e.g. ``/data/user/<userid>/<packagename>/cache/<AppName>``"""<N>        return self._append_app_name_and_version(_android_folder(), "cache")<N><N>
from __future__ import annotations<N><N>import os<N>import sys<N>from abc import ABC, abstractmethod<N>from pathlib import Path<N><N>if sys.version_info >= (3, 8):  # pragma: no branch<N>    from typing import Literal  # pragma: no cover<N><N><N>class PlatformDirsABC(ABC):<N>    """<N>    Abstract base class for platform directories.<N>    """<N><N>
    def __init__(<N>        self,<N>        appname: str | None = None,<N>        appauthor: str | None | Literal[False] = None,<N>        version: str | None = None,<N>        roaming: bool = False,<N>        multipath: bool = False,<N>        opinion: bool = True,<N>    ):<N>        """<N>        Create a new platform directory.<N><N>
from __future__ import annotations<N><N>import os<N>import sys<N>from configparser import ConfigParser<N>from pathlib import Path<N><N>from .api import PlatformDirsABC<N><N>if sys.platform.startswith("linux"):  # pragma: no branch # no op check, only to please the type checker<N>    from os import getuid<N>else:<N><N>
"""<N>Utilities for determining application-specific dirs. See <https://github.com/platformdirs/platformdirs> for details and<N>usage.<N>"""<N>from __future__ import annotations<N><N>import importlib<N>import os<N>import sys<N>from pathlib import Path<N>from typing import TYPE_CHECKING<N><N>
from __future__ import annotations<N><N>from pip._vendor.platformdirs import PlatformDirs, __version__<N><N>PROPS = (<N>    "user_data_dir",<N>    "user_config_dir",<N>    "user_cache_dir",<N>    "user_state_dir",<N>    "user_log_dir",<N>    "user_documents_dir",<N>    "user_runtime_dir",<N>    "site_data_dir",<N>    "site_config_dir",<N>)<N><N>
<N>def main() -> None:<N>    app_name = "MyApp"<N>    app_author = "MyCompany"<N><N>    print(f"-- platformdirs {__version__} --")<N><N>    print("-- app dirs (with optional 'version')")<N>    dirs = PlatformDirs(app_name, app_author, version="1.0")<N>    for prop in PROPS:<N>        print(f"{prop}: {getattr(dirs, prop)}")<N><N>
    print("\n-- app dirs (without optional 'version')")<N>    dirs = PlatformDirs(app_name, app_author)<N>    for prop in PROPS:<N>        print(f"{prop}: {getattr(dirs, prop)}")<N><N>    print("\n-- app dirs (without optional 'appauthor')")<N>    dirs = PlatformDirs(app_name)<N>    for prop in PROPS:<N>        print(f"{prop}: {getattr(dirs, prop)}")<N><N>
"""<N>    pygments.cmdline<N>    ~~~~~~~~~~~~~~~~<N><N>    Command line interface.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import os<N>import sys<N>import shutil<N>import argparse<N>from textwrap import dedent<N><N>
"""<N>    pygments.console<N>    ~~~~~~~~~~~~~~~~<N><N>    Format colored console output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>esc = "\x1b["<N><N>codes = {}<N>codes[""] = ""<N>codes["reset"] = esc + "39;49;00m"<N><N>
codes["bold"] = esc + "01m"<N>codes["faint"] = esc + "02m"<N>codes["standout"] = esc + "03m"<N>codes["underline"] = esc + "04m"<N>codes["blink"] = esc + "05m"<N>codes["overline"] = esc + "06m"<N><N>dark_colors = ["black", "red", "green", "yellow", "blue",<N>               "magenta", "cyan", "gray"]<N>light_colors = ["brightblack", "brightred", "brightgreen", "brightyellow", "brightblue",<N>                "brightmagenta", "brightcyan", "white"]<N><N>
x = 30<N>for d, l in zip(dark_colors, light_colors):<N>    codes[d] = esc + "%im" % x<N>    codes[l] = esc + "%im" % (60 + x)<N>    x += 1<N><N>del d, l, x<N><N>codes["white"] = codes["bold"]<N><N><N>def reset_color():<N>    return codes["reset"]<N><N>
"""<N>    pygments.formatter<N>    ~~~~~~~~~~~~~~~~~~<N><N>    Base formatter class.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import codecs<N><N>from pip._vendor.pygments.util import get_bool_opt<N>from pip._vendor.pygments.styles import get_style_by_name<N><N>
__all__ = ['Formatter']<N><N><N>def _lookup_style(style):<N>    if isinstance(style, str):<N>        return get_style_by_name(style)<N>    return style<N><N><N>class Formatter:<N>    """<N>    Converts a token stream to text.<N><N>    Options accepted:<N><N>
"""<N>    pygments.lexer<N>    ~~~~~~~~~~~~~~<N><N>    Base lexer classes.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N>import sys<N>import time<N><N>
from pip._vendor.pygments.filter import apply_filters, Filter<N>from pip._vendor.pygments.filters import get_filter_by_name<N>from pip._vendor.pygments.token import Error, Text, Other, _TokenType<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \<N>    make_analysator, Future, guess_decode<N>from pip._vendor.pygments.regexopt import regex_opt<N><N>
__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer',<N>           'LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this',<N>           'default', 'words']<N><N><N>_encoding_map = [(b'\xef\xbb\xbf', 'utf-8'),<N>                 (b'\xff\xfe\0\0', 'utf-32'),<N>                 (b'\0\0\xfe\xff', 'utf-32be'),<N>                 (b'\xff\xfe', 'utf-16'),<N>                 (b'\xfe\xff', 'utf-16be')]<N><N>
_default_analyse = staticmethod(lambda x: 0.0)<N><N><N>class LexerMeta(type):<N>    """<N>    This metaclass automagically converts ``analyse_text`` methods into<N>    static methods which always return float values.<N>    """<N><N>    def __new__(mcs, name, bases, d):<N>        if 'analyse_text' in d:<N>            d['analyse_text'] = make_analysator(d['analyse_text'])<N>        return type.__new__(mcs, name, bases, d)<N><N>
<N>class Lexer(metaclass=LexerMeta):<N>    """<N>    Lexer for a specific language.<N><N>    Basic options recognized:<N>    ``stripnl``<N>        Strip leading and trailing newlines from the input (default: True).<N>    ``stripall``<N>        Strip all leading and trailing whitespace from the input<N>        (default: False).<N>    ``ensurenl``<N>        Make sure that the input ends with a newline (default: True).  This<N>        is required for some lexers that consume input linewise.<N><N>
"""<N>    pygments.modeline<N>    ~~~~~~~~~~~~~~~~~<N><N>    A simple modeline parser (based on pymodeline).<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N><N>
__all__ = ['get_filetype_from_buffer']<N><N><N>modeline_re = re.compile(r'''<N>    (?: vi | vim | ex ) (?: [<=>]? \d* )? :<N>    .* (?: ft | filetype | syn | syntax ) = ( [^:\s]+ )<N>''', re.VERBOSE)<N><N><N>def get_filetype_from_line(l):<N>    m = modeline_re.search(l)<N>    if m:<N>        return m.group(1)<N><N>
<N>def get_filetype_from_buffer(buf, max_lines=5):<N>    """<N>    Scan the buffer for modelines and return filetype if one is found.<N>    """<N>    lines = buf.splitlines()<N>    for l in lines[-1:-max_lines-1:-1]:<N>        ret = get_filetype_from_line(l)<N>        if ret:<N>            return ret<N>    for i in range(max_lines, -1, -1):<N>        if i < len(lines):<N>            ret = get_filetype_from_line(lines[i])<N>            if ret:<N>                return ret<N><N>
"""<N>    pygments.plugin<N>    ~~~~~~~~~~~~~~~<N><N>    Pygments setuptools plugin interface. The methods defined<N>    here also work if setuptools isn't installed but they just<N>    return nothing.<N><N>    lexer plugins::<N><N>        [pygments.lexers]<N>        yourlexer = yourmodule:YourLexer<N><N>
    formatter plugins::<N><N>        [pygments.formatters]<N>        yourformatter = yourformatter:YourFormatter<N>        /.ext = yourformatter:YourFormatter<N><N>    As you can see, you can define extensions for the formatter<N>    with a leading slash.<N><N>
    syntax plugins::<N><N>        [pygments.styles]<N>        yourstyle = yourstyle:YourStyle<N><N>    filter plugin::<N><N>        [pygments.filter]<N>        yourfilter = yourfilter:YourFilter<N><N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N>LEXER_ENTRY_POINT = 'pygments.lexers'<N>FORMATTER_ENTRY_POINT = 'pygments.formatters'<N>STYLE_ENTRY_POINT = 'pygments.styles'<N>FILTER_ENTRY_POINT = 'pygments.filters'<N><N>
<N>def iter_entry_points(group_name):<N>    try:<N>        from pip._vendor import pkg_resources<N>    except (ImportError, OSError):<N>        return []<N><N>    return pkg_resources.iter_entry_points(group_name)<N><N><N>def find_plugin_lexers():<N>    for entrypoint in iter_entry_points(LEXER_ENTRY_POINT):<N>        yield entrypoint.load()<N><N>
<N>def find_plugin_formatters():<N>    for entrypoint in iter_entry_points(FORMATTER_ENTRY_POINT):<N>        yield entrypoint.name, entrypoint.load()<N><N><N>def find_plugin_styles():<N>    for entrypoint in iter_entry_points(STYLE_ENTRY_POINT):<N>        yield entrypoint.name, entrypoint.load()<N><N>
"""<N>    pygments.regexopt<N>    ~~~~~~~~~~~~~~~~~<N><N>    An algorithm that generates optimized regexes for matching long lists of<N>    literal strings.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
import re<N>from re import escape<N>from os.path import commonprefix<N>from itertools import groupby<N>from operator import itemgetter<N><N>CS_ESCAPE = re.compile(r'[\[\^\\\-\]]')<N>FIRST_ELEMENT = itemgetter(0)<N><N><N>def make_charset(letters):<N>    return '[' + CS_ESCAPE.sub(lambda m: '\\' + m.group(), ''.join(letters)) + ']'<N><N>
"""<N>    pygments.scanner<N>    ~~~~~~~~~~~~~~~~<N><N>    This library implements a regex based scanner. Some languages<N>    like Pascal are easy to parse but have some keywords that<N>    depend on the context. Because of this it's impossible to lex<N>    that just by using a regular expression lexer like the<N>    `RegexLexer`.<N><N>
    Have a look at the `DelphiLexer` to get an idea of how to use<N>    this scanner.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N>import re<N><N><N>class EndOfText(RuntimeError):<N>    """<N>    Raise if end of text is reached and the user<N>    tried to call a match function.<N>    """<N><N>
"""<N>    pygments.sphinxext<N>    ~~~~~~~~~~~~~~~~~~<N><N>    Sphinx extension to generate automatic documentation of lexers,<N>    formatters and filters.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
import sys<N><N>from docutils import nodes<N>from docutils.statemachine import ViewList<N>from docutils.parsers.rst import Directive<N>from sphinx.util.nodes import nested_parse_with_titles<N><N><N>MODULEDOC = '''<N>.. module:: %s<N><N>%s<N>%s<N>'''<N><N>
LEXERDOC = '''<N>.. class:: %s<N><N>    :Short names: %s<N>    :Filenames:   %s<N>    :MIME types:  %s<N><N>    %s<N><N>'''<N><N>FMTERDOC = '''<N>.. class:: %s<N><N>    :Short names: %s<N>    :Filenames: %s<N><N>    %s<N><N>'''<N><N>FILTERDOC = '''<N>.. class:: %s<N><N>
    :Name: %s<N><N>    %s<N><N>'''<N><N><N>class PygmentsDoc(Directive):<N>    """<N>    A directive to collect all lexers/formatters/filters and generate<N>    autoclass directives for them.<N>    """<N>    has_content = False<N>    required_arguments = 1<N>    optional_arguments = 0<N>    final_argument_whitespace = False<N>    option_spec = {}<N><N>
"""<N>    pygments.style<N>    ~~~~~~~~~~~~~~<N><N>    Basic style object.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from pip._vendor.pygments.token import Token, STANDARD_TYPES<N><N>
"""<N>    pygments.token<N>    ~~~~~~~~~~~~~~<N><N>    Basic token types and the standard tokens.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N><N>class _TokenType(tuple):<N>    parent = None<N><N>
    def split(self):<N>        buf = []<N>        node = self<N>        while node is not None:<N>            buf.append(node)<N>            node = node.parent<N>        buf.reverse()<N>        return buf<N><N>    def __init__(self, *args):<N>        # no need to call super.__init__<N>        self.subtypes = set()<N><N>
    def __contains__(self, val):<N>        return self is val or (<N>            type(val) is self.__class__ and<N>            val[:len(self)] == self<N>        )<N><N>    def __getattr__(self, val):<N>        if not val or not val[0].isupper():<N>            return tuple.__getattribute__(self, val)<N>        new = _TokenType(self + (val,))<N>        setattr(self, val, new)<N>        self.subtypes.add(new)<N>        new.parent = self<N>        return new<N><N>
    def __repr__(self):<N>        return 'Token' + (self and '.' or '') + '.'.join(self)<N><N>    def __copy__(self):<N>        # These instances are supposed to be singletons<N>        return self<N><N>    def __deepcopy__(self, memo):<N>        # These instances are supposed to be singletons<N>        return self<N><N>
<N>Token = _TokenType()<N><N># Special token types<N>Text = Token.Text<N>Whitespace = Text.Whitespace<N>Escape = Token.Escape<N>Error = Token.Error<N># Text that doesn't belong to this lexer (e.g. HTML in PHP)<N>Other = Token.Other<N><N># Common token types for source code<N>Keyword = Token.Keyword<N>Name = Token.Name<N>Literal = Token.Literal<N>String = Literal.String<N>Number = Literal.Number<N>Punctuation = Token.Punctuation<N>Operator = Token.Operator<N>Comment = Token.Comment<N><N>
# Generic types for non-source code<N>Generic = Token.Generic<N><N># String and some others are not direct children of Token.<N># alias them:<N>Token.Token = Token<N>Token.String = String<N>Token.Number = Number<N><N><N>def is_token_subtype(ttype, other):<N>    """<N>    Return True if ``ttype`` is a subtype of ``other``.<N><N>
    exists for backwards compatibility. use ``ttype in other`` now.<N>    """<N>    return ttype in other<N><N><N>def string_to_tokentype(s):<N>    """<N>    Convert a string into a token type::<N><N>        >>> string_to_token('String.Double')<N>        Token.Literal.String.Double<N>        >>> string_to_token('Token.Literal.Number')<N>        Token.Literal.Number<N>        >>> string_to_token('')<N>        Token<N><N>
    Tokens that are already tokens are returned unchanged:<N><N>        >>> string_to_token(String)<N>        Token.Literal.String<N>    """<N>    if isinstance(s, _TokenType):<N>        return s<N>    if not s:<N>        return Token<N>    node = Token<N>    for item in s.split('.'):<N>        node = getattr(node, item)<N>    return node<N><N>
<N># Map standard token types to short names, used in CSS class naming.<N># If you add a new item, please be sure to run this file to perform<N># a consistency check for duplicate values.<N>STANDARD_TYPES = {<N>    Token:                         '',<N><N>
"""<N>    pygments.unistring<N>    ~~~~~~~~~~~~~~~~~~<N><N>    Strings of all Unicode characters of a certain category.<N>    Used for matching in Unicode-aware languages. Run to regenerate.<N><N>    Inspired by chartypes_create.py from the MoinMoin project.<N><N>
    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>Cc = '\x00-\x1f\x7f-\x9f'<N><N>Cf = '\xad\u0600-\u0605\u061c\u06dd\u070f\u08e2\u180e\u200b-\u200f\u202a-\u202e\u2060-\u2064\u2066-\u206f\ufeff\ufff9-\ufffb\U000110bd\U000110cd\U0001bca0-\U0001bca3\U0001d173-\U0001d17a\U000e0001\U000e0020-\U000e007f'<N><N>
"""<N>    pygments.util<N>    ~~~~~~~~~~~~~<N><N>    Utility functions.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N>from io import TextIOWrapper<N><N>
<N>split_path_re = re.compile(r'[/\\ ]')<N>doctype_lookup_re = re.compile(r'''<N>    <!DOCTYPE\s+(<N>     [a-zA-Z_][a-zA-Z0-9]*<N>     (?: \s+      # optional in HTML5<N>     [a-zA-Z_][a-zA-Z0-9]*\s+<N>     "[^"]*")?<N>     )<N>     [^>]*><N>''', re.DOTALL | re.MULTILINE | re.VERBOSE)<N>tag_re = re.compile(r'<(.+?)(\s.*?)?>.*?</.+?>',<N>                    re.UNICODE | re.IGNORECASE | re.DOTALL | re.MULTILINE)<N>xml_decl_re = re.compile(r'\s*<\?xml[^>]*\?>', re.I)<N><N>
"""<N>    Pygments<N>    ~~~~~~~~<N><N>    Pygments is a syntax highlighting package written in Python.<N><N>    It is a generic syntax highlighter for general use in all kinds of software<N>    such as forum systems, wikis or other applications that need to prettify<N>    source code. Highlights are:<N><N>
    * a wide range of common languages and markup formats is supported<N>    * special attention is paid to details, increasing quality by a fair amount<N>    * support for new languages and formats are added easily<N>    * a number of output formats, presently HTML, LaTeX, RTF, SVG, all image<N>      formats that PIL supports, and ANSI sequences<N>    * it is usable as a command-line tool and as a library<N>    * ... and it highlights even Brainfuck!<N><N>
    The `Pygments master branch`_ is installable with ``easy_install Pygments==dev``.<N><N>    .. _Pygments master branch:<N>       https://github.com/pygments/pygments/archive/master.zip#egg=Pygments-dev<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N>from io import StringIO, BytesIO<N><N>
"""<N>    pygments.__main__<N>    ~~~~~~~~~~~~~~~~~<N><N>    Main entry point for ``python -m pygments``.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import sys<N>from pip._vendor.pygments.cmdline import main<N><N>try:<N>    sys.exit(main(sys.argv))<N>except KeyboardInterrupt:<N>    sys.exit(1)<N>
"""<N>    pygments.filters<N>    ~~~~~~~~~~~~~~~~<N><N>    Module containing filter lookup functions and default<N>    filters.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
import re<N><N>from pip._vendor.pygments.token import String, Comment, Keyword, Name, Error, Whitespace, \<N>    string_to_tokentype<N>from pip._vendor.pygments.filter import Filter<N>from pip._vendor.pygments.util import get_list_opt, get_int_opt, get_bool_opt, \<N>    get_choice_opt, ClassNotFound, OptionError<N>from pip._vendor.pygments.plugin import find_plugin_filters<N><N>
<N>def find_filter_class(filtername):<N>    """Lookup a filter by name. Return None if not found."""<N>    if filtername in FILTERS:<N>        return FILTERS[filtername]<N>    for name, cls in find_plugin_filters():<N>        if name == filtername:<N>            return cls<N>    return None<N><N>
<N>def get_filter_by_name(filtername, **options):<N>    """Return an instantiated filter.<N><N>    Options are passed to the filter initializer if wanted.<N>    Raise a ClassNotFound if not found.<N>    """<N>    cls = find_filter_class(filtername)<N>    if cls:<N>        return cls(**options)<N>    else:<N>        raise ClassNotFound('filter %r not found' % filtername)<N><N>
"""<N>    pygments.formatters.bbcode<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    BBcode formatter.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N><N>from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.util import get_bool_opt<N><N>
__all__ = ['BBCodeFormatter']<N><N><N>class BBCodeFormatter(Formatter):<N>    """<N>    Format tokens with BBcodes. These formatting codes are used by many<N>    bulletin boards, so you can highlight your sourcecode with pygments before<N>    posting it there.<N><N>
    This formatter has no support for background colors and borders, as there<N>    are no common BBcode tags for that.<N><N>    Some board systems (e.g. phpBB) don't support colors in their [code] tag,<N>    so you can't use the highlighting together with that tag.<N>    Text in a [code] tag usually is shown with a monospace font (which this<N>    formatter can do with the ``monofont`` option) and no spaces (which you<N>    need for indentation) are removed.<N><N>
    Additional options accepted:<N><N>    `style`<N>        The style to use, can be a string or a Style subclass (default:<N>        ``'default'``).<N><N>    `codetag`<N>        If set to true, put the output into ``[code]`` tags (default:<N>        ``false``)<N><N>
    `monofont`<N>        If set to true, add a tag to show the code with a monospace font<N>        (default: ``false``).<N>    """<N>    name = 'BBCode'<N>    aliases = ['bbcode', 'bb']<N>    filenames = []<N><N>    def __init__(self, **options):<N>        Formatter.__init__(self, **options)<N>        self._code = get_bool_opt(options, 'codetag', False)<N>        self._mono = get_bool_opt(options, 'monofont', False)<N><N>
"""<N>    pygments.formatters.groff<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for groff output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import math<N>from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt<N><N>
__all__ = ['GroffFormatter']<N><N><N>class GroffFormatter(Formatter):<N>    """<N>    Format tokens with groff escapes to change their color and font style.<N><N>    .. versionadded:: 2.11<N><N>    Additional options accepted:<N><N>    `style`<N>        The style to use, can be a string or a Style subclass (default:<N>        ``'default'``).<N><N>
    `monospaced`<N>        If set to true, monospace font will be used (default: ``true``).<N><N>    `linenos`<N>        If set to true, print the line numbers (default: ``false``).<N><N>    `wrap`<N>        Wrap lines to the specified number of characters. Disabled if set to 0<N>        (default: ``0``).<N>    """<N><N>
    name = 'groff'<N>    aliases = ['groff','troff','roff']<N>    filenames = []<N><N>    def __init__(self, **options):<N>        Formatter.__init__(self, **options)<N><N>        self.monospaced = get_bool_opt(options, 'monospaced', True)<N>        self.linenos = get_bool_opt(options, 'linenos', False)<N>        self._lineno = 0<N>        self.wrap = get_int_opt(options, 'wrap', 0)<N>        self._linelen = 0<N><N>
        self.styles = {}<N>        self._make_styles()<N><N><N>    def _make_styles(self):<N>        regular = '\\f[CR]' if self.monospaced else '\\f[R]'<N>        bold = '\\f[CB]' if self.monospaced else '\\f[B]'<N>        italic = '\\f[CI]' if self.monospaced else '\\f[I]'<N><N>
"""<N>    pygments.formatters.html<N>    ~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for HTML output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import functools<N>import os<N>import sys<N>import os.path<N>from io import StringIO<N><N>
from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.token import Token, Text, STANDARD_TYPES<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt<N><N>try:<N>    import ctags<N>except ImportError:<N>    ctags = None<N><N>
__all__ = ['HtmlFormatter']<N><N><N>_escape_html_table = {<N>    ord('&'): '&amp;',<N>    ord('<'): '&lt;',<N>    ord('>'): '&gt;',<N>    ord('"'): '&quot;',<N>    ord("'"): '&#39;',<N>}<N><N><N>def escape_html(text, table=_escape_html_table):<N>    """Escape &, <, > as well as single and double quotes for HTML."""<N>    return text.translate(table)<N><N>
<N>def webify(color):<N>    if color.startswith('calc') or color.startswith('var'):<N>        return color<N>    else:<N>        return '#' + color<N><N><N>def _get_ttype_class(ttype):<N>    fname = STANDARD_TYPES.get(ttype)<N>    if fname:<N>        return fname<N>    aname = ''<N>    while fname is None:<N>        aname = '-' + ttype[-1] + aname<N>        ttype = ttype.parent<N>        fname = STANDARD_TYPES.get(ttype)<N>    return fname + aname<N><N>
"""<N>    pygments.formatters.img<N>    ~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for Pixmap output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import os<N>import sys<N><N>
from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \<N>    get_choice_opt<N><N>import subprocess<N><N># Import this carefully<N>try:<N>    from PIL import Image, ImageDraw, ImageFont<N>    pil_available = True<N>except ImportError:<N>    pil_available = False<N><N>
try:<N>    import _winreg<N>except ImportError:<N>    try:<N>        import winreg as _winreg<N>    except ImportError:<N>        _winreg = None<N><N>__all__ = ['ImageFormatter', 'GifImageFormatter', 'JpgImageFormatter',<N>           'BmpImageFormatter']<N><N>
<N># For some unknown reason every font calls it something different<N>STYLES = {<N>    'NORMAL':     ['', 'Roman', 'Book', 'Normal', 'Regular', 'Medium'],<N>    'ITALIC':     ['Oblique', 'Italic'],<N>    'BOLD':       ['Bold'],<N>    'BOLDITALIC': ['Bold Oblique', 'Bold Italic'],<N>}<N><N>
# A sane default for modern systems<N>DEFAULT_FONT_NAME_NIX = 'DejaVu Sans Mono'<N>DEFAULT_FONT_NAME_WIN = 'Courier New'<N>DEFAULT_FONT_NAME_MAC = 'Menlo'<N><N><N>class PilNotAvailable(ImportError):<N>    """When Python imaging library is not available"""<N><N>
"""<N>    pygments.formatters.irc<N>    ~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for IRC output<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \<N>    Number, Operator, Generic, Token, Whitespace<N>from pip._vendor.pygments.util import get_choice_opt<N><N>
"""<N>    pygments.formatters.latex<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for LaTeX fancyvrb output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from io import StringIO<N><N>
from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.lexer import Lexer, do_insertions<N>from pip._vendor.pygments.token import Token, STANDARD_TYPES<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt<N><N><N>__all__ = ['LatexFormatter']<N><N>
"""<N>    pygments.formatters.other<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Other formatters: NullFormatter, RawTokenFormatter.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.util import get_choice_opt<N>from pip._vendor.pygments.token import Token<N>from pip._vendor.pygments.console import colorize<N><N>__all__ = ['NullFormatter', 'RawTokenFormatter', 'TestcaseFormatter']<N><N>
<N>class NullFormatter(Formatter):<N>    """<N>    Output the text unchanged without any formatting.<N>    """<N>    name = 'Text only'<N>    aliases = ['text', 'null']<N>    filenames = ['*.txt']<N><N>    def format(self, tokensource, outfile):<N>        enc = self.encoding<N>        for ttype, value in tokensource:<N>            if enc:<N>                outfile.write(value.encode(enc))<N>            else:<N>                outfile.write(value)<N><N>
<N>class RawTokenFormatter(Formatter):<N>    r"""<N>    Format tokens as a raw representation for storing token streams.<N><N>    The format is ``tokentype<TAB>repr(tokenstring)\n``. The output can later<N>    be converted to a token stream with the `RawTokenLexer`, described in the<N>    :doc:`lexer list <lexers>`.<N><N>
    Only two options are accepted:<N><N>    `compress`<N>        If set to ``'gz'`` or ``'bz2'``, compress the output with the given<N>        compression algorithm after encoding (default: ``''``).<N>    `error_color`<N>        If set to a color name, highlight error tokens using that color.  If<N>        set but with no value, defaults to ``'red'``.<N><N>
"""<N>    pygments.formatters.pangomarkup<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for Pango markup output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
from pip._vendor.pygments.formatter import Formatter<N><N><N>__all__ = ['PangoMarkupFormatter']<N><N><N>_escape_table = {<N>    ord('&'): '&amp;',<N>    ord('<'): '&lt;',<N>}<N><N><N>def escape_special_chars(text, table=_escape_table):<N>    """Escape & and < for Pango Markup."""<N>    return text.translate(table)<N><N>
<N>class PangoMarkupFormatter(Formatter):<N>    """<N>    Format tokens as Pango Markup code. It can then be rendered to an SVG.<N><N>    .. versionadded:: 2.9<N>    """<N><N>    name = 'Pango Markup'<N>    aliases = ['pango', 'pangomarkup']<N>    filenames = []<N><N>
"""<N>    pygments.formatters.rtf<N>    ~~~~~~~~~~~~~~~~~~~~~~~<N><N>    A formatter that generates RTF files.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.util import get_int_opt, surrogatepair<N><N>
<N>__all__ = ['RtfFormatter']<N><N><N>class RtfFormatter(Formatter):<N>    """<N>    Format tokens as RTF markup. This formatter automatically outputs full RTF<N>    documents with color information and other useful stuff. Perfect for Copy and<N>    Paste into Microsoft(R) Word(R) documents.<N><N>
    Please note that ``encoding`` and ``outencoding`` options are ignored.<N>    The RTF format is ASCII natively, but handles unicode characters correctly<N>    thanks to escape sequences.<N><N>    .. versionadded:: 0.6<N><N>    Additional options accepted:<N><N>
    `style`<N>        The style to use, can be a string or a Style subclass (default:<N>        ``'default'``).<N><N>    `fontface`<N>        The used font family, for example ``Bitstream Vera Sans``. Defaults to<N>        some generic font which is supposed to have fixed width.<N><N>
    `fontsize`<N>        Size of the font used. Size is specified in half points. The<N>        default is 24 half-points, giving a size 12 font.<N><N>        .. versionadded:: 2.0<N>    """<N>    name = 'RTF'<N>    aliases = ['rtf']<N>    filenames = ['*.rtf']<N><N>
    def __init__(self, **options):<N>        r"""<N>        Additional options accepted:<N><N>        ``fontface``<N>            Name of the font used. Could for example be ``'Courier New'``<N>            to further specify the default which is ``'\fmodern'``. The RTF<N>            specification claims that ``\fmodern`` are "Fixed-pitch serif<N>            and sans serif fonts". Hope every RTF implementation thinks<N>            the same about modern...<N><N>
        """<N>        Formatter.__init__(self, **options)<N>        self.fontface = options.get('fontface') or ''<N>        self.fontsize = get_int_opt(options, 'fontsize', 0)<N><N>    def _escape(self, text):<N>        return text.replace('\\', '\\\\') \<N>                   .replace('{', '\\{') \<N>                   .replace('}', '\\}')<N><N>
"""<N>    pygments.formatters.svg<N>    ~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for SVG output.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.token import Comment<N>from pip._vendor.pygments.util import get_bool_opt, get_int_opt<N><N>
__all__ = ['SvgFormatter']<N><N><N>def escape_html(text):<N>    """Escape &, <, > as well as single and double quotes for HTML."""<N>    return text.replace('&', '&amp;').  \<N>                replace('<', '&lt;').   \<N>                replace('>', '&gt;').   \<N>                replace('"', '&quot;'). \<N>                replace("'", '&#39;')<N><N>
<N>class2style = {}<N><N>class SvgFormatter(Formatter):<N>    """<N>    Format tokens as an SVG graphics file.  This formatter is still experimental.<N>    Each line of code is a ``<text>`` element with explicit ``x`` and ``y``<N>    coordinates containing ``<tspan>`` elements with the individual token styles.<N><N>
    By default, this formatter outputs a full SVG document including doctype<N>    declaration and the ``<svg>`` root element.<N><N>    .. versionadded:: 0.9<N><N>    Additional options accepted:<N><N>    `nowrap`<N>        Don't wrap the SVG ``<text>`` elements in ``<svg><g>`` elements and<N>        don't add a XML declaration and a doctype.  If true, the `fontfamily`<N>        and `fontsize` options are ignored.  Defaults to ``False``.<N><N>
    `fontfamily`<N>        The value to give the wrapping ``<g>`` element's ``font-family``<N>        attribute, defaults to ``"monospace"``.<N><N>    `fontsize`<N>        The value to give the wrapping ``<g>`` element's ``font-size``<N>        attribute, defaults to ``"14px"``.<N><N>
"""<N>    pygments.formatters.terminal<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for terminal output with ANSI sequences.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>
from pip._vendor.pygments.formatter import Formatter<N>from pip._vendor.pygments.token import Keyword, Name, Comment, String, Error, \<N>    Number, Operator, Generic, Token, Whitespace<N>from pip._vendor.pygments.console import ansiformat<N>from pip._vendor.pygments.util import get_choice_opt<N><N>
"""<N>    pygments.formatters.terminal256<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter for 256-color terminal output with ANSI sequences.<N><N>    RGB-to-XTERM color conversion routines adapted from xterm256-conv<N>    tool (http://frexx.de/xterm-256-notes/data/xterm256-conv2.tar.bz2)<N>    by Wolfgang Frisch.<N><N>
"""<N>    pygments.formatters._mapping<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Formatter mapping definitions. This file is generated by itself. Everytime<N>    you change something on a builtin formatter definition, run this script from<N>    the formatters folder to update it.<N><N>
"""<N>    pygments.formatters<N>    ~~~~~~~~~~~~~~~~~~~<N><N>    Pygments formatters.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N>import sys<N>import types<N>import fnmatch<N>from os.path import basename<N><N>
from pip._vendor.pygments.formatters._mapping import FORMATTERS<N>from pip._vendor.pygments.plugin import find_plugin_formatters<N>from pip._vendor.pygments.util import ClassNotFound<N><N>__all__ = ['get_formatter_by_name', 'get_formatter_for_filename',<N>           'get_all_formatters', 'load_formatter_from_file'] + list(FORMATTERS)<N><N>
_formatter_cache = {}  # classes by name<N>_pattern_cache = {}<N><N><N>def _fn_matches(fn, glob):<N>    """Return whether the supplied file name fn matches pattern filename."""<N>    if glob not in _pattern_cache:<N>        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))<N>        return pattern.match(fn)<N>    return _pattern_cache[glob].match(fn)<N><N>
<N>def _load_formatters(module_name):<N>    """Load a formatter (and all others in the module too)."""<N>    mod = __import__(module_name, None, None, ['__all__'])<N>    for formatter_name in mod.__all__:<N>        cls = getattr(mod, formatter_name)<N>        _formatter_cache[cls.name] = cls<N><N>
<N>def get_all_formatters():<N>    """Return a generator for all formatter classes."""<N>    # NB: this returns formatter classes, not info like get_all_lexers().<N>    for info in FORMATTERS.values():<N>        if info[1] not in _formatter_cache:<N>            _load_formatters(info[0])<N>        yield _formatter_cache[info[1]]<N>    for _, formatter in find_plugin_formatters():<N>        yield formatter<N><N>
<N>def find_formatter_class(alias):<N>    """Lookup a formatter by alias.<N><N>    Returns None if not found.<N>    """<N>    for module_name, name, aliases, _, _ in FORMATTERS.values():<N>        if alias in aliases:<N>            if name not in _formatter_cache:<N>                _load_formatters(module_name)<N>            return _formatter_cache[name]<N>    for _, cls in find_plugin_formatters():<N>        if alias in cls.aliases:<N>            return cls<N><N>
<N>def get_formatter_by_name(_alias, **options):<N>    """Lookup and instantiate a formatter by alias.<N><N>    Raises ClassNotFound if not found.<N>    """<N>    cls = find_formatter_class(_alias)<N>    if cls is None:<N>        raise ClassNotFound("no formatter found for name %r" % _alias)<N>    return cls(**options)<N><N>
<N>def load_formatter_from_file(filename, formattername="CustomFormatter",<N>                             **options):<N>    """Load a formatter from a file.<N><N>    This method expects a file located relative to the current working<N>    directory, which contains a class named CustomFormatter. By default,<N>    it expects the Formatter to be named CustomFormatter; you can specify<N>    your own class name as the second argument to this function.<N><N>
"""<N>    pygments.lexers.python<N>    ~~~~~~~~~~~~~~~~~~~~~~<N><N>    Lexers for Python and related languages.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N>import keyword<N><N>
from pip._vendor.pygments.lexer import Lexer, RegexLexer, include, bygroups, using, \<N>    default, words, combined, do_insertions, this<N>from pip._vendor.pygments.util import get_bool_opt, shebang_matches<N>from pip._vendor.pygments.token import Text, Comment, Operator, Keyword, Name, String, \<N>    Number, Punctuation, Generic, Other, Error<N>from pip._vendor.pygments import unistring as uni<N><N>
__all__ = ['PythonLexer', 'PythonConsoleLexer', 'PythonTracebackLexer',<N>           'Python2Lexer', 'Python2TracebackLexer',<N>           'CythonLexer', 'DgLexer', 'NumPyLexer']<N><N>line_re = re.compile('.*?\n')<N><N><N>class PythonLexer(RegexLexer):<N>    """<N>    For `Python <http://www.python.org>`_ source code (version 3.x).<N><N>
"""<N>    pygments.lexers._mapping<N>    ~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    Lexer mapping definitions. This file is generated by itself. Everytime<N>    you change something on a builtin lexer definition, run this script from<N>    the lexers folder to update it.<N><N>
"""<N>    pygments.lexers<N>    ~~~~~~~~~~~~~~~<N><N>    Pygments lexers.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>import re<N>import sys<N>import types<N>import fnmatch<N>from os.path import basename<N><N>
from pip._vendor.pygments.lexers._mapping import LEXERS<N>from pip._vendor.pygments.modeline import get_filetype_from_buffer<N>from pip._vendor.pygments.plugin import find_plugin_lexers<N>from pip._vendor.pygments.util import ClassNotFound, guess_decode<N><N>
COMPAT = {<N>    'Python3Lexer': 'PythonLexer',<N>    'Python3TracebackLexer': 'PythonTracebackLexer',<N>}<N><N>__all__ = ['get_lexer_by_name', 'get_lexer_for_filename', 'find_lexer_class',<N>           'guess_lexer', 'load_lexer_from_file'] + list(LEXERS) + list(COMPAT)<N><N>
_lexer_cache = {}<N>_pattern_cache = {}<N><N><N>def _fn_matches(fn, glob):<N>    """Return whether the supplied file name fn matches pattern filename."""<N>    if glob not in _pattern_cache:<N>        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))<N>        return pattern.match(fn)<N>    return _pattern_cache[glob].match(fn)<N><N>
<N>def _load_lexers(module_name):<N>    """Load a lexer (and all others in the module too)."""<N>    mod = __import__(module_name, None, None, ['__all__'])<N>    for lexer_name in mod.__all__:<N>        cls = getattr(mod, lexer_name)<N>        _lexer_cache[cls.name] = cls<N><N>
<N>def get_all_lexers():<N>    """Return a generator of tuples in the form ``(name, aliases,<N>    filenames, mimetypes)`` of all know lexers.<N>    """<N>    for item in LEXERS.values():<N>        yield item[1:]<N>    for lexer in find_plugin_lexers():<N>        yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes<N><N>
"""<N>    pygments.styles<N>    ~~~~~~~~~~~~~~~<N><N>    Contains built-in styles.<N><N>    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.<N>    :license: BSD, see LICENSE for details.<N>"""<N><N>from pip._vendor.pygments.plugin import find_plugin_styles<N>from pip._vendor.pygments.util import ClassNotFound<N><N>
# actions.py<N><N>from .exceptions import ParseException<N>from .util import col<N><N><N>class OnlyOnce:<N>    """<N>    Wrapper for parse actions, to ensure they are only called once.<N>    """<N><N>    def __init__(self, method_call):<N>        from .core import _trim_arity<N><N>
        self.callable = _trim_arity(method_call)<N>        self.called = False<N><N>    def __call__(self, s, l, t):<N>        if not self.called:<N>            results = self.callable(s, l, t)<N>            self.called = True<N>            return results<N>        raise ParseException(s, l, "OnlyOnce obj called multiple times w/out reset")<N><N>
    def reset(self):<N>        """<N>        Allow the associated parse action to be called once more.<N>        """<N><N>        self.called = False<N><N><N>def match_only_at_col(n):<N>    """<N>    Helper method for defining parse actions that require matching at<N>    a specific column in the input text.<N>    """<N><N>
    def verify_col(strg, locn, toks):<N>        if col(locn, strg) != n:<N>            raise ParseException(strg, locn, "matched token not at column {}".format(n))<N><N>    return verify_col<N><N><N>def replace_with(repl_str):<N>    """<N>    Helper method for common parse actions that simply return<N>    a literal value.  Especially useful when used with<N>    :class:`transform_string<ParserElement.transform_string>` ().<N><N>
    Example::<N><N>        num = Word(nums).set_parse_action(lambda toks: int(toks[0]))<N>        na = one_of("N/A NA").set_parse_action(replace_with(math.nan))<N>        term = na | num<N><N>        OneOrMore(term).parse_string("324 234 N/A 234") # -> [324, 234, nan, 234]<N>    """<N>    return lambda s, l, t: [repl_str]<N><N>
<N>def remove_quotes(s, l, t):<N>    """<N>    Helper parse action for removing quotation marks from parsed<N>    quoted strings.<N><N>    Example::<N><N>        # by default, quotation marks are included in parsed results<N>        quoted_string.parse_string("'Now is the Winter of our Discontent'") # -> ["'Now is the Winter of our Discontent'"]<N><N>
        # use remove_quotes to strip quotation marks from parsed results<N>        quoted_string.set_parse_action(remove_quotes)<N>        quoted_string.parse_string("'Now is the Winter of our Discontent'") # -> ["Now is the Winter of our Discontent"]<N>    """<N>    return t[0][1:-1]<N><N>
<N>def with_attribute(*args, **attr_dict):<N>    """<N>    Helper to create a validating parse action to be used with start<N>    tags created with :class:`make_xml_tags` or<N>    :class:`make_html_tags`. Use ``with_attribute`` to qualify<N>    a starting tag with a required attribute value, to avoid false<N>    matches on common tags such as ``<TD>`` or ``<DIV>``.<N><N>
    Call ``with_attribute`` with a series of attribute names and<N>    values. Specify the list of filter attributes names and values as:<N><N>    - keyword arguments, as in ``(align="right")``, or<N>    - as an explicit dict with ``**`` operator, when an attribute<N>      name is also a Python reserved word, as in ``**{"class":"Customer", "align":"right"}``<N>    - a list of name-value tuples, as in ``(("ns1:class", "Customer"), ("ns2:align", "right"))``<N><N>
    For attribute names with a namespace prefix, you must use the second<N>    form.  Attribute names are matched insensitive to upper/lower case.<N><N>    If just testing for ``class`` (with or without a namespace), use<N>    :class:`with_class`.<N><N>
    To verify that the attribute exists, but without specifying a value,<N>    pass ``with_attribute.ANY_VALUE`` as the value.<N><N>    Example::<N><N>        html = '''<N>            <div><N>            Some text<N>            <div type="grid">1 4 0 1 0</div><N>            <div type="graph">1,3 2,3 1,1</div><N>            <div>this has no type</div><N>            </div><N><N>
        '''<N>        div,div_end = make_html_tags("div")<N><N>        # only match div tag having a type attribute with value "grid"<N>        div_grid = div().set_parse_action(with_attribute(type="grid"))<N>        grid_expr = div_grid + SkipTo(div | div_end)("body")<N>        for grid_header in grid_expr.search_string(html):<N>            print(grid_header.body)<N><N>
        # construct a match with any div tag having a type attribute, regardless of the value<N>        div_any_type = div().set_parse_action(with_attribute(type=with_attribute.ANY_VALUE))<N>        div_expr = div_any_type + SkipTo(div | div_end)("body")<N>        for div_header in div_expr.search_string(html):<N>            print(div_header.body)<N><N>
# common.py<N>from .core import *<N>from .helpers import delimited_list, any_open_tag, any_close_tag<N>from datetime import datetime<N><N><N># some other useful expressions - using lower-case class name since we are really using this as a namespace<N>class pyparsing_common:<N>    """Here are some common low-level expressions that may be useful in<N>    jump-starting parser development:<N><N>
    - numeric forms (:class:`integers<integer>`, :class:`reals<real>`,<N>      :class:`scientific notation<sci_real>`)<N>    - common :class:`programming identifiers<identifier>`<N>    - network addresses (:class:`MAC<mac_address>`,<N>      :class:`IPv4<ipv4_address>`, :class:`IPv6<ipv6_address>`)<N>    - ISO8601 :class:`dates<iso8601_date>` and<N>      :class:`datetime<iso8601_datetime>`<N>    - :class:`UUID<uuid>`<N>    - :class:`comma-separated list<comma_separated_list>`<N>    - :class:`url`<N><N>
    Parse actions:<N><N>    - :class:`convertToInteger`<N>    - :class:`convertToFloat`<N>    - :class:`convertToDate`<N>    - :class:`convertToDatetime`<N>    - :class:`stripHTMLTags`<N>    - :class:`upcaseTokens`<N>    - :class:`downcaseTokens`<N><N>
    Example::<N><N>        pyparsing_common.number.runTests('''<N>            # any int or real number, returned as the appropriate type<N>            100<N>            -100<N>            +100<N>            3.14159<N>            6.02e23<N>            1e-12<N>            ''')<N><N>
        pyparsing_common.fnumber.runTests('''<N>            # any int or real number, returned as float<N>            100<N>            -100<N>            +100<N>            3.14159<N>            6.02e23<N>            1e-12<N>            ''')<N><N>        pyparsing_common.hex_integer.runTests('''<N>            # hex numbers<N>            100<N>            FF<N>            ''')<N><N>
        pyparsing_common.fraction.runTests('''<N>            # fractions<N>            1/2<N>            -3/4<N>            ''')<N><N>        pyparsing_common.mixed_integer.runTests('''<N>            # mixed fractions<N>            1<N>            1/2<N>            -3/4<N>            1-3/4<N>            ''')<N><N>
        import uuid<N>        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))<N>        pyparsing_common.uuid.runTests('''<N>            # uuid<N>            12345678-1234-5678-1234-567812345678<N>            ''')<N><N>    prints::<N><N>        # any int or real number, returned as the appropriate type<N>        100<N>        [100]<N><N>
        -100<N>        [-100]<N><N>        +100<N>        [100]<N><N>        3.14159<N>        [3.14159]<N><N>        6.02e23<N>        [6.02e+23]<N><N>        1e-12<N>        [1e-12]<N><N>        # any int or real number, returned as float<N>        100<N>        [100.0]<N><N>
        -100<N>        [-100.0]<N><N>        +100<N>        [100.0]<N><N>        3.14159<N>        [3.14159]<N><N>        6.02e23<N>        [6.02e+23]<N><N>        1e-12<N>        [1e-12]<N><N>        # hex numbers<N>        100<N>        [256]<N><N>
        FF<N>        [255]<N><N>        # fractions<N>        1/2<N>        [0.5]<N><N>        -3/4<N>        [-0.75]<N><N>        # mixed fractions<N>        1<N>        [1]<N><N>        1/2<N>        [0.5]<N><N>        -3/4<N>        [-0.75]<N><N>        1-3/4<N>        [1.75]<N><N>
        # uuid<N>        12345678-1234-5678-1234-567812345678<N>        [UUID('12345678-1234-5678-1234-567812345678')]<N>    """<N><N>    convert_to_integer = token_map(int)<N>    """<N>    Parse action for converting parsed integers to Python int<N>    """<N><N>
    convert_to_float = token_map(float)<N>    """<N>    Parse action for converting parsed numbers to Python float<N>    """<N><N>    integer = Word(nums).set_name("integer").set_parse_action(convert_to_integer)<N>    """expression that parses an unsigned integer, returns an int"""<N><N>
    hex_integer = (<N>        Word(hexnums).set_name("hex integer").set_parse_action(token_map(int, 16))<N>    )<N>    """expression that parses a hexadecimal integer, returns an int"""<N><N>    signed_integer = (<N>        Regex(r"[+-]?\d+")<N>        .set_name("signed integer")<N>        .set_parse_action(convert_to_integer)<N>    )<N>    """expression that parses an integer with optional leading sign, returns an int"""<N><N>
    fraction = (<N>        signed_integer().set_parse_action(convert_to_float)<N>        + "/"<N>        + signed_integer().set_parse_action(convert_to_float)<N>    ).set_name("fraction")<N>    """fractional expression of an integer divided by an integer, returns a float"""<N>    fraction.add_parse_action(lambda tt: tt[0] / tt[-1])<N><N>
    mixed_integer = (<N>        fraction | signed_integer + Opt(Opt("-").suppress() + fraction)<N>    ).set_name("fraction or mixed integer-fraction")<N>    """mixed integer of the form 'integer - fraction', with optional leading integer, returns float"""<N>    mixed_integer.add_parse_action(sum)<N><N>
# exceptions.py<N><N>import re<N>import sys<N>from typing import Optional<N><N>from .util import col, line, lineno, _collapse_string_to_ranges<N>from .unicode import pyparsing_unicode as ppu<N><N><N>class ExceptionWordUnicode(ppu.Latin1, ppu.LatinA, ppu.LatinB, ppu.Greek, ppu.Cyrillic):<N>    pass<N><N>
<N>_extract_alphanums = _collapse_string_to_ranges(ExceptionWordUnicode.alphanums)<N>_exception_word_extractor = re.compile("([" + _extract_alphanums + "]{1,16})|.")<N><N><N>class ParseBaseException(Exception):<N>    """base exception class for all parsing runtime exceptions"""<N><N>
# results.py<N>from collections.abc import MutableMapping, Mapping, MutableSequence, Iterator<N>import pprint<N>from weakref import ref as wkref<N>from typing import Tuple, Any<N><N>str_type: Tuple[type, ...] = (str, bytes)<N>_generator_type = type((_ for _ in ()))<N><N>
<N>class _ParseResultsWithOffset:<N>    __slots__ = ["tup"]<N><N>    def __init__(self, p1, p2):<N>        self.tup = (p1, p2)<N><N>    def __getitem__(self, i):<N>        return self.tup[i]<N><N>    def __getstate__(self):<N>        return self.tup<N><N>
    def __setstate__(self, *args):<N>        self.tup = args[0]<N><N><N>class ParseResults:<N>    """Structured parse results, to provide multiple means of access to<N>    the parsed data:<N><N>    - as a list (``len(results)``)<N>    - by list index (``results[0], results[1]``, etc.)<N>    - by attribute (``results.<results_name>`` - see :class:`ParserElement.set_results_name`)<N><N>
    Example::<N><N>        integer = Word(nums)<N>        date_str = (integer.set_results_name("year") + '/'<N>                    + integer.set_results_name("month") + '/'<N>                    + integer.set_results_name("day"))<N>        # equivalent form:<N>        # date_str = (integer("year") + '/'<N>        #             + integer("month") + '/'<N>        #             + integer("day"))<N><N>
        # parse_string returns a ParseResults object<N>        result = date_str.parse_string("1999/12/31")<N><N>        def test(s, fn=repr):<N>            print("{} -> {}".format(s, fn(eval(s))))<N>        test("list(result)")<N>        test("result[0]")<N>        test("result['month']")<N>        test("result.day")<N>        test("'month' in result")<N>        test("'minutes' in result")<N>        test("result.dump()", str)<N><N>
    prints::<N><N>        list(result) -> ['1999', '/', '12', '/', '31']<N>        result[0] -> '1999'<N>        result['month'] -> '12'<N>        result.day -> '31'<N>        'month' in result -> True<N>        'minutes' in result -> False<N>        result.dump() -> ['1999', '/', '12', '/', '31']<N>        - day: 31<N>        - month: 12<N>        - year: 1999<N>    """<N><N>
    _null_values: Tuple[Any, ...] = (None, [], "", ())<N><N>    __slots__ = [<N>        "_name",<N>        "_parent",<N>        "_all_names",<N>        "_modal",<N>        "_toklist",<N>        "_tokdict",<N>        "__weakref__",<N>    ]<N><N>    class List(list):<N>        """<N>        Simple wrapper class to distinguish parsed list results that should be preserved<N>        as actual Python lists, instead of being converted to :class:`ParseResults`:<N><N>
            LBRACK, RBRACK = map(pp.Suppress, "[]")<N>            element = pp.Forward()<N>            item = ppc.integer<N>            element_list = LBRACK + pp.delimited_list(element) + RBRACK<N><N>            # add parse actions to convert from ParseResults to actual Python collection types<N>            def as_python_list(t):<N>                return pp.ParseResults.List(t.as_list())<N>            element_list.add_parse_action(as_python_list)<N><N>
            element <<= item | element_list<N><N>            element.run_tests('''<N>                100<N>                [2,3,4]<N>                [[2, 1],3,4]<N>                [(2, 1),3,4]<N>                (2,3,4)<N>                ''', post_parse=lambda s, r: (r[0], type(r[0])))<N><N>
        prints:<N><N>            100<N>            (100, <class 'int'>)<N><N>            [2,3,4]<N>            ([2, 3, 4], <class 'list'>)<N><N>            [[2, 1],3,4]<N>            ([[2, 1], 3, 4], <class 'list'>)<N><N>        (Used internally by :class:`Group` when `aslist=True`.)<N>        """<N><N>
        def __new__(cls, contained=None):<N>            if contained is None:<N>                contained = []<N><N>            if not isinstance(contained, list):<N>                raise TypeError(<N>                    "{} may only be constructed with a list,"<N>                    " not {}".format(cls.__name__, type(contained).__name__)<N>                )<N><N>
            return list.__new__(cls)<N><N>    def __new__(cls, toklist=None, name=None, **kwargs):<N>        if isinstance(toklist, ParseResults):<N>            return toklist<N>        self = object.__new__(cls)<N>        self._name = None<N>        self._parent = None<N>        self._all_names = set()<N><N>
        if toklist is None:<N>            self._toklist = []<N>        elif isinstance(toklist, (list, _generator_type)):<N>            self._toklist = (<N>                [toklist[:]]<N>                if isinstance(toklist, ParseResults.List)<N>                else list(toklist)<N>            )<N>        else:<N>            self._toklist = [toklist]<N>        self._tokdict = dict()<N>        return self<N><N>
# util.py<N>import warnings<N>import types<N>import collections<N>import itertools<N>from functools import lru_cache<N>from typing import List, Union, Iterable<N><N>_bslash = chr(92)<N><N><N>class __config_flags:<N>    """Internal class for defining compatibility and debugging flags"""<N><N>
import railroad<N>from pip._vendor import pyparsing<N>from pip._vendor.pkg_resources import resource_filename<N>from typing import (<N>    List,<N>    Optional,<N>    NamedTuple,<N>    Generic,<N>    TypeVar,<N>    Dict,<N>    Callable,<N>    Set,<N>    Iterable,<N>)<N>from jinja2 import Template<N>from io import StringIO<N>import inspect<N><N>
with open(resource_filename(__name__, "template.jinja2"), encoding="utf-8") as fp:<N>    template = Template(fp.read())<N><N># Note: ideally this would be a dataclass, but we're supporting Python 3.5+ so we can't do this yet<N>NamedDiagram = NamedTuple(<N>    "NamedDiagram",<N>    [("name", str), ("diagram", Optional[railroad.DiagramItem]), ("index", int)],<N>)<N>"""<N>A simple structure for associating a name with a railroad diagram<N>"""<N><N>
T = TypeVar("T")<N><N><N>class EachItem(railroad.Group):<N>    """<N>    Custom railroad item to compose a:<N>    - Group containing a<N>      - OneOrMore containing a<N>        - Choice of the elements in the Each<N>    with the group label indicating that all must be matched<N>    """<N><N>
    all_label = "[ALL]"<N><N>    def __init__(self, *items):<N>        choice_item = railroad.Choice(len(items) - 1, *items)<N>        one_or_more_item = railroad.OneOrMore(item=choice_item)<N>        super().__init__(one_or_more_item, label=self.all_label)<N><N>
<N>class AnnotatedItem(railroad.Group):<N>    """<N>    Simple subclass of Group that creates an annotation label<N>    """<N><N>    def __init__(self, label: str, item):<N>        super().__init__(item=item, label="[{}]".format(label))<N><N><N>class EditablePartial(Generic[T]):<N>    """<N>    Acts like a functools.partial, but can be edited. In other words, it represents a type that hasn't yet been<N>    constructed.<N>    """<N><N>
    # We need this here because the railroad constructors actually transform the data, so can't be called until the<N>    # entire tree is assembled<N><N>    def __init__(self, func: Callable[..., T], args: list, kwargs: dict):<N>        self.func = func<N>        self.args = args<N>        self.kwargs = kwargs<N><N>
    @classmethod<N>    def from_call(cls, func: Callable[..., T], *args, **kwargs) -> "EditablePartial[T]":<N>        """<N>        If you call this function in the same way that you would call the constructor, it will store the arguments<N>        as you expect. For example EditablePartial.from_call(Fraction, 1, 3)() == Fraction(1, 3)<N>        """<N>        return EditablePartial(func=func, args=list(args), kwargs=kwargs)<N><N>
    @property<N>    def name(self):<N>        return self.kwargs["name"]<N><N>    def __call__(self) -> T:<N>        """<N>        Evaluate the partial and return the result<N>        """<N>        args = self.args.copy()<N>        kwargs = self.kwargs.copy()<N><N>
        # This is a helpful hack to allow you to specify varargs parameters (e.g. *args) as keyword args (e.g.<N>        # args=['list', 'of', 'things'])<N>        arg_spec = inspect.getfullargspec(self.func)<N>        if arg_spec.varargs in self.kwargs:<N>            args += kwargs.pop(arg_spec.varargs)<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.api<N>~~~~~~~~~~~~<N><N>This module implements the Requests API.<N><N>:copyright: (c) 2012 by Kenneth Reitz.<N>:license: Apache2, see LICENSE for more details.<N>"""<N><N>from . import sessions<N><N><N>def request(method, url, **kwargs):<N>    """Constructs and sends a :class:`Request <Request>`.<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.auth<N>~~~~~~~~~~~~~<N><N>This module contains the authentication handlers for Requests.<N>"""<N><N>import os<N>import re<N>import time<N>import hashlib<N>import threading<N>import warnings<N><N>from base64 import b64encode<N><N>
from .compat import urlparse, str, basestring<N>from .cookies import extract_cookies_to_jar<N>from ._internal_utils import to_native_string<N>from .utils import parse_dict_header<N><N>CONTENT_TYPE_FORM_URLENCODED = 'application/x-www-form-urlencoded'<N>CONTENT_TYPE_MULTI_PART = 'multipart/form-data'<N><N>
#!/usr/bin/env python<N># -*- coding: utf-8 -*-<N><N>"""<N>requests.certs<N>~~~~~~~~~~~~~~<N><N>This module returns the preferred default CA certificate bundle. There is<N>only one — the one from the certifi package.<N><N>If you are packaging Requests, e.g., for a Linux distribution or a managed<N>environment, you can change the definition of where() to return a separately<N>packaged CA bundle.<N>"""<N>from pip._vendor.certifi import where<N><N>if __name__ == '__main__':<N>    print(where())<N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.compat<N>~~~~~~~~~~~~~~~<N><N>This module handles import compatibility issues between Python 2 and<N>Python 3.<N>"""<N><N>from pip._vendor import chardet<N><N>import sys<N><N># -------<N># Pythons<N># -------<N><N>
# Syntax sugar.<N>_ver = sys.version_info<N><N>#: Python 2.x?<N>is_py2 = (_ver[0] == 2)<N><N>#: Python 3.x?<N>is_py3 = (_ver[0] == 3)<N><N># Note: We've patched out simplejson support in pip because it prevents<N>#       upgrading simplejson on Windows.<N># try:<N>#     import simplejson as json<N># except (ImportError, SyntaxError):<N>#     # simplejson does not support Python 3.2, it throws a SyntaxError<N>#     # because of u'...' Unicode literals.<N>import json<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.cookies<N>~~~~~~~~~~~~~~~~<N><N>Compatibility code to be able to use `cookielib.CookieJar` with requests.<N><N>requests.utils imports from here, so be careful with imports.<N>"""<N><N>import copy<N>import time<N>import calendar<N><N>
from ._internal_utils import to_native_string<N>from .compat import cookielib, urlparse, urlunparse, Morsel, MutableMapping<N><N>try:<N>    import threading<N>except ImportError:<N>    import dummy_threading as threading<N><N><N>class MockRequest(object):<N>    """Wraps a `requests.Request` to mimic a `urllib2.Request`.<N><N>
    The code in `cookielib.CookieJar` expects this interface in order to correctly<N>    manage cookie policies, i.e., determine whether a cookie can be set, given the<N>    domains of the request and the cookie.<N><N>    The original request object is read-only. The client is responsible for collecting<N>    the new headers via `get_new_headers()` and interpreting them appropriately. You<N>    probably want `get_cookie_header`, defined below.<N>    """<N><N>
    def __init__(self, request):<N>        self._r = request<N>        self._new_headers = {}<N>        self.type = urlparse(self._r.url).scheme<N><N>    def get_type(self):<N>        return self.type<N><N>    def get_host(self):<N>        return urlparse(self._r.url).netloc<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.exceptions<N>~~~~~~~~~~~~~~~~~~~<N><N>This module contains the set of Requests' exceptions.<N>"""<N>from pip._vendor.urllib3.exceptions import HTTPError as BaseHTTPError<N><N>from .compat import JSONDecodeError as CompatJSONDecodeError<N><N>
"""Module containing bug report helper(s)."""<N>from __future__ import print_function<N><N>import json<N>import platform<N>import sys<N>import ssl<N><N>from pip._vendor import idna<N>from pip._vendor import urllib3<N><N>from . import __version__ as requests_version<N><N>
charset_normalizer = None<N><N>try:<N>    from pip._vendor import chardet<N>except ImportError:<N>    chardet = None<N><N>try:<N>    from pip._vendor.urllib3.contrib import pyopenssl<N>except ImportError:<N>    pyopenssl = None<N>    OpenSSL = None<N>    cryptography = None<N>else:<N>    import OpenSSL<N>    import cryptography<N><N>
<N>def _implementation():<N>    """Return a dict with the Python implementation and version.<N><N>    Provide both the name and the version of the Python implementation<N>    currently running. For example, on CPython 2.7.5 it will return<N>    {'name': 'CPython', 'version': '2.7.5'}.<N><N>
    This function works best on CPython and PyPy: in particular, it probably<N>    doesn't work for Jython or IronPython. Future investigation should be done<N>    to work out the correct shape of the code for those platforms.<N>    """<N>    implementation = platform.python_implementation()<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.hooks<N>~~~~~~~~~~~~~~<N><N>This module provides the capabilities for the Requests hooks system.<N><N>Available hooks:<N><N>``response``:<N>    The response generated from a Request.<N>"""<N>HOOKS = ['response']<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.models<N>~~~~~~~~~~~~~~~<N><N>This module contains the primary objects that power Requests.<N>"""<N><N>import datetime<N>import sys<N><N># Import encoding now, to avoid implicit import later.<N># Implicit import within threads may cause LookupError when standard library is in a ZIP,<N># such as in Embedded Python. See https://github.com/psf/requests/issues/3578.<N>import encodings.idna<N><N>
from pip._vendor.urllib3.fields import RequestField<N>from pip._vendor.urllib3.filepost import encode_multipart_formdata<N>from pip._vendor.urllib3.util import parse_url<N>from pip._vendor.urllib3.exceptions import (<N>    DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.sessions<N>~~~~~~~~~~~~~~~~~<N><N>This module provides a Session object to manage and persist settings across<N>requests (cookies, auth, proxies).<N>"""<N>import os<N>import sys<N>import time<N>from datetime import timedelta<N>from collections import OrderedDict<N><N>
# -*- coding: utf-8 -*-<N><N>r"""<N>The ``codes`` object defines a mapping from common names for HTTP statuses<N>to their numerical codes, accessible either as attributes or as dictionary<N>items.<N><N>Example::<N><N>    >>> import requests<N>    >>> requests.codes['temporary_redirect']<N>    307<N>    >>> requests.codes.teapot<N>    418<N>    >>> requests.codes['\o/']<N>    200<N><N>
Some codes have multiple names, and both upper- and lower-case versions of<N>the names are allowed. For example, ``codes.ok``, ``codes.OK``, and<N>``codes.okay`` all correspond to the HTTP status code 200.<N>"""<N><N>from .structures import LookupDict<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.structures<N>~~~~~~~~~~~~~~~~~~~<N><N>Data structures that power Requests.<N>"""<N><N>from collections import OrderedDict<N><N>from .compat import Mapping, MutableMapping<N><N><N>class CaseInsensitiveDict(MutableMapping):<N>    """A case-insensitive ``dict``-like object.<N><N>
    Implements all methods and operations of<N>    ``MutableMapping`` as well as dict's ``copy``. Also<N>    provides ``lower_items``.<N><N>    All keys are expected to be strings. The structure remembers the<N>    case of the last key to be set, and ``iter(instance)``,<N>    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``<N>    will contain case-sensitive keys. However, querying and contains<N>    testing is case insensitive::<N><N>
        cid = CaseInsensitiveDict()<N>        cid['Accept'] = 'application/json'<N>        cid['aCCEPT'] == 'application/json'  # True<N>        list(cid) == ['Accept']  # True<N><N>    For example, ``headers['content-encoding']`` will return the<N>    value of a ``'Content-Encoding'`` response header, regardless<N>    of how the header name was originally stored.<N><N>
    If the constructor, ``.update``, or equality comparison<N>    operations are given keys that have equal ``.lower()``s, the<N>    behavior is undefined.<N>    """<N><N>    def __init__(self, data=None, **kwargs):<N>        self._store = OrderedDict()<N>        if data is None:<N>            data = {}<N>        self.update(data, **kwargs)<N><N>
    def __setitem__(self, key, value):<N>        # Use the lowercased key for lookups, but store the actual<N>        # key alongside the value.<N>        self._store[key.lower()] = (key, value)<N><N>    def __getitem__(self, key):<N>        return self._store[key.lower()][1]<N><N>
    def __delitem__(self, key):<N>        del self._store[key.lower()]<N><N>    def __iter__(self):<N>        return (casedkey for casedkey, mappedvalue in self._store.values())<N><N>    def __len__(self):<N>        return len(self._store)<N><N>    def lower_items(self):<N>        """Like iteritems(), but with all lowercase keys."""<N>        return (<N>            (lowerkey, keyval[1])<N>            for (lowerkey, keyval)<N>            in self._store.items()<N>        )<N><N>
    def __eq__(self, other):<N>        if isinstance(other, Mapping):<N>            other = CaseInsensitiveDict(other)<N>        else:<N>            return NotImplemented<N>        # Compare insensitively<N>        return dict(self.lower_items()) == dict(other.lower_items())<N><N>
    # Copy is required<N>    def copy(self):<N>        return CaseInsensitiveDict(self._store.values())<N><N>    def __repr__(self):<N>        return str(dict(self.items()))<N><N><N>class LookupDict(dict):<N>    """Dictionary lookup object."""<N><N>    def __init__(self, name=None):<N>        self.name = name<N>        super(LookupDict, self).__init__()<N><N>
    def __repr__(self):<N>        return '<lookup \'%s\'>' % (self.name)<N><N>    def __getitem__(self, key):<N>        # We allow fall-through here, so values default to None<N><N>        return self.__dict__.get(key, None)<N><N>    def get(self, key, default=None):<N>        return self.__dict__.get(key, default)<N><N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests._internal_utils<N>~~~~~~~~~~~~~~<N><N>Provides utility functions that are consumed internally by Requests<N>which depend on extremely few external helpers (such as compat)<N>"""<N><N>from .compat import is_py2, builtin_str, str<N><N>
<N>def to_native_string(string, encoding='ascii'):<N>    """Given a string object, regardless of type, returns a representation of<N>    that string in the native string type, encoding and decoding where<N>    necessary. This assumes ASCII unless told otherwise.<N>    """<N>    if isinstance(string, builtin_str):<N>        out = string<N>    else:<N>        if is_py2:<N>            out = string.encode(encoding)<N>        else:<N>            out = string.decode(encoding)<N><N>
    return out<N><N><N>def unicode_is_ascii(u_string):<N>    """Determine if unicode string only contains ASCII characters.<N><N>    :param str u_string: unicode string to check. Must be unicode<N>        and not Python 2 `str`.<N>    :rtype: bool<N>    """<N>    assert isinstance(u_string, str)<N>    try:<N>        u_string.encode('ascii')<N>        return True<N>    except UnicodeEncodeError:<N>        return False<N><N><N>
# -*- coding: utf-8 -*-<N><N>#   __<N>#  /__)  _  _     _   _ _/   _<N># / (   (- (/ (/ (- _)  /  _)<N>#          /<N><N>"""<N>Requests HTTP Library<N>~~~~~~~~~~~~~~~~~~~~~<N><N>Requests is an HTTP library, written in Python, for human beings.<N>Basic GET usage:<N><N>
   >>> import requests<N>   >>> r = requests.get('https://www.python.org')<N>   >>> r.status_code<N>   200<N>   >>> b'Python is a programming language' in r.content<N>   True<N><N>... or POST:<N><N>   >>> payload = dict(key1='value1', key2='value2')<N>   >>> r = requests.post('https://httpbin.org/post', data=payload)<N>   >>> print(r.text)<N>   {<N>     ...<N>     "form": {<N>       "key1": "value1",<N>       "key2": "value2"<N>     },<N>     ...<N>   }<N><N>
The other HTTP methods are supported - see `requests.api`. Full documentation<N>is at <https://requests.readthedocs.io>.<N><N>:copyright: (c) 2017 by Kenneth Reitz.<N>:license: Apache 2.0, see LICENSE for more details.<N>"""<N><N>from pip._vendor import urllib3<N>import warnings<N>from .exceptions import RequestsDependencyWarning<N><N>
charset_normalizer_version = None<N><N>try:<N>    from pip._vendor.chardet import __version__ as chardet_version<N>except ImportError:<N>    chardet_version = None<N><N>def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):<N>    urllib3_version = urllib3_version.split('.')<N>    assert urllib3_version != ['dev']  # Verify urllib3 isn't installed from git.<N><N>
    # Sometimes, urllib3 only reports its version as 16.1.<N>    if len(urllib3_version) == 2:<N>        urllib3_version.append('0')<N><N>    # Check urllib3 for compatibility.<N>    major, minor, patch = urllib3_version  # noqa: F811<N>    major, minor, patch = int(major), int(minor), int(patch)<N>    # urllib3 >= 1.21.1, <= 1.26<N>    assert major == 1<N>    assert minor >= 21<N>    assert minor <= 26<N><N>
# .-. .-. .-. . . .-. .-. .-. .-.<N># |(  |-  |.| | | |-  `-.  |  `-.<N># ' ' `-' `-`.`-' `-' `-'  '  `-'<N><N>__title__ = 'requests'<N>__description__ = 'Python HTTP for Humans.'<N>__url__ = 'https://requests.readthedocs.io'<N>__version__ = '2.27.1'<N>__build__ = 0x022701<N>__author__ = 'Kenneth Reitz'<N>__author_email__ = 'me@kennethreitz.org'<N>__license__ = 'Apache 2.0'<N>__copyright__ = 'Copyright 2022 Kenneth Reitz'<N>__cake__ = u'\u2728 \U0001f370 \u2728'<N>
class AbstractProvider(object):<N>    """Delegate class to provide requirement interface for the resolver."""<N><N>    def identify(self, requirement_or_candidate):<N>        """Given a requirement, return an identifier for it.<N><N>        This is used to identify a requirement, e.g. whether two requirements<N>        should have their specifier parts merged.<N>        """<N>        raise NotImplementedError<N><N>
    def get_preference(<N>        self,<N>        identifier,<N>        resolutions,<N>        candidates,<N>        information,<N>        backtrack_causes,<N>    ):<N>        """Produce a sort key for given requirement based on preference.<N><N>        The preference is defined as "I think this requirement should be<N>        resolved first". The lower the return value is, the more preferred<N>        this group of arguments is.<N><N>
class BaseReporter(object):<N>    """Delegate class to provider progress reporting for the resolver."""<N><N>    def starting(self):<N>        """Called before the resolution actually starts."""<N><N>    def starting_round(self, index):<N>        """Called before each round of resolution starts.<N><N>
        The index is zero-based.<N>        """<N><N>    def ending_round(self, index, state):<N>        """Called before each round of resolution ends.<N><N>        This is NOT called if the resolution ends at this round. Use `ending`<N>        if you want to report finalization. The index is zero-based.<N>        """<N><N>
import collections<N>import operator<N><N>from .providers import AbstractResolver<N>from .structs import DirectedGraph, IteratorMapping, build_iter_view<N><N>RequirementInformation = collections.namedtuple(<N>    "RequirementInformation", ["requirement", "parent"]<N>)<N><N>
<N>class ResolverException(Exception):<N>    """A base class for all exceptions raised by this module.<N><N>    Exceptions derived by this class should all be handled in this module. Any<N>    bubbling pass the resolver should be treated as a bug.<N>    """<N><N>
<N>class RequirementsConflicted(ResolverException):<N>    def __init__(self, criterion):<N>        super(RequirementsConflicted, self).__init__(criterion)<N>        self.criterion = criterion<N><N>    def __str__(self):<N>        return "Requirements conflict: {}".format(<N>            ", ".join(repr(r) for r in self.criterion.iter_requirement()),<N>        )<N><N>
<N>class InconsistentCandidate(ResolverException):<N>    def __init__(self, candidate, criterion):<N>        super(InconsistentCandidate, self).__init__(candidate, criterion)<N>        self.candidate = candidate<N>        self.criterion = criterion<N><N>
    def __str__(self):<N>        return "Provided candidate {!r} does not satisfy {}".format(<N>            self.candidate,<N>            ", ".join(repr(r) for r in self.criterion.iter_requirement()),<N>        )<N><N><N>class Criterion(object):<N>    """Representation of possible resolution results of a package.<N><N>
import itertools<N><N>from .compat import collections_abc<N><N><N>class DirectedGraph(object):<N>    """A graph structure with directed edges."""<N><N>    def __init__(self):<N>        self._vertices = set()<N>        self._forwards = {}  # <key> -> Set[<key>]<N>        self._backwards = {}  # <key> -> Set[<key>]<N><N>
__all__ = [<N>    "__version__",<N>    "AbstractProvider",<N>    "AbstractResolver",<N>    "BaseReporter",<N>    "InconsistentCandidate",<N>    "Resolver",<N>    "RequirementsConflicted",<N>    "ResolutionError",<N>    "ResolutionImpossible",<N>    "ResolutionTooDeep",<N>]<N><N>
__version__ = "0.8.1"<N><N><N>from .providers import AbstractProvider, AbstractResolver<N>from .reporters import BaseReporter<N>from .resolvers import (<N>    InconsistentCandidate,<N>    RequirementsConflicted,<N>    ResolutionError,<N>    ResolutionImpossible,<N>    ResolutionTooDeep,<N>    Resolver,<N>)<N><N><N>
__all__ = ["Mapping", "Sequence"]<N><N>try:<N>    from collections.abc import Mapping, Sequence<N>except ImportError:<N>    from collections import Mapping, Sequence<N>
from abc import ABC<N><N><N>class RichRenderable(ABC):<N>    """An abstract base class for Rich renderables.<N><N>    Note that there is no need to extend this class, the intended use is to check if an<N>    object supports the Rich renderable protocol. For example::<N><N>
        if isinstance(my_object, RichRenderable):<N>            console.print(my_object)<N><N>    """<N><N>    @classmethod<N>    def __subclasshook__(cls, other: type) -> bool:<N>        """Check if this class supports the rich render protocol."""<N>        return hasattr(other, "__rich_console__") or hasattr(other, "__rich__")<N><N>
<N>if __name__ == "__main__":  # pragma: no cover<N>    from pip._vendor.rich.text import Text<N><N>    t = Text()<N>    print(isinstance(Text, RichRenderable))<N>    print(isinstance(t, RichRenderable))<N><N>    class Foo:<N>        pass<N><N>    f = Foo()<N>    print(isinstance(f, RichRenderable))<N>    print(isinstance("", RichRenderable))<N><N><N>
import sys<N>from itertools import chain<N>from typing import TYPE_CHECKING, Iterable, Optional<N><N>if sys.version_info >= (3, 8):<N>    from typing import Literal<N>else:<N>    from pip._vendor.typing_extensions import Literal  # pragma: no cover<N><N>
from .constrain import Constrain<N>from .jupyter import JupyterMixin<N>from .measure import Measurement<N>from .segment import Segment<N>from .style import StyleType<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderableType, RenderResult<N><N>
AlignMethod = Literal["left", "center", "right"]<N>VerticalAlignMethod = Literal["top", "middle", "bottom"]<N>AlignValues = AlignMethod  # TODO: deprecate AlignValues<N><N><N>class Align(JupyterMixin):<N>    """Align a renderable by adding spaces if necessary.<N><N>
from contextlib import suppress<N>import re<N>from typing import Iterable, NamedTuple<N><N>from .color import Color<N>from .style import Style<N>from .text import Text<N><N>re_ansi = re.compile(r"(?:\x1b\[(.*?)m)|(?:\x1b\](.*?)\x1b\\)")<N>re_csi = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")<N><N>
<N>class _AnsiToken(NamedTuple):<N>    """Result of ansi tokenized string."""<N><N>    plain: str = ""<N>    sgr: str = ""<N>    osc: str = ""<N><N><N>def _ansi_tokenize(ansi_text: str) -> Iterable[_AnsiToken]:<N>    """Tokenize a string in to plain text and ANSI codes.<N><N>
    Args:<N>        ansi_text (str): A String containing ANSI codes.<N><N>    Yields:<N>        AnsiToken: A named tuple of (plain, sgr, osc)<N>    """<N><N>    def remove_csi(ansi_text: str) -> str:<N>        """Remove unknown CSI sequences."""<N>        return re_csi.sub("", ansi_text)<N><N>
    position = 0<N>    for match in re_ansi.finditer(ansi_text):<N>        start, end = match.span(0)<N>        sgr, osc = match.groups()<N>        if start > position:<N>            yield _AnsiToken(remove_csi(ansi_text[position:start]))<N>        yield _AnsiToken("", sgr, osc)<N>        position = end<N>    if position < len(ansi_text):<N>        yield _AnsiToken(remove_csi(ansi_text[position:]))<N><N>
import platform<N>import re<N>from colorsys import rgb_to_hls<N>from enum import IntEnum<N>from functools import lru_cache<N>from typing import TYPE_CHECKING, NamedTuple, Optional, Tuple<N><N>from ._palettes import EIGHT_BIT_PALETTE, STANDARD_PALETTE, WINDOWS_PALETTE<N>from .color_triplet import ColorTriplet<N>from .repr import rich_repr, Result<N>from .terminal_theme import DEFAULT_TERMINAL_THEME<N><N>
if TYPE_CHECKING:  # pragma: no cover<N>    from .terminal_theme import TerminalTheme<N>    from .text import Text<N><N><N>WINDOWS = platform.system() == "Windows"<N><N><N>class ColorSystem(IntEnum):<N>    """One of the 3 color system supported by terminals."""<N><N>
    STANDARD = 1<N>    EIGHT_BIT = 2<N>    TRUECOLOR = 3<N>    WINDOWS = 4<N><N>    def __repr__(self) -> str:<N>        return f"ColorSystem.{self.name}"<N><N><N>class ColorType(IntEnum):<N>    """Type of color stored in Color class."""<N><N>    DEFAULT = 0<N>    STANDARD = 1<N>    EIGHT_BIT = 2<N>    TRUECOLOR = 3<N>    WINDOWS = 4<N><N>
from typing import NamedTuple, Tuple<N><N><N>class ColorTriplet(NamedTuple):<N>    """The red, green, and blue components of a color."""<N><N>    red: int<N>    """Red component in 0 to 255 range."""<N>    green: int<N>    """Green component in 0 to 255 range."""<N>    blue: int<N>    """Blue component in 0 to 255 range."""<N><N>
    @property<N>    def hex(self) -> str:<N>        """get the color triplet in CSS style."""<N>        red, green, blue = self<N>        return f"#{red:02x}{green:02x}{blue:02x}"<N><N>    @property<N>    def rgb(self) -> str:<N>        """The color in RGB format.<N><N>
        Returns:<N>            str: An rgb color, e.g. ``"rgb(100,23,255)"``.<N>        """<N>        red, green, blue = self<N>        return f"rgb({red},{green},{blue})"<N><N>    @property<N>    def normalized(self) -> Tuple[float, float, float]:<N>        """Convert components into floats between 0 and 1.<N><N>
from collections import defaultdict<N>from itertools import chain<N>from operator import itemgetter<N>from typing import Dict, Iterable, List, Optional, Tuple<N><N>from .align import Align, AlignMethod<N>from .console import Console, ConsoleOptions, RenderableType, RenderResult<N>from .constrain import Constrain<N>from .measure import Measurement<N>from .padding import Padding, PaddingDimensions<N>from .table import Table<N>from .text import TextType<N>from .jupyter import JupyterMixin<N><N>
from typing import Optional, TYPE_CHECKING<N><N>from .jupyter import JupyterMixin<N>from .measure import Measurement<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderableType, RenderResult<N><N><N>class Constrain(JupyterMixin):<N>    """Constrain the width of a renderable to a given number of characters.<N><N>
    Args:<N>        renderable (RenderableType): A renderable object.<N>        width (int, optional): The maximum width (in characters) to render. Defaults to 80.<N>    """<N><N>    def __init__(self, renderable: "RenderableType", width: Optional[int] = 80) -> None:<N>        self.renderable = renderable<N>        self.width = width<N><N>
    def __rich_console__(<N>        self, console: "Console", options: "ConsoleOptions"<N>    ) -> "RenderResult":<N>        if self.width is None:<N>            yield self.renderable<N>        else:<N>            child_options = options.update_width(min(self.width, options.max_width))<N>            yield from console.render(self.renderable, child_options)<N><N>
    def __rich_measure__(<N>        self, console: "Console", options: "ConsoleOptions"<N>    ) -> "Measurement":<N>        if self.width is not None:<N>            options = options.update_width(self.width)<N>        measurement = Measurement.get(console, options, self.renderable)<N>        return measurement<N><N><N>
from itertools import zip_longest<N>from typing import (<N>    Iterator,<N>    Iterable,<N>    List,<N>    Optional,<N>    Union,<N>    overload,<N>    TypeVar,<N>    TYPE_CHECKING,<N>)<N><N>if TYPE_CHECKING:<N>    from .console import (<N>        Console,<N>        ConsoleOptions,<N>        JustifyMethod,<N>        OverflowMethod,<N>        RenderResult,<N>        RenderableType,<N>    )<N>    from .text import Text<N><N>
from .cells import cell_len<N>from .measure import Measurement<N><N>T = TypeVar("T")<N><N><N>class Renderables:<N>    """A list subclass which renders its contents to the console."""<N><N>    def __init__(<N>        self, renderables: Optional[Iterable["RenderableType"]] = None<N>    ) -> None:<N>        self._renderables: List["RenderableType"] = (<N>            list(renderables) if renderables is not None else []<N>        )<N><N>
from typing import Any, Callable, Dict, Iterable, List, TYPE_CHECKING, Union<N><N>from .segment import ControlCode, ControlType, Segment<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderResult<N><N>STRIP_CONTROL_CODES = [<N>    8,  # Backspace<N>    11,  # Vertical tab<N>    12,  # Form feed<N>    13,  # Carriage return<N>]<N>_CONTROL_TRANSLATE = {_codepoint: None for _codepoint in STRIP_CONTROL_CODES}<N><N>
if __name__ == "__main__":  # pragma: no cover<N>    from pip._vendor.rich.console import Console<N>    from pip._vendor.rich import inspect<N><N>    console = Console()<N>    inspect(console)<N>
import sys<N>from typing import TYPE_CHECKING, Optional, Union<N><N>from .jupyter import JupyterMixin<N>from .segment import Segment<N>from .style import Style<N>from ._emoji_codes import EMOJI<N>from ._emoji_replace import _emoji_replace<N><N>if sys.version_info >= (3, 8):<N>    from typing import Literal<N>else:<N>    from pip._vendor.typing_extensions import Literal  # pragma: no cover<N><N>
<N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderResult<N><N><N>EmojiVariant = Literal["emoji", "text"]<N><N><N>class NoEmoji(Exception):<N>    """No emoji by that name."""<N><N><N>class Emoji(JupyterMixin):<N>    __slots__ = ["name", "style", "_char", "variant"]<N><N>
    VARIANTS = {"text": "\uFE0E", "emoji": "\uFE0F"}<N><N>    def __init__(<N>        self,<N>        name: str,<N>        style: Union[str, Style] = "none",<N>        variant: Optional[EmojiVariant] = None,<N>    ) -> None:<N>        """A single emoji character.<N><N>
class ConsoleError(Exception):<N>    """An error in console operation."""<N><N><N>class StyleError(Exception):<N>    """An error in styles."""<N><N><N>class StyleSyntaxError(ConsoleError):<N>    """Style was badly formatted."""<N><N><N>class MissingStyle(StyleError):<N>    """No such style."""<N><N>
<N>class StyleStackError(ConsoleError):<N>    """Style stack is invalid."""<N><N><N>class NotRenderableError(ConsoleError):<N>    """Object is not renderable."""<N><N><N>class MarkupError(ConsoleError):<N>    """Markup was badly formatted."""<N><N><N>class LiveError(ConsoleError):<N>    """Error related to Live display."""<N><N>
# coding: utf-8<N>"""Functions for reporting filesizes. Borrowed from https://github.com/PyFilesystem/pyfilesystem2<N><N>The functions declared in this module should cover the different<N>usecases needed to generate a string representation of a file size<N>using several different units. Since there are many standards regarding<N>file size units, three different functions have been implemented.<N><N>
See Also:<N>    * `Wikipedia: Binary prefix <https://en.wikipedia.org/wiki/Binary_prefix>`_<N><N>"""<N><N>__all__ = ["decimal"]<N><N>from typing import Iterable, List, Tuple, Optional<N><N><N>def _to_str(<N>    size: int,<N>    suffixes: Iterable[str],<N>    base: int,<N>    *,<N>    precision: Optional[int] = 1,<N>    separator: Optional[str] = " ",<N>) -> str:<N>    if size == 1:<N>        return "1 byte"<N>    elif size < base:<N>        return "{:,} bytes".format(size)<N><N>
    for i, suffix in enumerate(suffixes, 2):  # noqa: B007<N>        unit = base ** i<N>        if size < unit:<N>            break<N>    return "{:,.{precision}f}{separator}{}".format(<N>        (base * size / unit),<N>        suffix,<N>        precision=precision,<N>        separator=separator,<N>    )<N><N>
<N>def pick_unit_and_suffix(size: int, suffixes: List[str], base: int) -> Tuple[int, str]:<N>    """Pick a suffix and base for the given size."""<N>    for i, suffix in enumerate(suffixes):<N>        unit = base ** i<N>        if size < unit * base:<N>            break<N>    return unit, suffix<N><N>
<N>def decimal(<N>    size: int,<N>    *,<N>    precision: Optional[int] = 1,<N>    separator: Optional[str] = " ",<N>) -> str:<N>    """Convert a filesize in to a string (powers of 1000, SI prefixes).<N><N>    In this convention, ``1000 B = 1 kB``.<N><N>
    This is typically the format used to advertise the storage<N>    capacity of USB flash drives and the like (*256 MB* meaning<N>    actually a storage capacity of more than *256 000 000 B*),<N>    or used by **Mac OS X** since v10.6 to report file sizes.<N><N>
    Arguments:<N>        int (size): A file size.<N>        int (precision): The number of decimal places to include (default = 1).<N>        str (separator): The string to separate the value from the units (default = " ").<N><N>    Returns:<N>        `str`: A string containing a abbreviated file size and units.<N><N>
    Example:<N>        >>> filesize.decimal(30000)<N>        '30.0 kB'<N>        >>> filesize.decimal(30000, precision=2, separator="")<N>        '30.00kB'<N><N>    """<N>    return _to_str(<N>        size,<N>        ("kB", "MB", "GB", "TB", "PB", "EB", "ZB", "YB"),<N>        1000,<N>        precision=precision,<N>        separator=separator,<N>    )<N><N><N>
import io<N>from typing import List, Any, IO, TYPE_CHECKING<N><N>from .ansi import AnsiDecoder<N>from .text import Text<N><N>if TYPE_CHECKING:<N>    from .console import Console<N><N><N>class FileProxy(io.TextIOBase):<N>    """Wraps a file (e.g. sys.stdout) and redirects writes to a console."""<N><N>
    def __init__(self, console: "Console", file: IO[str]) -> None:<N>        self.__console = console<N>        self.__file = file<N>        self.__buffer: List[str] = []<N>        self.__ansi_decoder = AnsiDecoder()<N><N>    @property<N>    def rich_proxied_file(self) -> IO[str]:<N>        """Get proxied file."""<N>        return self.__file<N><N>
from abc import ABC, abstractmethod<N>from typing import List, Union<N><N>from .text import Text<N><N><N>def _combine_regex(*regexes: str) -> str:<N>    """Combine a number of regexes in to a single regex.<N><N>    Returns:<N>        str: New regex with all regexes ORed together.<N>    """<N>    return "|".join(regexes)<N><N>
<N>class Highlighter(ABC):<N>    """Abstract base class for highlighters."""<N><N>    def __call__(self, text: Union[str, Text]) -> Text:<N>        """Highlight a str or Text instance.<N><N>        Args:<N>            text (Union[str, ~Text]): Text to highlight.<N><N>
        Raises:<N>            TypeError: If not called with text or str.<N><N>        Returns:<N>            Text: A test instance with highlighting applied.<N>        """<N>        if isinstance(text, str):<N>            highlight_text = Text(text)<N>        elif isinstance(text, Text):<N>            highlight_text = text.copy()<N>        else:<N>            raise TypeError(f"str or Text instance required, not {text!r}")<N>        self.highlight(highlight_text)<N>        return highlight_text<N><N>
    @abstractmethod<N>    def highlight(self, text: Text) -> None:<N>        """Apply highlighting in place to text.<N><N>        Args:<N>            text (~Text): A text object highlight.<N>        """<N><N><N>class NullHighlighter(Highlighter):<N>    """A highlighter object that doesn't highlight.<N><N>
    May be used to disable highlighting entirely.<N><N>    """<N><N>    def highlight(self, text: Text) -> None:<N>        """Nothing to do"""<N><N><N>class RegexHighlighter(Highlighter):<N>    """Applies highlighting from a list of regular expressions."""<N><N>
    highlights: List[str] = []<N>    base_style: str = ""<N><N>    def highlight(self, text: Text) -> None:<N>        """Highlight :class:`rich.text.Text` using regular expressions.<N><N>        Args:<N>            text (~Text): Text to highlighted.<N><N>
        """<N><N>        highlight_regex = text.highlight_regex<N>        for re_highlight in self.highlights:<N>            highlight_regex(re_highlight, style_prefix=self.base_style)<N><N><N>class ReprHighlighter(RegexHighlighter):<N>    """Highlights the text typically produced from ``__repr__`` methods."""<N><N>
from typing import Any, Dict, Iterable, List<N><N>from . import get_console<N>from .segment import Segment<N>from .terminal_theme import DEFAULT_TERMINAL_THEME<N><N>JUPYTER_HTML_FORMAT = """\<N><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">{code}</pre><N>"""<N><N>
import sys<N>from threading import Event, RLock, Thread<N>from types import TracebackType<N>from typing import IO, Any, Callable, List, Optional, TextIO, Type, cast<N><N>from . import get_console<N>from .console import Console, ConsoleRenderable, RenderableType, RenderHook<N>from .control import Control<N>from .file_proxy import FileProxy<N>from .jupyter import JupyterMixin<N>from .live_render import LiveRender, VerticalOverflowMethod<N>from .screen import Screen<N>from .text import Text<N><N>
<N>class _RefreshThread(Thread):<N>    """A thread that calls refresh() at regular intervals."""<N><N>    def __init__(self, live: "Live", refresh_per_second: float) -> None:<N>        self.live = live<N>        self.refresh_per_second = refresh_per_second<N>        self.done = Event()<N>        super().__init__(daemon=True)<N><N>
    def stop(self) -> None:<N>        self.done.set()<N><N>    def run(self) -> None:<N>        while not self.done.wait(1 / self.refresh_per_second):<N>            with self.live._lock:<N>                if not self.done.is_set():<N>                    self.live.refresh()<N><N>
import sys<N>from typing import Optional, Tuple<N><N>if sys.version_info >= (3, 8):<N>    from typing import Literal<N>else:<N>    from pip._vendor.typing_extensions import Literal  # pragma: no cover<N><N><N>from ._loop import loop_last<N>from .console import Console, ConsoleOptions, RenderableType, RenderResult<N>from .control import Control<N>from .segment import ControlType, Segment<N>from .style import StyleType<N>from .text import Text<N><N>
VerticalOverflowMethod = Literal["crop", "ellipsis", "visible"]<N><N><N>class LiveRender:<N>    """Creates a renderable that may be updated.<N><N>    Args:<N>        renderable (RenderableType): Any renderable object.<N>        style (StyleType, optional): An optional style to apply to the renderable. Defaults to "".<N>    """<N><N>
    def __init__(<N>        self,<N>        renderable: RenderableType,<N>        style: StyleType = "",<N>        vertical_overflow: VerticalOverflowMethod = "ellipsis",<N>    ) -> None:<N>        self.renderable = renderable<N>        self.style = style<N>        self.vertical_overflow = vertical_overflow<N>        self._shape: Optional[Tuple[int, int]] = None<N><N>
    def set_renderable(self, renderable: RenderableType) -> None:<N>        """Set a new renderable.<N><N>        Args:<N>            renderable (RenderableType): Any renderable object, including str.<N>        """<N>        self.renderable = renderable<N><N>
import logging<N>from datetime import datetime<N>from logging import Handler, LogRecord<N>from pathlib import Path<N>from typing import ClassVar, List, Optional, Type, Union<N><N>from . import get_console<N>from ._log_render import LogRender, FormatTimeCallable<N>from .console import Console, ConsoleRenderable<N>from .highlighter import Highlighter, ReprHighlighter<N>from .text import Text<N>from .traceback import Traceback<N><N>
<N>class RichHandler(Handler):<N>    """A logging handler that renders output with Rich. The time / level / message and file are displayed in columns.<N>    The level is color coded, and the message is syntax highlighted.<N><N>    Note:<N>        Be careful when enabling console markup in log messages if you have configured logging for libraries not<N>        under your control. If a dependency writes messages containing square brackets, it may not produce the intended output.<N><N>
from ast import literal_eval<N>from operator import attrgetter<N>import re<N>from typing import Callable, Iterable, List, Match, NamedTuple, Optional, Tuple, Union<N><N>from .errors import MarkupError<N>from .style import Style<N>from .text import Span, Text<N>from .emoji import EmojiVariant<N>from ._emoji_replace import _emoji_replace<N><N>
<N>RE_TAGS = re.compile(<N>    r"""((\\*)\[([a-z#\/@].*?)\])""",<N>    re.VERBOSE,<N>)<N><N>RE_HANDLER = re.compile(r"^([\w\.]*?)(\(.*?\))?$")<N><N><N>class Tag(NamedTuple):<N>    """A tag in console markup."""<N><N>    name: str<N>    """The tag name. e.g. 'bold'."""<N>    parameters: Optional[str]<N>    """Any additional parameters after the name."""<N><N>
    def __str__(self) -> str:<N>        return (<N>            self.name if self.parameters is None else f"{self.name} {self.parameters}"<N>        )<N><N>    @property<N>    def markup(self) -> str:<N>        """Get the string representation of this tag."""<N>        return (<N>            f"[{self.name}]"<N>            if self.parameters is None<N>            else f"[{self.name}={self.parameters}]"<N>        )<N><N>
<N>_ReStringMatch = Match[str]  # regex match object<N>_ReSubCallable = Callable[[_ReStringMatch], str]  # Callable invoked by re.sub<N>_EscapeSubMethod = Callable[[_ReSubCallable, str], str]  # Sub method of a compiled re<N><N><N>def escape(<N>    markup: str, _escape: _EscapeSubMethod = re.compile(r"(\\*)(\[[a-z#\/@].*?\])").sub<N>) -> str:<N>    """Escapes text so that it won't be interpreted as markup.<N><N>
    Args:<N>        markup (str): Content to be inserted in to markup.<N><N>    Returns:<N>        str: Markup with square brackets escaped.<N>    """<N><N>    def escape_backslashes(match: Match[str]) -> str:<N>        """Called by re.sub replace matches."""<N>        backslashes, text = match.groups()<N>        return f"{backslashes}{backslashes}\\{text}"<N><N>
    markup = _escape(escape_backslashes, markup)<N>    return markup<N><N><N>def _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:<N>    """Parse markup in to an iterable of tuples of (position, text, tag).<N><N>    Args:<N>        markup (str): A string containing console markup<N><N>
from operator import itemgetter<N>from typing import Callable, Iterable, NamedTuple, Optional, TYPE_CHECKING<N><N>from . import errors<N>from .protocol import is_renderable, rich_cast<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderableType<N><N>
<N>class Measurement(NamedTuple):<N>    """Stores the minimum and maximum widths (in characters) required to render an object."""<N><N>    minimum: int<N>    """Minimum number of cells required to render."""<N>    maximum: int<N>    """Maximum number of cells required to render."""<N><N>
    @property<N>    def span(self) -> int:<N>        """Get difference between maximum and minimum."""<N>        return self.maximum - self.minimum<N><N>    def normalize(self) -> "Measurement":<N>        """Get measurement that ensures that minimum <= maximum and minimum >= 0<N><N>
        Returns:<N>            Measurement: A normalized measurement.<N>        """<N>        minimum, maximum = self<N>        minimum = min(max(0, minimum), maximum)<N>        return Measurement(max(0, minimum), max(0, max(minimum, maximum)))<N><N>
    def with_maximum(self, width: int) -> "Measurement":<N>        """Get a RenderableWith where the widths are <= width.<N><N>        Args:<N>            width (int): Maximum desired width.<N><N>        Returns:<N>            Measurement: New Measurement object.<N>        """<N>        minimum, maximum = self<N>        return Measurement(min(minimum, width), min(maximum, width))<N><N>
    def with_minimum(self, width: int) -> "Measurement":<N>        """Get a RenderableWith where the widths are >= width.<N><N>        Args:<N>            width (int): Minimum desired width.<N><N>        Returns:<N>            Measurement: New Measurement object.<N>        """<N>        minimum, maximum = self<N>        width = max(0, width)<N>        return Measurement(max(minimum, width), max(maximum, width))<N><N>
    def clamp(<N>        self, min_width: Optional[int] = None, max_width: Optional[int] = None<N>    ) -> "Measurement":<N>        """Clamp a measurement within the specified range.<N><N>        Args:<N>            min_width (int): Minimum desired width, or ``None`` for no minimum. Defaults to None.<N>            max_width (int): Maximum desired width, or ``None`` for no maximum. Defaults to None.<N><N>
        Returns:<N>            Measurement: New Measurement object.<N>        """<N>        measurement = self<N>        if min_width is not None:<N>            measurement = measurement.with_minimum(min_width)<N>        if max_width is not None:<N>            measurement = measurement.with_maximum(max_width)<N>        return measurement<N><N>
    @classmethod<N>    def get(<N>        cls, console: "Console", options: "ConsoleOptions", renderable: "RenderableType"<N>    ) -> "Measurement":<N>        """Get a measurement for a renderable.<N><N>        Args:<N>            console (~rich.console.Console): Console instance.<N>            options (~rich.console.ConsoleOptions): Console options.<N>            renderable (RenderableType): An object that may be rendered with Rich.<N><N>
from typing import cast, List, Optional, Tuple, TYPE_CHECKING, Union<N><N>if TYPE_CHECKING:<N>    from .console import (<N>        Console,<N>        ConsoleOptions,<N>        RenderableType,<N>        RenderResult,<N>    )<N>from .jupyter import JupyterMixin<N>from .measure import Measurement<N>from .style import Style<N>from .segment import Segment<N><N>
from abc import ABC, abstractmethod<N>from typing import Any, Callable<N><N><N>class Pager(ABC):<N>    """Base class for a pager."""<N><N>    @abstractmethod<N>    def show(self, content: str) -> None:<N>        """Show content in pager.<N><N>        Args:<N>            content (str): Content to be displayed.<N>        """<N><N>
<N>class SystemPager(Pager):<N>    """Uses the pager installed on the system."""<N><N>    def _pager(self, content: str) -> Any:  #  pragma: no cover<N>        return __import__("pydoc").pager(content)<N><N>    def show(self, content: str) -> None:<N>        """Use the same pager used by pydoc."""<N>        self._pager(content)<N><N>
from math import sqrt<N>from functools import lru_cache<N>from typing import Sequence, Tuple, TYPE_CHECKING<N><N>from .color_triplet import ColorTriplet<N><N>if TYPE_CHECKING:<N>    from pip._vendor.rich.table import Table<N><N><N>class Palette:<N>    """A palette of available colors."""<N><N>
    def __init__(self, colors: Sequence[Tuple[int, int, int]]):<N>        self._colors = colors<N><N>    def __getitem__(self, number: int) -> ColorTriplet:<N>        return ColorTriplet(*self._colors[number])<N><N>    def __rich__(self) -> "Table":<N>        from pip._vendor.rich.color import Color<N>        from pip._vendor.rich.style import Style<N>        from pip._vendor.rich.text import Text<N>        from pip._vendor.rich.table import Table<N><N>
from typing import Optional, TYPE_CHECKING<N><N>from .box import Box, ROUNDED<N><N>from .align import AlignMethod<N>from .jupyter import JupyterMixin<N>from .measure import Measurement, measure_renderables<N>from .padding import Padding, PaddingDimensions<N>from .style import StyleType<N>from .text import Text, TextType<N>from .segment import Segment<N><N>
if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderableType, RenderResult<N><N><N>class Panel(JupyterMixin):<N>    """A console renderable that draws a border around its contents.<N><N>    Example:<N>        >>> console.print(Panel("Hello, World!"))<N><N>
from typing import Any, Generic, List, Optional, TextIO, TypeVar, Union, overload<N><N>from . import get_console<N>from .console import Console<N>from .text import Text, TextType<N><N>PromptType = TypeVar("PromptType")<N>DefaultType = TypeVar("DefaultType")<N><N>
<N>class PromptError(Exception):<N>    """Exception base class for prompt related errors."""<N><N><N>class InvalidResponse(PromptError):<N>    """Exception to indicate a response was invalid. Raise this within process_response() to indicate an error<N>    and provide an error message.<N><N>
    Args:<N>        message (Union[str, Text]): Error message.<N>    """<N><N>    def __init__(self, message: TextType) -> None:<N>        self.message = message<N><N>    def __rich__(self) -> TextType:<N>        return self.message<N><N><N>class PromptBase(Generic[PromptType]):<N>    """Ask the user for input until a valid response is received. This is the base class, see one of<N>    the concrete classes for examples.<N><N>
from typing import NamedTuple<N><N><N>class Region(NamedTuple):<N>    """Defines a rectangular region of the screen."""<N><N>    x: int<N>    y: int<N>    width: int<N>    height: int<N>
from functools import partial<N>import inspect<N><N>from typing import (<N>    Any,<N>    Callable,<N>    Iterable,<N>    List,<N>    Optional,<N>    overload,<N>    Union,<N>    Tuple,<N>    Type,<N>    TypeVar,<N>)<N><N><N>T = TypeVar("T")<N><N><N>Result = Iterable[Union[Any, Tuple[Any], Tuple[str, Any], Tuple[str, Any, Any]]]<N>RichReprResult = Result<N><N>
<N>class ReprError(Exception):<N>    """An error occurred when attempting to build a repr."""<N><N><N>@overload<N>def auto(cls: Optional[T]) -> T:<N>    ...<N><N><N>@overload<N>def auto(*, angular: bool = False) -> Callable[[T], T]:<N>    ...<N><N><N>def auto(<N>    cls: Optional[T] = None, *, angular: Optional[bool] = None<N>) -> Union[T, Callable[[T], T]]:<N>    """Class decorator to create __repr__ from __rich_repr__"""<N><N>
    def do_replace(cls: Type[T], angular: Optional[bool] = None) -> Type[T]:<N>        def auto_repr(self: Type[T]) -> str:<N>            """Create repr string from __rich_repr__"""<N>            repr_str: List[str] = []<N>            append = repr_str.append<N><N>
from typing import Union<N><N>from .align import AlignMethod<N>from .cells import cell_len, set_cell_size<N>from .console import Console, ConsoleOptions, RenderResult<N>from .jupyter import JupyterMixin<N>from .style import Style<N>from .text import Text<N><N>
from collections.abc import Mapping<N>from typing import TYPE_CHECKING, Any, Optional, Tuple<N><N>from .highlighter import ReprHighlighter<N>from .panel import Panel<N>from .pretty import Pretty<N>from .table import Table<N>from .text import Text, TextType<N><N>
if TYPE_CHECKING:<N>    from .console import ConsoleRenderable<N><N><N>def render_scope(<N>    scope: "Mapping[str, Any]",<N>    *,<N>    title: Optional[TextType] = None,<N>    sort_keys: bool = True,<N>    indent_guides: bool = False,<N>    max_length: Optional[int] = None,<N>    max_string: Optional[int] = None,<N>) -> "ConsoleRenderable":<N>    """Render python variables in a given scope.<N><N>
from typing import Optional, TYPE_CHECKING<N><N>from .segment import Segment<N>from .style import StyleType<N>from ._loop import loop_last<N><N><N>if TYPE_CHECKING:<N>    from .console import (<N>        Console,<N>        ConsoleOptions,<N>        RenderResult,<N>        RenderableType,<N>        Group,<N>    )<N><N>
<N>class Screen:<N>    """A renderable that fills the terminal screen and crops excess.<N><N>    Args:<N>        renderable (RenderableType): Child renderable.<N>        style (StyleType, optional): Optional background style. Defaults to None.<N>    """<N><N>
    renderable: "RenderableType"<N><N>    def __init__(<N>        self,<N>        *renderables: "RenderableType",<N>        style: Optional[StyleType] = None,<N>        application_mode: bool = False,<N>    ) -> None:<N>        from pip._vendor.rich.console import Group<N><N>
from enum import IntEnum<N>from functools import lru_cache<N>from itertools import filterfalse<N>from logging import getLogger<N>from operator import attrgetter<N>from typing import (<N>    TYPE_CHECKING,<N>    Dict,<N>    Iterable,<N>    List,<N>    NamedTuple,<N>    Optional,<N>    Sequence,<N>    Tuple,<N>    Type,<N>    Union,<N>)<N><N>
from .cells import (<N>    _is_single_cell_widths,<N>    cell_len,<N>    get_character_cell_size,<N>    set_cell_size,<N>)<N>from .repr import Result, rich_repr<N>from .style import Style<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderResult<N><N>
log = getLogger("rich")<N><N><N>class ControlType(IntEnum):<N>    """Non-printable control codes which typically translate to ANSI codes."""<N><N>    BELL = 1<N>    CARRIAGE_RETURN = 2<N>    HOME = 3<N>    CLEAR = 4<N>    SHOW_CURSOR = 5<N>    HIDE_CURSOR = 6<N>    ENABLE_ALT_SCREEN = 7<N>    DISABLE_ALT_SCREEN = 8<N>    CURSOR_UP = 9<N>    CURSOR_DOWN = 10<N>    CURSOR_FORWARD = 11<N>    CURSOR_BACKWARD = 12<N>    CURSOR_MOVE_TO_COLUMN = 13<N>    CURSOR_MOVE_TO = 14<N>    ERASE_IN_LINE = 15<N><N>
<N>ControlCode = Union[<N>    Tuple[ControlType], Tuple[ControlType, int], Tuple[ControlType, int, int]<N>]<N><N><N>@rich_repr()<N>class Segment(NamedTuple):<N>    """A piece of text with associated style. Segments are produced by the Console render process and<N>    are ultimately converted in to strings to be written to the terminal.<N><N>
    Args:<N>        text (str): A piece of text.<N>        style (:class:`~rich.style.Style`, optional): An optional style to apply to the text.<N>        control (Tuple[ControlCode..], optional): Optional sequence of control codes.<N>    """<N><N>    text: str = ""<N>    """Raw text."""<N>    style: Optional[Style] = None<N>    """An optional style."""<N>    control: Optional[Sequence[ControlCode]] = None<N>    """Optional sequence of control codes."""<N><N>
    def __rich_repr__(self) -> Result:<N>        yield self.text<N>        if self.control is None:<N>            if self.style is not None:<N>                yield self.style<N>        else:<N>            yield self.style<N>            yield self.control<N><N>
    def __bool__(self) -> bool:<N>        """Check if the segment contains text."""<N>        return bool(self.text)<N><N>    @property<N>    def cell_length(self) -> int:<N>        """Get cell length of segment."""<N>        return 0 if self.control else cell_len(self.text)<N><N>
    @property<N>    def is_control(self) -> bool:<N>        """Check if the segment contains control codes."""<N>        return self.control is not None<N><N>    @classmethod<N>    @lru_cache(1024 * 16)<N>    def _split_cells(cls, segment: "Segment", cut: int) -> Tuple["Segment", "Segment"]:  # type: ignore<N><N>
        text, style, control = segment<N>        _Segment = Segment<N><N>        cell_length = segment.cell_length<N>        if cut >= cell_length:<N>            return segment, _Segment("", style, control)<N><N>        cell_size = get_character_cell_size<N><N>
from typing import cast, List, Optional, TYPE_CHECKING<N><N>from ._spinners import SPINNERS<N>from .measure import Measurement<N>from .table import Table<N>from .text import Text<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderResult, RenderableType<N>    from .style import StyleType<N><N>
<N>class Spinner:<N>    def __init__(<N>        self,<N>        name: str,<N>        text: "RenderableType" = "",<N>        *,<N>        style: Optional["StyleType"] = None,<N>        speed: float = 1.0,<N>    ) -> None:<N>        """A spinner animation.<N><N>
        Args:<N>            name (str): Name of spinner (run python -m rich.spinner).<N>            text (RenderableType, optional): A renderable to display at the right of the spinner (str or Text typically). Defaults to "".<N>            style (StyleType, optional): Style for spinner animation. Defaults to None.<N>            speed (float, optional): Speed factor for animation. Defaults to 1.0.<N><N>
from types import TracebackType<N>from typing import Optional, Type<N><N>from .console import Console, RenderableType<N>from .jupyter import JupyterMixin<N>from .live import Live<N>from .spinner import Spinner<N>from .style import StyleType<N><N><N>class Status(JupyterMixin):<N>    """Displays a status indicator with a 'spinner' animation.<N><N>
import sys<N>from functools import lru_cache<N>from marshal import loads, dumps<N>from random import randint<N>from typing import Any, cast, Dict, Iterable, List, Optional, Type, Union<N><N>from . import errors<N>from .color import Color, ColorParseError, ColorSystem, blend_rgb<N>from .repr import rich_repr, Result<N>from .terminal_theme import DEFAULT_TERMINAL_THEME, TerminalTheme<N><N>
<N># Style instances and style definitions are often interchangeable<N>StyleType = Union[str, "Style"]<N><N><N>class _Bit:<N>    """A descriptor to get/set a style attribute bit."""<N><N>    __slots__ = ["bit"]<N><N>    def __init__(self, bit_no: int) -> None:<N>        self.bit = 1 << bit_no<N><N>
    def __get__(self, obj: "Style", objtype: Type["Style"]) -> Optional[bool]:<N>        if obj._set_attributes & self.bit:<N>            return obj._attributes & self.bit != 0<N>        return None<N><N><N>@rich_repr<N>class Style:<N>    """A terminal style.<N><N>
    A terminal style consists of a color (`color`), a background color (`bgcolor`), and a number of attributes, such<N>    as bold, italic etc. The attributes have 3 states: they can either be on<N>    (``True``), off (``False``), or not set (``None``).<N><N>
from typing import TYPE_CHECKING<N><N>from .measure import Measurement<N>from .segment import Segment<N>from .style import StyleType<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleOptions, RenderResult, RenderableType<N><N><N>class Styled:<N>    """Apply a style to a renderable.<N><N>
    Args:<N>        renderable (RenderableType): Any renderable.<N>        style (StyleType): A style to apply across the entire renderable.<N>    """<N><N>    def __init__(self, renderable: "RenderableType", style: "StyleType") -> None:<N>        self.renderable = renderable<N>        self.style = style<N><N>
    def __rich_console__(<N>        self, console: "Console", options: "ConsoleOptions"<N>    ) -> "RenderResult":<N>        style = console.get_style(self.style)<N>        rendered_segments = console.render(self.renderable, options)<N>        segments = Segment.apply_style(rendered_segments, style)<N>        return segments<N><N>
    def __rich_measure__(<N>        self, console: "Console", options: "ConsoleOptions"<N>    ) -> Measurement:<N>        return Measurement.get(console, options, self.renderable)<N><N><N>if __name__ == "__main__":  # pragma: no cover<N>    from pip._vendor.rich import print<N>    from pip._vendor.rich.panel import Panel<N><N>
from collections.abc import Mapping<N>from typing import Any, Optional<N>import warnings<N><N>from pip._vendor.rich.console import JustifyMethod<N><N>from . import box<N>from .highlighter import ReprHighlighter<N>from .pretty import Pretty<N>from .table import Table<N><N>
<N>def tabulate_mapping(<N>    mapping: "Mapping[Any, Any]",<N>    title: Optional[str] = None,<N>    caption: Optional[str] = None,<N>    title_justify: Optional[JustifyMethod] = None,<N>    caption_justify: Optional[JustifyMethod] = None,<N>) -> Table:<N>    """Generate a simple table from a mapping.<N><N>
    Args:<N>        mapping (Mapping): A mapping object (e.g. a dict);<N>        title (str, optional): Optional title to be displayed over the table.<N>        caption (str, optional): Optional caption to be displayed below the table.<N>        title_justify (str, optional): Justify method for title. Defaults to None.<N>        caption_justify (str, optional): Justify method for caption. Defaults to None.<N><N>
from typing import List, Optional, Tuple<N><N>from .color_triplet import ColorTriplet<N>from .palette import Palette<N><N>_ColorTuple = Tuple[int, int, int]<N><N><N>class TerminalTheme:<N>    """A color theme used when exporting console content.<N><N>
    Args:<N>        background (Tuple[int, int, int]): The background color.<N>        foreground (Tuple[int, int, int]): The foreground (text) color.<N>        normal (List[Tuple[int, int, int]]): A list of 8 normal intensity colors.<N>        bright (List[Tuple[int, int, int]], optional): A list of 8 bright colors, or None<N>            to repeat normal intensity. Defaults to None.<N>    """<N><N>
    def __init__(<N>        self,<N>        background: _ColorTuple,<N>        foreground: _ColorTuple,<N>        normal: List[_ColorTuple],<N>        bright: Optional[List[_ColorTuple]] = None,<N>    ) -> None:<N>        self.background_color = ColorTriplet(*background)<N>        self.foreground_color = ColorTriplet(*foreground)<N>        self.ansi_colors = Palette(normal + (bright or normal))<N><N>
import re<N>from functools import partial, reduce<N>from math import gcd<N>from operator import itemgetter<N>from pip._vendor.rich.emoji import EmojiVariant<N>from typing import (<N>    TYPE_CHECKING,<N>    Any,<N>    Callable,<N>    Dict,<N>    Iterable,<N>    List,<N>    NamedTuple,<N>    Optional,<N>    Tuple,<N>    Union,<N>)<N><N>
from ._loop import loop_last<N>from ._pick import pick_bool<N>from ._wrap import divide_line<N>from .align import AlignMethod<N>from .cells import cell_len, set_cell_size<N>from .containers import Lines<N>from .control import strip_control_codes<N>from .emoji import EmojiVariant<N>from .jupyter import JupyterMixin<N>from .measure import Measurement<N>from .segment import Segment<N>from .style import Style, StyleType<N><N>
if TYPE_CHECKING:  # pragma: no cover<N>    from .console import Console, ConsoleOptions, JustifyMethod, OverflowMethod<N><N>DEFAULT_JUSTIFY: "JustifyMethod" = "default"<N>DEFAULT_OVERFLOW: "OverflowMethod" = "fold"<N><N><N>_re_whitespace = re.compile(r"\s+$")<N><N>
TextType = Union[str, "Text"]<N><N>GetStyleCallable = Callable[[str], Optional[StyleType]]<N><N><N>class Span(NamedTuple):<N>    """A marked up region in some text."""<N><N>    start: int<N>    """Span start index."""<N>    end: int<N>    """Span end index."""<N>    style: Union[str, Style]<N>    """Style associated with the span."""<N><N>
    def __repr__(self) -> str:<N>        return (<N>            f"Span({self.start}, {self.end}, {self.style!r})"<N>            if (isinstance(self.style, Style) and self.style._meta)<N>            else f"Span({self.start}, {self.end}, {repr(self.style)})"<N>        )<N><N>
    def __bool__(self) -> bool:<N>        return self.end > self.start<N><N>    def split(self, offset: int) -> Tuple["Span", Optional["Span"]]:<N>        """Split a span in to 2 from a given offset."""<N><N>        if offset < self.start:<N>            return self, None<N>        if offset >= self.end:<N>            return self, None<N><N>
        start, end, style = self<N>        span1 = Span(start, min(end, offset), style)<N>        span2 = Span(span1.end, end, style)<N>        return span1, span2<N><N>    def move(self, offset: int) -> "Span":<N>        """Move start and end by a given offset.<N><N>
        Args:<N>            offset (int): Number of characters to add to start and end.<N><N>        Returns:<N>            TextSpan: A new TextSpan with adjusted position.<N>        """<N>        start, end, style = self<N>        return Span(start + offset, end + offset, style)<N><N>
    def right_crop(self, offset: int) -> "Span":<N>        """Crop the span at the given offset.<N><N>        Args:<N>            offset (int): A value between start and end.<N><N>        Returns:<N>            Span: A new (possibly smaller) span.<N>        """<N>        start, end, style = self<N>        if offset >= end:<N>            return self<N>        return Span(start, min(offset, end), style)<N><N>
import configparser<N>from typing import Dict, List, IO, Mapping, Optional<N><N>from .default_styles import DEFAULT_STYLES<N>from .style import Style, StyleType<N><N><N>class Theme:<N>    """A container for style information, used by :class:`~rich.console.Console`.<N><N>
    Args:<N>        styles (Dict[str, Style], optional): A mapping of style names on to styles. Defaults to None for a theme with no styles.<N>        inherit (bool, optional): Inherit default styles. Defaults to True.<N>    """<N><N>    styles: Dict[str, Style]<N><N>
    def __init__(<N>        self, styles: Optional[Mapping[str, StyleType]] = None, inherit: bool = True<N>    ):<N>        self.styles = DEFAULT_STYLES.copy() if inherit else {}<N>        if styles is not None:<N>            self.styles.update(<N>                {<N>                    name: style if isinstance(style, Style) else Style.parse(style)<N>                    for name, style in styles.items()<N>                }<N>            )<N><N>
    @property<N>    def config(self) -> str:<N>        """Get contents of a config file for this theme."""<N>        config = "[styles]\n" + "\n".join(<N>            f"{name} = {style}" for name, style in sorted(self.styles.items())<N>        )<N>        return config<N><N>
    @classmethod<N>    def from_file(<N>        cls, config_file: IO[str], source: Optional[str] = None, inherit: bool = True<N>    ) -> "Theme":<N>        """Load a theme from a text mode file.<N><N>        Args:<N>            config_file (IO[str]): An open conf file.<N>            source (str, optional): The filename of the open file. Defaults to None.<N>            inherit (bool, optional): Inherit default styles. Defaults to True.<N><N>
        Returns:<N>            Theme: A New theme instance.<N>        """<N>        config = configparser.ConfigParser()<N>        config.read_file(config_file, source=source)<N>        styles = {name: Style.parse(value) for name, value in config.items("styles")}<N>        theme = Theme(styles, inherit=inherit)<N>        return theme<N><N>
    @classmethod<N>    def read(cls, path: str, inherit: bool = True) -> "Theme":<N>        """Read a theme from a path.<N><N>        Args:<N>            path (str): Path to a config file readable by Python configparser module.<N>            inherit (bool, optional): Inherit default styles. Defaults to True.<N><N>
        Returns:<N>            Theme: A new theme instance.<N>        """<N>        with open(path, "rt") as config_file:<N>            return cls.from_file(config_file, source=path, inherit=inherit)<N><N><N>class ThemeStackError(Exception):<N>    """Base exception for errors related to the theme stack."""<N><N>
<N>class ThemeStack:<N>    """A stack of themes.<N><N>    Args:<N>        theme (Theme): A theme instance<N>    """<N><N>    def __init__(self, theme: Theme) -> None:<N>        self._entries: List[Dict[str, Style]] = [theme.styles]<N>        self.get = self._entries[-1].get<N><N>
from .default_styles import DEFAULT_STYLES<N>from .theme import Theme<N><N><N>DEFAULT = Theme(DEFAULT_STYLES)<N>
from typing import Callable, Match, Optional<N>import re<N><N>from ._emoji_codes import EMOJI<N><N><N>_ReStringMatch = Match[str]  # regex match object<N>_ReSubCallable = Callable[[_ReStringMatch], str]  # Callable invoked by re.sub<N>_EmojiSubMethod = Callable[[_ReSubCallable, str], str]  # Sub method of a compiled re<N><N>
<N>def _emoji_replace(<N>    text: str,<N>    default_variant: Optional[str] = None,<N>    _emoji_sub: _EmojiSubMethod = re.compile(r"(:(\S*?)(?:(?:\-)(emoji|text))?:)").sub,<N>) -> str:<N>    """Replace emoji code in text."""<N>    get_emoji = EMOJI.__getitem__<N>    variants = {"text": "\uFE0E", "emoji": "\uFE0F"}<N>    get_variant = variants.get<N>    default_variant_code = variants.get(default_variant, "") if default_variant else ""<N><N>
    def do_replace(match: Match[str]) -> str:<N>        emoji_code, emoji_name, variant = match.groups()<N>        try:<N>            return get_emoji(emoji_name.lower()) + get_variant(<N>                variant, default_variant_code<N>            )<N>        except KeyError:<N>            return emoji_code<N><N>
from typing import Any<N><N><N>def load_ipython_extension(ip: Any) -> None:  # pragma: no cover<N>    # prevent circular import<N>    from pip._vendor.rich.pretty import install<N>    from pip._vendor.rich.traceback import install as tr_install<N><N>    install()<N>    tr_install()<N>
from __future__ import absolute_import<N><N>from inspect import cleandoc, getdoc, getfile, isclass, ismodule, signature<N>from typing import Any, Iterable, Optional, Tuple<N><N>from .console import RenderableType, Group<N>from .highlighter import ReprHighlighter<N>from .jupyter import JupyterMixin<N>from .panel import Panel<N>from .pretty import Pretty<N>from .table import Table<N>from .text import Text, TextType<N><N>
<N>def _first_paragraph(doc: str) -> str:<N>    """Get the first paragraph from a docstring."""<N>    paragraph, _, _ = doc.partition("\n\n")<N>    return paragraph<N><N><N>def _reformat_doc(doc: str) -> str:<N>    """Reformat docstring."""<N>    doc = cleandoc(doc).strip()<N>    return doc<N><N>
from datetime import datetime<N>from typing import Iterable, List, Optional, TYPE_CHECKING, Union, Callable<N><N><N>from .text import Text, TextType<N><N>if TYPE_CHECKING:<N>    from .console import Console, ConsoleRenderable, RenderableType<N>    from .table import Table<N><N>
from typing import Iterable, Tuple, TypeVar<N><N>T = TypeVar("T")<N><N><N>def loop_first(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:<N>    """Iterate and generate a tuple with a flag for first value."""<N>    iter_values = iter(values)<N>    try:<N>        value = next(iter_values)<N>    except StopIteration:<N>        return<N>    yield True, value<N>    for value in iter_values:<N>        yield False, value<N><N>
<N>def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:<N>    """Iterate and generate a tuple with a flag for last value."""<N>    iter_values = iter(values)<N>    try:<N>        previous_value = next(iter_values)<N>    except StopIteration:<N>        return<N>    for value in iter_values:<N>        yield False, previous_value<N>        previous_value = value<N>    yield True, previous_value<N><N>
<N>def loop_first_last(values: Iterable[T]) -> Iterable[Tuple[bool, bool, T]]:<N>    """Iterate and generate a tuple with a flag for first and last value."""<N>    iter_values = iter(values)<N>    try:<N>        previous_value = next(iter_values)<N>    except StopIteration:<N>        return<N>    first = True<N>    for value in iter_values:<N>        yield first, False, previous_value<N>        first = False<N>        previous_value = value<N>    yield first, True, previous_value<N><N><N>
from collections import OrderedDict<N>from typing import Dict, Generic, TypeVar<N><N><N>CacheKey = TypeVar("CacheKey")<N>CacheValue = TypeVar("CacheValue")<N><N><N>class LRUCache(Generic[CacheKey, CacheValue], OrderedDict):  # type: ignore # https://github.com/python/mypy/issues/6904<N>    """<N>    A dictionary-like container that stores a given maximum items.<N><N>
    If an additional item is added when the LRUCache is full, the least<N>    recently used key is discarded to make room for the new item.<N><N>    """<N><N>    def __init__(self, cache_size: int) -> None:<N>        self.cache_size = cache_size<N>        super(LRUCache, self).__init__()<N><N>
    def __setitem__(self, key: CacheKey, value: CacheValue) -> None:<N>        """Store a new views, potentially discarding an old value."""<N>        if key not in self:<N>            if len(self) >= self.cache_size:<N>                self.popitem(last=False)<N>        OrderedDict.__setitem__(self, key, value)<N><N>
    def __getitem__(self: Dict[CacheKey, CacheValue], key: CacheKey) -> CacheValue:<N>        """Gets the item, but also makes it most recent."""<N>        value: CacheValue = OrderedDict.__getitem__(self, key)<N>        OrderedDict.__delitem__(self, key)<N>        OrderedDict.__setitem__(self, key, value)<N>        return value<N><N><N>
from typing import Optional<N><N><N>def pick_bool(*values: Optional[bool]) -> bool:<N>    """Pick the first non-none bool or return the last value.<N><N>    Args:<N>        *values (bool): Any number of boolean or None values.<N><N>    Returns:<N>        bool: First non-none boolean.<N>    """<N>    assert values, "1 or more values required"<N>    for value in values:<N>        if value is not None:<N>            return value<N>    return bool(value)<N>
import sys<N>from fractions import Fraction<N>from math import ceil<N>from typing import cast, List, Optional, Sequence<N><N>if sys.version_info >= (3, 8):<N>    from typing import Protocol<N>else:<N>    from pip._vendor.typing_extensions import Protocol  # pragma: no cover<N><N>
<N>class Edge(Protocol):<N>    """Any object that defines an edge (such as Layout)."""<N><N>    size: Optional[int] = None<N>    ratio: int = 1<N>    minimum_size: int = 1<N><N><N>def ratio_resolve(total: int, edges: Sequence[Edge]) -> List[int]:<N>    """Divide total space to satisfy size, ratio, and minimum_size, constraints.<N><N>
    The returned list of integers should add up to total in most cases, unless it is<N>    impossible to satisfy all the constraints. For instance, if there are two edges<N>    with a minimum size of 20 each and `total` is 30 then the returned list will be<N>    greater than total. In practice, this would mean that a Layout object would<N>    clip the rows that would overflow the screen height.<N><N>
    Args:<N>        total (int): Total number of characters.<N>        edges (List[Edge]): Edges within total space.<N><N>    Returns:<N>        List[int]: Number of characters for each edge.<N>    """<N>    # Size of edge or None for yet to be determined<N>    sizes = [(edge.size or None) for edge in edges]<N><N>
from typing import List, TypeVar<N><N>T = TypeVar("T")<N><N><N>class Stack(List[T]):<N>    """A small shim over builtin list."""<N><N>    @property<N>    def top(self) -> T:<N>        """Get top of stack."""<N>        return self[-1]<N><N>    def push(self, item: T) -> None:<N>        """Push an item on to the stack (append in stack nomenclature)."""<N>        self.append(item)<N>
"""<N>Timer context manager, only used in debug.<N><N>"""<N><N>from time import time<N><N>import contextlib<N>from typing import Generator<N><N><N>@contextlib.contextmanager<N>def timer(subject: str = "time") -> Generator[None, None, None]:<N>    """print the elapsed time. (only used in debugging)"""<N>    start = time()<N>    yield<N>    elapsed = time() - start<N>    elapsed_ms = elapsed * 1000<N>    print(f"{subject} elapsed {elapsed_ms:.1f}ms")<N>
import sys<N>from dataclasses import dataclass<N><N><N>@dataclass<N>class WindowsConsoleFeatures:<N>    """Windows features available."""<N><N>    vt: bool = False<N>    """The console supports VT codes."""<N>    truecolor: bool = False<N>    """The console supports truecolor."""<N><N>
<N>try:<N>    import ctypes<N>    from ctypes import LibraryLoader, wintypes<N><N>    if sys.platform == "win32":<N>        windll = LibraryLoader(ctypes.WinDLL)<N>    else:<N>        windll = None<N>        raise ImportError("Not windows")<N>except (AttributeError, ImportError, ValueError):<N><N>
    # Fallback if we can't load the Windows DLL<N>    def get_windows_console_features() -> WindowsConsoleFeatures:<N>        features = WindowsConsoleFeatures()<N>        return features<N><N>else:<N><N>    STDOUT = -11<N>    ENABLE_VIRTUAL_TERMINAL_PROCESSING = 4<N>    _GetConsoleMode = windll.kernel32.GetConsoleMode<N>    _GetConsoleMode.argtypes = [wintypes.HANDLE, wintypes.LPDWORD]<N>    _GetConsoleMode.restype = wintypes.BOOL<N><N>
    _GetStdHandle = windll.kernel32.GetStdHandle<N>    _GetStdHandle.argtypes = [<N>        wintypes.DWORD,<N>    ]<N>    _GetStdHandle.restype = wintypes.HANDLE<N><N>    def get_windows_console_features() -> WindowsConsoleFeatures:<N>        """Get windows console features.<N><N>
import re<N>from typing import Iterable, List, Tuple<N><N>from .cells import cell_len, chop_cells<N>from ._loop import loop_last<N><N>re_word = re.compile(r"\s*\S+\s*")<N><N><N>def words(text: str) -> Iterable[Tuple[int, int, str]]:<N>    position = 0<N>    word_match = re_word.match(text, position)<N>    while word_match is not None:<N>        start, end = word_match.span()<N>        word = word_match.group(0)<N>        yield start, end, word<N>        word_match = re_word.match(text, end)<N><N>
"""Rich text and beautiful formatting in the terminal."""<N><N>import os<N>from typing import Callable, IO, TYPE_CHECKING, Any, Optional<N><N>from ._extension import load_ipython_extension<N><N>__all__ = ["get_console", "reconfigure", "print", "inspect"]<N><N>
if TYPE_CHECKING:<N>    from .console import Console<N><N># Global console used by alternative print<N>_console: Optional["Console"] = None<N><N>_IMPORT_CWD = os.path.abspath(os.getcwd())<N><N><N>def get_console() -> "Console":<N>    """Get a global :class:`~rich.console.Console` instance. This function is used when Rich requires a Console,<N>    and hasn't been explicitly given one.<N><N>
    Returns:<N>        Console: A console instance.<N>    """<N>    global _console<N>    if _console is None:<N>        from .console import Console<N><N>        _console = Console()<N><N>    return _console<N><N><N>def reconfigure(*args: Any, **kwargs: Any) -> None:<N>    """Reconfigures the global console by replacing it with another.<N><N>
    Args:<N>        console (Console): Replacement console instance.<N>    """<N>    from pip._vendor.rich.console import Console<N><N>    new_console = Console(*args, **kwargs)<N>    _console = get_console()<N>    _console.__dict__ = new_console.__dict__<N><N>
<N>def print(<N>    *objects: Any,<N>    sep: str = " ",<N>    end: str = "\n",<N>    file: Optional[IO[str]] = None,<N>    flush: bool = False,<N>) -> None:<N>    r"""Print object(s) supplied via positional arguments.<N>    This function has an identical signature to the built-in print.<N>    For more advanced features, see the :class:`~rich.console.Console` class.<N><N>
    Args:<N>        sep (str, optional): Separator between printed objects. Defaults to " ".<N>        end (str, optional): Character to write at end of output. Defaults to "\\n".<N>        file (IO[str], optional): File to write to, or None for stdout. Defaults to None.<N>        flush (bool, optional): Has no effect as Rich always flushes output. Defaults to False.<N><N>
import string<N>from types import MappingProxyType<N>from typing import (<N>    TYPE_CHECKING,<N>    Any,<N>    Callable,<N>    Dict,<N>    FrozenSet,<N>    Iterable,<N>    Optional,<N>    TextIO,<N>    Tuple,<N>)<N><N>from pip._vendor.tomli._re import (<N>    RE_BIN,<N>    RE_DATETIME,<N>    RE_HEX,<N>    RE_LOCALTIME,<N>    RE_NUMBER,<N>    RE_OCT,<N>    match_to_datetime,<N>    match_to_localtime,<N>    match_to_number,<N>)<N><N>
if TYPE_CHECKING:<N>    from re import Pattern<N><N><N>ASCII_CTRL = frozenset(chr(i) for i in range(32)) | frozenset(chr(127))<N><N># Neither of these sets include quotation mark or backslash. They are<N># currently handled as separate cases in the parser functions.<N>ILLEGAL_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t")<N>ILLEGAL_MULTILINE_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t\n\r")<N><N>
ILLEGAL_LITERAL_STR_CHARS = ILLEGAL_BASIC_STR_CHARS<N>ILLEGAL_MULTILINE_LITERAL_STR_CHARS = ASCII_CTRL - frozenset("\t\n")<N><N>ILLEGAL_COMMENT_CHARS = ILLEGAL_BASIC_STR_CHARS<N><N>TOML_WS = frozenset(" \t")<N>TOML_WS_AND_NEWLINE = TOML_WS | frozenset("\n")<N>BARE_KEY_CHARS = frozenset(string.ascii_letters + string.digits + "-_")<N>KEY_INITIAL_CHARS = BARE_KEY_CHARS | frozenset("\"'")<N><N>
BASIC_STR_ESCAPE_REPLACEMENTS = MappingProxyType(<N>    {<N>        "\\b": "\u0008",  # backspace<N>        "\\t": "\u0009",  # tab<N>        "\\n": "\u000A",  # linefeed<N>        "\\f": "\u000C",  # form feed<N>        "\\r": "\u000D",  # carriage return<N>        '\\"': "\u0022",  # quote<N>        "\\\\": "\u005C",  # backslash<N>    }<N>)<N><N>
# Type annotations<N>ParseFloat = Callable[[str], Any]<N>Key = Tuple[str, ...]<N>Pos = int<N><N><N>class TOMLDecodeError(ValueError):<N>    """An error raised if a document is not valid TOML."""<N><N><N>def load(fp: TextIO, *, parse_float: ParseFloat = float) -> Dict[str, Any]:<N>    """Parse TOML from a file object."""<N>    s = fp.read()<N>    return loads(s, parse_float=parse_float)<N><N>
<N>def loads(s: str, *, parse_float: ParseFloat = float) -> Dict[str, Any]:  # noqa: C901<N>    """Parse TOML from a string."""<N><N>    # The spec allows converting "\r\n" to "\n", even in string<N>    # literals. Let's do so to simplify parsing.<N>    src = s.replace("\r\n", "\n")<N>    pos = 0<N>    state = State()<N><N>
from datetime import date, datetime, time, timedelta, timezone, tzinfo<N>import re<N>from typing import TYPE_CHECKING, Any, Optional, Union<N><N>if TYPE_CHECKING:<N>    from re import Match<N><N>    from pip._vendor.tomli._parser import ParseFloat<N><N>
"""A lil' TOML parser."""<N><N>__all__ = ("loads", "load", "TOMLDecodeError")<N>__version__ = "1.0.3"  # DO NOT EDIT THIS LINE MANUALLY. LET bump2version UTILITY DO IT<N><N>from pip._vendor.tomli._parser import TOMLDecodeError, load, loads<N>
from __future__ import absolute_import<N><N>import datetime<N>import logging<N>import os<N>import re<N>import socket<N>import warnings<N>from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>from .packages import six<N>from .packages.six.moves.http_client import HTTPConnection as _HTTPConnection<N>from .packages.six.moves.http_client import HTTPException  # noqa: F401<N>from .util.proxy import create_proxy_ssl_context<N><N>
try:  # Compiled with SSL?<N>    import ssl<N><N>    BaseSSLError = ssl.SSLError<N>except (ImportError, AttributeError):  # Platform-specific: No SSL.<N>    ssl = None<N><N>    class BaseSSLError(BaseException):<N>        pass<N><N><N>try:<N>    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.<N>    ConnectionError = ConnectionError<N>except NameError:<N>    # Python 2<N>    class ConnectionError(Exception):<N>        pass<N><N>
from __future__ import absolute_import<N><N>from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead<N><N># Base Exceptions<N><N><N>class HTTPError(Exception):<N>    """Base exception used by this module."""<N><N>    pass<N><N>
<N>class HTTPWarning(Warning):<N>    """Base warning used by this module."""<N><N>    pass<N><N><N>class PoolError(HTTPError):<N>    """Base exception for errors caused within a pool."""<N><N>    def __init__(self, pool, message):<N>        self.pool = pool<N>        HTTPError.__init__(self, "%s: %s" % (pool, message))<N><N>
    def __reduce__(self):<N>        # For pickling purposes.<N>        return self.__class__, (None, None)<N><N><N>class RequestError(PoolError):<N>    """Base exception for PoolErrors that have associated URLs."""<N><N>    def __init__(self, pool, url, message):<N>        self.url = url<N>        PoolError.__init__(self, pool, message)<N><N>
    def __reduce__(self):<N>        # For pickling purposes.<N>        return self.__class__, (None, self.url, None)<N><N><N>class SSLError(HTTPError):<N>    """Raised when SSL certificate fails in an HTTPS connection."""<N><N>    pass<N><N><N>class ProxyError(HTTPError):<N>    """Raised when the connection to a proxy fails."""<N><N>
    def __init__(self, message, error, *args):<N>        super(ProxyError, self).__init__(message, error, *args)<N>        self.original_error = error<N><N><N>class DecodeError(HTTPError):<N>    """Raised when automatic decoding based on Content-Type fails."""<N><N>
    pass<N><N><N>class ProtocolError(HTTPError):<N>    """Raised when something unexpected happens mid-request/response."""<N><N>    pass<N><N><N>#: Renamed to ProtocolError but aliased for backwards compatibility.<N>ConnectionError = ProtocolError<N><N>
<N># Leaf Exceptions<N><N><N>class MaxRetryError(RequestError):<N>    """Raised when the maximum number of retries is exceeded.<N><N>    :param pool: The connection pool<N>    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`<N>    :param string url: The requested Url<N>    :param exceptions.Exception reason: The underlying error<N><N>
    """<N><N>    def __init__(self, pool, url, reason=None):<N>        self.reason = reason<N><N>        message = "Max retries exceeded with url: %s (Caused by %r)" % (url, reason)<N><N>        RequestError.__init__(self, pool, url, message)<N><N><N>class HostChangedError(RequestError):<N>    """Raised when an existing pool gets a request for a foreign host."""<N><N>
    def __init__(self, pool, url, retries=3):<N>        message = "Tried to open a foreign host with url: %s" % url<N>        RequestError.__init__(self, pool, url, message)<N>        self.retries = retries<N><N><N>class TimeoutStateError(HTTPError):<N>    """Raised when passing an invalid state to a timeout"""<N><N>
    pass<N><N><N>class TimeoutError(HTTPError):<N>    """Raised when a socket timeout error occurs.<N><N>    Catching this error will catch both :exc:`ReadTimeoutErrors<N>    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.<N>    """<N><N>
    pass<N><N><N>class ReadTimeoutError(TimeoutError, RequestError):<N>    """Raised when a socket timeout occurs while receiving data from a server"""<N><N>    pass<N><N><N># This timeout error does not have a URL attached and needs to inherit from the<N># base HTTPError<N>class ConnectTimeoutError(TimeoutError):<N>    """Raised when a socket timeout occurs while connecting to a server"""<N><N>
    pass<N><N><N>class NewConnectionError(ConnectTimeoutError, PoolError):<N>    """Raised when we fail to establish a new connection. Usually ECONNREFUSED."""<N><N>    pass<N><N><N>class EmptyPoolError(PoolError):<N>    """Raised when a pool runs out of connections and no more are allowed."""<N><N>
    pass<N><N><N>class ClosedPoolError(PoolError):<N>    """Raised when a request enters a pool after the pool has been closed."""<N><N>    pass<N><N><N>class LocationValueError(ValueError, HTTPError):<N>    """Raised when there is something wrong with a given URL input."""<N><N>
    pass<N><N><N>class LocationParseError(LocationValueError):<N>    """Raised when get_host or similar fails to parse the URL input."""<N><N>    def __init__(self, location):<N>        message = "Failed to parse: %s" % location<N>        HTTPError.__init__(self, message)<N><N>
        self.location = location<N><N><N>class URLSchemeUnknown(LocationValueError):<N>    """Raised when a URL input has an unsupported scheme."""<N><N>    def __init__(self, scheme):<N>        message = "Not supported URL scheme %s" % scheme<N>        super(URLSchemeUnknown, self).__init__(message)<N><N>
        self.scheme = scheme<N><N><N>class ResponseError(HTTPError):<N>    """Used as a container for an error reason supplied in a MaxRetryError."""<N><N>    GENERIC_ERROR = "too many error responses"<N>    SPECIFIC_ERROR = "too many {status_code} error responses"<N><N>
<N>class SecurityWarning(HTTPWarning):<N>    """Warned when performing security reducing actions"""<N><N>    pass<N><N><N>class SubjectAltNameWarning(SecurityWarning):<N>    """Warned when connecting to a host with a certificate missing a SAN."""<N><N>
    pass<N><N><N>class InsecureRequestWarning(SecurityWarning):<N>    """Warned when making an unverified HTTPS request."""<N><N>    pass<N><N><N>class SystemTimeWarning(SecurityWarning):<N>    """Warned when system time is suspected to be wrong"""<N><N>
    pass<N><N><N>class InsecurePlatformWarning(SecurityWarning):<N>    """Warned when certain TLS/SSL configuration is not available on a platform."""<N><N>    pass<N><N><N>class SNIMissingWarning(HTTPWarning):<N>    """Warned when making a HTTPS request without SNI available."""<N><N>
    pass<N><N><N>class DependencyWarning(HTTPWarning):<N>    """<N>    Warned when an attempt is made to import a module with missing optional<N>    dependencies.<N>    """<N><N>    pass<N><N><N>class ResponseNotChunked(ProtocolError, ValueError):<N>    """Response needs to be chunked in order to read it as chunks."""<N><N>
    pass<N><N><N>class BodyNotHttplibCompatible(HTTPError):<N>    """<N>    Body should be :class:`http.client.HTTPResponse` like<N>    (have an fp attribute which returns raw chunks) for read_chunked().<N>    """<N><N>    pass<N><N><N>class IncompleteRead(HTTPError, httplib_IncompleteRead):<N>    """<N>    Response length doesn't match expected Content-Length<N><N>
    Subclass of :class:`http.client.IncompleteRead` to allow int value<N>    for ``partial`` to avoid creating large objects on streamed reads.<N>    """<N><N>    def __init__(self, partial, expected):<N>        super(IncompleteRead, self).__init__(partial, expected)<N><N>
    def __repr__(self):<N>        return "IncompleteRead(%i bytes read, %i more expected)" % (<N>            self.partial,<N>            self.expected,<N>        )<N><N><N>class InvalidChunkLength(HTTPError, httplib_IncompleteRead):<N>    """Invalid chunk length in a chunked response."""<N><N>
    def __init__(self, response, length):<N>        super(InvalidChunkLength, self).__init__(<N>            response.tell(), response.length_remaining<N>        )<N>        self.response = response<N>        self.length = length<N><N>    def __repr__(self):<N>        return "InvalidChunkLength(got length %r, %i bytes read)" % (<N>            self.length,<N>            self.partial,<N>        )<N><N>
<N>class InvalidHeader(HTTPError):<N>    """The header provided was somehow invalid."""<N><N>    pass<N><N><N>class ProxySchemeUnknown(AssertionError, URLSchemeUnknown):<N>    """ProxyManager does not support the supplied scheme"""<N><N>    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.<N><N>
from __future__ import absolute_import<N><N>import email.utils<N>import mimetypes<N>import re<N><N>from .packages import six<N><N><N>def guess_content_type(filename, default="application/octet-stream"):<N>    """<N>    Guess the "Content-Type" of a file.<N><N>
    :param filename:<N>        The filename to guess the "Content-Type" of using :mod:`mimetypes`.<N>    :param default:<N>        If no "Content-Type" can be guessed, default to `default`.<N>    """<N>    if filename:<N>        return mimetypes.guess_type(filename)[0] or default<N>    return default<N><N>
<N>def format_header_param_rfc2231(name, value):<N>    """<N>    Helper function to format and quote a single header parameter using the<N>    strategy defined in RFC 2231.<N><N>    Particularly useful for header parameters which might contain<N>    non-ASCII values, like file names. This follows<N>    `RFC 2388 Section 4.4 <https://tools.ietf.org/html/rfc2388#section-4.4>`_.<N><N>
    :param name:<N>        The name of the parameter, a string expected to be ASCII only.<N>    :param value:<N>        The value of the parameter, provided as ``bytes`` or `str``.<N>    :ret:<N>        An RFC-2231-formatted unicode string.<N>    """<N>    if isinstance(value, six.binary_type):<N>        value = value.decode("utf-8")<N><N>
    if not any(ch in value for ch in '"\\\r\n'):<N>        result = u'%s="%s"' % (name, value)<N>        try:<N>            result.encode("ascii")<N>        except (UnicodeEncodeError, UnicodeDecodeError):<N>            pass<N>        else:<N>            return result<N><N>
    if six.PY2:  # Python 2:<N>        value = value.encode("utf-8")<N><N>    # encode_rfc2231 accepts an encoded string and returns an ascii-encoded<N>    # string in Python 2 but accepts and returns unicode strings in Python 3<N>    value = email.utils.encode_rfc2231(value, "utf-8")<N>    value = "%s*=%s" % (name, value)<N><N>
    if six.PY2:  # Python 2:<N>        value = value.decode("utf-8")<N><N>    return value<N><N><N>_HTML5_REPLACEMENTS = {<N>    u"\u0022": u"%22",<N>    # Replace "\" with "\\".<N>    u"\u005C": u"\u005C\u005C",<N>}<N><N># All control characters from 0x00 to 0x1F *except* 0x1B.<N>_HTML5_REPLACEMENTS.update(<N>    {<N>        six.unichr(cc): u"%{:02X}".format(cc)<N>        for cc in range(0x00, 0x1F + 1)<N>        if cc not in (0x1B,)<N>    }<N>)<N><N>
<N>def _replace_multiple(value, needles_and_replacements):<N>    def replacer(match):<N>        return needles_and_replacements[match.group(0)]<N><N>    pattern = re.compile(<N>        r"|".join([re.escape(needle) for needle in needles_and_replacements.keys()])<N>    )<N><N>
    result = pattern.sub(replacer, value)<N><N>    return result<N><N><N>def format_header_param_html5(name, value):<N>    """<N>    Helper function to format and quote a single header parameter using the<N>    HTML5 strategy.<N><N>    Particularly useful for header parameters which might contain<N>    non-ASCII values, like file names. This follows the `HTML5 Working Draft<N>    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.<N><N>
    .. _HTML5 Working Draft Section 4.10.22.7:<N>        https://w3c.github.io/html/sec-forms.html#multipart-form-data<N><N>    :param name:<N>        The name of the parameter, a string expected to be ASCII only.<N>    :param value:<N>        The value of the parameter, provided as ``bytes`` or `str``.<N>    :ret:<N>        A unicode string, stripped of troublesome characters.<N>    """<N>    if isinstance(value, six.binary_type):<N>        value = value.decode("utf-8")<N><N>
    value = _replace_multiple(value, _HTML5_REPLACEMENTS)<N><N>    return u'%s="%s"' % (name, value)<N><N><N># For backwards-compatibility.<N>format_header_param = format_header_param_html5<N><N><N>class RequestField(object):<N>    """<N>    A data container for request body parameters.<N><N>
    :param name:<N>        The name of this request field. Must be unicode.<N>    :param data:<N>        The data/value body.<N>    :param filename:<N>        An optional filename of the request field. Must be unicode.<N>    :param headers:<N>        An optional dict-like object of headers to initially use for the field.<N>    :param header_formatter:<N>        An optional callable that is used to encode and format the headers. By<N>        default, this is :func:`format_header_param_html5`.<N>    """<N><N>
    def __init__(<N>        self,<N>        name,<N>        data,<N>        filename=None,<N>        headers=None,<N>        header_formatter=format_header_param_html5,<N>    ):<N>        self._name = name<N>        self._filename = filename<N>        self.data = data<N>        self.headers = {}<N>        if headers:<N>            self.headers = dict(headers)<N>        self.header_formatter = header_formatter<N><N>
    @classmethod<N>    def from_tuples(cls, fieldname, value, header_formatter=format_header_param_html5):<N>        """<N>        A :class:`~urllib3.fields.RequestField` factory from old-style tuple parameters.<N><N>        Supports constructing :class:`~urllib3.fields.RequestField` from<N>        parameter of key/value strings AND key/filetuple. A filetuple is a<N>        (filename, data, MIME type) tuple where the MIME type is optional.<N>        For example::<N><N>
            'foo': 'bar',<N>            'fakefile': ('foofile.txt', 'contents of foofile'),<N>            'realfile': ('barfile.txt', open('realfile').read()),<N>            'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'),<N>            'nonamefile': 'contents of nonamefile field',<N><N>
        Field names and filenames must be unicode.<N>        """<N>        if isinstance(value, tuple):<N>            if len(value) == 3:<N>                filename, data, content_type = value<N>            else:<N>                filename, data = value<N>                content_type = guess_content_type(filename)<N>        else:<N>            filename = None<N>            content_type = None<N>            data = value<N><N>
        request_param = cls(<N>            fieldname, data, filename=filename, header_formatter=header_formatter<N>        )<N>        request_param.make_multipart(content_type=content_type)<N><N>        return request_param<N><N>    def _render_part(self, name, value):<N>        """<N>        Overridable helper function to format a single header parameter. By<N>        default, this calls ``self.header_formatter``.<N><N>
        :param name:<N>            The name of the parameter, a string expected to be ASCII only.<N>        :param value:<N>            The value of the parameter, provided as a unicode string.<N>        """<N><N>        return self.header_formatter(name, value)<N><N>
from __future__ import absolute_import<N><N>import binascii<N>import codecs<N>import os<N>from io import BytesIO<N><N>from .fields import RequestField<N>from .packages import six<N>from .packages.six import b<N><N>writer = codecs.lookup("utf-8")[3]<N><N>
<N>def choose_boundary():<N>    """<N>    Our embarrassingly-simple replacement for mimetools.choose_boundary.<N>    """<N>    boundary = binascii.hexlify(os.urandom(16))<N>    if not six.PY2:<N>        boundary = boundary.decode("ascii")<N>    return boundary<N><N>
<N>def iter_field_objects(fields):<N>    """<N>    Iterate over fields.<N><N>    Supports list of (k, v) tuples and dicts, and lists of<N>    :class:`~urllib3.fields.RequestField`.<N><N>    """<N>    if isinstance(fields, dict):<N>        i = six.iteritems(fields)<N>    else:<N>        i = iter(fields)<N><N>
    for field in i:<N>        if isinstance(field, RequestField):<N>            yield field<N>        else:<N>            yield RequestField.from_tuples(*field)<N><N><N>def iter_fields(fields):<N>    """<N>    .. deprecated:: 1.6<N><N>    Iterate over fields.<N><N>
    The addition of :class:`~urllib3.fields.RequestField` makes this function<N>    obsolete. Instead, use :func:`iter_field_objects`, which returns<N>    :class:`~urllib3.fields.RequestField` objects.<N><N>    Supports list of (k, v) tuples and dicts.<N>    """<N>    if isinstance(fields, dict):<N>        return ((k, v) for k, v in six.iteritems(fields))<N><N>
    return ((k, v) for k, v in fields)<N><N><N>def encode_multipart_formdata(fields, boundary=None):<N>    """<N>    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.<N><N>    :param fields:<N>        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).<N><N>
    :param boundary:<N>        If not specified, then a random boundary will be generated using<N>        :func:`urllib3.filepost.choose_boundary`.<N>    """<N>    body = BytesIO()<N>    if boundary is None:<N>        boundary = choose_boundary()<N><N>
    for field in iter_field_objects(fields):<N>        body.write(b("--%s\r\n" % (boundary)))<N><N>        writer(body).write(field.render_headers())<N>        data = field.data<N><N>        if isinstance(data, int):<N>            data = str(data)  # Backwards compatibility<N><N>
        if isinstance(data, six.text_type):<N>            writer(body).write(data)<N>        else:<N>            body.write(data)<N><N>        body.write(b"\r\n")<N><N>    body.write(b("--%s--\r\n" % (boundary)))<N><N>    content_type = str("multipart/form-data; boundary=%s" % boundary)<N><N>
from __future__ import absolute_import<N><N>from .filepost import encode_multipart_formdata<N>from .packages.six.moves.urllib.parse import urlencode<N><N>__all__ = ["RequestMethods"]<N><N><N>class RequestMethods(object):<N>    """<N>    Convenience mixin for classes who implement a :meth:`urlopen` method, such<N>    as :class:`urllib3.HTTPConnectionPool` and<N>    :class:`urllib3.PoolManager`.<N><N>
    Provides behavior for making common types of HTTP request methods and<N>    decides which type of request field encoding to use.<N><N>    Specifically,<N><N>    :meth:`.request_encode_url` is for sending requests whose fields are<N>    encoded in the URL (such as GET, HEAD, DELETE).<N><N>
    :meth:`.request_encode_body` is for sending requests whose fields are<N>    encoded in the *body* of the request using multipart or www-form-urlencoded<N>    (such as for POST, PUT, PATCH).<N><N>    :meth:`.request` is for making any kind of request, it will look up the<N>    appropriate encoding format and use one of the above two methods to make<N>    the request.<N><N>
    Initializer parameters:<N><N>    :param headers:<N>        Headers to include with all requests, unless other headers are given<N>        explicitly.<N>    """<N><N>    _encode_url_methods = {"DELETE", "GET", "HEAD", "OPTIONS"}<N><N>    def __init__(self, headers=None):<N>        self.headers = headers or {}<N><N>
    def urlopen(<N>        self,<N>        method,<N>        url,<N>        body=None,<N>        headers=None,<N>        encode_multipart=True,<N>        multipart_boundary=None,<N>        **kw<N>    ):  # Abstract<N>        raise NotImplementedError(<N>            "Classes extending RequestMethods must implement "<N>            "their own ``urlopen`` method."<N>        )<N><N>
from __future__ import absolute_import<N><N>import io<N>import logging<N>import zlib<N>from contextlib import contextmanager<N>from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>try:<N>    import brotli<N>except ImportError:<N>    brotli = None<N><N>
from ._collections import HTTPHeaderDict<N>from .connection import BaseSSLError, HTTPException<N>from .exceptions import (<N>    BodyNotHttplibCompatible,<N>    DecodeError,<N>    HTTPError,<N>    IncompleteRead,<N>    InvalidChunkLength,<N>    InvalidHeader,<N>    ProtocolError,<N>    ReadTimeoutError,<N>    ResponseNotChunked,<N>    SSLError,<N>)<N>from .packages import six<N>from .util.response import is_fp_closed, is_response_to_head<N><N>
log = logging.getLogger(__name__)<N><N><N>class DeflateDecoder(object):<N>    def __init__(self):<N>        self._first_try = True<N>        self._data = b""<N>        self._obj = zlib.decompressobj()<N><N>    def __getattr__(self, name):<N>        return getattr(self._obj, name)<N><N>
from __future__ import absolute_import<N><N>try:<N>    from collections.abc import Mapping, MutableMapping<N>except ImportError:<N>    from collections import Mapping, MutableMapping<N>try:<N>    from threading import RLock<N>except ImportError:  # Platform-specific: No threads available<N><N>
    class RLock:<N>        def __enter__(self):<N>            pass<N><N>        def __exit__(self, exc_type, exc_value, traceback):<N>            pass<N><N><N>from collections import OrderedDict<N><N>from .exceptions import InvalidHeader<N>from .packages import six<N>from .packages.six import iterkeys, itervalues<N><N>
__all__ = ["RecentlyUsedContainer", "HTTPHeaderDict"]<N><N><N>_Null = object()<N><N><N>class RecentlyUsedContainer(MutableMapping):<N>    """<N>    Provides a thread-safe dict-like container which maintains up to<N>    ``maxsize`` keys while throwing away the least-recently-used keys beyond<N>    ``maxsize``.<N><N>
    :param maxsize:<N>        Maximum number of recent elements to retain.<N><N>    :param dispose_func:<N>        Every time an item is evicted from the container,<N>        ``dispose_func(value)`` is called.  Callback which will get called<N>    """<N><N>
    ContainerCls = OrderedDict<N><N>    def __init__(self, maxsize=10, dispose_func=None):<N>        self._maxsize = maxsize<N>        self.dispose_func = dispose_func<N><N>        self._container = self.ContainerCls()<N>        self.lock = RLock()<N><N>
    def __getitem__(self, key):<N>        # Re-insert the item, moving it to the end of the eviction line.<N>        with self.lock:<N>            item = self._container.pop(key)<N>            self._container[key] = item<N>            return item<N><N>
    def __setitem__(self, key, value):<N>        evicted_value = _Null<N>        with self.lock:<N>            # Possibly evict the existing value of 'key'<N>            evicted_value = self._container.get(key, _Null)<N>            self._container[key] = value<N><N>
            # If we didn't evict an existing value, we might have to evict the<N>            # least recently used item from the beginning of the container.<N>            if len(self._container) > self._maxsize:<N>                _key, evicted_value = self._container.popitem(last=False)<N><N>
        if self.dispose_func and evicted_value is not _Null:<N>            self.dispose_func(evicted_value)<N><N>    def __delitem__(self, key):<N>        with self.lock:<N>            value = self._container.pop(key)<N><N>        if self.dispose_func:<N>            self.dispose_func(value)<N><N>
    def __len__(self):<N>        with self.lock:<N>            return len(self._container)<N><N>    def __iter__(self):<N>        raise NotImplementedError(<N>            "Iteration over this class is unlikely to be threadsafe."<N>        )<N><N>    def clear(self):<N>        with self.lock:<N>            # Copy pointers to all values, then wipe the mapping<N>            values = list(itervalues(self._container))<N>            self._container.clear()<N><N>
        if self.dispose_func:<N>            for value in values:<N>                self.dispose_func(value)<N><N>    def keys(self):<N>        with self.lock:<N>            return list(iterkeys(self._container))<N><N><N>class HTTPHeaderDict(MutableMapping):<N>    """<N>    :param headers:<N>        An iterable of field-value pairs. Must not contain multiple field names<N>        when compared case-insensitively.<N><N>
    :param kwargs:<N>        Additional field-value pairs to pass in to ``dict.update``.<N><N>    A ``dict`` like container for storing HTTP Headers.<N><N>    Field names are stored and compared case-insensitively in compliance with<N>    RFC 7230. Iteration provides the first case-sensitive key seen for each<N>    case-insensitive pair.<N><N>
    Using ``__setitem__`` syntax overwrites fields that compare equal<N>    case-insensitively in order to maintain ``dict``'s api. For fields that<N>    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``<N>    in a loop.<N><N>    If multiple fields that are equal case-insensitively are passed to the<N>    constructor or ``.update``, the behavior is undefined and some will be<N>    lost.<N><N>
    >>> headers = HTTPHeaderDict()<N>    >>> headers.add('Set-Cookie', 'foo=bar')<N>    >>> headers.add('set-cookie', 'baz=quxx')<N>    >>> headers['content-length'] = '7'<N>    >>> headers['SET-cookie']<N>    'foo=bar, baz=quxx'<N>    >>> headers['Content-Length']<N>    '7'<N>    """<N><N>
    def __init__(self, headers=None, **kwargs):<N>        super(HTTPHeaderDict, self).__init__()<N>        self._container = OrderedDict()<N>        if headers is not None:<N>            if isinstance(headers, HTTPHeaderDict):<N>                self._copy_from(headers)<N>            else:<N>                self.extend(headers)<N>        if kwargs:<N>            self.extend(kwargs)<N><N>
    def __setitem__(self, key, val):<N>        self._container[key.lower()] = [key, val]<N>        return self._container[key.lower()]<N><N>    def __getitem__(self, key):<N>        val = self._container[key.lower()]<N>        return ", ".join(val[1:])<N><N>
"""<N>Python HTTP library with thread-safe connection pooling, file post support, user friendly, and more<N>"""<N>from __future__ import absolute_import<N><N># Set default logging handler to avoid "No handler found" warnings.<N>import logging<N>import warnings<N>from logging import NullHandler<N><N>
from . import exceptions<N>from ._version import __version__<N>from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url<N>from .filepost import encode_multipart_formdata<N>from .poolmanager import PoolManager, ProxyManager, proxy_from_url<N>from .response import HTTPResponse<N>from .util.request import make_headers<N>from .util.retry import Retry<N>from .util.timeout import Timeout<N>from .util.url import get_host<N><N>
__author__ = "Andrey Petrov (andrey.petrov@shazow.net)"<N>__license__ = "MIT"<N>__version__ = __version__<N><N>__all__ = (<N>    "HTTPConnectionPool",<N>    "HTTPSConnectionPool",<N>    "PoolManager",<N>    "ProxyManager",<N>    "HTTPResponse",<N>    "Retry",<N>    "Timeout",<N>    "add_stderr_logger",<N>    "connection_from_url",<N>    "disable_warnings",<N>    "encode_multipart_formdata",<N>    "get_host",<N>    "make_headers",<N>    "proxy_from_url",<N>)<N><N>
"""<N>This module provides a pool manager that uses Google App Engine's<N>`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.<N><N>Example usage::<N><N>    from pip._vendor.urllib3 import PoolManager<N>    from pip._vendor.urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox<N><N>
    if is_appengine_sandbox():<N>        # AppEngineManager uses AppEngine's URLFetch API behind the scenes<N>        http = AppEngineManager()<N>    else:<N>        # PoolManager uses a socket-level API behind the scenes<N>        http = PoolManager()<N><N>
    r = http.request('GET', 'https://google.com/')<N><N>There are `limitations <https://cloud.google.com/appengine/docs/python/\<N>urlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be<N>the best choice for your application. There are three options for using<N>urllib3 on Google App Engine:<N><N>
"""<N>NTLM authenticating pool, contributed by erikcederstran<N><N>Issue #10, see: http://code.google.com/p/urllib3/issues/detail?id=10<N>"""<N>from __future__ import absolute_import<N><N>import warnings<N>from logging import getLogger<N><N>from ntlm import ntlm<N><N>
from .. import HTTPSConnectionPool<N>from ..packages.six.moves.http_client import HTTPSConnection<N><N>warnings.warn(<N>    "The 'urllib3.contrib.ntlmpool' module is deprecated and will be removed "<N>    "in urllib3 v2.0 release, urllib3 is not able to support it properly due "<N>    "to reasons listed in issue: https://github.com/urllib3/urllib3/issues/2282. "<N>    "If you are a user of this module please comment in the mentioned issue.",<N>    DeprecationWarning,<N>)<N><N>
"""<N>TLS with SNI_-support for Python 2. Follow these instructions if you would<N>like to verify TLS certificates in Python 2. Note, the default libraries do<N>*not* do certificate checking; you need to do additional work to validate<N>certificates yourself.<N><N>
This needs the following packages installed:<N><N>* `pyOpenSSL`_ (tested with 16.0.0)<N>* `cryptography`_ (minimum 1.3.4, from pyopenssl)<N>* `idna`_ (minimum 2.0, from cryptography)<N><N>However, pyopenssl depends on cryptography, which depends on idna, so while we<N>use all three directly here we end up having relatively few packages required.<N><N>
You can install them with the following command:<N><N>.. code-block:: bash<N><N>    $ python -m pip install pyopenssl cryptography idna<N><N>To activate certificate checking, call<N>:func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code<N>before you begin making HTTP requests. This can be done in a ``sitecustomize``<N>module, or at any other time before your application begins using ``urllib3``,<N>like this:<N><N>
.. code-block:: python<N><N>    try:<N>        import pip._vendor.urllib3.contrib.pyopenssl as pyopenssl<N>        pyopenssl.inject_into_urllib3()<N>    except ImportError:<N>        pass<N><N>Now you can use :mod:`urllib3` as you normally would, and it will support SNI<N>when the required modules are installed.<N><N>
Activating this module also has the positive side effect of disabling SSL/TLS<N>compression in Python 2 (see `CRIME attack`_).<N><N>.. _sni: https://en.wikipedia.org/wiki/Server_Name_Indication<N>.. _crime attack: https://en.wikipedia.org/wiki/CRIME_(security_exploit)<N>.. _pyopenssl: https://www.pyopenssl.org<N>.. _cryptography: https://cryptography.io<N>.. _idna: https://github.com/kjd/idna<N>"""<N>from __future__ import absolute_import<N><N>
import OpenSSL.SSL<N>from cryptography import x509<N>from cryptography.hazmat.backends.openssl import backend as openssl_backend<N>from cryptography.hazmat.backends.openssl.x509 import _Certificate<N><N>try:<N>    from cryptography.x509 import UnsupportedExtension<N>except ImportError:<N>    # UnsupportedExtension is gone in cryptography >= 2.1.0<N>    class UnsupportedExtension(Exception):<N>        pass<N><N>
<N>from io import BytesIO<N>from socket import error as SocketError<N>from socket import timeout<N><N>try:  # Platform-specific: Python 2<N>    from socket import _fileobject<N>except ImportError:  # Platform-specific: Python 3<N>    _fileobject = None<N>    from ..packages.backports.makefile import backport_makefile<N><N>
import logging<N>import ssl<N>import sys<N><N>from .. import util<N>from ..packages import six<N>from ..util.ssl_ import PROTOCOL_TLS_CLIENT<N><N>__all__ = ["inject_into_urllib3", "extract_from_urllib3"]<N><N># SNI always works.<N>HAS_SNI = True<N><N>
# Map from urllib3 to PyOpenSSL compatible parameter-values.<N>_openssl_versions = {<N>    util.PROTOCOL_TLS: OpenSSL.SSL.SSLv23_METHOD,<N>    PROTOCOL_TLS_CLIENT: OpenSSL.SSL.SSLv23_METHOD,<N>    ssl.PROTOCOL_TLSv1: OpenSSL.SSL.TLSv1_METHOD,<N>}<N><N>
if hasattr(ssl, "PROTOCOL_SSLv3") and hasattr(OpenSSL.SSL, "SSLv3_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_SSLv3] = OpenSSL.SSL.SSLv3_METHOD<N><N>if hasattr(ssl, "PROTOCOL_TLSv1_1") and hasattr(OpenSSL.SSL, "TLSv1_1_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_TLSv1_1] = OpenSSL.SSL.TLSv1_1_METHOD<N><N>
if hasattr(ssl, "PROTOCOL_TLSv1_2") and hasattr(OpenSSL.SSL, "TLSv1_2_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_TLSv1_2] = OpenSSL.SSL.TLSv1_2_METHOD<N><N><N>_stdlib_to_openssl_verify = {<N>    ssl.CERT_NONE: OpenSSL.SSL.VERIFY_NONE,<N>    ssl.CERT_OPTIONAL: OpenSSL.SSL.VERIFY_PEER,<N>    ssl.CERT_REQUIRED: OpenSSL.SSL.VERIFY_PEER<N>    + OpenSSL.SSL.VERIFY_FAIL_IF_NO_PEER_CERT,<N>}<N>_openssl_to_stdlib_verify = dict((v, k) for k, v in _stdlib_to_openssl_verify.items())<N><N>
# OpenSSL will only write 16K at a time<N>SSL_WRITE_BLOCKSIZE = 16384<N><N>orig_util_HAS_SNI = util.HAS_SNI<N>orig_util_SSLContext = util.ssl_.SSLContext<N><N><N>log = logging.getLogger(__name__)<N><N><N>def inject_into_urllib3():<N>    "Monkey-patch urllib3 with PyOpenSSL-backed SSL-support."<N><N>
    _validate_dependencies_met()<N><N>    util.SSLContext = PyOpenSSLContext<N>    util.ssl_.SSLContext = PyOpenSSLContext<N>    util.HAS_SNI = HAS_SNI<N>    util.ssl_.HAS_SNI = HAS_SNI<N>    util.IS_PYOPENSSL = True<N>    util.ssl_.IS_PYOPENSSL = True<N><N>
<N>def extract_from_urllib3():<N>    "Undo monkey-patching by :func:`inject_into_urllib3`."<N><N>    util.SSLContext = orig_util_SSLContext<N>    util.ssl_.SSLContext = orig_util_SSLContext<N>    util.HAS_SNI = orig_util_HAS_SNI<N>    util.ssl_.HAS_SNI = orig_util_HAS_SNI<N>    util.IS_PYOPENSSL = False<N>    util.ssl_.IS_PYOPENSSL = False<N><N>
<N>def _validate_dependencies_met():<N>    """<N>    Verifies that PyOpenSSL's package-level dependencies have been met.<N>    Throws `ImportError` if they are not met.<N>    """<N>    # Method added in `cryptography==1.1`; not available in older versions<N>    from cryptography.x509.extensions import Extensions<N><N>
    if getattr(Extensions, "get_extension_for_class", None) is None:<N>        raise ImportError(<N>            "'cryptography' module missing required functionality.  "<N>            "Try upgrading to v1.3.4 or newer."<N>        )<N><N>    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509<N>    # attribute is only present on those versions.<N>    from OpenSSL.crypto import X509<N><N>
    x509 = X509()<N>    if getattr(x509, "_x509", None) is None:<N>        raise ImportError(<N>            "'pyOpenSSL' module missing required functionality. "<N>            "Try upgrading to v0.14 or newer."<N>        )<N><N><N>def _dnsname_to_stdlib(name):<N>    """<N>    Converts a dNSName SubjectAlternativeName field to the form used by the<N>    standard library on the given Python version.<N><N>
    Cryptography produces a dNSName as a unicode string that was idna-decoded<N>    from ASCII bytes. We need to idna-encode that string to get it back, and<N>    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib<N>    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).<N><N>
    If the name cannot be idna-encoded then we return None signalling that<N>    the name given should be skipped.<N>    """<N><N>    def idna_encode(name):<N>        """<N>        Borrowed wholesale from the Python Cryptography Project. It turns out<N>        that we can't just safely call `idna.encode`: it can explode for<N>        wildcard names. This avoids that problem.<N>        """<N>        from pip._vendor import idna<N><N>
        try:<N>            for prefix in [u"*.", u"."]:<N>                if name.startswith(prefix):<N>                    name = name[len(prefix) :]<N>                    return prefix.encode("ascii") + idna.encode(name)<N>            return idna.encode(name)<N>        except idna.core.IDNAError:<N>            return None<N><N>
    # Don't send IPv6 addresses through the IDNA encoder.<N>    if ":" in name:<N>        return name<N><N>    name = idna_encode(name)<N>    if name is None:<N>        return None<N>    elif sys.version_info >= (3, 0):<N>        name = name.decode("utf-8")<N>    return name<N><N>
"""<N>SecureTranport support for urllib3 via ctypes.<N><N>This makes platform-native TLS available to urllib3 users on macOS without the<N>use of a compiler. This is an important feature because the Python Package<N>Index is moving to become a TLSv1.2-or-higher server, and the default OpenSSL<N>that ships with macOS is not capable of doing TLSv1.2. The only way to resolve<N>this is to give macOS users an alternative solution to the problem, and that<N>solution is to use SecureTransport.<N><N>
We use ctypes here because this solution must not require a compiler. That's<N>because pip is not allowed to require a compiler either.<N><N>This is not intended to be a seriously long-term solution to this problem.<N>The hope is that PEP 543 will eventually solve this issue for us, at which<N>point we can retire this contrib module. But in the short term, we need to<N>solve the impending tire fire that is Python on Mac without this kind of<N>contrib module. So...here we are.<N><N>
To use this module, simply import and inject it::<N><N>    import pip._vendor.urllib3.contrib.securetransport as securetransport<N>    securetransport.inject_into_urllib3()<N><N>Happy TLSing!<N><N>This code is a bastardised version of the code found in Will Bond's oscrypto<N>library. An enormous debt is owed to him for blazing this trail for us. For<N>that reason, this code should be considered to be covered both by urllib3's<N>license and by oscrypto's:<N><N>
# -*- coding: utf-8 -*-<N>"""<N>This module contains provisional support for SOCKS proxies from within<N>urllib3. This module supports SOCKS4, SOCKS4A (an extension of SOCKS4), and<N>SOCKS5. To enable its functionality, either install PySocks or install this<N>module with the ``socks`` extra.<N><N>
The SOCKS implementation supports the full range of urllib3 features. It also<N>supports the following SOCKS features:<N><N>- SOCKS4A (``proxy_url='socks4a://...``)<N>- SOCKS4 (``proxy_url='socks4://...``)<N>- SOCKS5 with remote DNS (``proxy_url='socks5h://...``)<N>- SOCKS5 with local DNS (``proxy_url='socks5://...``)<N>- Usernames and passwords for the SOCKS proxy<N><N>
.. note::<N>   It is recommended to use ``socks5h://`` or ``socks4a://`` schemes in<N>   your ``proxy_url`` to ensure that DNS resolution is done from the remote<N>   server instead of client-side when connecting to a domain name.<N><N>SOCKS4 supports IPv4 and domain names with the SOCKS4A extension. SOCKS5<N>supports IPv4, IPv6, and domain names.<N><N>
When connecting to a SOCKS4 proxy the ``username`` portion of the ``proxy_url``<N>will be sent as the ``userid`` section of the SOCKS request:<N><N>.. code-block:: python<N><N>    proxy_url="socks4a://<userid>@proxy-host"<N><N>When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion<N>of the ``proxy_url`` will be sent as the username/password to authenticate<N>with the proxy:<N><N>
.. code-block:: python<N><N>    proxy_url="socks5h://<username>:<password>@proxy-host"<N><N>"""<N>from __future__ import absolute_import<N><N>try:<N>    import socks<N>except ImportError:<N>    import warnings<N><N>    from ..exceptions import DependencyWarning<N><N>
    warnings.warn(<N>        (<N>            "SOCKS support in urllib3 requires the installation of optional "<N>            "dependencies: specifically, PySocks.  For more information, see "<N>            "https://urllib3.readthedocs.io/en/1.26.x/contrib.html#socks-proxies"<N>        ),<N>        DependencyWarning,<N>    )<N>    raise<N><N>
from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>from ..connection import HTTPConnection, HTTPSConnection<N>from ..connectionpool import HTTPConnectionPool, HTTPSConnectionPool<N>from ..exceptions import ConnectTimeoutError, NewConnectionError<N>from ..poolmanager import PoolManager<N>from ..util.url import parse_url<N><N>
try:<N>    import ssl<N>except ImportError:<N>    ssl = None<N><N><N>class SOCKSConnection(HTTPConnection):<N>    """<N>    A plain-text HTTP connection that connects via a SOCKS proxy.<N>    """<N><N>    def __init__(self, *args, **kwargs):<N>        self._socks_options = kwargs.pop("_socks_options")<N>        super(SOCKSConnection, self).__init__(*args, **kwargs)<N><N>
    def _new_conn(self):<N>        """<N>        Establish a new connection via the SOCKS proxy.<N>        """<N>        extra_kw = {}<N>        if self.source_address:<N>            extra_kw["source_address"] = self.source_address<N><N>        if self.socket_options:<N>            extra_kw["socket_options"] = self.socket_options<N><N>
"""<N>This module provides means to detect the App Engine environment.<N>"""<N><N>import os<N><N><N>def is_appengine():<N>    return is_local_appengine() or is_prod_appengine()<N><N><N>def is_appengine_sandbox():<N>    """Reports if the app is running in the first generation sandbox.<N><N>
    The second generation runtimes are technically still in a sandbox, but it<N>    is much less restrictive, so generally you shouldn't need to check for it.<N>    see https://cloud.google.com/appengine/docs/standard/runtimes<N>    """<N>    return is_appengine() and os.environ["APPENGINE_RUNTIME"] == "python27"<N><N>
<N>def is_local_appengine():<N>    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(<N>        "SERVER_SOFTWARE", ""<N>    ).startswith("Development/")<N><N><N>def is_prod_appengine():<N>    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(<N>        "SERVER_SOFTWARE", ""<N>    ).startswith("Google App Engine/")<N><N>
"""<N>This module uses ctypes to bind a whole bunch of functions and constants from<N>SecureTransport. The goal here is to provide the low-level API to<N>SecureTransport. These are essentially the C-level functions and constants, and<N>they're pretty gross to work with.<N><N>
This code is a bastardised version of the code found in Will Bond's oscrypto<N>library. An enormous debt is owed to him for blazing this trail for us. For<N>that reason, this code should be considered to be covered both by urllib3's<N>license and by oscrypto's:<N><N>
from __future__ import absolute_import<N><N>import socket<N><N>from ..contrib import _appengine_environ<N>from ..exceptions import LocationParseError<N>from ..packages import six<N>from .wait import NoWayToWaitForSocketError, wait_for_read<N><N><N>def is_connection_dropped(conn):  # Platform-specific<N>    """<N>    Returns True if the connection is dropped and should be closed.<N><N>
from .ssl_ import create_urllib3_context, resolve_cert_reqs, resolve_ssl_version<N><N><N>def connection_requires_http_tunnel(<N>    proxy_url=None, proxy_config=None, destination_scheme=None<N>):<N>    """<N>    Returns True if the connection requires an HTTP CONNECT through the proxy.<N><N>
    :param URL proxy_url:<N>        URL of the proxy.<N>    :param ProxyConfig proxy_config:<N>        Proxy configuration from poolmanager.py<N>    :param str destination_scheme:<N>        The scheme of the destination. (i.e https, http, etc)<N>    """<N>    # If we're not using a proxy, no way to use a tunnel.<N>    if proxy_url is None:<N>        return False<N><N>
    # HTTP destinations never require tunneling, we always forward.<N>    if destination_scheme == "http":<N>        return False<N><N>    # Support for forwarding with HTTPS proxies and HTTPS destinations.<N>    if (<N>        proxy_url.scheme == "https"<N>        and proxy_config<N>        and proxy_config.use_forwarding_for_https<N>    ):<N>        return False<N><N>
    # Otherwise always use a tunnel.<N>    return True<N><N><N>def create_proxy_ssl_context(<N>    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None<N>):<N>    """<N>    Generates a default proxy ssl context if one hasn't been provided by the<N>    user.<N>    """<N>    ssl_context = create_urllib3_context(<N>        ssl_version=resolve_ssl_version(ssl_version),<N>        cert_reqs=resolve_cert_reqs(cert_reqs),<N>    )<N><N>
import collections<N><N>from ..packages import six<N>from ..packages.six.moves import queue<N><N>if six.PY2:<N>    # Queue is imported for side effects on MS Windows. See issue #229.<N>    import Queue as _unused_module_Queue  # noqa: F401<N><N><N>class LifoQueue(queue.Queue):<N>    def _init(self, _):<N>        self.queue = collections.deque()<N><N>    def _qsize(self, len=len):<N>        return len(self.queue)<N><N>    def _put(self, item):<N>        self.queue.append(item)<N><N>    def _get(self):<N>        return self.queue.pop()<N>
from __future__ import absolute_import<N><N>from base64 import b64encode<N><N>from ..exceptions import UnrewindableBodyError<N>from ..packages.six import b, integer_types<N><N># Pass as a value within ``headers`` to skip<N># emitting some HTTP headers that are added automatically.<N># The only headers that are supported are ``Accept-Encoding``,<N># ``Host``, and ``User-Agent``.<N>SKIP_HEADER = "@@@SKIP_HEADER@@@"<N>SKIPPABLE_HEADERS = frozenset(["accept-encoding", "host", "user-agent"])<N><N>
ACCEPT_ENCODING = "gzip,deflate"<N>try:<N>    import brotli as _unused_module_brotli  # noqa: F401<N>except ImportError:<N>    pass<N>else:<N>    ACCEPT_ENCODING += ",br"<N><N>_FAILEDTELL = object()<N><N><N>def make_headers(<N>    keep_alive=None,<N>    accept_encoding=None,<N>    user_agent=None,<N>    basic_auth=None,<N>    proxy_basic_auth=None,<N>    disable_cache=None,<N>):<N>    """<N>    Shortcuts for generating request headers.<N><N>
    :param keep_alive:<N>        If ``True``, adds 'connection: keep-alive' header.<N><N>    :param accept_encoding:<N>        Can be a boolean, list, or string.<N>        ``True`` translates to 'gzip,deflate'.<N>        List will get joined by comma.<N>        String will be used as provided.<N><N>
    :param user_agent:<N>        String representing the user-agent you want, such as<N>        "python-urllib3/0.6"<N><N>    :param basic_auth:<N>        Colon-separated username:password string for 'authorization: basic ...'<N>        auth header.<N><N>
    :param proxy_basic_auth:<N>        Colon-separated username:password string for 'proxy-authorization: basic ...'<N>        auth header.<N><N>    :param disable_cache:<N>        If ``True``, adds 'cache-control: no-cache' header.<N><N>    Example::<N><N>
from __future__ import absolute_import<N><N>from email.errors import MultipartInvariantViolationDefect, StartBoundaryNotFoundDefect<N><N>from ..exceptions import HeaderParsingError<N>from ..packages.six.moves import http_client as httplib<N><N><N>def is_fp_closed(obj):<N>    """<N>    Checks whether a given file-like object is closed.<N><N>
    :param obj:<N>        The file-like object to check.<N>    """<N><N>    try:<N>        # Check `isclosed()` first, in case Python3 doesn't set `closed`.<N>        # GH Issue #928<N>        return obj.isclosed()<N>    except AttributeError:<N>        pass<N><N>
    try:<N>        # Check via the official file-like-object way.<N>        return obj.closed<N>    except AttributeError:<N>        pass<N><N>    try:<N>        # Check if the object is a container for another file-like object that<N>        # gets released on exhaustion (e.g. HTTPResponse).<N>        return obj.fp is None<N>    except AttributeError:<N>        pass<N><N>
    raise ValueError("Unable to determine whether fp is closed.")<N><N><N>def assert_header_parsing(headers):<N>    """<N>    Asserts whether all headers have been successfully parsed.<N>    Extracts encountered errors from the result of parsing headers.<N><N>
    Only works on Python 3.<N><N>    :param http.client.HTTPMessage headers: Headers to verify.<N><N>    :raises urllib3.exceptions.HeaderParsingError:<N>        If parsing errors are found.<N>    """<N><N>    # This will fail silently if we pass in the wrong kind of parameter.<N>    # To make debugging easier add an explicit check.<N>    if not isinstance(headers, httplib.HTTPMessage):<N>        raise TypeError("expected httplib.Message, got {0}.".format(type(headers)))<N><N>
    defects = getattr(headers, "defects", None)<N>    get_payload = getattr(headers, "get_payload", None)<N><N>    unparsed_data = None<N>    if get_payload:<N>        # get_payload is actually email.message.Message.get_payload;<N>        # we're only interested in the result if it's not a multipart message<N>        if not headers.is_multipart():<N>            payload = get_payload()<N><N>
            if isinstance(payload, (bytes, str)):<N>                unparsed_data = payload<N>    if defects:<N>        # httplib is assuming a response body is available<N>        # when parsing headers even when httplib only sends<N>        # header data to parse_headers() This results in<N>        # defects on multipart responses in particular.<N>        # See: https://github.com/urllib3/urllib3/issues/800<N><N>
from __future__ import absolute_import<N><N>import email<N>import logging<N>import re<N>import time<N>import warnings<N>from collections import namedtuple<N>from itertools import takewhile<N><N>from ..exceptions import (<N>    ConnectTimeoutError,<N>    InvalidHeader,<N>    MaxRetryError,<N>    ProtocolError,<N>    ProxyError,<N>    ReadTimeoutError,<N>    ResponseError,<N>)<N>from ..packages import six<N><N>
log = logging.getLogger(__name__)<N><N><N># Data structure for representing the metadata of requests that result in a retry.<N>RequestHistory = namedtuple(<N>    "RequestHistory", ["method", "url", "error", "status", "redirect_location"]<N>)<N><N><N># TODO: In v2 we can remove this sentinel and metaclass with deprecated options.<N>_Default = object()<N><N>
<N>class _RetryMeta(type):<N>    @property<N>    def DEFAULT_METHOD_WHITELIST(cls):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_ALLOWED_METHODS<N><N>
    @DEFAULT_METHOD_WHITELIST.setter<N>    def DEFAULT_METHOD_WHITELIST(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_ALLOWED_METHODS = value<N><N>
    @property<N>    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT<N><N>
    @DEFAULT_REDIRECT_HEADERS_BLACKLIST.setter<N>    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT = value<N><N>
    @property<N>    def BACKOFF_MAX(cls):<N>        warnings.warn(<N>            "Using 'Retry.BACKOFF_MAX' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_BACKOFF_MAX<N><N>
    @BACKOFF_MAX.setter<N>    def BACKOFF_MAX(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.BACKOFF_MAX' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_BACKOFF_MAX = value<N><N>
<N>@six.add_metaclass(_RetryMeta)<N>class Retry(object):<N>    """Retry configuration.<N><N>    Each retry attempt will create a new Retry object with updated values, so<N>    they can be safely reused.<N><N>    Retries can be defined as a default for a pool::<N><N>
        retries = Retry(connect=5, read=2, redirect=5)<N>        http = PoolManager(retries=retries)<N>        response = http.request('GET', 'http://example.com/')<N><N>    Or per-request (which overrides the default for the pool)::<N><N>        response = http.request('GET', 'http://example.com/', retries=Retry(10))<N><N>
    Retries can be disabled by passing ``False``::<N><N>        response = http.request('GET', 'http://example.com/', retries=False)<N><N>    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless<N>    retries are disabled, in which case the causing exception will be raised.<N><N>
    :param int total:<N>        Total number of retries to allow. Takes precedence over other counts.<N><N>        Set to ``None`` to remove this constraint and fall back on other<N>        counts.<N><N>        Set to ``0`` to fail on the first retry.<N><N>
        Set to ``False`` to disable and imply ``raise_on_redirect=False``.<N><N>    :param int connect:<N>        How many connection-related errors to retry on.<N><N>        These are errors raised before the request is sent to the remote server,<N>        which we assume has not triggered the server to process the request.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int read:<N>        How many times to retry on read errors.<N><N>        These errors are raised after the request was sent to the server, so the<N>        request may have side-effects.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int redirect:<N>        How many redirects to perform. Limit this to avoid infinite redirect<N>        loops.<N><N>        A redirect is a HTTP response with a status code 301, 302, 303, 307 or<N>        308.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>        Set to ``False`` to disable and imply ``raise_on_redirect=False``.<N><N>    :param int status:<N>        How many times to retry on bad status codes.<N><N>        These are retries made on responses, where status code matches<N>        ``status_forcelist``.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int other:<N>        How many times to retry on other errors.<N><N>        Other errors are errors that are not connect, read, redirect or status errors.<N>        These errors might be raised after the request was sent to the server, so the<N>        request might have side-effects.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>        If ``total`` is not set, it's a good idea to set this to 0 to account<N>        for unexpected edge cases and avoid infinite retry loops.<N><N>    :param iterable allowed_methods:<N>        Set of uppercased HTTP method verbs that we should retry on.<N><N>
        By default, we only retry on methods which are considered to be<N>        idempotent (multiple requests with the same parameters end with the<N>        same state). See :attr:`Retry.DEFAULT_ALLOWED_METHODS`.<N><N>        Set to a ``False`` value to retry on any verb.<N><N>
        .. warning::<N><N>            Previously this parameter was named ``method_whitelist``, that<N>            usage is deprecated in v1.26.0 and will be removed in v2.0.<N><N>    :param iterable status_forcelist:<N>        A set of integer HTTP status codes that we should force a retry on.<N>        A retry is initiated if the request method is in ``allowed_methods``<N>        and the response status code is in ``status_forcelist``.<N><N>
        By default, this is disabled with ``None``.<N><N>    :param float backoff_factor:<N>        A backoff factor to apply between attempts after the second try<N>        (most errors are resolved immediately by a second try without a<N>        delay). urllib3 will sleep for::<N><N>
            {backoff factor} * (2 ** ({number of total retries} - 1))<N><N>        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep<N>        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer<N>        than :attr:`Retry.DEFAULT_BACKOFF_MAX`.<N><N>
        By default, backoff is disabled (set to 0).<N><N>    :param bool raise_on_redirect: Whether, if the number of redirects is<N>        exhausted, to raise a MaxRetryError, or to return a response with a<N>        response code in the 3xx range.<N><N>
    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:<N>        whether we should raise an exception, or return a response,<N>        if status falls in ``status_forcelist`` range and retries have<N>        been exhausted.<N><N>    :param tuple history: The history of the request encountered during<N>        each call to :meth:`~Retry.increment`. The list is in the order<N>        the requests occurred. Each list item is of class :class:`RequestHistory`.<N><N>
    :param bool respect_retry_after_header:<N>        Whether to respect Retry-After header on status codes defined as<N>        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.<N><N>    :param iterable remove_headers_on_redirect:<N>        Sequence of headers to remove from the request when a response<N>        indicating a redirect is returned before firing off the redirected<N>        request.<N>    """<N><N>
    #: Default methods to be used for ``allowed_methods``<N>    DEFAULT_ALLOWED_METHODS = frozenset(<N>        ["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]<N>    )<N><N>    #: Default status codes to be used for ``status_forcelist``<N>    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])<N><N>
import io<N>import socket<N>import ssl<N><N>from ..exceptions import ProxySchemeUnsupported<N>from ..packages import six<N><N>SSL_BLOCKSIZE = 16384<N><N><N>class SSLTransport:<N>    """<N>    The SSLTransport wraps an existing socket and establishes an SSL connection.<N><N>
    Contrary to Python's implementation of SSLSocket, it allows you to chain<N>    multiple TLS connections together. It's particularly useful if you need to<N>    implement TLS within TLS.<N><N>    The class supports most of the socket API operations.<N>    """<N><N>
    @staticmethod<N>    def _validate_ssl_context_for_tls_in_tls(ssl_context):<N>        """<N>        Raises a ProxySchemeUnsupported if the provided ssl_context can't be used<N>        for TLS in TLS.<N><N>        The only requirement is that the ssl_context provides the 'wrap_bio'<N>        methods.<N>        """<N><N>
        if not hasattr(ssl_context, "wrap_bio"):<N>            if six.PY2:<N>                raise ProxySchemeUnsupported(<N>                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "<N>                    "supported on Python 2"<N>                )<N>            else:<N>                raise ProxySchemeUnsupported(<N>                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "<N>                    "available on non-native SSLContext"<N>                )<N><N>
    def __init__(<N>        self, socket, ssl_context, server_hostname=None, suppress_ragged_eofs=True<N>    ):<N>        """<N>        Create an SSLTransport around socket using the provided ssl_context.<N>        """<N>        self.incoming = ssl.MemoryBIO()<N>        self.outgoing = ssl.MemoryBIO()<N><N>
        self.suppress_ragged_eofs = suppress_ragged_eofs<N>        self.socket = socket<N><N>        self.sslobj = ssl_context.wrap_bio(<N>            self.incoming, self.outgoing, server_hostname=server_hostname<N>        )<N><N>        # Perform initial handshake.<N>        self._ssl_io_loop(self.sslobj.do_handshake)<N><N>
    def __enter__(self):<N>        return self<N><N>    def __exit__(self, *_):<N>        self.close()<N><N>    def fileno(self):<N>        return self.socket.fileno()<N><N>    def read(self, len=1024, buffer=None):<N>        return self._wrap_ssl_read(len, buffer)<N><N>
from __future__ import absolute_import<N><N>import hmac<N>import os<N>import sys<N>import warnings<N>from binascii import hexlify, unhexlify<N>from hashlib import md5, sha1, sha256<N><N>from ..exceptions import (<N>    InsecurePlatformWarning,<N>    ProxySchemeUnsupported,<N>    SNIMissingWarning,<N>    SSLError,<N>)<N>from ..packages import six<N>from .url import BRACELESS_IPV6_ADDRZ_RE, IPV4_RE<N><N>
SSLContext = None<N>SSLTransport = None<N>HAS_SNI = False<N>IS_PYOPENSSL = False<N>IS_SECURETRANSPORT = False<N>ALPN_PROTOCOLS = ["http/1.1"]<N><N># Maps the length of a digest to a possible hash function producing this digest<N>HASHFUNC_MAP = {32: md5, 40: sha1, 64: sha256}<N><N>
<N>def _const_compare_digest_backport(a, b):<N>    """<N>    Compare two digests of equal length in constant time.<N><N>    The digests must be of type str/bytes.<N>    Returns True if the digests match, and False otherwise.<N>    """<N>    result = abs(len(a) - len(b))<N>    for left, right in zip(bytearray(a), bytearray(b)):<N>        result |= left ^ right<N>    return result == 0<N><N>
<N>_const_compare_digest = getattr(hmac, "compare_digest", _const_compare_digest_backport)<N><N>try:  # Test for SSL features<N>    import ssl<N>    from ssl import CERT_REQUIRED, wrap_socket<N>except ImportError:<N>    pass<N><N>try:<N>    from ssl import HAS_SNI  # Has SNI?<N>except ImportError:<N>    pass<N><N>
try:<N>    from .ssltransport import SSLTransport<N>except ImportError:<N>    pass<N><N><N>try:  # Platform-specific: Python 3.6<N>    from ssl import PROTOCOL_TLS<N><N>    PROTOCOL_SSLv23 = PROTOCOL_TLS<N>except ImportError:<N>    try:<N>        from ssl import PROTOCOL_SSLv23 as PROTOCOL_TLS<N><N>
        PROTOCOL_SSLv23 = PROTOCOL_TLS<N>    except ImportError:<N>        PROTOCOL_SSLv23 = PROTOCOL_TLS = 2<N><N>try:<N>    from ssl import PROTOCOL_TLS_CLIENT<N>except ImportError:<N>    PROTOCOL_TLS_CLIENT = PROTOCOL_TLS<N><N><N>try:<N>    from ssl import OP_NO_COMPRESSION, OP_NO_SSLv2, OP_NO_SSLv3<N>except ImportError:<N>    OP_NO_SSLv2, OP_NO_SSLv3 = 0x1000000, 0x2000000<N>    OP_NO_COMPRESSION = 0x20000<N><N>
from __future__ import absolute_import<N><N>import time<N><N># The default socket timeout, used by httplib to indicate that no timeout was<N># specified by the user<N>from socket import _GLOBAL_DEFAULT_TIMEOUT<N><N>from ..exceptions import TimeoutStateError<N><N>
# A sentinel value to indicate that no timeout was specified by the user in<N># urllib3<N>_Default = object()<N><N><N># Use time.monotonic if available.<N>current_time = getattr(time, "monotonic", time.time)<N><N><N>class Timeout(object):<N>    """Timeout configuration.<N><N>
    Timeouts can be defined as a default for a pool:<N><N>    .. code-block:: python<N><N>       timeout = Timeout(connect=2.0, read=7.0)<N>       http = PoolManager(timeout=timeout)<N>       response = http.request('GET', 'http://example.com/')<N><N>
    Or per-request (which overrides the default for the pool):<N><N>    .. code-block:: python<N><N>       response = http.request('GET', 'http://example.com/', timeout=Timeout(10))<N><N>    Timeouts can be disabled by setting all the parameters to ``None``:<N><N>
from __future__ import absolute_import<N><N>import re<N>from collections import namedtuple<N><N>from ..exceptions import LocationParseError<N>from ..packages import six<N><N>url_attrs = ["scheme", "auth", "host", "port", "path", "query", "fragment"]<N><N>
import errno<N>import select<N>import sys<N>from functools import partial<N><N>try:<N>    from time import monotonic<N>except ImportError:<N>    from time import time as monotonic<N><N>__all__ = ["NoWayToWaitForSocketError", "wait_for_read", "wait_for_write"]<N><N>
"""<N><N>    webencodings.labels<N>    ~~~~~~~~~~~~~~~~~~~<N><N>    Map encoding labels to their name.<N><N>    :copyright: Copyright 2012 by Simon Sapin<N>    :license: BSD, see LICENSE for details.<N><N>"""<N><N># XXX Do not edit!<N># This file is automatically generated by mklabels.py<N><N>
"""<N><N>    webencodings.mklabels<N>    ~~~~~~~~~~~~~~~~~~~~~<N><N>    Regenarate the webencodings.labels module.<N><N>    :copyright: Copyright 2012 by Simon Sapin<N>    :license: BSD, see LICENSE for details.<N><N>"""<N><N>import json<N>try:<N>    from urllib import urlopen<N>except ImportError:<N>    from urllib.request import urlopen<N><N>
<N>def assert_lower(string):<N>    assert string == string.lower()<N>    return string<N><N><N>def generate(url):<N>    parts = ['''\<N>"""<N><N>    webencodings.labels<N>    ~~~~~~~~~~~~~~~~~~~<N><N>    Map encoding labels to their name.<N><N>    :copyright: Copyright 2012 by Simon Sapin<N>    :license: BSD, see LICENSE for details.<N><N>
# coding: utf-8<N>"""<N><N>    webencodings.tests<N>    ~~~~~~~~~~~~~~~~~~<N><N>    A basic test suite for Encoding.<N><N>    :copyright: Copyright 2012 by Simon Sapin<N>    :license: BSD, see LICENSE for details.<N><N>"""<N><N>from __future__ import unicode_literals<N><N>
from . import (lookup, LABELS, decode, encode, iter_decode, iter_encode,<N>               IncrementalDecoder, IncrementalEncoder, UTF8)<N><N><N>def assert_raises(exception, function, *args, **kwargs):<N>    try:<N>        function(*args, **kwargs)<N>    except exception:<N>        return<N>    else:  # pragma: no cover<N>        raise AssertionError('Did not raise %s.' % exception)<N><N>
<N>def test_labels():<N>    assert lookup('utf-8').name == 'utf-8'<N>    assert lookup('Utf-8').name == 'utf-8'<N>    assert lookup('UTF-8').name == 'utf-8'<N>    assert lookup('utf8').name == 'utf-8'<N>    assert lookup('utf8').name == 'utf-8'<N>    assert lookup('utf8 ').name == 'utf-8'<N>    assert lookup(' \r\nutf8\t').name == 'utf-8'<N>    assert lookup('u8') is None  # Python label.<N>    assert lookup('utf-8 ') is None  # Non-ASCII white space.<N><N>
    assert lookup('US-ASCII').name == 'windows-1252'<N>    assert lookup('iso-8859-1').name == 'windows-1252'<N>    assert lookup('latin1').name == 'windows-1252'<N>    assert lookup('LATIN1').name == 'windows-1252'<N>    assert lookup('latin-1') is None<N>    assert lookup('LATİN1') is None  # ASCII-only case insensitivity.<N><N>
# coding: utf-8<N>"""<N><N>    webencodings.x_user_defined<N>    ~~~~~~~~~~~~~~~~~~~~~~~~~~~<N><N>    An implementation of the x-user-defined encoding.<N><N>    :copyright: Copyright 2012 by Simon Sapin<N>    :license: BSD, see LICENSE for details.<N><N>
"""<N><N>from __future__ import unicode_literals<N><N>import codecs<N><N><N>### Codec APIs<N><N>class Codec(codecs.Codec):<N><N>    def encode(self, input, errors='strict'):<N>        return codecs.charmap_encode(input, errors, encoding_table)<N><N>    def decode(self, input, errors='strict'):<N>        return codecs.charmap_decode(input, errors, decoding_table)<N><N>
<N>class IncrementalEncoder(codecs.IncrementalEncoder):<N>    def encode(self, input, final=False):<N>        return codecs.charmap_encode(input, self.errors, encoding_table)[0]<N><N><N>class IncrementalDecoder(codecs.IncrementalDecoder):<N>    def decode(self, input, final=False):<N>        return codecs.charmap_decode(input, self.errors, decoding_table)[0]<N><N>
<N>class StreamWriter(Codec, codecs.StreamWriter):<N>    pass<N><N><N>class StreamReader(Codec, codecs.StreamReader):<N>    pass<N><N><N>### encodings module API<N><N>codec_info = codecs.CodecInfo(<N>    name='x-user-defined',<N>    encode=Codec().encode,<N>    decode=Codec().decode,<N>    incrementalencoder=IncrementalEncoder,<N>    incrementaldecoder=IncrementalDecoder,<N>    streamreader=StreamReader,<N>    streamwriter=StreamWriter,<N>)<N><N>
"""<N>Package resource API<N>--------------------<N><N>A resource is a logical file contained within a package, or a logical<N>subdirectory thereof.  The package resource API expects resource names<N>to have their path parts separated with ``/``, *not* whatever the local<N>path separator is.  Do not use os.path operations to manipulate resource<N>names being passed into the API.<N><N>
The package resource API is designed to work with normal filesystem packages,<N>.egg files, and unpacked .egg files.  It can also work in a limited way with<N>.zip files and with custom PEP 302 loaders that support the ``get_data()``<N>method.<N>"""<N><N>
import sys<N>import os<N>import io<N>import time<N>import re<N>import types<N>import zipfile<N>import zipimport<N>import warnings<N>import stat<N>import functools<N>import pkgutil<N>import operator<N>import platform<N>import collections<N>import plistlib<N>import email.parser<N>import errno<N>import tempfile<N>import textwrap<N>import itertools<N>import inspect<N>import ntpath<N>import posixpath<N>import importlib<N>from pkgutil import get_importer<N><N>
try:<N>    import _imp<N>except ImportError:<N>    # Python 3.2 compatibility<N>    import imp as _imp<N><N>try:<N>    FileExistsError<N>except NameError:<N>    FileExistsError = OSError<N><N># capture these to bypass sandboxing<N>from os import utime<N>try:<N>    from os import mkdir, rename, unlink<N>    WRITE_SUPPORT = True<N>except ImportError:<N>    # no write support, probably under GAE<N>    WRITE_SUPPORT = False<N><N>
from os import open as os_open<N>from os.path import isdir, split<N><N>try:<N>    import importlib.machinery as importlib_machinery<N>    # access attribute to force import under delayed import mechanisms.<N>    importlib_machinery.__name__<N>except ImportError:<N>    importlib_machinery = None<N><N>
from pkg_resources.extern.jaraco.text import (<N>    yield_lines,<N>    drop_comment,<N>    join_continuation,<N>)<N><N>from pkg_resources.extern import appdirs<N>from pkg_resources.extern import packaging<N>__import__('pkg_resources.extern.packaging.version')<N>__import__('pkg_resources.extern.packaging.specifiers')<N>__import__('pkg_resources.extern.packaging.requirements')<N>__import__('pkg_resources.extern.packaging.markers')<N>__import__('pkg_resources.extern.packaging.utils')<N><N>
import importlib.util<N>import sys<N><N><N>class VendorImporter:<N>    """<N>    A PEP 302 meta path importer for finding optionally-vendored<N>    or otherwise naturally-installed packages from root_name.<N>    """<N><N>    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):<N>        self.root_name = root_name<N>        self.vendored_names = set(vendored_names)<N>        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')<N><N>
    @property<N>    def search_path(self):<N>        """<N>        Search first the vendor package then as a natural package.<N>        """<N>        yield self.vendor_pkg + '.'<N>        yield ''<N><N>    def _module_matches_namespace(self, fullname):<N>        """Figure out if the target module is vendored."""<N>        root, base, target = fullname.partition(self.root_name + '.')<N>        return not root and any(map(target.startswith, self.vendored_names))<N><N>
import io<N>import posixpath<N>import zipfile<N>import itertools<N>import contextlib<N>import sys<N>import pathlib<N><N>if sys.version_info < (3, 7):<N>    from collections import OrderedDict<N>else:<N>    OrderedDict = dict<N><N><N>__all__ = ['Path']<N><N>
<N>def _parents(path):<N>    """<N>    Given a path with elements separated by<N>    posixpath.sep, generate all parents of that path.<N><N>    >>> list(_parents('b/d'))<N>    ['b']<N>    >>> list(_parents('/b/d/'))<N>    ['/b']<N>    >>> list(_parents('b/d/f/'))<N>    ['b/d', 'b']<N>    >>> list(_parents('b'))<N>    []<N>    >>> list(_parents(''))<N>    []<N>    """<N>    return itertools.islice(_ancestry(path), 1, None)<N><N>
import abc<N>from typing import BinaryIO, Iterable, Text<N><N>from ._compat import runtime_checkable, Protocol<N><N><N>class ResourceReader(metaclass=abc.ABCMeta):<N>    """Abstract base class for loaders to provide resource reading support."""<N><N>
import collections<N>import pathlib<N>import operator<N><N>from . import abc<N><N>from ._itertools import unique_everseen<N>from ._compat import ZipPath<N><N><N>def remove_duplicates(items):<N>    return iter(collections.OrderedDict.fromkeys(items))<N><N>
<N>class FileReader(abc.TraversableResources):<N>    def __init__(self, loader):<N>        self.path = pathlib.Path(loader.path).parent<N><N>    def resource_path(self, resource):<N>        """<N>        Return the file system path to prevent<N>        `resources.path()` from creating a temporary<N>        copy.<N>        """<N>        return str(self.path.joinpath(resource))<N><N>
    def files(self):<N>        return self.path<N><N><N>class ZipReader(abc.TraversableResources):<N>    def __init__(self, loader, module):<N>        _, _, name = module.rpartition('.')<N>        self.prefix = loader.prefix.replace('\\', '/') + name + '/'<N>        self.archive = loader.archive<N><N>
    def open_resource(self, resource):<N>        try:<N>            return super().open_resource(resource)<N>        except KeyError as exc:<N>            raise FileNotFoundError(exc.args[0])<N><N>    def is_resource(self, path):<N>        # workaround for `zipfile.Path.is_file` returning true<N>        # for non-existent paths.<N>        target = self.files().joinpath(path)<N>        return target.is_file() and target.exists()<N><N>
    def files(self):<N>        return ZipPath(self.archive, self.prefix)<N><N><N>class MultiplexedPath(abc.Traversable):<N>    """<N>    Given a series of Traversable objects, implement a merged<N>    version of the interface across all objects. Useful for<N>    namespace packages which may be multihomed at a single<N>    name.<N>    """<N><N>
    def __init__(self, *paths):<N>        self._paths = list(map(pathlib.Path, remove_duplicates(paths)))<N>        if not self._paths:<N>            message = 'MultiplexedPath must contain at least one path'<N>            raise FileNotFoundError(message)<N>        if not all(path.is_dir() for path in self._paths):<N>            raise NotADirectoryError('MultiplexedPath only supports directories')<N><N>
    def iterdir(self):<N>        files = (file for path in self._paths for file in path.iterdir())<N>        return unique_everseen(files, key=operator.attrgetter('name'))<N><N>    def read_bytes(self):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>
    def read_text(self, *args, **kwargs):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>    def is_dir(self):<N>        return True<N><N>    def is_file(self):<N>        return False<N><N>    def joinpath(self, child):<N>        # first try to find child in current paths<N>        for file in self.iterdir():<N>            if file.name == child:<N>                return file<N>        # if it does not exist, construct it with the first path<N>        return self._paths[0] / child<N><N>
    __truediv__ = joinpath<N><N>    def open(self, *args, **kwargs):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>    @property<N>    def name(self):<N>        return self._paths[0].name<N><N>    def __repr__(self):<N>        paths = ', '.join(f"'{path}'" for path in self._paths)<N>        return f'MultiplexedPath({paths})'<N><N>
<N>class NamespaceReader(abc.TraversableResources):<N>    def __init__(self, namespace_path):<N>        if 'NamespacePath' not in str(namespace_path):<N>            raise ValueError('Invalid path')<N>        self.path = MultiplexedPath(*list(namespace_path))<N><N>
    def resource_path(self, resource):<N>        """<N>        Return the file system path to prevent<N>        `resources.path()` from creating a temporary<N>        copy.<N>        """<N>        return str(self.path.joinpath(resource))<N><N>    def files(self):<N>        return self.path<N><N><N>
"""<N>Interface adapters for low-level readers.<N>"""<N><N>import abc<N>import io<N>import itertools<N>from typing import BinaryIO, List<N><N>from .abc import Traversable, TraversableResources<N><N><N>class SimpleReader(abc.ABC):<N>    """<N>    The minimum, low-level interface required from a resource<N>    provider.<N>    """<N><N>
    @abc.abstractproperty<N>    def package(self):<N>        # type: () -> str<N>        """<N>        The name of the package for which this reader loads resources.<N>        """<N><N>    @abc.abstractmethod<N>    def children(self):<N>        # type: () -> List['SimpleReader']<N>        """<N>        Obtain an iterable of SimpleReader for available<N>        child containers (e.g. directories).<N>        """<N><N>
    @abc.abstractmethod<N>    def resources(self):<N>        # type: () -> List[str]<N>        """<N>        Obtain available named resources for this virtual package.<N>        """<N><N>    @abc.abstractmethod<N>    def open_binary(self, resource):<N>        # type: (str) -> BinaryIO<N>        """<N>        Obtain a File-like for a named resource.<N>        """<N><N>
    @property<N>    def name(self):<N>        return self.package.split('.')[-1]<N><N><N>class ResourceHandle(Traversable):<N>    """<N>    Handle to a named resource in a ResourceReader.<N>    """<N><N>    def __init__(self, parent, name):<N>        # type: (ResourceContainer, str) -> None<N>        self.parent = parent<N>        self.name = name  # type: ignore<N><N>
    def is_file(self):<N>        return True<N><N>    def is_dir(self):<N>        return False<N><N>    def open(self, mode='r', *args, **kwargs):<N>        stream = self.parent.reader.open_binary(self.name)<N>        if 'b' not in mode:<N>            stream = io.TextIOWrapper(*args, **kwargs)<N>        return stream<N><N>
    def joinpath(self, name):<N>        raise RuntimeError("Cannot traverse into a resource")<N><N><N>class ResourceContainer(Traversable):<N>    """<N>    Traversable container for a package's resources via its reader.<N>    """<N><N>    def __init__(self, reader):<N>        # type: (SimpleReader) -> None<N>        self.reader = reader<N><N>
    def is_dir(self):<N>        return True<N><N>    def is_file(self):<N>        return False<N><N>    def iterdir(self):<N>        files = (ResourceHandle(self, name) for name in self.reader.resources)<N>        dirs = map(ResourceContainer, self.reader.children())<N>        return itertools.chain(files, dirs)<N><N>
from contextlib import suppress<N>from io import TextIOWrapper<N><N>from . import abc<N><N><N>class SpecLoaderAdapter:<N>    """<N>    Adapt a package spec to adapt the underlying loader.<N>    """<N><N>    def __init__(self, spec, adapter=lambda spec: spec.loader):<N>        self.spec = spec<N>        self.loader = adapter(spec)<N><N>
    def __getattr__(self, name):<N>        return getattr(self.spec, name)<N><N><N>class TraversableResourcesLoader:<N>    """<N>    Adapt a loader to provide TraversableResources.<N>    """<N><N>    def __init__(self, spec):<N>        self.spec = spec<N><N>
    def get_resource_reader(self, name):<N>        return CompatibilityFiles(self.spec)._native()<N><N><N>def _io_wrapper(file, mode='r', *args, **kwargs):<N>    if mode == 'r':<N>        return TextIOWrapper(file, *args, **kwargs)<N>    elif mode == 'rb':<N>        return file<N>    raise ValueError(<N>        "Invalid mode value '{}', only 'r' and 'rb' are supported".format(mode)<N>    )<N><N>
<N>class CompatibilityFiles:<N>    """<N>    Adapter for an existing or non-existent resource reader<N>    to provide a compatibility .files().<N>    """<N><N>    class SpecPath(abc.Traversable):<N>        """<N>        Path tied to a module spec.<N>        Can be read and exposes the resource reader children.<N>        """<N><N>
        def __init__(self, spec, reader):<N>            self._spec = spec<N>            self._reader = reader<N><N>        def iterdir(self):<N>            if not self._reader:<N>                return iter(())<N>            return iter(<N>                CompatibilityFiles.ChildPath(self._reader, path)<N>                for path in self._reader.contents()<N>            )<N><N>
        def is_file(self):<N>            return False<N><N>        is_dir = is_file<N><N>        def joinpath(self, other):<N>            if not self._reader:<N>                return CompatibilityFiles.OrphanPath(other)<N>            return CompatibilityFiles.ChildPath(self._reader, other)<N><N>
        @property<N>        def name(self):<N>            return self._spec.name<N><N>        def open(self, mode='r', *args, **kwargs):<N>            return _io_wrapper(self._reader.open_resource(None), mode, *args, **kwargs)<N><N>    class ChildPath(abc.Traversable):<N>        """<N>        Path tied to a resource reader child.<N>        Can be read but doesn't expose any meaningful children.<N>        """<N><N>
        def __init__(self, reader, name):<N>            self._reader = reader<N>            self._name = name<N><N>        def iterdir(self):<N>            return iter(())<N><N>        def is_file(self):<N>            return self._reader.is_resource(self.name)<N><N>
        def is_dir(self):<N>            return not self.is_file()<N><N>        def joinpath(self, other):<N>            return CompatibilityFiles.OrphanPath(self.name, other)<N><N>        @property<N>        def name(self):<N>            return self._name<N><N>
        def open(self, mode='r', *args, **kwargs):<N>            return _io_wrapper(<N>                self._reader.open_resource(self.name), mode, *args, **kwargs<N>            )<N><N>    class OrphanPath(abc.Traversable):<N>        """<N>        Orphan path, not tied to a module spec or resource reader.<N>        Can't be read and doesn't expose any meaningful children.<N>        """<N><N>
        def __init__(self, *path_parts):<N>            if len(path_parts) < 1:<N>                raise ValueError('Need at least one path part to construct a path')<N>            self._path = path_parts<N><N>        def iterdir(self):<N>            return iter(())<N><N>
        def is_file(self):<N>            return False<N><N>        is_dir = is_file<N><N>        def joinpath(self, other):<N>            return CompatibilityFiles.OrphanPath(*self._path, other)<N><N>        @property<N>        def name(self):<N>            return self._path[-1]<N><N>
        def open(self, mode='r', *args, **kwargs):<N>            raise FileNotFoundError("Can't open orphan path")<N><N>    def __init__(self, spec):<N>        self.spec = spec<N><N>    @property<N>    def _reader(self):<N>        with suppress(AttributeError):<N>            return self.spec.loader.get_resource_reader(self.spec.name)<N><N>
    def _native(self):<N>        """<N>        Return the native reader if it supports files().<N>        """<N>        reader = self._reader<N>        return reader if hasattr(reader, 'files') else self<N><N>    def __getattr__(self, attr):<N>        return getattr(self._reader, attr)<N><N>
    def files(self):<N>        return CompatibilityFiles.SpecPath(self.spec, self._reader)<N><N><N>def wrap_spec(package):<N>    """<N>    Construct a package spec with traversable compatibility<N>    on the spec/loader/reader.<N>    """<N>    return SpecLoaderAdapter(package.__spec__, TraversableResourcesLoader)<N><N><N>
import os<N>import pathlib<N>import tempfile<N>import functools<N>import contextlib<N>import types<N>import importlib<N><N>from typing import Union, Optional<N>from .abc import ResourceReader, Traversable<N><N>from ._compat import wrap_spec<N><N>Package = Union[types.ModuleType, str]<N><N>
# flake8: noqa<N><N>import abc<N>import sys<N>import pathlib<N>from contextlib import suppress<N><N>if sys.version_info >= (3, 10):<N>    from zipfile import Path as ZipPath  # type: ignore<N>else:<N>    from ..zipp import Path as ZipPath  # type: ignore<N><N>
<N>try:<N>    from typing import runtime_checkable  # type: ignore<N>except ImportError:<N><N>    def runtime_checkable(cls):  # type: ignore<N>        return cls<N><N><N>try:<N>    from typing import Protocol  # type: ignore<N>except ImportError:<N>    Protocol = abc.ABC  # type: ignore<N><N>
<N>class TraversableResourcesLoader:<N>    """<N>    Adapt loaders to provide TraversableResources and other<N>    compatibility.<N><N>    Used primarily for Python 3.9 and earlier where the native<N>    loaders do not yet implement TraversableResources.<N>    """<N><N>
    def __init__(self, spec):<N>        self.spec = spec<N><N>    @property<N>    def path(self):<N>        return self.spec.origin<N><N>    def get_resource_reader(self, name):<N>        from . import readers, _adapters<N><N>        def _zip_reader(spec):<N>            with suppress(AttributeError):<N>                return readers.ZipReader(spec.loader, spec.name)<N><N>
        def _namespace_reader(spec):<N>            with suppress(AttributeError, ValueError):<N>                return readers.NamespaceReader(spec.submodule_search_locations)<N><N>        def _available_reader(spec):<N>            with suppress(AttributeError):<N>                return spec.loader.get_resource_reader(spec.name)<N><N>
        def _native_reader(spec):<N>            reader = _available_reader(spec)<N>            return reader if hasattr(reader, 'files') else None<N><N>        def _file_reader(spec):<N>            try:<N>                path = pathlib.Path(self.path)<N>            except TypeError:<N>                return None<N>            if path.exists():<N>                return readers.FileReader(self)<N><N>
from itertools import filterfalse<N><N>from typing import (<N>    Callable,<N>    Iterable,<N>    Iterator,<N>    Optional,<N>    Set,<N>    TypeVar,<N>    Union,<N>)<N><N># Type and type variable definitions<N>_T = TypeVar('_T')<N>_U = TypeVar('_U')<N><N>
import functools<N>import os<N>import pathlib<N>import types<N>import warnings<N><N>from typing import Union, Iterable, ContextManager, BinaryIO, TextIO, Any<N><N>from . import _common<N><N>Package = Union[types.ModuleType, str]<N>Resource = str<N><N>
<N>def deprecated(func):<N>    @functools.wraps(func)<N>    def wrapper(*args, **kwargs):<N>        warnings.warn(<N>            f"{func.__name__} is deprecated. Use files() instead. "<N>            "Refer to https://importlib-resources.readthedocs.io"<N>            "/en/latest/using.html#migrating-from-legacy for migration advice.",<N>            DeprecationWarning,<N>            stacklevel=2,<N>        )<N>        return func(*args, **kwargs)<N><N>
    return wrapper<N><N><N>def normalize_path(path):<N>    # type: (Any) -> str<N>    """Normalize a path by ensuring it is a string.<N><N>    If the resulting string contains path separators, an exception is raised.<N>    """<N>    str_path = str(path)<N>    parent, file_name = os.path.split(str_path)<N>    if parent:<N>        raise ValueError(f'{path!r} must be only a file name')<N>    return file_name<N><N>
<N>@deprecated<N>def open_binary(package: Package, resource: Resource) -> BinaryIO:<N>    """Return a file-like object opened for binary reading of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).open('rb')<N><N><N>@deprecated<N>def read_binary(package: Package, resource: Resource) -> bytes:<N>    """Return the binary contents of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).read_bytes()<N><N>
<N>@deprecated<N>def open_text(<N>    package: Package,<N>    resource: Resource,<N>    encoding: str = 'utf-8',<N>    errors: str = 'strict',<N>) -> TextIO:<N>    """Return a file-like object opened for text reading of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).open(<N>        'r', encoding=encoding, errors=errors<N>    )<N><N>
<N>@deprecated<N>def read_text(<N>    package: Package,<N>    resource: Resource,<N>    encoding: str = 'utf-8',<N>    errors: str = 'strict',<N>) -> str:<N>    """Return the decoded string of the resource.<N><N>    The decoding-related arguments have the same semantics as those of<N>    bytes.decode().<N>    """<N>    with open_text(package, resource, encoding, errors) as fp:<N>        return fp.read()<N><N>
<N>@deprecated<N>def contents(package: Package) -> Iterable[str]:<N>    """Return an iterable of entries in `package`.<N><N>    Note that not all entries are resources.  Specifically, directories are<N>    not considered resources.  Use `is_resource()` on each entry returned here<N>    to check if it is a resource or not.<N>    """<N>    return [path.name for path in _common.files(package).iterdir()]<N><N>
<N>@deprecated<N>def is_resource(package: Package, name: str) -> bool:<N>    """True if `name` is a resource inside `package`.<N><N>    Directories are *not* resources.<N>    """<N>    resource = normalize_path(name)<N>    return any(<N>        traversable.name == resource and traversable.is_file()<N>        for traversable in _common.files(package).iterdir()<N>    )<N><N>
"""Read resources contained within a package."""<N><N>from ._common import (<N>    as_file,<N>    files,<N>    Package,<N>)<N><N>from ._legacy import (<N>    contents,<N>    open_binary,<N>    read_binary,<N>    open_text,<N>    read_text,<N>    is_resource,<N>    path,<N>    Resource,<N>)<N><N>from .abc import ResourceReader<N><N><N>__all__ = [<N>    'Package',<N>    'Resource',<N>    'ResourceReader',<N>    'as_file',<N>    'contents',<N>    'files',<N>    'is_resource',<N>    'open_binary',<N>    'open_text',<N>    'path',<N>    'read_binary',<N>    'read_text',<N>]<N>
import os<N>import subprocess<N>import contextlib<N>import functools<N>import tempfile<N>import shutil<N>import operator<N><N><N>@contextlib.contextmanager<N>def pushd(dir):<N>    orig = os.getcwd()<N>    os.chdir(dir)<N>    try:<N>        yield dir<N>    finally:<N>        os.chdir(orig)<N><N>
import functools<N>import time<N>import inspect<N>import collections<N>import types<N>import itertools<N><N>import pkg_resources.extern.more_itertools<N><N>from typing import Callable, TypeVar<N><N><N>CallableT = TypeVar("CallableT", bound=Callable[..., object])<N><N>
<N>def compose(*funcs):<N>    """<N>    Compose any number of unary functions into a single unary function.<N><N>    >>> import textwrap<N>    >>> expected = str.strip(textwrap.dedent(compose.__doc__))<N>    >>> strip_and_dedent = compose(str.strip, textwrap.dedent)<N>    >>> strip_and_dedent(compose.__doc__) == expected<N>    True<N><N>
    Compose also allows the innermost function to take arbitrary arguments.<N><N>    >>> round_three = lambda x: round(x, ndigits=3)<N>    >>> f = compose(round_three, int.__truediv__)<N>    >>> [f(3*x, x+1) for x in range(1,10)]<N>    [1.5, 2.0, 2.25, 2.4, 2.5, 2.571, 2.625, 2.667, 2.7]<N>    """<N><N>
    def compose_two(f1, f2):<N>        return lambda *args, **kwargs: f1(f2(*args, **kwargs))<N><N>    return functools.reduce(compose_two, funcs)<N><N><N>def method_caller(method_name, *args, **kwargs):<N>    """<N>    Return a function that will call a named method on the<N>    target object with optional positional and keyword<N>    arguments.<N><N>
    >>> lower = method_caller('lower')<N>    >>> lower('MyString')<N>    'mystring'<N>    """<N><N>    def call_method(target):<N>        func = getattr(target, method_name)<N>        return func(*args, **kwargs)<N><N>    return call_method<N><N><N>def once(func):<N>    """<N>    Decorate func so it's only ever called the first time.<N><N>
    This decorator can ensure that an expensive or non-idempotent function<N>    will not be expensive on subsequent calls and is idempotent.<N><N>    >>> add_three = once(lambda a: a+3)<N>    >>> add_three(3)<N>    6<N>    >>> add_three(9)<N>    6<N>    >>> add_three('12')<N>    6<N><N>
    To reset the stored value, simply clear the property ``saved_result``.<N><N>    >>> del add_three.saved_result<N>    >>> add_three(9)<N>    12<N>    >>> add_three(8)<N>    12<N><N>    Or invoke 'reset()' on it.<N><N>    >>> add_three.reset()<N>    >>> add_three(-3)<N>    0<N>    >>> add_three(0)<N>    0<N>    """<N><N>
    @functools.wraps(func)<N>    def wrapper(*args, **kwargs):<N>        if not hasattr(wrapper, 'saved_result'):<N>            wrapper.saved_result = func(*args, **kwargs)<N>        return wrapper.saved_result<N><N>    wrapper.reset = lambda: vars(wrapper).__delitem__('saved_result')<N>    return wrapper<N><N>
<N>def method_cache(<N>    method: CallableT,<N>    cache_wrapper: Callable[<N>        [CallableT], CallableT<N>    ] = functools.lru_cache(),  # type: ignore[assignment]<N>) -> CallableT:<N>    """<N>    Wrap lru_cache to support storing the cache data in the object instances.<N><N>
    Abstracts the common paradigm where the method explicitly saves an<N>    underscore-prefixed protected property on first call and returns that<N>    subsequently.<N><N>    >>> class MyClass:<N>    ...     calls = 0<N>    ...<N>    ...     @method_cache<N>    ...     def method(self, value):<N>    ...         self.calls += 1<N>    ...         return value<N><N>
    >>> a = MyClass()<N>    >>> a.method(3)<N>    3<N>    >>> for x in range(75):<N>    ...     res = a.method(x)<N>    >>> a.calls<N>    75<N><N>    Note that the apparent behavior will be exactly like that of lru_cache<N>    except that the cache is stored on each instance, so values in one<N>    instance will not flush values from another, and when an instance is<N>    deleted, so are the cached values for that instance.<N><N>
    >>> b = MyClass()<N>    >>> for x in range(35):<N>    ...     res = b.method(x)<N>    >>> b.calls<N>    35<N>    >>> a.method(0)<N>    0<N>    >>> a.calls<N>    75<N><N>    Note that if method had been decorated with ``functools.lru_cache()``,<N>    a.calls would have been 76 (due to the cached value of 0 having been<N>    flushed by the 'b' instance).<N><N>
    Clear the cache with ``.cache_clear()``<N><N>    >>> a.method.cache_clear()<N><N>    Same for a method that hasn't yet been called.<N><N>    >>> c = MyClass()<N>    >>> c.method.cache_clear()<N><N>    Another cache wrapper may be supplied:<N><N>    >>> cache = functools.lru_cache(maxsize=2)<N>    >>> MyClass.method2 = method_cache(lambda self: 3, cache_wrapper=cache)<N>    >>> a = MyClass()<N>    >>> a.method2()<N>    3<N><N>
    Caution - do not subsequently wrap the method with another decorator, such<N>    as ``@property``, which changes the semantics of the function.<N><N>    See also<N>    http://code.activestate.com/recipes/577452-a-memoize-decorator-for-instance-methods/<N>    for another implementation and additional justification.<N>    """<N><N>
    def wrapper(self: object, *args: object, **kwargs: object) -> object:<N>        # it's the first call, replace the method with a cached, bound method<N>        bound_method: CallableT = types.MethodType(  # type: ignore[assignment]<N>            method, self<N>        )<N>        cached_method = cache_wrapper(bound_method)<N>        setattr(self, method.__name__, cached_method)<N>        return cached_method(*args, **kwargs)<N><N>
    # Support cache clear even before cache has been created.<N>    wrapper.cache_clear = lambda: None  # type: ignore[attr-defined]<N><N>    return (  # type: ignore[return-value]<N>        _special_method_cache(method, cache_wrapper) or wrapper<N>    )<N><N>
<N>def _special_method_cache(method, cache_wrapper):<N>    """<N>    Because Python treats special methods differently, it's not<N>    possible to use instance attributes to implement the cached<N>    methods.<N><N>    Instead, install the wrapper method under a different name<N>    and return a simple proxy to that wrapper.<N><N>
import re<N>import itertools<N>import textwrap<N>import functools<N><N>try:<N>    from importlib.resources import files  # type: ignore<N>except ImportError:  # pragma: nocover<N>    from pkg_resources.extern.importlib_resources import files  # type: ignore<N><N>
from pkg_resources.extern.jaraco.functools import compose, method_cache<N>from pkg_resources.extern.jaraco.context import ExceptionTrap<N><N><N>def substitution(old, new):<N>    """<N>    Return a function that will perform a substitution on a string<N>    """<N>    return lambda s: s.replace(old, new)<N><N>
"""Imported from the recipes section of the itertools documentation.<N><N>All functions taken from the recipes section of the itertools library docs<N>[1]_.<N>Some backward-compatible usability improvements have been made.<N><N>.. [1] http://docs.python.org/library/itertools.html#recipes<N><N>
"""<N>import warnings<N>from collections import deque<N>from itertools import (<N>    chain,<N>    combinations,<N>    count,<N>    cycle,<N>    groupby,<N>    islice,<N>    repeat,<N>    starmap,<N>    tee,<N>    zip_longest,<N>)<N>import operator<N>from random import randrange, sample, choice<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import operator<N>import os<N>import platform<N>import sys<N>from typing import Any, Callable, Dict, List, Optional, Tuple, Union<N><N>
from pkg_resources.extern.pyparsing import (  # noqa: N817<N>    Forward,<N>    Group,<N>    Literal as L,<N>    ParseException,<N>    ParseResults,<N>    QuotedString,<N>    ZeroOrMore,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .specifiers import InvalidSpecifier, Specifier<N><N>
__all__ = [<N>    "InvalidMarker",<N>    "UndefinedComparison",<N>    "UndefinedEnvironmentName",<N>    "Marker",<N>    "default_environment",<N>]<N><N>Operator = Callable[[str, str], bool]<N><N><N>class InvalidMarker(ValueError):<N>    """<N>    An invalid marker was found, users should refer to PEP 508.<N>    """<N><N>
<N>class UndefinedComparison(ValueError):<N>    """<N>    An invalid operation was attempted on a value that doesn't support it.<N>    """<N><N><N>class UndefinedEnvironmentName(ValueError):<N>    """<N>    A name was attempted to be used that does not exist inside of the<N>    environment.<N>    """<N><N>
<N>class Node:<N>    def __init__(self, value: Any) -> None:<N>        self.value = value<N><N>    def __str__(self) -> str:<N>        return str(self.value)<N><N>    def __repr__(self) -> str:<N>        return f"<{self.__class__.__name__}('{self}')>"<N><N>
    def serialize(self) -> str:<N>        raise NotImplementedError<N><N><N>class Variable(Node):<N>    def serialize(self) -> str:<N>        return str(self)<N><N><N>class Value(Node):<N>    def serialize(self) -> str:<N>        return f'"{self}"'<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>import string<N>import urllib.parse<N>from typing import List, Optional as TOptional, Set<N><N>
from pkg_resources.extern.pyparsing import (  # noqa<N>    Combine,<N>    Literal as L,<N>    Optional,<N>    ParseException,<N>    Regex,<N>    Word,<N>    ZeroOrMore,<N>    originalTextFor,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .markers import MARKER_EXPR, Marker<N>from .specifiers import LegacySpecifier, Specifier, SpecifierSet<N><N>
<N>class InvalidRequirement(ValueError):<N>    """<N>    An invalid requirement was found, users should refer to PEP 508.<N>    """<N><N><N>ALPHANUM = Word(string.ascii_letters + string.digits)<N><N>LBRACKET = L("[").suppress()<N>RBRACKET = L("]").suppress()<N>LPAREN = L("(").suppress()<N>RPAREN = L(")").suppress()<N>COMMA = L(",").suppress()<N>SEMICOLON = L(";").suppress()<N>AT = L("@").suppress()<N><N>
PUNCTUATION = Word("-_.")<N>IDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM)<N>IDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END))<N><N>NAME = IDENTIFIER("name")<N>EXTRA = IDENTIFIER<N><N>URI = Regex(r"[^ ]+")("url")<N>URL = AT + URI<N><N>
EXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA)<N>EXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)("extras")<N><N>VERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE)<N>VERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE)<N><N>
VERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY<N>VERSION_MANY = Combine(<N>    VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=",", adjacent=False<N>)("_raw_spec")<N>_VERSION_SPEC = Optional((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY)<N>_VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or "")<N><N>
VERSION_SPEC = originalTextFor(_VERSION_SPEC)("specifier")<N>VERSION_SPEC.setParseAction(lambda s, l, t: t[1])<N><N>MARKER_EXPR = originalTextFor(MARKER_EXPR())("marker")<N>MARKER_EXPR.setParseAction(<N>    lambda s, l, t: Marker(s[t._original_start : t._original_end])<N>)<N>MARKER_SEPARATOR = SEMICOLON<N>MARKER = MARKER_SEPARATOR + MARKER_EXPR<N><N>
VERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER)<N>URL_AND_MARKER = URL + Optional(MARKER)<N><N>NAMED_REQUIREMENT = NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER)<N><N>REQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd<N># pkg_resources.extern.pyparsing isn't thread safe during initialization, so we do it eagerly, see<N># issue #104<N>REQUIREMENT.parseString("x[]")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import abc<N>import functools<N>import itertools<N>import re<N>import warnings<N>from typing import (<N>    Callable,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Pattern,<N>    Set,<N>    Tuple,<N>    TypeVar,<N>    Union,<N>)<N><N>
from .utils import canonicalize_version<N>from .version import LegacyVersion, Version, parse<N><N>ParsedVersion = Union[Version, LegacyVersion]<N>UnparsedVersion = Union[Version, LegacyVersion, str]<N>VersionTypeVar = TypeVar("VersionTypeVar", bound=UnparsedVersion)<N>CallableOperator = Callable[[ParsedVersion, str], bool]<N><N>
<N>class InvalidSpecifier(ValueError):<N>    """<N>    An invalid specifier was found, users should refer to PEP 440.<N>    """<N><N><N>class BaseSpecifier(metaclass=abc.ABCMeta):<N>    @abc.abstractmethod<N>    def __str__(self) -> str:<N>        """<N>        Returns the str representation of this Specifier like object. This<N>        should be representative of the Specifier itself.<N>        """<N><N>
    @abc.abstractmethod<N>    def __hash__(self) -> int:<N>        """<N>        Returns a hash value for this Specifier like object.<N>        """<N><N>    @abc.abstractmethod<N>    def __eq__(self, other: object) -> bool:<N>        """<N>        Returns a boolean representing whether or not the two Specifier like<N>        objects are equal.<N>        """<N><N>
    @abc.abstractproperty<N>    def prereleases(self) -> Optional[bool]:<N>        """<N>        Returns whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>    @prereleases.setter<N>    def prereleases(self, value: bool) -> None:<N>        """<N>        Sets whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import logging<N>import platform<N>import sys<N>import sysconfig<N>from importlib.machinery import EXTENSION_SUFFIXES<N>from typing import (<N>    Dict,<N>    FrozenSet,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Sequence,<N>    Tuple,<N>    Union,<N>    cast,<N>)<N><N>
from . import _manylinux, _musllinux<N><N>logger = logging.getLogger(__name__)<N><N>PythonVersion = Sequence[int]<N>MacVersion = Tuple[int, int]<N><N>INTERPRETER_SHORT_NAMES: Dict[str, str] = {<N>    "python": "py",  # Generic.<N>    "cpython": "cp",<N>    "pypy": "pp",<N>    "ironpython": "ip",<N>    "jython": "jy",<N>}<N><N>
<N>_32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32<N><N><N>class Tag:<N>    """<N>    A representation of the tag triple for a wheel.<N><N>    Instances are considered immutable and thus are hashable. Equality checking<N>    is also supported.<N>    """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>from typing import FrozenSet, NewType, Tuple, Union, cast<N><N>
from .tags import Tag, parse_tag<N>from .version import InvalidVersion, Version<N><N>BuildTag = Union[Tuple[()], Tuple[int, str]]<N>NormalizedName = NewType("NormalizedName", str)<N><N><N>class InvalidWheelFilename(ValueError):<N>    """<N>    An invalid wheel filename was found, users should refer to PEP 427.<N>    """<N><N>
<N>class InvalidSdistFilename(ValueError):<N>    """<N>    An invalid sdist filename was found, users should refer to the packaging user guide.<N>    """<N><N><N>_canonicalize_regex = re.compile(r"[-_.]+")<N># PEP 427: The build number must start with a digit.<N>_build_tag_regex = re.compile(r"(\d+)(.*)")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import collections<N>import itertools<N>import re<N>import warnings<N>from typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union<N><N>
"""PEP 656 support.<N><N>This module implements logic to detect if the currently running Python is<N>linked against musl, and what musl version is used.<N>"""<N><N>import contextlib<N>import functools<N>import operator<N>import os<N>import re<N>import struct<N>import subprocess<N>import sys<N>from typing import IO, Iterator, NamedTuple, Optional, Tuple<N><N>
<N>def _read_unpacked(f: IO[bytes], fmt: str) -> Tuple[int, ...]:<N>    return struct.unpack(fmt, f.read(struct.calcsize(fmt)))<N><N><N>def _parse_ld_musl_from_elf(f: IO[bytes]) -> Optional[str]:<N>    """Detect musl libc location by parsing the Python executable.<N><N>
    Based on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca<N>    ELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html<N>    """<N>    f.seek(0)<N>    try:<N>        ident = _read_unpacked(f, "16B")<N>    except struct.error:<N>        return None<N>    if ident[:4] != tuple(b"\x7fELF"):  # Invalid magic, not ELF.<N>        return None<N>    f.seek(struct.calcsize("HHI"), 1)  # Skip file type, machine, and version.<N><N>
    try:<N>        # e_fmt: Format for program header.<N>        # p_fmt: Format for section header.<N>        # p_idx: Indexes to find p_type, p_offset, and p_filesz.<N>        e_fmt, p_fmt, p_idx = {<N>            1: ("IIIIHHH", "IIIIIIII", (0, 1, 4)),  # 32-bit.<N>            2: ("QQQIHHH", "IIQQQQQQ", (0, 2, 5)),  # 64-bit.<N>        }[ident[4]]<N>    except KeyError:<N>        return None<N>    else:<N>        p_get = operator.itemgetter(*p_idx)<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N><N>class InfinityType:<N>    def __repr__(self) -> str:<N>        return "Infinity"<N><N>
    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return False<N><N>    def __le__(self, other: object) -> bool:<N>        return False<N><N>    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>
    def __gt__(self, other: object) -> bool:<N>        return True<N><N>    def __ge__(self, other: object) -> bool:<N>        return True<N><N>    def __neg__(self: object) -> "NegativeInfinityType":<N>        return NegativeInfinity<N><N><N>Infinity = InfinityType()<N><N>
<N>class NegativeInfinityType:<N>    def __repr__(self) -> str:<N>        return "-Infinity"<N><N>    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return True<N><N>    def __le__(self, other: object) -> bool:<N>        return True<N><N>
    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>    def __gt__(self, other: object) -> bool:<N>        return False<N><N>    def __ge__(self, other: object) -> bool:<N>        return False<N><N>    def __neg__(self: object) -> InfinityType:<N>        return Infinity<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N><N>
__title__ = "packaging"<N>__summary__ = "Core utilities for Python packages"<N>__uri__ = "https://github.com/pypa/packaging"<N><N>__version__ = "21.3"<N><N>__author__ = "Donald Stufft and individual contributors"<N>__email__ = "donald@stufft.io"<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>from .__about__ import (<N>    __author__,<N>    __copyright__,<N>    __email__,<N>    __license__,<N>    __summary__,<N>    __title__,<N>    __uri__,<N>    __version__,<N>)<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2005-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import atexit<N>import os<N>import sys<N>import tempfile<N><N>pixbuf_file = os.path.join(sys._MEIPASS, 'lib', 'gdk-pixbuf', 'loaders.cache')<N><N># If we are not on Windows, we need to rewrite the cache -> we rewrite on Mac OS to support --onefile mode<N>if os.path.exists(pixbuf_file) and sys.platform != 'win32':<N>    with open(pixbuf_file, 'rb') as fp:<N>        contents = fp.read()<N><N>
    # Create a temporary file with the cache and cleverly replace the prefix we injected with the actual path.<N>    fd, pixbuf_file = tempfile.mkstemp()<N>    with os.fdopen(fd, 'wb') as fp:<N>        libpath = os.path.join(sys._MEIPASS, 'lib').encode('utf-8')<N>        fp.write(contents.replace(b'@executable_path/lib', libpath))<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N># Without this environment variable set to 'no' importing 'gst' causes 100% CPU load. (Tested on Mac OS.)<N>os.environ['GST_REGISTRY_FORK'] = 'no'<N><N>gst_plugin_paths = [sys._MEIPASS, os.path.join(sys._MEIPASS, 'gst-plugins')]<N>os.environ['GST_PLUGIN_PATH'] = os.pathsep.join(gst_plugin_paths)<N><N>
# Prevent permission issues on Windows<N>os.environ['GST_REGISTRY'] = os.path.join(sys._MEIPASS, 'registry.bin')<N><N># Only use packaged plugins to prevent GStreamer from crashing when it finds plugins from another version which are<N># installed system wide.<N>os.environ['GST_PLUGIN_SYSTEM_PATH'] = ''<N><N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N>os.environ['GTK_DATA_PREFIX'] = sys._MEIPASS<N>os.environ['GTK_EXE_PREFIX'] = sys._MEIPASS<N>os.environ['GTK_PATH'] = sys._MEIPASS<N><N># Include these here, as GTK will import pango automatically.<N>os.environ['PANGO_LIBDIR'] = sys._MEIPASS<N>os.environ['PANGO_SYSCONFDIR'] = os.path.join(sys._MEIPASS, 'etc')  # TODO?<N><N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2017-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import multiprocessing<N>import multiprocessing.spawn as spawn<N># 'spawn' multiprocessing needs some adjustments on osx<N>import os<N>import sys<N>from subprocess import _args_from_interpreter_flags<N><N># prevent spawn from trying to read __main__ in from the main script<N>multiprocessing.process.ORIGINAL_DIR = None<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2014-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N># See ``pyi_rth_qt5.py`: use a "standard" PyQt5 layout.<N>if sys.platform == 'darwin':<N>    # Try PyQt5 5.15.4-style path first...<N>    pyqt_path = os.path.join(sys._MEIPASS, 'PyQt5', 'Qt5')<N>    if not os.path.isdir(pyqt_path):<N>        # ... and fall back to the older version<N>        pyqt_path = os.path.join(sys._MEIPASS, 'PyQt5', 'Qt')<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N># See ``pyi_rth_qt6.py`: use a "standard" PyQt6 layout.<N>if sys.platform == 'darwin':<N>    # NOTE: QtWebEngine support was added in Qt6 6.2.x series, so we do not need to worry about pre-6.0.3 path layout.<N>    pyqt_path = os.path.join(sys._MEIPASS, 'PyQt6', 'Qt6')<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import subprocess<N>import sys<N>import io<N><N><N>class Popen(subprocess.Popen):<N><N>    # In windowed mode, force any unused pipes (stdin, stdout and stderr) to be DEVNULL instead of inheriting the<N>    # invalid corresponding handles from this parent process.<N>    if sys.platform == "win32" and not isinstance(sys.stdout, io.IOBase):<N><N>
        def _get_handles(self, stdin, stdout, stderr):<N>            stdin, stdout, stderr = (subprocess.DEVNULL if pipe is None else pipe for pipe in (stdin, stdout, stderr))<N>            return super()._get_handles(stdin, stdout, stderr)<N><N><N>subprocess.Popen = Popen<N><N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
# Starting with Python 3.8, win32api failed with "ImportError: DLL load failed while importing win32clipboard: The<N># specified module could not be found." This seems to be caused by pywintypes.dll not being found in various situations.<N># See https://github.com/mhammond/pywin32/pull/1430 and<N># https://github.com/mhammond/pywin32/commit/71afa71e11e6631be611ca5cb57cda526<N># As a work-around, import pywintypes prior to win32api.<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
# The win32.client.gencache code must be allowed to create the cache in %temp% (user's temp). It is necessary to get the<N># gencache code to use a suitable directory other than the default in lib\site-packages\win32com\client\gen_py.<N># PyInstaller does not provide this directory structure and the frozen executable could be placed in a non-writable<N># directory like 'C:\Program Files. That's the reason for %temp% directory.<N>#<N># http://www.py2exe.org/index.cgi/UsingEnsureDispatch<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N>tcldir = os.path.join(sys._MEIPASS, 'tcl')<N>tkdir = os.path.join(sys._MEIPASS, 'tk')<N><N># Notify "tkinter" of data directories. On macOS, we do not collect data directories if system Tcl/Tk framework is used.<N># On other OSes, we always collect them, so their absence is considered an error.<N>is_darwin = sys.platform == 'darwin'<N><N>
if os.path.isdir(tcldir):<N>    os.environ["TCL_LIBRARY"] = tcldir<N>elif not is_darwin:<N>    raise FileNotFoundError('Tcl data directory "%s" not found.' % tcldir)<N><N>if os.path.isdir(tkdir):<N>    os.environ["TK_LIBRARY"] = tkdir<N>elif not is_darwin:<N>    raise FileNotFoundError('Tk data directory "%s" not found.' % tkdir)<N><N><N>
"""<N>modulegraph.find_modules - High-level module dependency finding interface<N>=========================================================================<N><N>History<N>........<N><N>Originally (loosely) based on code in py2exe's build_exe.py by Thomas Heller.<N>"""<N>import sys<N>import os<N>import imp<N>import warnings<N>import pkgutil<N><N>
from . import modulegraph<N>from .modulegraph import Alias, Script, Extension<N>from .util import imp_find_module<N><N>__all__ = [<N>    'find_modules', 'parse_mf_results'<N>]<N><N>_PLATFORM_MODULES = {'posix', 'nt', 'os2', 'mac', 'ce', 'riscos'}<N><N>
<N># Filter DeprecationWarnings until the code has been revised<N>import warnings<N>warnings.filterwarnings("ignore", "the imp module is deprecated in")<N>warnings.filterwarnings("ignore", "imp_walk will be removed in a future")<N><N>import os<N>import imp<N>import sys<N>import re<N>import marshal<N>import warnings<N>import inspect<N><N>
"""<N>A helper module that can work with paths<N>that can refer to data inside a zipfile<N><N>XXX: Need to determine if isdir("zipfile.zip")<N>should return True or False. Currently returns<N>True, but that might do the wrong thing with<N>data-files that are zipfiles.<N>"""<N>import os as _os<N>import zipfile as _zipfile<N>import errno as _errno<N>import time as _time<N>import sys as _sys<N>import stat as _stat<N><N>
_DFLT_DIR_MODE = (<N>    _stat.S_IXOTH<N>    | _stat.S_IXGRP<N>    | _stat.S_IXUSR<N>    | _stat.S_IROTH<N>    | _stat.S_IRGRP<N>    | _stat.S_IRUSR)<N><N>_DFLT_FILE_MODE = (<N>    _stat.S_IROTH<N>    | _stat.S_IRGRP<N>    | _stat.S_IRUSR)<N><N><N>if _sys.version_info[0] == 2:<N>    from StringIO import StringIO as _BaseStringIO<N>    from StringIO import StringIO as _BaseBytesIO<N><N>
    class _StringIO (_BaseStringIO):<N>        def __enter__(self):<N>            return self<N><N>        def __exit__(self, exc_type, exc_value, traceback):<N>            self.close()<N>            return False<N><N>    class _BytesIO (_BaseBytesIO):<N>        def __enter__(self):<N>            return self<N><N>
        def __exit__(self, exc_type, exc_value, traceback):<N>            self.close()<N>            return False<N><N>else:<N>    from io import StringIO as _StringIO<N>    from io import BytesIO as _BytesIO<N><N><N>def _locate(path):<N>    full_path = path<N>    if _os.path.exists(path):<N>        return path, None<N><N>
    else:<N>        rest = []<N>        root = _os.path.splitdrive(path)<N>        while path and path != root:<N>            path, bn = _os.path.split(path)<N>            rest.append(bn)<N>            if _os.path.exists(path):<N>                break<N><N>
        if path == root:<N>            raise IOError(<N>                _errno.ENOENT, full_path,<N>                "No such file or directory")<N><N>        if not _os.path.isfile(path):<N>            raise IOError(<N>                _errno.ENOENT, full_path,<N>                "No such file or directory")<N><N>
        rest.reverse()<N>        return path, '/'.join(rest).strip('/')<N><N><N>_open = open<N><N><N>def open(path, mode='r'):<N>    if 'w' in mode or 'a' in mode:<N>        raise IOError(<N>            _errno.EINVAL, path, "Write access not supported")<N>    elif 'r+' in mode:<N>        raise IOError(<N>            _errno.EINVAL, path, "Write access not supported")<N><N>
    full_path = path<N>    path, rest = _locate(path)<N>    if not rest:<N>        return _open(path, mode)<N><N>    else:<N>        try:<N>            zf = _zipfile.ZipFile(path, 'r')<N><N>        except _zipfile.error:<N>            raise IOError(<N>                _errno.ENOENT, full_path,<N>                "No such file or directory")<N><N>
        try:<N>            data = zf.read(rest)<N>        except (_zipfile.error, KeyError):<N>            zf.close()<N>            raise IOError(<N>                _errno.ENOENT, full_path,<N>                "No such file or directory")<N>        zf.close()<N><N>
        if mode == 'rb':<N>            return _BytesIO(data)<N><N>        else:<N>            if _sys.version_info[0] == 3:<N>                data = data.decode('ascii')<N><N>            return _StringIO(data)<N><N><N>def listdir(path):<N>    full_path = path<N>    path, rest = _locate(path)<N>    if not rest and not _os.path.isfile(path):<N>        return _os.listdir(path)<N><N>
import sys<N><N>if sys.version_info[0] == 2:<N>    PY2 = True<N><N>    from StringIO import StringIO<N>    BytesIO = StringIO<N>    from urllib import pathname2url<N>    _cOrd = ord<N><N>    # File open mode for reading (univeral newlines)<N>    _READ_MODE = "rU"<N><N>else:<N>    PY2 = False<N><N>    from urllib.request import pathname2url<N>    from io import BytesIO, StringIO<N>    _cOrd = int<N>    _READ_MODE = "r"<N><N>if sys.version_info < (3,):<N>    from dis3 import get_instructions<N>else:<N>    from dis import get_instructions<N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2005-2021, PyInstaller Development Team.<N>#<N># Distributed under the terms of the GNU General Public License with exception<N># for distributing bootloader.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#-----------------------------------------------------------------------------<N>"""<N>Hooks to make ctypes.CDLL, .PyDLL, etc. look in sys._MEIPASS first.<N>"""<N><N>
import sys<N><N><N>def install():<N>    """<N>    Install the hooks.<N><N>    This must be done from a function as opposed to at module-level, because when the module is imported/executed,<N>    the import machinery is not completely set up yet.<N>    """<N><N>
#<N># The content of this file will be filled in with meaningful data when creating an archive using `git archive` or by<N># downloading an archive from github, e.g., from github.com/.../archive/develop.zip<N>#<N>rev = "$Format:%h$"  # abbreviated commit hash<N>commit = "$Format:%H$"  # commit hash<N>date = "$Format:%ci$"  # commit date<N>author = "$Format:%an$ <$Format:%ae$>"<N>ref_names = "$Format:%D$"  # incl. current branch<N>commit_message = """$Format:%B$"""<N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.api<N>~~~~~~~~~~~~<N><N>This module implements the Requests API.<N><N>:copyright: (c) 2012 by Kenneth Reitz.<N>:license: Apache2, see LICENSE for more details.<N>"""<N><N>from . import sessions<N><N><N>def request(method, url, **kwargs):<N>    """Constructs and sends a :class:`Request <Request>`.<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.auth<N>~~~~~~~~~~~~~<N><N>This module contains the authentication handlers for Requests.<N>"""<N><N>import os<N>import re<N>import time<N>import hashlib<N>import threading<N>import warnings<N><N>from base64 import b64encode<N><N>
from .compat import urlparse, str, basestring<N>from .cookies import extract_cookies_to_jar<N>from ._internal_utils import to_native_string<N>from .utils import parse_dict_header<N><N>CONTENT_TYPE_FORM_URLENCODED = 'application/x-www-form-urlencoded'<N>CONTENT_TYPE_MULTI_PART = 'multipart/form-data'<N><N>
#!/usr/bin/env python<N># -*- coding: utf-8 -*-<N><N>"""<N>requests.certs<N>~~~~~~~~~~~~~~<N><N>This module returns the preferred default CA certificate bundle. There is<N>only one — the one from the certifi package.<N><N>If you are packaging Requests, e.g., for a Linux distribution or a managed<N>environment, you can change the definition of where() to return a separately<N>packaged CA bundle.<N>"""<N>from certifi import where<N><N>if __name__ == '__main__':<N>    print(where())<N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.compat<N>~~~~~~~~~~~~~~~<N><N>This module handles import compatibility issues between Python 2 and<N>Python 3.<N>"""<N><N>try:<N>    import chardet<N>except ImportError:<N>    import charset_normalizer as chardet<N><N>
import sys<N><N># -------<N># Pythons<N># -------<N><N># Syntax sugar.<N>_ver = sys.version_info<N><N>#: Python 2.x?<N>is_py2 = (_ver[0] == 2)<N><N>#: Python 3.x?<N>is_py3 = (_ver[0] == 3)<N><N>has_simplejson = False<N>try:<N>    import simplejson as json<N>    has_simplejson = True<N>except ImportError:<N>    import json<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.cookies<N>~~~~~~~~~~~~~~~~<N><N>Compatibility code to be able to use `cookielib.CookieJar` with requests.<N><N>requests.utils imports from here, so be careful with imports.<N>"""<N><N>import copy<N>import time<N>import calendar<N><N>
from ._internal_utils import to_native_string<N>from .compat import cookielib, urlparse, urlunparse, Morsel, MutableMapping<N><N>try:<N>    import threading<N>except ImportError:<N>    import dummy_threading as threading<N><N><N>class MockRequest(object):<N>    """Wraps a `requests.Request` to mimic a `urllib2.Request`.<N><N>
    The code in `cookielib.CookieJar` expects this interface in order to correctly<N>    manage cookie policies, i.e., determine whether a cookie can be set, given the<N>    domains of the request and the cookie.<N><N>    The original request object is read-only. The client is responsible for collecting<N>    the new headers via `get_new_headers()` and interpreting them appropriately. You<N>    probably want `get_cookie_header`, defined below.<N>    """<N><N>
    def __init__(self, request):<N>        self._r = request<N>        self._new_headers = {}<N>        self.type = urlparse(self._r.url).scheme<N><N>    def get_type(self):<N>        return self.type<N><N>    def get_host(self):<N>        return urlparse(self._r.url).netloc<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.exceptions<N>~~~~~~~~~~~~~~~~~~~<N><N>This module contains the set of Requests' exceptions.<N>"""<N>from urllib3.exceptions import HTTPError as BaseHTTPError<N><N>from .compat import JSONDecodeError as CompatJSONDecodeError<N><N>
"""Module containing bug report helper(s)."""<N>from __future__ import print_function<N><N>import json<N>import platform<N>import sys<N>import ssl<N><N>import idna<N>import urllib3<N><N>from . import __version__ as requests_version<N><N>try:<N>    import charset_normalizer<N>except ImportError:<N>    charset_normalizer = None<N><N>
try:<N>    import chardet<N>except ImportError:<N>    chardet = None<N><N>try:<N>    from urllib3.contrib import pyopenssl<N>except ImportError:<N>    pyopenssl = None<N>    OpenSSL = None<N>    cryptography = None<N>else:<N>    import OpenSSL<N>    import cryptography<N><N>
<N>def _implementation():<N>    """Return a dict with the Python implementation and version.<N><N>    Provide both the name and the version of the Python implementation<N>    currently running. For example, on CPython 2.7.5 it will return<N>    {'name': 'CPython', 'version': '2.7.5'}.<N><N>
    This function works best on CPython and PyPy: in particular, it probably<N>    doesn't work for Jython or IronPython. Future investigation should be done<N>    to work out the correct shape of the code for those platforms.<N>    """<N>    implementation = platform.python_implementation()<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.hooks<N>~~~~~~~~~~~~~~<N><N>This module provides the capabilities for the Requests hooks system.<N><N>Available hooks:<N><N>``response``:<N>    The response generated from a Request.<N>"""<N>HOOKS = ['response']<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.models<N>~~~~~~~~~~~~~~~<N><N>This module contains the primary objects that power Requests.<N>"""<N><N>import datetime<N>import sys<N><N># Import encoding now, to avoid implicit import later.<N># Implicit import within threads may cause LookupError when standard library is in a ZIP,<N># such as in Embedded Python. See https://github.com/psf/requests/issues/3578.<N>import encodings.idna<N><N>
from urllib3.fields import RequestField<N>from urllib3.filepost import encode_multipart_formdata<N>from urllib3.util import parse_url<N>from urllib3.exceptions import (<N>    DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)<N><N>from io import UnsupportedOperation<N>from .hooks import default_hooks<N>from .structures import CaseInsensitiveDict<N><N>
import sys<N><N>try:<N>    import chardet<N>except ImportError:<N>    import charset_normalizer as chardet<N>    import warnings<N><N>    warnings.filterwarnings('ignore', 'Trying to detect', module='charset_normalizer')<N><N># This code exists for backwards compatibility reasons.<N># I don't like it either. Just look the other way. :)<N><N>
for package in ('urllib3', 'idna'):<N>    locals()[package] = __import__(package)<N>    # This traversal is apparently necessary such that the identities are<N>    # preserved (requests.packages.urllib3.* is urllib3.*)<N>    for mod in list(sys.modules):<N>        if mod == package or mod.startswith(package + '.'):<N>            sys.modules['requests.packages.' + mod] = sys.modules[mod]<N><N>
target = chardet.__name__<N>for mod in list(sys.modules):<N>    if mod == target or mod.startswith(target + '.'):<N>        sys.modules['requests.packages.' + target.replace(target, 'chardet')] = sys.modules[mod]<N># Kinda cool, though, right?<N><N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.sessions<N>~~~~~~~~~~~~~~~~~<N><N>This module provides a Session object to manage and persist settings across<N>requests (cookies, auth, proxies).<N>"""<N>import os<N>import sys<N>import time<N>from datetime import timedelta<N>from collections import OrderedDict<N><N>
# -*- coding: utf-8 -*-<N><N>r"""<N>The ``codes`` object defines a mapping from common names for HTTP statuses<N>to their numerical codes, accessible either as attributes or as dictionary<N>items.<N><N>Example::<N><N>    >>> import requests<N>    >>> requests.codes['temporary_redirect']<N>    307<N>    >>> requests.codes.teapot<N>    418<N>    >>> requests.codes['\o/']<N>    200<N><N>
Some codes have multiple names, and both upper- and lower-case versions of<N>the names are allowed. For example, ``codes.ok``, ``codes.OK``, and<N>``codes.okay`` all correspond to the HTTP status code 200.<N>"""<N><N>from .structures import LookupDict<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.structures<N>~~~~~~~~~~~~~~~~~~~<N><N>Data structures that power Requests.<N>"""<N><N>from collections import OrderedDict<N><N>from .compat import Mapping, MutableMapping<N><N><N>class CaseInsensitiveDict(MutableMapping):<N>    """A case-insensitive ``dict``-like object.<N><N>
    Implements all methods and operations of<N>    ``MutableMapping`` as well as dict's ``copy``. Also<N>    provides ``lower_items``.<N><N>    All keys are expected to be strings. The structure remembers the<N>    case of the last key to be set, and ``iter(instance)``,<N>    ``keys()``, ``items()``, ``iterkeys()``, and ``iteritems()``<N>    will contain case-sensitive keys. However, querying and contains<N>    testing is case insensitive::<N><N>
        cid = CaseInsensitiveDict()<N>        cid['Accept'] = 'application/json'<N>        cid['aCCEPT'] == 'application/json'  # True<N>        list(cid) == ['Accept']  # True<N><N>    For example, ``headers['content-encoding']`` will return the<N>    value of a ``'Content-Encoding'`` response header, regardless<N>    of how the header name was originally stored.<N><N>
    If the constructor, ``.update``, or equality comparison<N>    operations are given keys that have equal ``.lower()``s, the<N>    behavior is undefined.<N>    """<N><N>    def __init__(self, data=None, **kwargs):<N>        self._store = OrderedDict()<N>        if data is None:<N>            data = {}<N>        self.update(data, **kwargs)<N><N>
    def __setitem__(self, key, value):<N>        # Use the lowercased key for lookups, but store the actual<N>        # key alongside the value.<N>        self._store[key.lower()] = (key, value)<N><N>    def __getitem__(self, key):<N>        return self._store[key.lower()][1]<N><N>
    def __delitem__(self, key):<N>        del self._store[key.lower()]<N><N>    def __iter__(self):<N>        return (casedkey for casedkey, mappedvalue in self._store.values())<N><N>    def __len__(self):<N>        return len(self._store)<N><N>    def lower_items(self):<N>        """Like iteritems(), but with all lowercase keys."""<N>        return (<N>            (lowerkey, keyval[1])<N>            for (lowerkey, keyval)<N>            in self._store.items()<N>        )<N><N>
    def __eq__(self, other):<N>        if isinstance(other, Mapping):<N>            other = CaseInsensitiveDict(other)<N>        else:<N>            return NotImplemented<N>        # Compare insensitively<N>        return dict(self.lower_items()) == dict(other.lower_items())<N><N>
    # Copy is required<N>    def copy(self):<N>        return CaseInsensitiveDict(self._store.values())<N><N>    def __repr__(self):<N>        return str(dict(self.items()))<N><N><N>class LookupDict(dict):<N>    """Dictionary lookup object."""<N><N>    def __init__(self, name=None):<N>        self.name = name<N>        super(LookupDict, self).__init__()<N><N>
    def __repr__(self):<N>        return '<lookup \'%s\'>' % (self.name)<N><N>    def __getitem__(self, key):<N>        # We allow fall-through here, so values default to None<N><N>        return self.__dict__.get(key, None)<N><N>    def get(self, key, default=None):<N>        return self.__dict__.get(key, default)<N><N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests.utils<N>~~~~~~~~~~~~~~<N><N>This module provides utility functions that are used within Requests<N>that are also useful for external consumption.<N>"""<N><N>import codecs<N>import contextlib<N>import io<N>import os<N>import re<N>import socket<N>import struct<N>import sys<N>import tempfile<N>import warnings<N>import zipfile<N>from collections import OrderedDict<N>from urllib3.util import make_headers<N>from urllib3.util import parse_url<N><N>
# -*- coding: utf-8 -*-<N><N>"""<N>requests._internal_utils<N>~~~~~~~~~~~~~~<N><N>Provides utility functions that are consumed internally by Requests<N>which depend on extremely few external helpers (such as compat)<N>"""<N><N>from .compat import is_py2, builtin_str, str<N><N>
<N>def to_native_string(string, encoding='ascii'):<N>    """Given a string object, regardless of type, returns a representation of<N>    that string in the native string type, encoding and decoding where<N>    necessary. This assumes ASCII unless told otherwise.<N>    """<N>    if isinstance(string, builtin_str):<N>        out = string<N>    else:<N>        if is_py2:<N>            out = string.encode(encoding)<N>        else:<N>            out = string.decode(encoding)<N><N>
    return out<N><N><N>def unicode_is_ascii(u_string):<N>    """Determine if unicode string only contains ASCII characters.<N><N>    :param str u_string: unicode string to check. Must be unicode<N>        and not Python 2 `str`.<N>    :rtype: bool<N>    """<N>    assert isinstance(u_string, str)<N>    try:<N>        u_string.encode('ascii')<N>        return True<N>    except UnicodeEncodeError:<N>        return False<N><N><N>
# -*- coding: utf-8 -*-<N><N>#   __<N>#  /__)  _  _     _   _ _/   _<N># / (   (- (/ (/ (- _)  /  _)<N>#          /<N><N>"""<N>Requests HTTP Library<N>~~~~~~~~~~~~~~~~~~~~~<N><N>Requests is an HTTP library, written in Python, for human beings.<N>Basic GET usage:<N><N>
   >>> import requests<N>   >>> r = requests.get('https://www.python.org')<N>   >>> r.status_code<N>   200<N>   >>> b'Python is a programming language' in r.content<N>   True<N><N>... or POST:<N><N>   >>> payload = dict(key1='value1', key2='value2')<N>   >>> r = requests.post('https://httpbin.org/post', data=payload)<N>   >>> print(r.text)<N>   {<N>     ...<N>     "form": {<N>       "key1": "value1",<N>       "key2": "value2"<N>     },<N>     ...<N>   }<N><N>
The other HTTP methods are supported - see `requests.api`. Full documentation<N>is at <https://requests.readthedocs.io>.<N><N>:copyright: (c) 2017 by Kenneth Reitz.<N>:license: Apache 2.0, see LICENSE for more details.<N>"""<N><N>import urllib3<N>import warnings<N>from .exceptions import RequestsDependencyWarning<N><N>
try:<N>    from charset_normalizer import __version__ as charset_normalizer_version<N>except ImportError:<N>    charset_normalizer_version = None<N><N>try:<N>    from chardet import __version__ as chardet_version<N>except ImportError:<N>    chardet_version = None<N><N>
def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):<N>    urllib3_version = urllib3_version.split('.')<N>    assert urllib3_version != ['dev']  # Verify urllib3 isn't installed from git.<N><N>    # Sometimes, urllib3 only reports its version as 16.1.<N>    if len(urllib3_version) == 2:<N>        urllib3_version.append('0')<N><N>
    # Check urllib3 for compatibility.<N>    major, minor, patch = urllib3_version  # noqa: F811<N>    major, minor, patch = int(major), int(minor), int(patch)<N>    # urllib3 >= 1.21.1, <= 1.26<N>    assert major == 1<N>    assert minor >= 21<N>    assert minor <= 26<N><N>
# .-. .-. .-. . . .-. .-. .-. .-.<N># |(  |-  |.| | | |-  `-.  |  `-.<N># ' ' `-' `-`.`-' `-' `-'  '  `-'<N><N>__title__ = 'requests'<N>__description__ = 'Python HTTP for Humans.'<N>__url__ = 'https://requests.readthedocs.io'<N>__version__ = '2.27.1'<N>__build__ = 0x022701<N>__author__ = 'Kenneth Reitz'<N>__author_email__ = 'me@kennethreitz.org'<N>__license__ = 'Apache 2.0'<N>__copyright__ = 'Copyright 2022 Kenneth Reitz'<N>__cake__ = u'\u2728 \U0001f370 \u2728'<N>
# coding: utf-8<N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List, Union, Optional, Iterator  # NOQA<N><N>anchor_attrib = '_yaml_anchor'<N><N><N>class Anchor:<N>    __slots__ = 'value', 'always_dump'<N>    attrib = anchor_attrib<N><N>    def __init__(self):<N>        # type: () -> None<N>        self.value = None<N>        self.always_dump = False<N><N>    def __repr__(self):<N>        # type: () -> Any<N>        ad = ', (always dump)' if self.always_dump else ""<N>        return 'Anchor({!r}{})'.format(self.value, ad)<N>
# coding: utf-8<N><N>"""<N>stuff to deal with comments and formatting on dict/list/ordereddict/set<N>these are not really related, formatting could be factored out as<N>a separate base<N>"""<N><N>import sys<N>import copy<N><N><N>from ruamel.yaml.compat import ordereddict<N>from ruamel.yaml.compat import MutableSliceableSequence, _F, nprintf  # NOQA<N>from ruamel.yaml.scalarstring import ScalarString<N>from ruamel.yaml.anchor import Anchor<N><N>
from collections.abc import MutableSet, Sized, Set, Mapping<N><N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List, Union, Optional, Iterator  # NOQA<N><N># fmt: off<N>__all__ = ['CommentedSeq', 'CommentedKeySeq',<N>           'CommentedMap', 'CommentedOrderedMap',<N>           'CommentedSet', 'comment_attrib', 'merge_attrib',<N>           'C_POST', 'C_PRE', 'C_SPLIT_ON_FIRST_BLANK', 'C_BLANK_LINE_PRESERVE_SPACE',<N>           ]<N># fmt: on<N><N>
# coding: utf-8<N><N># partially from package six by Benjamin Peterson<N><N>import sys<N>import os<N>import io<N>import traceback<N>from abc import abstractmethod<N>import collections.abc<N><N><N># fmt: off<N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List, Union, BinaryIO, IO, Text, Tuple  # NOQA<N>    from typing import Optional  # NOQA<N># fmt: on<N><N>
_DEFAULT_YAML_VERSION = (1, 2)<N><N>try:<N>    from collections import OrderedDict<N>except ImportError:<N>    from ordereddict import OrderedDict  # type: ignore<N><N>    # to get the right name import ... as ordereddict doesn't do that<N><N><N>class ordereddict(OrderedDict):  # type: ignore<N>    if not hasattr(OrderedDict, 'insert'):<N><N>
        def insert(self, pos, key, value):<N>            # type: (int, Any, Any) -> None<N>            if pos >= len(self):<N>                self[key] = value<N>                return<N>            od = ordereddict()<N>            od.update(self)<N>            for k in od:<N>                del self[k]<N>            for index, old_key in enumerate(od):<N>                if pos == index:<N>                    self[key] = value<N>                self[old_key] = od[old_key]<N><N>
<N>PY2 = sys.version_info[0] == 2<N>PY3 = sys.version_info[0] == 3<N><N><N># replace with f-strings when 3.5 support is dropped<N># ft = '42'<N># assert _F('abc {ft!r}', ft=ft) == 'abc %r' % ft<N># 'abc %r' % ft -> _F('abc {ft!r}' -> f'abc {ft!r}'<N>def _F(s, *superfluous, **kw):<N>    # type: (Any, Any, Any) -> Any<N>    if superfluous:<N>        raise TypeError<N>    return s.format(**kw)<N><N>
<N>StringIO = io.StringIO<N>BytesIO = io.BytesIO<N><N>if False:  # MYPY<N>    # StreamType = Union[BinaryIO, IO[str], IO[unicode],  StringIO]<N>    # StreamType = Union[BinaryIO, IO[str], StringIO]  # type: ignore<N>    StreamType = Any<N><N>    StreamTextType = StreamType  # Union[Text, StreamType]<N>    VersionType = Union[List[int], str, Tuple[int, int]]<N><N>
builtins_module = 'builtins'<N><N><N>def with_metaclass(meta, *bases):<N>    # type: (Any, Any) -> Any<N>    """Create a base class with a metaclass."""<N>    return meta('NewBase', bases, {})<N><N><N>DBG_TOKEN = 1<N>DBG_EVENT = 2<N>DBG_NODE = 4<N><N>
<N>_debug = None  # type: Optional[int]<N>if 'RUAMELDEBUG' in os.environ:<N>    _debugx = os.environ.get('RUAMELDEBUG')<N>    if _debugx is None:<N>        _debug = 0<N>    else:<N>        _debug = int(_debugx)<N><N><N>if bool(_debug):<N><N>    class ObjectCounter:<N>        def __init__(self):<N>            # type: () -> None<N>            self.map = {}  # type: Dict[Any, Any]<N><N>
        def __call__(self, k):<N>            # type: (Any) -> None<N>            self.map[k] = self.map.get(k, 0) + 1<N><N>        def dump(self):<N>            # type: () -> None<N>            for k in sorted(self.map):<N>                sys.stdout.write('{} -> {}'.format(k, self.map[k]))<N><N>
    object_counter = ObjectCounter()<N><N><N># used from yaml util when testing<N>def dbg(val=None):<N>    # type: (Any) -> Any<N>    global _debug<N>    if _debug is None:<N>        # set to true or false<N>        _debugx = os.environ.get('YAMLDEBUG')<N>        if _debugx is None:<N>            _debug = 0<N>        else:<N>            _debug = int(_debugx)<N>    if val is None:<N>        return _debug<N>    return _debug & val<N><N>
# coding: utf-8<N><N>import warnings<N><N>from ruamel.yaml.error import MarkedYAMLError, ReusedAnchorWarning<N>from ruamel.yaml.compat import _F, nprint, nprintf  # NOQA<N><N>from ruamel.yaml.events import (<N>    StreamStartEvent,<N>    StreamEndEvent,<N>    MappingStartEvent,<N>    MappingEndEvent,<N>    SequenceStartEvent,<N>    SequenceEndEvent,<N>    AliasEvent,<N>    ScalarEvent,<N>)<N>from ruamel.yaml.nodes import MappingNode, ScalarNode, SequenceNode<N><N>
if False:  # MYPY<N>    from typing import Any, Dict, Optional, List  # NOQA<N><N>__all__ = ['Composer', 'ComposerError']<N><N><N>class ComposerError(MarkedYAMLError):<N>    pass<N><N><N>class Composer:<N>    def __init__(self, loader=None):<N>        # type: (Any) -> None<N>        self.loader = loader<N>        if self.loader is not None and getattr(self.loader, '_composer', None) is None:<N>            self.loader._composer = self<N>        self.anchors = {}  # type: Dict[Any, Any]<N><N>
    @property<N>    def parser(self):<N>        # type: () -> Any<N>        if hasattr(self.loader, 'typ'):<N>            self.loader.parser<N>        return self.loader._parser<N><N>    @property<N>    def resolver(self):<N>        # type: () -> Any<N>        # assert self.loader._resolver is not None<N>        if hasattr(self.loader, 'typ'):<N>            self.loader.resolver<N>        return self.loader._resolver<N><N>
    def check_node(self):<N>        # type: () -> Any<N>        # Drop the STREAM-START event.<N>        if self.parser.check_event(StreamStartEvent):<N>            self.parser.get_event()<N><N>        # If there are more documents available?<N>        return not self.parser.check_event(StreamEndEvent)<N><N>
    def get_node(self):<N>        # type: () -> Any<N>        # Get the root node of the next document.<N>        if not self.parser.check_event(StreamEndEvent):<N>            return self.compose_document()<N><N>    def get_single_node(self):<N>        # type: () -> Any<N>        # Drop the STREAM-START event.<N>        self.parser.get_event()<N><N>
# coding: utf-8<N><N>import warnings<N><N>from ruamel.yaml.util import configobj_walker as new_configobj_walker<N><N>if False:  # MYPY<N>    from typing import Any  # NOQA<N><N><N>def configobj_walker(cfg):<N>    # type: (Any) -> Any<N>    warnings.warn('configobj_walker has moved to ruamel.yaml.util, please update your code')<N>    return new_configobj_walker(cfg)<N>
# coding: utf-8<N><N>from _ruamel_yaml import CParser, CEmitter  # type: ignore<N><N>from ruamel.yaml.constructor import Constructor, BaseConstructor, SafeConstructor<N>from ruamel.yaml.representer import Representer, SafeRepresenter, BaseRepresenter<N>from ruamel.yaml.resolver import Resolver, BaseResolver<N><N>
if False:  # MYPY<N>    from typing import Any, Union, Optional  # NOQA<N>    from ruamel.yaml.compat import StreamTextType, StreamType, VersionType  # NOQA<N><N>__all__ = ['CBaseLoader', 'CSafeLoader', 'CLoader', 'CBaseDumper', 'CSafeDumper', 'CDumper']<N><N>
# coding: utf-8<N><N>from ruamel.yaml.emitter import Emitter<N>from ruamel.yaml.serializer import Serializer<N>from ruamel.yaml.representer import (<N>    Representer,<N>    SafeRepresenter,<N>    BaseRepresenter,<N>    RoundTripRepresenter,<N>)<N>from ruamel.yaml.resolver import Resolver, BaseResolver, VersionedResolver<N><N>
# coding: utf-8<N><N># Emitter expects events obeying the following grammar:<N># stream ::= STREAM-START document* STREAM-END<N># document ::= DOCUMENT-START node DOCUMENT-END<N># node ::= SCALAR | sequence | mapping<N># sequence ::= SEQUENCE-START node* SEQUENCE-END<N># mapping ::= MAPPING-START (node node)* MAPPING-END<N><N>
import sys<N>from ruamel.yaml.error import YAMLError, YAMLStreamError<N>from ruamel.yaml.events import *  # NOQA<N><N># fmt: off<N>from ruamel.yaml.compat import _F, nprint, dbg, DBG_EVENT, \<N>    check_anchorname_char, nprintf  # NOQA<N># fmt: on<N><N>
if False:  # MYPY<N>    from typing import Any, Dict, List, Union, Text, Tuple, Optional  # NOQA<N>    from ruamel.yaml.compat import StreamType  # NOQA<N><N>__all__ = ['Emitter', 'EmitterError']<N><N><N>class EmitterError(YAMLError):<N>    pass<N><N>
# coding: utf-8<N><N>import warnings<N>import textwrap<N><N>from ruamel.yaml.compat import _F<N><N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List, Text  # NOQA<N><N><N>__all__ = [<N>    'FileMark',<N>    'StringMark',<N>    'CommentMark',<N>    'YAMLError',<N>    'MarkedYAMLError',<N>    'ReusedAnchorWarning',<N>    'UnsafeLoaderWarning',<N>    'MarkedYAMLWarning',<N>    'MarkedYAMLFutureWarning',<N>]<N><N>
<N>class StreamMark:<N>    __slots__ = 'name', 'index', 'line', 'column'<N><N>    def __init__(self, name, index, line, column):<N>        # type: (Any, int, int, int) -> None<N>        self.name = name<N>        self.index = index<N>        self.line = line<N>        self.column = column<N><N>
    def __str__(self):<N>        # type: () -> Any<N>        where = _F(<N>            '  in "{sname!s}", line {sline1:d}, column {scolumn1:d}',<N>            sname=self.name,<N>            sline1=self.line + 1,<N>            scolumn1=self.column + 1,<N>        )<N>        return where<N><N>
    def __eq__(self, other):<N>        # type: (Any) -> bool<N>        if self.line != other.line or self.column != other.column:<N>            return False<N>        if self.name != other.name or self.index != other.index:<N>            return False<N>        return True<N><N>
    def __ne__(self, other):<N>        # type: (Any) -> bool<N>        return not self.__eq__(other)<N><N><N>class FileMark(StreamMark):<N>    __slots__ = ()<N><N><N>class StringMark(StreamMark):<N>    __slots__ = 'name', 'index', 'line', 'column', 'buffer', 'pointer'<N><N>
    def __init__(self, name, index, line, column, buffer, pointer):<N>        # type: (Any, int, int, int, Any, Any) -> None<N>        StreamMark.__init__(self, name, index, line, column)<N>        self.buffer = buffer<N>        self.pointer = pointer<N><N>
# coding: utf-8<N><N>from ruamel.yaml.compat import _F<N><N># Abstract classes.<N><N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List  # NOQA<N><N>SHOW_LINES = False<N><N><N>def CommentCheck():<N>    # type: () -> None<N>    pass<N><N>
<N>class Event:<N>    __slots__ = 'start_mark', 'end_mark', 'comment'<N><N>    def __init__(self, start_mark=None, end_mark=None, comment=CommentCheck):<N>        # type: (Any, Any, Any) -> None<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N>        # assert comment is not CommentCheck<N>        if comment is CommentCheck:<N>            comment = None<N>        self.comment = comment<N><N>
# coding: utf-8<N><N>from ruamel.yaml.reader import Reader<N>from ruamel.yaml.scanner import Scanner, RoundTripScanner<N>from ruamel.yaml.parser import Parser, RoundTripParser<N>from ruamel.yaml.composer import Composer<N>from ruamel.yaml.constructor import (<N>    BaseConstructor,<N>    SafeConstructor,<N>    Constructor,<N>    RoundTripConstructor,<N>)<N>from ruamel.yaml.resolver import VersionedResolver<N><N>
# coding: utf-8<N><N>import sys<N>import os<N>import warnings<N>import glob<N>from importlib import import_module<N><N><N>import ruamel.yaml<N>from ruamel.yaml.error import UnsafeLoaderWarning, YAMLError  # NOQA<N><N>from ruamel.yaml.tokens import *  # NOQA<N>from ruamel.yaml.events import *  # NOQA<N>from ruamel.yaml.nodes import *  # NOQA<N><N>
# coding: utf-8<N><N>import re<N><N>if False:  # MYPY<N>    from typing import Any, Dict, List, Union, Text, Optional  # NOQA<N>    from ruamel.yaml.compat import VersionType  # NOQA<N><N>from ruamel.yaml.compat import _DEFAULT_YAML_VERSION, _F  # NOQA<N>from ruamel.yaml.error import *  # NOQA<N>from ruamel.yaml.nodes import MappingNode, ScalarNode, SequenceNode  # NOQA<N>from ruamel.yaml.util import RegExp  # NOQA<N><N>
# coding: utf-8<N><N>"""<N>You cannot subclass bool, and this is necessary for round-tripping anchored<N>bool values (and also if you want to preserve the original way of writing)<N><N>bool.__bases__ is type 'int', so that is what is used as the basis for ScalarBoolean as well.<N><N>
# coding: utf-8<N><N>from ruamel.yaml.anchor import Anchor<N><N>if False:  # MYPY<N>    from typing import Text, Any, Dict, List  # NOQA<N><N>__all__ = [<N>    'ScalarString',<N>    'LiteralScalarString',<N>    'FoldedScalarString',<N>    'SingleQuotedScalarString',<N>    'DoubleQuotedScalarString',<N>    'PlainScalarString',<N>    # PreservedScalarString is the old name, as it was the first to be preserved on rt,<N>    # use LiteralScalarString instead<N>    'PreservedScalarString',<N>]<N><N>
<N>class ScalarString(str):<N>    __slots__ = Anchor.attrib<N><N>    def __new__(cls, *args, **kw):<N>        # type: (Any, Any) -> Any<N>        anchor = kw.pop('anchor', None)<N>        ret_val = str.__new__(cls, *args, **kw)<N>        if anchor is not None:<N>            ret_val.yaml_set_anchor(anchor, always_dump=True)<N>        return ret_val<N><N>
    def replace(self, old, new, maxreplace=-1):<N>        # type: (Any, Any, int) -> Any<N>        return type(self)((str.replace(self, old, new, maxreplace)))<N><N>    @property<N>    def anchor(self):<N>        # type: () -> Any<N>        if not hasattr(self, Anchor.attrib):<N>            setattr(self, Anchor.attrib, Anchor())<N>        return getattr(self, Anchor.attrib)<N><N>
    def yaml_anchor(self, any=False):<N>        # type: (bool) -> Any<N>        if not hasattr(self, Anchor.attrib):<N>            return None<N>        if any or self.anchor.always_dump:<N>            return self.anchor<N>        return None<N><N>    def yaml_set_anchor(self, value, always_dump=False):<N>        # type: (Any, bool) -> None<N>        self.anchor.value = value<N>        self.anchor.always_dump = always_dump<N><N>
<N>class LiteralScalarString(ScalarString):<N>    __slots__ = 'comment'  # the comment after the | on the first line<N><N>    style = '|'<N><N>    def __new__(cls, value, anchor=None):<N>        # type: (Text, Any) -> Any<N>        return ScalarString.__new__(cls, value, anchor=anchor)<N><N>
<N>PreservedScalarString = LiteralScalarString<N><N><N>class FoldedScalarString(ScalarString):<N>    __slots__ = ('fold_pos', 'comment')  # the comment after the > on the first line<N><N>    style = '>'<N><N>    def __new__(cls, value, anchor=None):<N>        # type: (Text, Any) -> Any<N>        return ScalarString.__new__(cls, value, anchor=anchor)<N><N>
<N>class SingleQuotedScalarString(ScalarString):<N>    __slots__ = ()<N><N>    style = "'"<N><N>    def __new__(cls, value, anchor=None):<N>        # type: (Text, Any) -> Any<N>        return ScalarString.__new__(cls, value, anchor=anchor)<N><N><N>class DoubleQuotedScalarString(ScalarString):<N>    __slots__ = ()<N><N>
    style = '"'<N><N>    def __new__(cls, value, anchor=None):<N>        # type: (Text, Any) -> Any<N>        return ScalarString.__new__(cls, value, anchor=anchor)<N><N><N>class PlainScalarString(ScalarString):<N>    __slots__ = ()<N><N>    style = ''<N><N>
    def __new__(cls, value, anchor=None):<N>        # type: (Text, Any) -> Any<N>        return ScalarString.__new__(cls, value, anchor=anchor)<N><N><N>def preserve_literal(s):<N>    # type: (Text) -> Text<N>    return LiteralScalarString(s.replace('\r\n', '\n').replace('\r', '\n'))<N><N>
<N>def walk_tree(base, map=None):<N>    # type: (Any, Any) -> None<N>    """<N>    the routine here walks over a simple yaml tree (recursing in<N>    dict values and list items) and converts strings that<N>    have multiple lines to literal scalars<N><N>
    You can also provide an explicit (ordered) mapping for multiple transforms<N>    (first of which is executed):<N>        map = ruamel.yaml.compat.ordereddict<N>        map['\n'] = preserve_literal<N>        map[':'] = SingleQuotedScalarString<N>        walk_tree(data, map=map)<N>    """<N>    from collections.abc import MutableMapping, MutableSequence<N><N>
# coding: utf-8<N><N>import datetime<N>import copy<N><N># ToDo: at least on PY3 you could probably attach the tzinfo correctly to the object<N>#       a more complete datetime might be used by safe loading as well<N><N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List  # NOQA<N><N>
<N>class TimeStamp(datetime.datetime):<N>    def __init__(self, *args, **kw):<N>        # type: (Any, Any) -> None<N>        self._yaml = dict(t=False, tz=None, delta=0)  # type: Dict[Any, Any]<N><N>    def __new__(cls, *args, **kw):  # datetime is immutable<N>        # type: (Any, Any) -> Any<N>        return datetime.datetime.__new__(cls, *args, **kw)<N><N>
# coding: utf-8<N><N>from ruamel.yaml.compat import _F, nprintf  # NOQA<N><N>if False:  # MYPY<N>    from typing import Text, Any, Dict, Optional, List  # NOQA<N>    from .error import StreamMark  # NOQA<N><N>SHOW_LINES = True<N><N><N>class Token:<N>    __slots__ = 'start_mark', 'end_mark', '_comment'<N><N>
# coding: utf-8<N><N>"""<N>some helper functions that might be generally useful<N>"""<N><N>import datetime<N>from functools import partial<N>import re<N><N><N>if False:  # MYPY<N>    from typing import Any, Dict, Optional, List, Text  # NOQA<N>    from .compat import StreamTextType  # NOQA<N><N>
<N>class LazyEval:<N>    """<N>    Lightweight wrapper around lazily evaluated func(*args, **kwargs).<N><N>    func is only evaluated when any attribute of its return value is accessed.<N>    Every attribute access is passed through to the wrapped value.<N>    (This only excludes special cases like method-wrappers, e.g., __hash__.)<N>    The sole additional attribute is the lazy_self function which holds the<N>    return value (or, prior to evaluation, func and arguments), in its closure.<N>    """<N><N>
    def __init__(self, func, *args, **kwargs):<N>        # type: (Any, Any, Any) -> None<N>        def lazy_self():<N>            # type: () -> Any<N>            return_value = func(*args, **kwargs)<N>            object.__setattr__(self, 'lazy_self', lambda: return_value)<N>            return return_value<N><N>
        object.__setattr__(self, 'lazy_self', lazy_self)<N><N>    def __getattribute__(self, name):<N>        # type: (Any) -> Any<N>        lazy_self = object.__getattribute__(self, 'lazy_self')<N>        if name == 'lazy_self':<N>            return lazy_self<N>        return getattr(lazy_self(), name)<N><N>
"""Utilities for extracting common archive formats"""<N><N>import zipfile<N>import tarfile<N>import os<N>import shutil<N>import posixpath<N>import contextlib<N>from distutils.errors import DistutilsError<N><N>from ._path import ensure_directory<N><N>
__all__ = [<N>    "unpack_archive", "unpack_zipfile", "unpack_tarfile", "default_filter",<N>    "UnrecognizedFormat", "extraction_drivers", "unpack_directory",<N>]<N><N><N>class UnrecognizedFormat(DistutilsError):<N>    """Couldn't recognize the archive type"""<N><N>
<N>def default_filter(src, dst):<N>    """The default progress/filter callback; returns True for all files"""<N>    return dst<N><N><N>def unpack_archive(<N>        filename, extract_dir, progress_filter=default_filter,<N>        drivers=None):<N>    """Unpack `filename` to `extract_dir`, or raise ``UnrecognizedFormat``<N><N>
    `progress_filter` is a function taking two arguments: a source path<N>    internal to the archive ('/'-separated), and a filesystem path where it<N>    will be extracted.  The callback must return the desired extract path<N>    (which may be the same as the one passed in), or else ``None`` to skip<N>    that file or directory.  The callback can thus be used to report on the<N>    progress of the extraction, as well as to filter the items extracted or<N>    alter their extraction paths.<N><N>
"""A PEP 517 interface to setuptools<N><N>Previously, when a user or a command line tool (let's call it a "frontend")<N>needed to make a request of setuptools to take a certain action, for<N>example, generating a list of installation requirements, the frontend would<N>would call "setup.py egg_info" or "setup.py bdist_wheel" on the command line.<N><N>
PEP 517 defines a different method of interfacing with setuptools. Rather<N>than calling "setup.py" directly, the frontend should:<N><N>  1. Set the current directory to the directory with a setup.py file<N>  2. Import this module into a safe python interpreter (one in which<N>     setuptools can potentially set global variables or crash hard).<N>  3. Call one of the functions defined in PEP 517.<N><N>
import sys<N>import marshal<N>import contextlib<N>import dis<N><N>from setuptools.extern.packaging import version<N><N>from ._imp import find_module, PY_COMPILED, PY_FROZEN, PY_SOURCE<N>from . import _imp<N><N><N>__all__ = [<N>    'Require', 'find_module', 'get_module_constant', 'extract_constant'<N>]<N><N>
<N>class Require:<N>    """A prerequisite to building or installing a distribution"""<N><N>    def __init__(<N>            self, name, requested_version, module, homepage='',<N>            attribute=None, format=None):<N><N>        if format is None and requested_version is not None:<N>            format = version.Version<N><N>
        if format is not None:<N>            requested_version = format(requested_version)<N>            if attribute is None:<N>                attribute = '__version__'<N><N>        self.__dict__.update(locals())<N>        del self.self<N><N>    def full_name(self):<N>        """Return full package/distribution name, w/version"""<N>        if self.requested_version is not None:<N>            return '%s-%s' % (self.name, self.requested_version)<N>        return self.name<N><N>
    def version_ok(self, version):<N>        """Is 'version' sufficiently up-to-date?"""<N>        return self.attribute is None or self.format is None or \<N>            str(version) != "unknown" and self.format(version) >= self.requested_version<N><N>
"""Automatic discovery of Python modules and packages (for inclusion in the<N>distribution) and other config values.<N><N>For the purposes of this module, the following nomenclature is used:<N><N>- "src-layout": a directory representing a Python project that contains a "src"<N>  folder. Everything under the "src" folder is meant to be included in the<N>  distribution when packaging the project. Example::<N><N>
    .<N>    ├── tox.ini<N>    ├── pyproject.toml<N>    └── src/<N>        └── mypkg/<N>            ├── __init__.py<N>            ├── mymodule.py<N>            └── my_data_file.txt<N><N>- "flat-layout": a Python project that does not use "src-layout" but instead<N>  have a directory under the project root for each package::<N><N>
    .<N>    ├── tox.ini<N>    ├── pyproject.toml<N>    └── mypkg/<N>        ├── __init__.py<N>        ├── mymodule.py<N>        └── my_data_file.txt<N><N>- "single-module": a project that contains a single Python script direct under<N>  the project root (no directory used)::<N><N>
    .<N>    ├── tox.ini<N>    ├── pyproject.toml<N>    └── mymodule.py<N><N>"""<N><N>import itertools<N>import os<N>from fnmatch import fnmatchcase<N>from glob import glob<N>from pathlib import Path<N>from typing import TYPE_CHECKING<N>from typing import Callable, Dict, Iterator, Iterable, List, Optional, Tuple, Union<N><N>
import _distutils_hack.override  # noqa: F401<N><N>from distutils import log<N>from distutils.util import convert_path<N><N>_Path = Union[str, os.PathLike]<N>_Filter = Callable[[str], bool]<N>StrIter = Iterator[str]<N><N>chain_iter = itertools.chain.from_iterable<N><N>
if TYPE_CHECKING:<N>    from setuptools import Distribution  # noqa<N><N><N>def _valid_name(path: _Path) -> bool:<N>    # Ignore invalid names that cannot be imported directly<N>    return os.path.basename(path).isidentifier()<N><N><N>class _Finder:<N>    """Base class that exposes functionality for module/package finders"""<N><N>
    ALWAYS_EXCLUDE: Tuple[str, ...] = ()<N>    DEFAULT_EXCLUDE: Tuple[str, ...] = ()<N><N>    @classmethod<N>    def find(<N>        cls,<N>        where: _Path = '.',<N>        exclude: Iterable[str] = (),<N>        include: Iterable[str] = ('*',)<N>    ) -> List[str]:<N>        """Return a list of all Python items (packages or modules, depending on<N>        the finder implementation) found within directory 'where'.<N><N>
        'where' is the root directory which will be searched.<N>        It should be supplied as a "cross-platform" (i.e. URL-style) path;<N>        it will be converted to the appropriate local path syntax.<N><N>        'exclude' is a sequence of names to exclude; '*' can be used<N>        as a wildcard in the names.<N>        When finding packages, 'foo.*' will exclude all subpackages of 'foo'<N>        (but not 'foo' itself).<N><N>
        'include' is a sequence of names to include.<N>        If it's specified, only the named items will be included.<N>        If it's not specified, all found items will be included.<N>        'include' can contain shell style wildcard patterns just like<N>        'exclude'.<N>        """<N><N>
        exclude = exclude or cls.DEFAULT_EXCLUDE<N>        return list(<N>            cls._find_iter(<N>                convert_path(str(where)),<N>                cls._build_filter(*cls.ALWAYS_EXCLUDE, *exclude),<N>                cls._build_filter(*include),<N>            )<N>        )<N><N>
    @classmethod<N>    def _find_iter(cls, where: _Path, exclude: _Filter, include: _Filter) -> StrIter:<N>        raise NotImplementedError<N><N>    @staticmethod<N>    def _build_filter(*patterns: str) -> _Filter:<N>        """<N>        Given a list of patterns, return a callable that will be true only if<N>        the input matches at least one of the patterns.<N>        """<N>        return lambda name: any(fnmatchcase(name, pat) for pat in patterns)<N><N>
import re<N>import functools<N>import distutils.core<N>import distutils.errors<N>import distutils.extension<N><N>from .monkey import get_unpatched<N><N><N>def _have_cython():<N>    """<N>    Return True if Cython can be imported.<N>    """<N>    cython_impl = 'Cython.Distutils.build_ext'<N>    try:<N>        # from (cython_impl) import build_ext<N>        __import__(cython_impl, fromlist=['build_ext']).build_ext<N>        return True<N>    except Exception:<N>        pass<N>    return False<N><N>
"""<N>Filename globbing utility. Mostly a copy of `glob` from Python 3.5.<N><N>Changes include:<N> * `yield from` and PEP3102 `*` removed.<N> * Hidden files are not ignored.<N>"""<N><N>import os<N>import re<N>import fnmatch<N><N>__all__ = ["glob", "iglob", "escape"]<N><N>
<N>def glob(pathname, recursive=False):<N>    """Return a list of paths matching a pathname pattern.<N><N>    The pattern may contain simple shell-style wildcards a la<N>    fnmatch. However, unlike fnmatch, filenames starting with a<N>    dot are special cases that are not matched by '*' and '?'<N>    patterns.<N><N>
    If recursive is true, the pattern '**' will match any files and<N>    zero or more directories and subdirectories.<N>    """<N>    return list(iglob(pathname, recursive=recursive))<N><N><N>def iglob(pathname, recursive=False):<N>    """Return an iterator which yields the paths matching a pathname pattern.<N><N>
import glob<N>import os<N>import subprocess<N>import sys<N>import tempfile<N>import warnings<N>from distutils import log<N>from distutils.errors import DistutilsError<N><N>import pkg_resources<N>from setuptools.wheel import Wheel<N>from ._deprecation_warning import SetuptoolsDeprecationWarning<N><N>
<N>def _fixup_find_links(find_links):<N>    """Ensure find-links option end-up being a list of strings."""<N>    if isinstance(find_links, str):<N>        return find_links.split()<N>    assert isinstance(find_links, (tuple, list))<N>    return find_links<N><N>
"""<N>Launch the Python script on the command line after<N>setuptools is bootstrapped via import.<N>"""<N><N># Note that setuptools gets imported implicitly by the<N># invocation of this script using python -m setuptools.launch<N><N>import tokenize<N>import sys<N><N>
<N>def run():<N>    """<N>    Run the script in sys.argv[1] as if it had<N>    been invoked naturally.<N>    """<N>    __builtins__<N>    script_name = sys.argv[1]<N>    namespace = dict(<N>        __file__=script_name,<N>        __name__='__main__',<N>        __doc__=None,<N>    )<N>    sys.argv[:] = sys.argv[1:]<N><N>
    open_ = getattr(tokenize, 'open', open)<N>    with open_(script_name) as fid:<N>        script = fid.read()<N>    norm_script = script.replace('\\r\\n', '\\n')<N>    code = compile(norm_script, script_name, 'exec')<N>    exec(code, namespace)<N><N>
"""<N>Monkey patching of distutils.<N>"""<N><N>import sys<N>import distutils.filelist<N>import platform<N>import types<N>import functools<N>from importlib import import_module<N>import inspect<N><N>import setuptools<N><N>__all__ = []<N>"""<N>Everything is private. Contact the project team<N>if you think you need this functionality.<N>"""<N><N>
"""<N>Improved support for Microsoft Visual C++ compilers.<N><N>Known supported compilers:<N>--------------------------<N>Microsoft Visual C++ 9.0:<N>    Microsoft Visual C++ Compiler for Python 2.7 (x86, amd64)<N>    Microsoft Windows SDK 6.1 (x86, x64, ia64)<N>    Microsoft Windows SDK 7.0 (x86, x64, ia64)<N><N>
Microsoft Visual C++ 10.0:<N>    Microsoft Windows SDK 7.1 (x86, x64, ia64)<N><N>Microsoft Visual C++ 14.X:<N>    Microsoft Visual C++ Build Tools 2015 (x86, x64, arm)<N>    Microsoft Visual Studio Build Tools 2017 (x86, x64, arm, arm64)<N>    Microsoft Visual Studio Build Tools 2019 (x86, x64, arm, arm64)<N><N>
This may also support compilers shipped with compatible Visual Studio versions.<N>"""<N><N>import json<N>from io import open<N>from os import listdir, pathsep<N>from os.path import join, isfile, isdir, dirname<N>import sys<N>import contextlib<N>import platform<N>import itertools<N>import subprocess<N>import distutils.errors<N>from setuptools.extern.packaging.version import LegacyVersion<N>from setuptools.extern.more_itertools import unique_everseen<N><N>
from .monkey import get_unpatched<N><N>if platform.system() == 'Windows':<N>    import winreg<N>    from os import environ<N>else:<N>    # Mock winreg and environ so the module can be imported on this platform.<N><N>    class winreg:<N>        HKEY_USERS = None<N>        HKEY_CURRENT_USER = None<N>        HKEY_LOCAL_MACHINE = None<N>        HKEY_CLASSES_ROOT = None<N><N>
    environ = dict()<N><N>_msvc9_suppress_errors = (<N>    # msvc9compiler isn't available on some platforms<N>    ImportError,<N><N>    # msvc9compiler raises DistutilsPlatformError in some<N>    # environments. See #1118.<N>    distutils.errors.DistutilsPlatformError,<N>)<N><N>
try:<N>    from distutils.msvc9compiler import Reg<N>except _msvc9_suppress_errors:<N>    pass<N><N><N>def msvc9_find_vcvarsall(version):<N>    """<N>    Patched "distutils.msvc9compiler.find_vcvarsall" to use the standalone<N>    compiler build for Python<N>    (VCForPython / Microsoft Visual C++ Compiler for Python 2.7).<N><N>
"""PyPI and direct package downloading"""<N>import sys<N>import os<N>import re<N>import io<N>import shutil<N>import socket<N>import base64<N>import hashlib<N>import itertools<N>import warnings<N>import configparser<N>import html<N>import http.client<N>import urllib.parse<N>import urllib.request<N>import urllib.error<N>from functools import wraps<N><N>
import setuptools<N>from pkg_resources import (<N>    CHECKOUT_DIST, Distribution, BINARY_DIST, normalize_path, SOURCE_DIST,<N>    Environment, find_distributions, safe_name, safe_version,<N>    to_filename, Requirement, DEVELOP_DIST, EGG_DIST, parse_version,<N>)<N>from distutils import log<N>from distutils.errors import DistutilsError<N>from fnmatch import translate<N>from setuptools.wheel import Wheel<N>from setuptools.extern.more_itertools import unique_everseen<N><N>
<N>EGG_FRAGMENT = re.compile(r'^egg=([-A-Za-z0-9_.+!]+)$')<N>HREF = re.compile(r"""href\s*=\s*['"]?([^'"> ]+)""", re.I)<N>PYPI_MD5 = re.compile(<N>    r'<a href="([^"#]+)">([^<]+)</a>\n\s+\(<a (?:title="MD5 hash"\n\s+)'<N>    r'href="[^?]+\?:action=show_md5&amp;digest=([0-9a-f]{32})">md5</a>\)'<N>)<N>URL_SCHEME = re.compile('([-+.a-z0-9]{2,}):', re.I).match<N>EXTENSIONS = ".tar.gz .tar.bz2 .tar .zip .tgz".split()<N><N>
__all__ = [<N>    'PackageIndex', 'distros_for_url', 'parse_bdist_wininst',<N>    'interpret_distro_name',<N>]<N><N>_SOCKET_TIMEOUT = 15<N><N>_tmpl = "setuptools/{setuptools.__version__} Python-urllib/{py_major}"<N>user_agent = _tmpl.format(<N>    py_major='{}.{}'.format(*sys.version_info), setuptools=setuptools)<N><N>
<N>def parse_requirement_arg(spec):<N>    try:<N>        return Requirement.parse(spec)<N>    except ValueError as e:<N>        raise DistutilsError(<N>            "Not a URL, existing file, or requirement spec: %r" % (spec,)<N>        ) from e<N><N>
import importlib<N><N>try:<N>    import importlib.util<N>except ImportError:<N>    pass<N><N><N>try:<N>    module_from_spec = importlib.util.module_from_spec<N>except AttributeError:<N>    def module_from_spec(spec):<N>        return spec.loader.load_module(spec.name)<N>
import os<N>import sys<N>import tempfile<N>import operator<N>import functools<N>import itertools<N>import re<N>import contextlib<N>import pickle<N>import textwrap<N>import builtins<N><N>import pkg_resources<N>from distutils.errors import DistutilsError<N>from pkg_resources import working_set<N><N>
if sys.platform.startswith('java'):<N>    import org.python.modules.posix.PosixModule as _os<N>else:<N>    _os = sys.modules[os.name]<N>try:<N>    _file = file<N>except NameError:<N>    _file = None<N>_open = open<N><N><N>__all__ = [<N>    "AbstractSandbox",<N>    "DirectorySandbox",<N>    "SandboxViolation",<N>    "run_setup",<N>]<N><N>
<N>def _execfile(filename, globals, locals=None):<N>    """<N>    Python 3 implementation of execfile.<N>    """<N>    mode = 'rb'<N>    with open(filename, mode) as stream:<N>        script = stream.read()<N>    if locals is None:<N>        locals = globals<N>    code = compile(script, filename, 'exec')<N>    exec(code, globals, locals)<N><N>
<N>@contextlib.contextmanager<N>def save_argv(repl=None):<N>    saved = sys.argv[:]<N>    if repl is not None:<N>        sys.argv[:] = repl<N>    try:<N>        yield saved<N>    finally:<N>        sys.argv[:] = saved<N><N><N>@contextlib.contextmanager<N>def save_path():<N>    saved = sys.path[:]<N>    try:<N>        yield saved<N>    finally:<N>        sys.path[:] = saved<N><N>
<N>@contextlib.contextmanager<N>def override_temp(replacement):<N>    """<N>    Monkey-patch tempfile.tempdir with replacement, ensuring it exists<N>    """<N>    os.makedirs(replacement, exist_ok=True)<N><N>    saved = tempfile.tempdir<N><N>    tempfile.tempdir = replacement<N><N>
    try:<N>        yield<N>    finally:<N>        tempfile.tempdir = saved<N><N><N>@contextlib.contextmanager<N>def pushd(target):<N>    saved = os.getcwd()<N>    os.chdir(target)<N>    try:<N>        yield saved<N>    finally:<N>        os.chdir(saved)<N><N>
import unicodedata<N>import sys<N><N><N># HFS Plus uses decomposed UTF-8<N>def decompose(path):<N>    if isinstance(path, str):<N>        return unicodedata.normalize('NFD', path)<N>    try:<N>        path = path.decode('utf-8')<N>        path = unicodedata.normalize('NFD', path)<N>        path = path.encode('utf-8')<N>    except UnicodeError:<N>        pass  # Not UTF-8<N>    return path<N><N>
<N>def filesys_decode(path):<N>    """<N>    Ensure that the given path is decoded,<N>    NONE when no expected encoding works<N>    """<N><N>    if isinstance(path, str):<N>        return path<N><N>    fs_enc = sys.getfilesystemencoding() or 'utf-8'<N>    candidates = fs_enc, 'utf-8'<N><N>
    for enc in candidates:<N>        try:<N>            return path.decode(enc)<N>        except UnicodeDecodeError:<N>            continue<N><N><N>def try_encode(string, enc):<N>    "turn unicode encoding into a functional routine"<N>    try:<N>        return string.encode(enc)<N>    except UnicodeEncodeError:<N>        return None<N><N><N>
import pkg_resources<N><N>try:<N>    __version__ = pkg_resources.get_distribution('setuptools').version<N>except Exception:<N>    __version__ = 'unknown'<N>
import platform<N><N><N>def windows_only(func):<N>    if platform.system() != 'Windows':<N>        return lambda *args, **kwargs: None<N>    return func<N><N><N>@windows_only<N>def hide_file(path):<N>    """<N>    Set the hidden attribute on a file or directory.<N><N>
    From http://stackoverflow.com/questions/19622133/<N><N>    `path` must be text.<N>    """<N>    import ctypes<N>    __import__('ctypes.wintypes')<N>    SetFileAttributes = ctypes.windll.kernel32.SetFileAttributesW<N>    SetFileAttributes.argtypes = ctypes.wintypes.LPWSTR, ctypes.wintypes.DWORD<N>    SetFileAttributes.restype = ctypes.wintypes.BOOL<N><N>
class SetuptoolsDeprecationWarning(Warning):<N>    """<N>    Base class for warning deprecations in ``setuptools``<N><N>    This class is not derived from ``DeprecationWarning``, and as such is<N>    visible by default.<N>    """<N>
import functools<N>import operator<N>import itertools<N><N>from .extern.jaraco.text import yield_lines<N>from .extern.jaraco.functools import pass_none<N>from ._importlib import metadata<N>from ._itertools import ensure_unique<N>from .extern.more_itertools import consume<N><N>
<N>def ensure_valid(ep):<N>    """<N>    Exercise one of the dynamic properties to trigger<N>    the pattern match.<N>    """<N>    ep.extras<N><N><N>def load_group(value, group):<N>    """<N>    Given a value of an entry point or series of entry points,<N>    return each as an EntryPoint.<N>    """<N>    # normalize to a single sequence of lines<N>    lines = yield_lines(value)<N>    text = f'[{group}]\n' + '\n'.join(lines)<N>    return metadata.EntryPoints._from_text(text)<N><N>
<N>def by_group_and_name(ep):<N>    return ep.group, ep.name<N><N><N>def validate(eps: metadata.EntryPoints):<N>    """<N>    Ensure entry points are unique by group and name and validate each.<N>    """<N>    consume(map(ensure_valid, ensure_unique(eps, key=by_group_and_name)))<N>    return eps<N><N>
<N>@functools.singledispatch<N>def load(eps):<N>    """<N>    Given a Distribution.entry_points, produce EntryPoints.<N>    """<N>    groups = itertools.chain.from_iterable(<N>        load_group(value, group)<N>        for group, value in eps.items())<N>    return validate(metadata.EntryPoints(groups))<N><N>
<N>@load.register(str)<N>def _(eps):<N>    r"""<N>    >>> ep, = load('[console_scripts]\nfoo=bar')<N>    >>> ep.group<N>    'console_scripts'<N>    >>> ep.name<N>    'foo'<N>    >>> ep.value<N>    'bar'<N>    """<N>    return validate(metadata.EntryPoints(metadata.EntryPoints._from_text(eps)))<N><N>
<N>load.register(type(None), lambda x: x)<N><N><N>@pass_none<N>def render(eps: metadata.EntryPoints):<N>    by_group = operator.attrgetter('group')<N>    groups = itertools.groupby(sorted(eps, key=by_group), by_group)<N><N>    return '\n'.join(<N>        f'[{group}]\n{render_items(items)}\n'<N>        for group, items in groups<N>    )<N><N>
"""<N>Re-implementation of find_module and get_frozen_object<N>from the deprecated imp module.<N>"""<N><N>import os<N>import importlib.util<N>import importlib.machinery<N><N>from .py34compat import module_from_spec<N><N><N>PY_SOURCE = 1<N>PY_COMPILED = 2<N>C_EXTENSION = 3<N>C_BUILTIN = 6<N>PY_FROZEN = 7<N><N>
from setuptools.extern.more_itertools import consume  # noqa: F401<N><N><N># copied from jaraco.itertools 6.1<N>def ensure_unique(iterable, key=lambda x: x):<N>    """<N>    Wrap an iterable to raise a ValueError if non-unique values are encountered.<N><N>
    >>> list(ensure_unique('abc'))<N>    ['a', 'b', 'c']<N>    >>> consume(ensure_unique('abca'))<N>    Traceback (most recent call last):<N>    ...<N>    ValueError: Duplicate element 'a' encountered.<N>    """<N>    seen = set()<N>    seen_add = seen.add<N>    for element in iterable:<N>        k = key(element)<N>        if k in seen:<N>            raise ValueError(f"Duplicate element {element!r} encountered.")<N>        seen_add(k)<N>        yield element<N><N><N>
import os<N><N><N>def ensure_directory(path):<N>    """Ensure that the parent directory of `path` exists"""<N>    dirname = os.path.dirname(path)<N>    os.makedirs(dirname, exist_ok=True)<N>
import setuptools.extern.jaraco.text as text<N><N>from pkg_resources import Requirement<N><N><N>def parse_strings(strs):<N>    """<N>    Yield requirement strings for each specification in `strs`.<N><N>    `strs` must be a string, or a (possibly-nested) iterable thereof.<N>    """<N>    return text.join_continuation(map(text.drop_comment, text.yield_lines(strs)))<N><N><N>def parse(strs):<N>    """<N>    Deprecated drop-in replacement for pkg_resources.parse_requirements.<N>    """<N>    return map(Requirement, parse_strings(strs))<N>
"""Extensions to the 'distutils' for large or complex distributions"""<N><N>import functools<N>import os<N>import re<N>import warnings<N><N>import _distutils_hack.override  # noqa: F401<N><N>import distutils.core<N>from distutils.errors import DistutilsOptionError<N>from distutils.util import convert_path as _convert_path<N><N>
from ._deprecation_warning import SetuptoolsDeprecationWarning<N><N>import setuptools.version<N>from setuptools.extension import Extension<N>from setuptools.dist import Distribution<N>from setuptools.depends import Require<N>from setuptools.discovery import PackageFinder, PEP420PackageFinder<N>from . import monkey<N>from . import logging<N><N>
<N>__all__ = [<N>    'setup',<N>    'Distribution',<N>    'Command',<N>    'Extension',<N>    'Require',<N>    'SetuptoolsDeprecationWarning',<N>    'find_packages',<N>    'find_namespace_packages',<N>]<N><N>__version__ = setuptools.version.__version__<N><N>
bootstrap_install_from = None<N><N><N>find_packages = PackageFinder.find<N>find_namespace_packages = PEP420PackageFinder.find<N><N><N>def _install_setup_requires(attrs):<N>    # Note: do not use `setuptools.Distribution` directly, as<N>    # our PEP 517 backend patch `distutils.core.Distribution`.<N>    class MinimalDistribution(distutils.core.Distribution):<N>        """<N>        A minimal version of a distribution for supporting the<N>        fetch_build_eggs interface.<N>        """<N><N>
        def __init__(self, attrs):<N>            _incl = 'dependency_links', 'setup_requires'<N>            filtered = {k: attrs[k] for k in set(_incl) & set(attrs)}<N>            super().__init__(filtered)<N>            # Prevent accidentally triggering discovery with incomplete set of attrs<N>            self.set_defaults._disable()<N><N>
        def _get_project_config_files(self, filenames=None):<N>            """Ignore ``pyproject.toml``, they are not related to setup_requires"""<N>            try:<N>                cfg, toml = super()._split_standard_project_metadata(filenames)<N>                return cfg, ()<N>            except Exception:<N>                return filenames, ()<N><N>
        def finalize_options(self):<N>            """<N>            Disable finalize_options to avoid building the working set.<N>            Ref #2158.<N>            """<N><N>    dist = MinimalDistribution(attrs)<N><N>    # Honor setup.cfg's options.<N>    dist.parse_config_files(ignore_option_errors=True)<N>    if dist.setup_requires:<N>        dist.fetch_build_eggs(dist.setup_requires)<N><N>
<N>def setup(**attrs):<N>    # Make sure we have any requirements needed to interpret 'attrs'.<N>    logging.configure()<N>    _install_setup_requires(attrs)<N>    return distutils.core.setup(**attrs)<N><N><N>setup.__doc__ = distutils.core.setup.__doc__<N><N>
<N>_Command = monkey.get_unpatched(distutils.core.Command)<N><N><N>class Command(_Command):<N>    __doc__ = _Command.__doc__<N><N>    command_consumes_arguments = False<N><N>    def __init__(self, dist, **kw):<N>        """<N>        Construct the command for dist, updating<N>        vars(self) with any keyword parameters.<N>        """<N>        super().__init__(dist)<N>        vars(self).update(kw)<N><N>
    def _ensure_stringlike(self, option, what, default=None):<N>        val = getattr(self, option)<N>        if val is None:<N>            setattr(self, option, default)<N>            return default<N>        elif not isinstance(val, str):<N>            raise DistutilsOptionError(<N>                "'%s' must be a %s (got `%s`)" % (option, what, val)<N>            )<N>        return val<N><N>
from distutils.errors import DistutilsOptionError<N><N>from setuptools.command.setopt import edit_config, option_base, config_file<N><N><N>def shquote(arg):<N>    """Quote an argument for later parsing by shlex.split()"""<N>    for c in '"', "'", "\\", "#":<N>        if c in arg:<N>            return repr(arg)<N>    if arg.split() != [arg]:<N>        return repr(arg)<N>    return arg<N><N>
<N>class alias(option_base):<N>    """Define a shortcut that invokes one or more commands"""<N><N>    description = "define a shortcut to invoke one or more commands"<N>    command_consumes_arguments = True<N><N>    user_options = [<N>        ('remove', 'r', 'remove (unset) the alias'),<N>    ] + option_base.user_options<N><N>
    boolean_options = option_base.boolean_options + ['remove']<N><N>    def initialize_options(self):<N>        option_base.initialize_options(self)<N>        self.args = None<N>        self.remove = None<N><N>    def finalize_options(self):<N>        option_base.finalize_options(self)<N>        if self.remove and len(self.args) != 1:<N>            raise DistutilsOptionError(<N>                "Must specify exactly one argument (the alias name) when "<N>                "using --remove"<N>            )<N><N>
    def run(self):<N>        aliases = self.distribution.get_option_dict('aliases')<N><N>        if not self.args:<N>            print("Command Aliases")<N>            print("---------------")<N>            for alias in aliases:<N>                print("setup.py alias", format_alias(alias, aliases))<N>            return<N><N>
        elif len(self.args) == 1:<N>            alias, = self.args<N>            if self.remove:<N>                command = None<N>            elif alias in aliases:<N>                print("setup.py alias", format_alias(alias, aliases))<N>                return<N>            else:<N>                print("No alias definition found for %r" % alias)<N>                return<N>        else:<N>            alias = self.args[0]<N>            command = ' '.join(map(shquote, self.args[1:]))<N><N>
        edit_config(self.filename, {'aliases': {alias: command}}, self.dry_run)<N><N><N>def format_alias(name, aliases):<N>    source, command = aliases[name]<N>    if source == config_file('global'):<N>        source = '--global-config '<N>    elif source == config_file('user'):<N>        source = '--user-config '<N>    elif source == config_file('local'):<N>        source = ''<N>    else:<N>        source = '--filename=%r' % source<N>    return source + name + ' ' + command<N><N><N>
"""setuptools.command.bdist_egg<N><N>Build .egg distributions"""<N><N>from distutils.dir_util import remove_tree, mkpath<N>from distutils import log<N>from types import CodeType<N>import sys<N>import os<N>import re<N>import textwrap<N>import marshal<N><N>
from pkg_resources import get_build_platform, Distribution<N>from setuptools.extension import Library<N>from setuptools import Command<N>from .._path import ensure_directory<N><N>from sysconfig import get_path, get_python_version<N><N><N>def _get_purelib():<N>    return get_path("purelib")<N><N>
<N>def strip_module(filename):<N>    if '.' in filename:<N>        filename = os.path.splitext(filename)[0]<N>    if filename.endswith('module'):<N>        filename = filename[:-6]<N>    return filename<N><N><N>def sorted_walk(dir):<N>    """Do os.walk in a reproducible way,<N>    independent of indeterministic filesystem readdir order<N>    """<N>    for base, dirs, files in os.walk(dir):<N>        dirs.sort()<N>        files.sort()<N>        yield base, dirs, files<N><N>
import distutils.command.bdist_rpm as orig<N>import warnings<N><N>from setuptools import SetuptoolsDeprecationWarning<N><N><N>class bdist_rpm(orig.bdist_rpm):<N>    """<N>    Override the default bdist_rpm behavior to do the following:<N><N>    1. Run egg_info to ensure the name and version are properly calculated.<N>    2. Always run 'install' using --single-version-externally-managed to<N>       disable eggs in RPM distributions.<N>    """<N><N>
    def run(self):<N>        warnings.warn(<N>            "bdist_rpm is deprecated and will be removed in a future "<N>            "version. Use bdist_wheel (wheel packages) instead.",<N>            SetuptoolsDeprecationWarning,<N>        )<N><N>        # ensure distro name is up-to-date<N>        self.run_command('egg_info')<N><N>
        orig.bdist_rpm.run(self)<N><N>    def _make_spec_file(self):<N>        spec = orig.bdist_rpm._make_spec_file(self)<N>        spec = [<N>            line.replace(<N>                "setup.py install ",<N>                "setup.py install --single-version-externally-managed "<N>            ).replace(<N>                "%setup",<N>                "%setup -n %{name}-%{unmangled_version}"<N>            )<N>            for line in spec<N>        ]<N>        return spec<N><N><N>
import distutils.command.build_clib as orig<N>from distutils.errors import DistutilsSetupError<N>from distutils import log<N>from setuptools.dep_util import newer_pairwise_group<N><N><N>class build_clib(orig.build_clib):<N>    """<N>    Override the default build_clib behaviour to do the following:<N><N>
import os<N>import sys<N>import itertools<N>from importlib.machinery import EXTENSION_SUFFIXES<N>from distutils.command.build_ext import build_ext as _du_build_ext<N>from distutils.file_util import copy_file<N>from distutils.ccompiler import new_compiler<N>from distutils.sysconfig import customize_compiler, get_config_var<N>from distutils.errors import DistutilsError<N>from distutils import log<N><N>
from setuptools.extension import Library<N><N>try:<N>    # Attempt to use Cython for building extensions, if available<N>    from Cython.Distutils.build_ext import build_ext as _build_ext<N>    # Additionally, assert that the compiler module will load<N>    # also. Ref #1229.<N>    __import__('Cython.Compiler.Main')<N>except ImportError:<N>    _build_ext = _du_build_ext<N><N>
from glob import glob<N>from distutils.util import convert_path<N>import distutils.command.build_py as orig<N>import os<N>import fnmatch<N>import textwrap<N>import io<N>import distutils.errors<N>import itertools<N>import stat<N>from setuptools.extern.more_itertools import unique_everseen<N><N>
<N>def make_writable(target):<N>    os.chmod(target, os.stat(target).st_mode | stat.S_IWRITE)<N><N><N>class build_py(orig.build_py):<N>    """Enhanced 'build_py' command that includes data files with packages<N><N>    The data files are specified via a 'package_data' argument to 'setup()'.<N>    See 'setuptools.dist.Distribution' for more details.<N><N>
    Also, this version of the 'build_py' command allows you to specify both<N>    'py_modules' and 'packages' in the same setup operation.<N>    """<N><N>    def finalize_options(self):<N>        orig.build_py.finalize_options(self)<N>        self.package_data = self.distribution.package_data<N>        self.exclude_package_data = self.distribution.exclude_package_data or {}<N>        if 'data_files' in self.__dict__:<N>            del self.__dict__['data_files']<N>        self.__updated_files = []<N><N>
    def run(self):<N>        """Build modules, packages, and copy data files to build directory"""<N>        if not self.py_modules and not self.packages:<N>            return<N><N>        if self.py_modules:<N>            self.build_modules()<N><N>        if self.packages:<N>            self.build_packages()<N>            self.build_package_data()<N><N>
        # Only compile actual .py files, using our base class' idea of what our<N>        # output files are.<N>        self.byte_compile(orig.build_py.get_outputs(self, include_bytecode=0))<N><N>    def __getattr__(self, attr):<N>        "lazily compute data files"<N>        if attr == 'data_files':<N>            self.data_files = self._get_data_files()<N>            return self.data_files<N>        return orig.build_py.__getattr__(self, attr)<N><N>
    def build_module(self, module, module_file, package):<N>        outfile, copied = orig.build_py.build_module(self, module, module_file, package)<N>        if copied:<N>            self.__updated_files.append(outfile)<N>        return outfile, copied<N><N>
from distutils.util import convert_path<N>from distutils import log<N>from distutils.errors import DistutilsError, DistutilsOptionError<N>import os<N>import glob<N>import io<N><N>import pkg_resources<N>from setuptools.command.easy_install import easy_install<N>from setuptools import namespaces<N>import setuptools<N><N>
<N>class develop(namespaces.DevelopInstaller, easy_install):<N>    """Set up package for development"""<N><N>    description = "install package in 'development mode'"<N><N>    user_options = easy_install.user_options + [<N>        ("uninstall", "u", "Uninstall this source package"),<N>        ("egg-path=", None, "Set the path to be used in the .egg-link file"),<N>    ]<N><N>
    boolean_options = easy_install.boolean_options + ['uninstall']<N><N>    command_consumes_arguments = False  # override base<N><N>    def run(self):<N>        if self.uninstall:<N>            self.multi_version = True<N>            self.uninstall_link()<N>            self.uninstall_namespaces()<N>        else:<N>            self.install_for_development()<N>        self.warn_deprecated_options()<N><N>
    def initialize_options(self):<N>        self.uninstall = None<N>        self.egg_path = None<N>        easy_install.initialize_options(self)<N>        self.setup_path = None<N>        self.always_copy_from = '.'  # always copy eggs installed in curdir<N><N>
    def finalize_options(self):<N>        ei = self.get_finalized_command("egg_info")<N>        if ei.broken_egg_info:<N>            template = "Please rename %r to %r before using 'develop'"<N>            args = ei.egg_info, ei.broken_egg_info<N>            raise DistutilsError(template % args)<N>        self.args = [ei.egg_name]<N><N>
        easy_install.finalize_options(self)<N>        self.expand_basedirs()<N>        self.expand_dirs()<N>        # pick up setup-dir .egg files only: no .egg-info<N>        self.package_index.scan(glob.glob('*.egg'))<N><N>        egg_link_fn = ei.egg_name + '.egg-link'<N>        self.egg_link = os.path.join(self.install_dir, egg_link_fn)<N>        self.egg_base = ei.egg_base<N>        if self.egg_path is None:<N>            self.egg_path = os.path.abspath(ei.egg_base)<N><N>
        target = pkg_resources.normalize_path(self.egg_base)<N>        egg_path = pkg_resources.normalize_path(<N>            os.path.join(self.install_dir, self.egg_path)<N>        )<N>        if egg_path != target:<N>            raise DistutilsOptionError(<N>                "--egg-path must be a relative path from the install"<N>                " directory to " + target<N>            )<N><N>
        # Make a distribution for the package's source<N>        self.dist = pkg_resources.Distribution(<N>            target,<N>            pkg_resources.PathMetadata(target, os.path.abspath(ei.egg_info)),<N>            project_name=ei.egg_name,<N>        )<N><N>
"""<N>Create a dist_info directory<N>As defined in the wheel specification<N>"""<N><N>import os<N>import re<N>import warnings<N>from inspect import cleandoc<N><N>from distutils.core import Command<N>from distutils import log<N>from setuptools.extern import packaging<N><N>
<N>class dist_info(Command):<N><N>    description = 'create a .dist-info directory'<N><N>    user_options = [<N>        ('egg-base=', 'e', "directory containing .egg-info directories"<N>                           " (default: top of the source tree)"),<N>    ]<N><N>
"""<N>Easy Install<N>------------<N><N>A tool for doing automatic download/extract/build of distutils-based Python<N>packages.  For detailed documentation, see the accompanying EasyInstall.txt<N>file, or visit the `EasyInstall home page`__.<N><N>__ https://setuptools.pypa.io/en/latest/deprecated/easy_install.html<N><N>
"""setuptools.command.egg_info<N><N>Create a distribution's .egg-info directory and contents"""<N><N>from distutils.filelist import FileList as _FileList<N>from distutils.errors import DistutilsInternalError<N>from distutils.util import convert_path<N>from distutils import log<N>import distutils.errors<N>import distutils.filelist<N>import functools<N>import os<N>import re<N>import sys<N>import io<N>import warnings<N>import time<N>import collections<N><N>
from .._importlib import metadata<N>from .. import _entry_points<N><N>from setuptools import Command<N>from setuptools.command.sdist import sdist<N>from setuptools.command.sdist import walk_revctrl<N>from setuptools.command.setopt import edit_config<N>from setuptools.command import bdist_egg<N>from pkg_resources import (<N>    Requirement, safe_name, parse_version,<N>    safe_version, to_filename)<N>import setuptools.unicode_utils as unicode_utils<N>from setuptools.glob import glob<N><N>
from distutils.errors import DistutilsArgError<N>import inspect<N>import glob<N>import warnings<N>import platform<N>import distutils.command.install as orig<N><N>import setuptools<N><N># Prior to numpy 1.9, NumPy relies on the '_install' name, so provide it for<N># now. See https://github.com/pypa/setuptools/issues/199/<N>_install = orig.install<N><N>
from distutils import log, dir_util<N>import os<N><N>from setuptools import Command<N>from setuptools import namespaces<N>from setuptools.archive_util import unpack_archive<N>from .._path import ensure_directory<N>import pkg_resources<N><N><N>class install_egg_info(namespaces.Installer, Command):<N>    """Install an .egg-info directory for the package"""<N><N>
import os<N>import sys<N>from itertools import product, starmap<N>import distutils.command.install_lib as orig<N><N><N>class install_lib(orig.install_lib):<N>    """Don't add compiled flags to filenames of non-Python files"""<N><N>    def run(self):<N>        self.build()<N>        outfiles = self.install()<N>        if outfiles is not None:<N>            # always compile, in case we have any extension stubs to deal with<N>            self.byte_compile(outfiles)<N><N>
    def get_exclusions(self):<N>        """<N>        Return a collections.Sized collections.Container of paths to be<N>        excluded for single_version_externally_managed installations.<N>        """<N>        all_packages = (<N>            pkg<N>            for ns_pkg in self._get_SVEM_NSPs()<N>            for pkg in self._all_packages(ns_pkg)<N>        )<N><N>
        excl_specs = product(all_packages, self._gen_exclusion_paths())<N>        return set(starmap(self._exclude_pkg_path, excl_specs))<N><N>    def _exclude_pkg_path(self, pkg, exclusion_path):<N>        """<N>        Given a package name and exclusion path within that package,<N>        compute the full exclusion path.<N>        """<N>        parts = pkg.split('.') + [exclusion_path]<N>        return os.path.join(self.install_dir, *parts)<N><N>
    @staticmethod<N>    def _all_packages(pkg_name):<N>        """<N>        >>> list(install_lib._all_packages('foo.bar.baz'))<N>        ['foo.bar.baz', 'foo.bar', 'foo']<N>        """<N>        while pkg_name:<N>            yield pkg_name<N>            pkg_name, sep, child = pkg_name.rpartition('.')<N><N>
    def _get_SVEM_NSPs(self):<N>        """<N>        Get namespace packages (list) but only for<N>        single_version_externally_managed installations and empty otherwise.<N>        """<N>        # TODO: is it necessary to short-circuit here? i.e. what's the cost<N>        # if get_finalized_command is called even when namespace_packages is<N>        # False?<N>        if not self.distribution.namespace_packages:<N>            return []<N><N>
        install_cmd = self.get_finalized_command('install')<N>        svem = install_cmd.single_version_externally_managed<N><N>        return self.distribution.namespace_packages if svem else []<N><N>    @staticmethod<N>    def _gen_exclusion_paths():<N>        """<N>        Generate file paths to be excluded for namespace packages (bytecode<N>        cache files).<N>        """<N>        # always exclude the package module itself<N>        yield '__init__.py'<N><N>
        yield '__init__.pyc'<N>        yield '__init__.pyo'<N><N>        if not hasattr(sys, 'implementation'):<N>            return<N><N>        base = os.path.join(<N>            '__pycache__', '__init__.' + sys.implementation.cache_tag)<N>        yield base + '.pyc'<N>        yield base + '.pyo'<N>        yield base + '.opt-1.pyc'<N>        yield base + '.opt-2.pyc'<N><N>
    def copy_tree(<N>            self, infile, outfile,<N>            preserve_mode=1, preserve_times=1, preserve_symlinks=0, level=1<N>    ):<N>        assert preserve_mode and preserve_times and not preserve_symlinks<N>        exclude = self.get_exclusions()<N><N>
        if not exclude:<N>            return orig.install_lib.copy_tree(self, infile, outfile)<N><N>        # Exclude namespace package __init__.py* files from the output<N><N>        from setuptools.archive_util import unpack_directory<N>        from distutils import log<N><N>
        outfiles = []<N><N>        def pf(src, dst):<N>            if dst in exclude:<N>                log.warn("Skipping installation of %s (namespace package)",<N>                         dst)<N>                return False<N><N>            log.info("copying %s -> %s", src, os.path.dirname(dst))<N>            outfiles.append(dst)<N>            return dst<N><N>
        unpack_directory(infile, outfile, pf)<N>        return outfiles<N><N>    def get_outputs(self):<N>        outputs = orig.install_lib.get_outputs(self)<N>        exclude = self.get_exclusions()<N>        if exclude:<N>            return [f for f in outputs if f not in exclude]<N>        return outputs<N><N><N>
from distutils import log<N>import distutils.command.install_scripts as orig<N>from distutils.errors import DistutilsModuleError<N>import os<N>import sys<N><N>from pkg_resources import Distribution, PathMetadata<N>from .._path import ensure_directory<N><N>
<N>class install_scripts(orig.install_scripts):<N>    """Do normal script install, plus any egg_info wrapper scripts"""<N><N>    def initialize_options(self):<N>        orig.install_scripts.initialize_options(self)<N>        self.no_ep = False<N><N>    def run(self):<N>        import setuptools.command.easy_install as ei<N><N>
        self.run_command("egg_info")<N>        if self.distribution.scripts:<N>            orig.install_scripts.run(self)  # run first to set up self.outfiles<N>        else:<N>            self.outfiles = []<N>        if self.no_ep:<N>            # don't install entry point scripts into .egg file!<N>            return<N><N>
import os<N>from glob import glob<N>from distutils.util import convert_path<N>from distutils.command import sdist<N><N><N>class sdist_add_defaults:<N>    """<N>    Mix-in providing forward-compatibility for functionality as found in<N>    distutils on Python 3.7.<N><N>
from distutils import log<N>import distutils.command.register as orig<N><N>from setuptools.errors import RemovedCommandError<N><N><N>class register(orig.register):<N>    """Formerly used to register packages on PyPI."""<N><N>    def run(self):<N>        msg = (<N>            "The register command has been removed, use twine to upload "<N>            + "instead (https://pypi.org/p/twine)"<N>        )<N><N>        self.announce("ERROR: " + msg, log.ERROR)<N><N>        raise RemovedCommandError(msg)<N>
from distutils.util import convert_path<N>from distutils import log<N>from distutils.errors import DistutilsOptionError<N>import os<N>import shutil<N><N>from setuptools import Command<N><N><N>class rotate(Command):<N>    """Delete older distributions"""<N><N>
    description = "delete older distributions, keeping N newest files"<N>    user_options = [<N>        ('match=', 'm', "patterns to match (required)"),<N>        ('dist-dir=', 'd', "directory where the distributions are"),<N>        ('keep=', 'k', "number of matching distributions to keep"),<N>    ]<N><N>
from setuptools.command.setopt import edit_config, option_base<N><N><N>class saveopts(option_base):<N>    """Save command-line options to a file"""<N><N>    description = "save supplied options to setup.cfg or other config file"<N><N>    def run(self):<N>        dist = self.distribution<N>        settings = {}<N><N>
        for cmd in dist.command_options:<N><N>            if cmd == 'saveopts':<N>                continue  # don't save our own options!<N><N>            for opt, (src, val) in dist.get_option_dict(cmd).items():<N>                if src == "command line":<N>                    settings.setdefault(cmd, {})[opt] = val<N><N>
from distutils import log<N>import distutils.command.sdist as orig<N>import os<N>import sys<N>import io<N>import contextlib<N><N>from .py36compat import sdist_add_defaults<N><N>from .._importlib import metadata<N><N>_default_revctrl = list<N><N><N>def walk_revctrl(dirname=''):<N>    """Find all files under revision control"""<N>    for ep in metadata.entry_points(group='setuptools.file_finders'):<N>        for item in ep.load()(dirname):<N>            yield item<N><N>
from distutils.util import convert_path<N>from distutils import log<N>from distutils.errors import DistutilsOptionError<N>import distutils<N>import os<N>import configparser<N><N>from setuptools import Command<N><N>__all__ = ['config_file', 'edit_config', 'option_base', 'setopt']<N><N>
from distutils import log<N>from distutils.command import upload as orig<N><N>from setuptools.errors import RemovedCommandError<N><N><N>class upload(orig.upload):<N>    """Formerly used to upload packages to PyPI."""<N><N>    def run(self):<N>        msg = (<N>            "The upload command has been removed, use twine to upload "<N>            + "instead (https://pypi.org/p/twine)"<N>        )<N><N>        self.announce("ERROR: " + msg, log.ERROR)<N>        raise RemovedCommandError(msg)<N>
# -*- coding: utf-8 -*-<N>"""upload_docs<N><N>Implements a Distutils 'upload_docs' subcommand (upload documentation to<N>sites other than PyPi such as devpi).<N>"""<N><N>from base64 import standard_b64encode<N>from distutils import log<N>from distutils.errors import DistutilsOptionError<N>import os<N>import socket<N>import zipfile<N>import tempfile<N>import shutil<N>import itertools<N>import functools<N>import http.client<N>import urllib.parse<N>import warnings<N><N>
from .._importlib import metadata<N>from .. import SetuptoolsDeprecationWarning<N><N>from .upload import upload<N><N><N>def _encode(s):<N>    return s.encode('utf-8', 'surrogateescape')<N><N><N>class upload_docs(upload):<N>    # override the default repository as upload_docs isn't<N>    # supported by Warehouse (and won't be).<N>    DEFAULT_REPOSITORY = 'https://pypi.python.org/pypi/'<N><N>
    description = 'Upload documentation to sites other than PyPi such as devpi'<N><N>    user_options = [<N>        ('repository=', 'r',<N>         "url of repository [default: %s]" % upload.DEFAULT_REPOSITORY),<N>        ('show-response', None,<N>         'display full response text from server'),<N>        ('upload-dir=', None, 'directory to upload'),<N>    ]<N>    boolean_options = upload.boolean_options<N><N>
    def has_sphinx(self):<N>        return bool(<N>            self.upload_dir is None<N>            and metadata.entry_points(group='distutils.commands', name='build_sphinx')<N>        )<N><N>    sub_commands = [('build_sphinx', has_sphinx)]<N><N>    def initialize_options(self):<N>        upload.initialize_options(self)<N>        self.upload_dir = None<N>        self.target_dir = None<N><N>
from distutils.command.bdist import bdist<N>import sys<N><N>if 'egg' not in bdist.format_commands:<N>    bdist.format_command['egg'] = ('bdist_egg', "Python .egg file")<N>    bdist.format_commands.append('egg')<N><N>del bdist, sys<N>
"""Utility functions to expand configuration directives or special values<N>(such glob patterns).<N><N>We can split the process of interpreting configuration files into 2 steps:<N><N>1. The parsing the file contents from strings to value objects<N>   that can be understand by Python (for example a string with a comma<N>   separated list of keywords into an actual Python list of strings).<N><N>
2. The expansion (or post-processing) of these values according to the<N>   semantics ``setuptools`` assign to them (for example a configuration field<N>   with the ``file:`` directive should be expanded from a list of file paths to<N>   a single string with the contents of those files concatenated)<N><N>
"""Load setuptools configuration from ``pyproject.toml`` files"""<N>import logging<N>import os<N>import warnings<N>from contextlib import contextmanager<N>from functools import partial<N>from typing import TYPE_CHECKING, Callable, Dict, Optional, Mapping, Union<N><N>
from setuptools.errors import FileError, OptionError<N><N>from . import expand as _expand<N>from ._apply_pyprojecttoml import apply as _apply<N>from ._apply_pyprojecttoml import _PREVIOUSLY_DEFINED, _WouldIgnoreField<N><N>if TYPE_CHECKING:<N>    from setuptools.dist import Distribution  # noqa<N><N>
_Path = Union[str, os.PathLike]<N>_logger = logging.getLogger(__name__)<N><N><N>def load_file(filepath: _Path) -> dict:<N>    from setuptools.extern import tomli  # type: ignore<N><N>    with open(filepath, "rb") as file:<N>        return tomli.load(file)<N><N>
<N>def validate(config: dict, filepath: _Path) -> bool:<N>    from . import _validate_pyproject as validator<N><N>    trove_classifier = validator.FORMAT_FUNCTIONS.get("trove-classifier")<N>    if hasattr(trove_classifier, "_disable_download"):<N>        # Improve reproducibility by default. See issue 31 for validate-pyproject.<N>        trove_classifier._disable_download()  # type: ignore<N><N>
    try:<N>        return validator.validate(config)<N>    except validator.ValidationError as ex:<N>        _logger.error(f"configuration error: {ex.summary}")  # type: ignore<N>        _logger.debug(ex.details)  # type: ignore<N>        error = ValueError(f"invalid pyproject.toml config: {ex.name}")  # type: ignore<N>        raise error from None<N><N>
<N>def apply_configuration(<N>    dist: "Distribution",<N>    filepath: _Path,<N>    ignore_option_errors=False,<N>) -> "Distribution":<N>    """Apply the configuration from a ``pyproject.toml`` file into an existing<N>    distribution object.<N>    """<N>    config = read_configuration(filepath, True, ignore_option_errors, dist)<N>    return _apply(dist, config, filepath)<N><N>
<N>def read_configuration(<N>    filepath: _Path,<N>    expand=True,<N>    ignore_option_errors=False,<N>    dist: Optional["Distribution"] = None,<N>):<N>    """Read given configuration file and returns options from it as a dict.<N><N>    :param str|unicode filepath: Path to configuration file in the ``pyproject.toml``<N>        format.<N><N>
    :param bool expand: Whether to expand directives and other computed values<N>        (i.e. post-process the given configuration)<N><N>    :param bool ignore_option_errors: Whether to silently ignore<N>        options, values of which could not be resolved (e.g. due to exceptions<N>        in directives such as file:, attr:, etc.).<N>        If False exceptions are propagated as expected.<N><N>
    :param Distribution|None: Distribution object to which the configuration refers.<N>        If not given a dummy object will be created and discarded after the<N>        configuration is read. This is used for auto-discovery of packages in the case<N>        a dynamic configuration (e.g. ``attr`` or ``cmdclass``) is expanded.<N>        When ``expand=False`` this object is simply ignored.<N><N>
"""Load setuptools configuration from ``setup.cfg`` files"""<N>import os<N><N>import warnings<N>import functools<N>from collections import defaultdict<N>from functools import partial<N>from functools import wraps<N>from typing import (TYPE_CHECKING, Callable, Any, Dict, Generic, Iterable, List,<N>                    Optional, Tuple, TypeVar, Union)<N><N>
from distutils.errors import DistutilsOptionError, DistutilsFileError<N>from setuptools.extern.packaging.version import Version, InvalidVersion<N>from setuptools.extern.packaging.specifiers import SpecifierSet<N><N>from . import expand<N><N>if TYPE_CHECKING:<N>    from setuptools.dist import Distribution  # noqa<N>    from distutils.dist import DistributionMetadata  # noqa<N><N>
"""For backward compatibility, expose main functions from<N>``setuptools.config.setupcfg``<N>"""<N>import warnings<N>from functools import wraps<N>from textwrap import dedent<N>from typing import Callable, TypeVar, cast<N><N>from .._deprecation_warning import SetuptoolsDeprecationWarning<N>from . import setupcfg<N><N>
Fn = TypeVar("Fn", bound=Callable)<N><N>__all__ = ('parse_configuration', 'read_configuration')<N><N><N>def _deprecation_notice(fn: Fn) -> Fn:<N>    @wraps(fn)<N>    def _wrapper(*args, **kwargs):<N>        msg = f"""\<N>        As setuptools moves its configuration towards `pyproject.toml`,<N>        `{__name__}.{fn.__name__}` became deprecated.<N><N>
        For the time being, you can use the `{setupcfg.__name__}` module<N>        to access a backward compatible API, but this module is provisional<N>        and might be removed in the future.<N>        """<N>        warnings.warn(dedent(msg), SetuptoolsDeprecationWarning)<N>        return fn(*args, **kwargs)<N><N>
import io<N>import json<N>import logging<N>import os<N>import re<N>from contextlib import contextmanager<N>from textwrap import indent, wrap<N>from typing import Any, Dict, Iterator, List, Optional, Sequence, Union, cast<N><N>from .fastjsonschema_exceptions import JsonSchemaValueException<N><N>
_logger = logging.getLogger(__name__)<N><N>_MESSAGE_REPLACEMENTS = {<N>    "must be named by propertyName definition": "keys must be named by",<N>    "one of contains definition": "at least one item that matches",<N>    " same as const definition:": "",<N>    "only specified items": "only items matching the definition",<N>}<N><N>
_SKIP_DETAILS = (<N>    "must not be empty",<N>    "is always invalid",<N>    "must not be there",<N>)<N><N>_NEED_DETAILS = {"anyOf", "oneOf", "anyOf", "contains", "propertyNames", "not", "items"}<N><N>_CAMEL_CASE_SPLITTER = re.compile(r"\W+|([A-Z][^A-Z\W]*)")<N>_IDENTIFIER = re.compile(r"^[\w_]+$", re.I)<N><N>
_TOML_JARGON = {<N>    "object": "table",<N>    "property": "key",<N>    "properties": "keys",<N>    "property names": "keys",<N>}<N><N><N>class ValidationError(JsonSchemaValueException):<N>    """Report violations of a given JSON schema.<N><N>    This class extends :exc:`~fastjsonschema.JsonSchemaValueException`<N>    by adding the following properties:<N><N>
    - ``summary``: an improved version of the ``JsonSchemaValueException`` error message<N>      with only the necessary information)<N><N>    - ``details``: more contextual information about the error like the failing schema<N>      itself and the value that violates the schema.<N><N>
    Depending on the level of the verbosity of the ``logging`` configuration<N>    the exception message will be only ``summary`` (default) or a combination of<N>    ``summary`` and ``details`` (when the logging level is set to :obj:`logging.DEBUG`).<N>    """<N><N>
"""The purpose of this module is implement PEP 621 validations that are<N>difficult to express as a JSON Schema (or that are not supported by the current<N>JSON Schema library).<N>"""<N><N>from typing import Mapping, TypeVar<N><N>from .fastjsonschema_exceptions import JsonSchemaValueException<N><N>
T = TypeVar("T", bound=Mapping)<N><N><N>class RedefiningStaticFieldAsDynamic(JsonSchemaValueException):<N>    """According to PEP 621:<N><N>    Build back-ends MUST raise an error if the metadata specifies a field<N>    statically as well as being listed in dynamic.<N>    """<N><N>
import re<N><N><N>SPLIT_RE = re.compile(r'[\.\[\]]+')<N><N><N>class JsonSchemaException(ValueError):<N>    """<N>    Base exception of ``fastjsonschema`` library.<N>    """<N><N><N>class JsonSchemaValueException(JsonSchemaException):<N>    """<N>    Exception raised by validation function. Available properties:<N><N>
# noqa<N># type: ignore<N># flake8: noqa<N># pylint: skip-file<N># mypy: ignore-errors<N># yapf: disable<N># pylama:skip=1<N><N><N># *** PLEASE DO NOT MODIFY DIRECTLY: Automatically generated code *** <N><N><N>VERSION = "2.15.3"<N>import re<N>from .fastjsonschema_exceptions import JsonSchemaValueException<N><N>
<N>REGEX_PATTERNS = {<N>    '^.*$': re.compile('^.*$'),<N>    '.+': re.compile('.+'),<N>    '^.+$': re.compile('^.+$'),<N>    'idn-email_re_pattern': re.compile('^[^@]+@[^@]+\\.[^@]+\\Z')<N>}<N><N>NoneType = type(None)<N><N>def validate(data, custom_formats={}, name_prefix=None):<N>    validate_https___packaging_python_org_en_latest_specifications_declaring_build_dependencies(data, custom_formats, (name_prefix or "data") + "")<N>    return data<N><N>
import logging<N>import os<N>import re<N>import string<N>import typing<N>from itertools import chain as _chain<N><N>_logger = logging.getLogger(__name__)<N><N># -------------------------------------------------------------------------------------<N># PEP 440<N><N>
from functools import reduce<N>from typing import Any, Callable, Dict<N><N>from . import formats<N>from .error_reporting import detailed_errors, ValidationError<N>from .extra_validations import EXTRA_VALIDATIONS<N>from .fastjsonschema_exceptions import JsonSchemaException, JsonSchemaValueException<N>from .fastjsonschema_validations import validate as _validate<N><N>
__all__ = [<N>    "validate",<N>    "FORMAT_FUNCTIONS",<N>    "EXTRA_VALIDATIONS",<N>    "ValidationError",<N>    "JsonSchemaException",<N>    "JsonSchemaValueException",<N>]<N><N><N>FORMAT_FUNCTIONS: Dict[str, Callable[[str], bool]] = {<N>    fn.__name__.replace("_", "-"): fn<N>    for fn in formats.__dict__.values()<N>    if callable(fn) and not fn.__name__.startswith("_")<N>}<N><N>
<N>def validate(data: Any) -> bool:<N>    """Validate the given ``data`` object using JSON Schema<N>    This function raises ``ValidationError`` if ``data`` is invalid.<N>    """<N>    with detailed_errors():<N>        _validate(data, custom_formats=FORMAT_FUNCTIONS)<N>    reduce(lambda acc, fn: fn(acc), EXTRA_VALIDATIONS, data)<N>    return True<N><N><N>
import importlib.util<N>import sys<N><N><N>class VendorImporter:<N>    """<N>    A PEP 302 meta path importer for finding optionally-vendored<N>    or otherwise naturally-installed packages from root_name.<N>    """<N><N>    def __init__(self, root_name, vendored_names=(), vendor_pkg=None):<N>        self.root_name = root_name<N>        self.vendored_names = set(vendored_names)<N>        self.vendor_pkg = vendor_pkg or root_name.replace('extern', '_vendor')<N><N>
    @property<N>    def search_path(self):<N>        """<N>        Search first the vendor package then as a natural package.<N>        """<N>        yield self.vendor_pkg + '.'<N>        yield ''<N><N>    def _module_matches_namespace(self, fullname):<N>        """Figure out if the target module is vendored."""<N>        root, base, target = fullname.partition(self.root_name + '.')<N>        return not root and any(map(target.startswith, self.vendored_names))<N><N>
"""distutils.archive_util<N><N>Utility functions for creating archive files (tarballs, zip files,<N>that sort of thing)."""<N><N>import os<N>from warnings import warn<N>import sys<N><N>try:<N>    import zipfile<N>except ImportError:<N>    zipfile = None<N><N>
<N>from distutils.errors import DistutilsExecError<N>from distutils.spawn import spawn<N>from distutils.dir_util import mkpath<N>from distutils import log<N><N>try:<N>    from pwd import getpwnam<N>except ImportError:<N>    getpwnam = None<N><N>try:<N>    from grp import getgrnam<N>except ImportError:<N>    getgrnam = None<N><N>
def _get_gid(name):<N>    """Returns a gid, given a group name."""<N>    if getgrnam is None or name is None:<N>        return None<N>    try:<N>        result = getgrnam(name)<N>    except KeyError:<N>        result = None<N>    if result is not None:<N>        return result[2]<N>    return None<N><N>
def _get_uid(name):<N>    """Returns an uid, given a user name."""<N>    if getpwnam is None or name is None:<N>        return None<N>    try:<N>        result = getpwnam(name)<N>    except KeyError:<N>        result = None<N>    if result is not None:<N>        return result[2]<N>    return None<N><N>
def make_tarball(base_name, base_dir, compress="gzip", verbose=0, dry_run=0,<N>                 owner=None, group=None):<N>    """Create a (possibly compressed) tar file from all the files under<N>    'base_dir'.<N><N>    'compress' must be "gzip" (the default), "bzip2", "xz", "compress", or<N>    None.  ("compress" will be deprecated in Python 3.2)<N><N>
    'owner' and 'group' can be used to define an owner and a group for the<N>    archive that is being built. If not provided, the current owner and group<N>    will be used.<N><N>    The output tar file will be named 'base_dir' +  ".tar", possibly plus<N>    the appropriate compression extension (".gz", ".bz2", ".xz" or ".Z").<N><N>
    Returns the output filename.<N>    """<N>    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', 'xz': 'xz', None: '',<N>                       'compress': ''}<N>    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2', 'xz': '.xz',<N>                    'compress': '.Z'}<N><N>
    # flags for compression program, each element of list will be an argument<N>    if compress is not None and compress not in compress_ext.keys():<N>        raise ValueError(<N>              "bad value for 'compress': must be None, 'gzip', 'bzip2', "<N>              "'xz' or 'compress'")<N><N>
    archive_name = base_name + '.tar'<N>    if compress != 'compress':<N>        archive_name += compress_ext.get(compress, '')<N><N>    mkpath(os.path.dirname(archive_name), dry_run=dry_run)<N><N>    # creating the tarball<N>    import tarfile  # late import so Python build itself doesn't break<N><N>
    log.info('Creating tar archive')<N><N>    uid = _get_uid(owner)<N>    gid = _get_gid(group)<N><N>    def _set_uid_gid(tarinfo):<N>        if gid is not None:<N>            tarinfo.gid = gid<N>            tarinfo.gname = group<N>        if uid is not None:<N>            tarinfo.uid = uid<N>            tarinfo.uname = owner<N>        return tarinfo<N><N>
"""distutils.bcppcompiler<N><N>Contains BorlandCCompiler, an implementation of the abstract CCompiler class<N>for the Borland C++ compiler.<N>"""<N><N># This implementation by Lyle Johnson, based on the original msvccompiler.py<N># module and using the directions originally published by Gordon Williams.<N><N>
# XXX looks like there's a LOT of overlap between these two classes:<N># someone should sit down and factor out the common code as<N># WindowsCCompiler!  --GPW<N><N><N>import os<N>from distutils.errors import \<N>     DistutilsExecError, \<N>     CompileError, LibError, LinkError, UnknownFileError<N>from distutils.ccompiler import \<N>     CCompiler, gen_preprocess_options<N>from distutils.file_util import write_file<N>from distutils.dep_util import newer<N>from distutils import log<N><N>
"""distutils.ccompiler<N><N>Contains CCompiler, an abstract base class that defines the interface<N>for the Distutils compiler abstraction model."""<N><N>import sys, os, re<N>from distutils.errors import *<N>from distutils.spawn import spawn<N>from distutils.file_util import move_file<N>from distutils.dir_util import mkpath<N>from distutils.dep_util import newer_group<N>from distutils.util import split_quoted, execute<N>from distutils import log<N><N>
"""distutils.cmd<N><N>Provides the Command class, the base class for the command classes<N>in the distutils.command package.<N>"""<N><N>import sys, os, re<N>from distutils.errors import DistutilsOptionError<N>from distutils import util, dir_util, file_util, archive_util, dep_util<N>from distutils import log<N><N>
"""distutils.pypirc<N><N>Provides the PyPIRCCommand class, the base class for the command classes<N>that uses .pypirc in the distutils.command package.<N>"""<N>import os<N>from configparser import RawConfigParser<N><N>from distutils.cmd import Command<N><N>
DEFAULT_PYPIRC = """\<N>[distutils]<N>index-servers =<N>    pypi<N><N>[pypi]<N>username:%s<N>password:%s<N>"""<N><N>class PyPIRCCommand(Command):<N>    """Base command that knows how to handle the .pypirc file<N>    """<N>    DEFAULT_REPOSITORY = 'https://upload.pypi.org/legacy/'<N>    DEFAULT_REALM = 'pypi'<N>    repository = None<N>    realm = None<N><N>
    user_options = [<N>        ('repository=', 'r',<N>         "url of repository [default: %s]" % \<N>            DEFAULT_REPOSITORY),<N>        ('show-response', None,<N>         'display full response text from server')]<N><N>    boolean_options = ['show-response']<N><N>
    def _get_rc_file(self):<N>        """Returns rc file path."""<N>        return os.path.join(os.path.expanduser('~'), '.pypirc')<N><N>    def _store_pypirc(self, username, password):<N>        """Creates a default .pypirc file."""<N>        rc = self._get_rc_file()<N>        with os.fdopen(os.open(rc, os.O_CREAT | os.O_WRONLY, 0o600), 'w') as f:<N>            f.write(DEFAULT_PYPIRC % (username, password))<N><N>
    def _read_pypirc(self):<N>        """Reads the .pypirc file."""<N>        rc = self._get_rc_file()<N>        if os.path.exists(rc):<N>            self.announce('Using PyPI login from %s' % rc)<N>            repository = self.repository or self.DEFAULT_REPOSITORY<N><N>
"""distutils.core<N><N>The only module that needs to be imported to use the Distutils; provides<N>the 'setup' function (which is to be called from the setup script).  Also<N>indirectly provides the Distribution and Command classes, although they are<N>really defined in distutils.dist and distutils.cmd.<N>"""<N><N>
import os<N>import sys<N>import tokenize<N><N>from distutils.debug import DEBUG<N>from distutils.errors import *<N><N># Mainly import these so setup scripts can "from distutils.core import" them.<N>from distutils.dist import Distribution<N>from distutils.cmd import Command<N>from distutils.config import PyPIRCCommand<N>from distutils.extension import Extension<N><N>
# This is a barebones help message generated displayed when the user<N># runs the setup script with no arguments at all.  More useful help<N># is generated with various --help options: global help, list commands,<N># and per-command help.<N>USAGE = """\<N>usage: %(script)s [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]<N>   or: %(script)s --help [cmd1 cmd2 ...]<N>   or: %(script)s --help-commands<N>   or: %(script)s cmd --help<N>"""<N><N>
"""distutils.cygwinccompiler<N><N>Provides the CygwinCCompiler class, a subclass of UnixCCompiler that<N>handles the Cygwin port of the GNU C compiler to Windows.  It also contains<N>the Mingw32CCompiler class which handles the mingw32 port of GCC (same as<N>cygwin in no-cygwin mode).<N>"""<N><N>
import os<N><N># If DISTUTILS_DEBUG is anything other than the empty string, we run in<N># debug mode.<N>DEBUG = os.environ.get('DISTUTILS_DEBUG')<N>
"""distutils.dep_util<N><N>Utility functions for simple, timestamp-based dependency of files<N>and groups of files; also, function based entirely on such<N>timestamp dependency analysis."""<N><N>import os<N>from distutils.errors import DistutilsFileError<N><N>
"""distutils.dir_util<N><N>Utility functions for manipulating directories and directory trees."""<N><N>import os<N>import errno<N>from distutils.errors import DistutilsFileError, DistutilsInternalError<N>from distutils import log<N><N># cache for by mkpath() -- in addition to cheapening redundant calls,<N># eliminates redundant "creating /foo/bar/baz" messages in dry-run mode<N>_path_created = {}<N><N>
# I don't use os.makedirs because a) it's new to Python 1.5.2, and<N># b) it blows up if the directory already exists (I want to silently<N># succeed in that case).<N>def mkpath(name, mode=0o777, verbose=1, dry_run=0):<N>    """Create a directory and any missing ancestor directories.<N><N>
    If the directory already exists (or if 'name' is the empty string, which<N>    means the current directory, which of course exists), then do nothing.<N>    Raise DistutilsFileError if unable to create some directory along the way<N>    (eg. some sub-path exists, but is a file rather than a directory).<N>    If 'verbose' is true, print a one-line summary of each mkdir to stdout.<N>    Return the list of directories actually created.<N>    """<N><N>
    global _path_created<N><N>    # Detect a common bug -- name is None<N>    if not isinstance(name, str):<N>        raise DistutilsInternalError(<N>              "mkpath: 'name' must be a string (got %r)" % (name,))<N><N>    # XXX what's the better way to handle verbosity? print as we create<N>    # each directory in the path (the current behaviour), or only announce<N>    # the creation of the whole path? (quite easy to do the latter since<N>    # we're not using a recursive algorithm)<N><N>
    name = os.path.normpath(name)<N>    created_dirs = []<N>    if os.path.isdir(name) or name == '':<N>        return created_dirs<N>    if _path_created.get(os.path.abspath(name)):<N>        return created_dirs<N><N>    (head, tail) = os.path.split(name)<N>    tails = [tail]                      # stack of lone dirs to create<N><N>
    while head and tail and not os.path.isdir(head):<N>        (head, tail) = os.path.split(head)<N>        tails.insert(0, tail)          # push next higher dir onto stack<N><N>    # now 'head' contains the deepest directory that already exists<N>    # (that is, the child of 'head' in 'name' is the highest directory<N>    # that does *not* exist)<N>    for d in tails:<N>        #print "head = %s, d = %s: " % (head, d),<N>        head = os.path.join(head, d)<N>        abs_head = os.path.abspath(head)<N><N>
        if _path_created.get(abs_head):<N>            continue<N><N>        if verbose >= 1:<N>            log.info("creating %s", head)<N><N>        if not dry_run:<N>            try:<N>                os.mkdir(head, mode)<N>            except OSError as exc:<N>                if not (exc.errno == errno.EEXIST and os.path.isdir(head)):<N>                    raise DistutilsFileError(<N>                          "could not create '%s': %s" % (head, exc.args[-1]))<N>            created_dirs.append(head)<N><N>
"""distutils.dist<N><N>Provides the Distribution class, which represents the module distribution<N>being built/installed/distributed.<N>"""<N><N>import sys<N>import os<N>import re<N>from email import message_from_file<N><N>try:<N>    import warnings<N>except ImportError:<N>    warnings = None<N><N>
"""distutils.errors<N><N>Provides exceptions used by the Distutils modules.  Note that Distutils<N>modules may raise standard exceptions; in particular, SystemExit is<N>usually raised for errors that are obviously the end-user's fault<N>(eg. bad command-line arguments).<N><N>
This module is safe to use in "from ... import *" mode; it only exports<N>symbols whose names start with "Distutils" and end with "Error"."""<N><N>class DistutilsError (Exception):<N>    """The root of all Distutils evil."""<N>    pass<N><N>class DistutilsModuleError (DistutilsError):<N>    """Unable to load an expected module, or to find an expected class<N>    within some module (in particular, command modules and classes)."""<N>    pass<N><N>
class DistutilsClassError (DistutilsError):<N>    """Some command class (or possibly distribution class, if anyone<N>    feels a need to subclass Distribution) is found not to be holding<N>    up its end of the bargain, ie. implementing some part of the<N>    "command "interface."""<N>    pass<N><N>
class DistutilsGetoptError (DistutilsError):<N>    """The option table provided to 'fancy_getopt()' is bogus."""<N>    pass<N><N>class DistutilsArgError (DistutilsError):<N>    """Raised by fancy_getopt in response to getopt.error -- ie. an<N>    error in the command line usage."""<N>    pass<N><N>
"""distutils.fancy_getopt<N><N>Wrapper around the standard getopt module that provides the following<N>additional features:<N>  * short and long options are tied together<N>  * options have help strings, so fancy_getopt could potentially<N>    create a complete usage summary<N>  * options set attributes of a passed-in object<N>"""<N><N>
import sys, string, re<N>import getopt<N>from distutils.errors import *<N><N># Much like command_re in distutils.core, this is close to but not quite<N># the same as a Python NAME -- except, in the spirit of most GNU<N># utilities, we use '-' in place of '_'.  (The spirit of LISP lives on!)<N># The similarities to NAME are again not a coincidence...<N>longopt_pat = r'[a-zA-Z](?:[a-zA-Z0-9-]*)'<N>longopt_re = re.compile(r'^%s$' % longopt_pat)<N><N>
# For recognizing "negative alias" options, eg. "quiet=!verbose"<N>neg_alias_re = re.compile("^(%s)=!(%s)$" % (longopt_pat, longopt_pat))<N><N># This is used to translate long options to legitimate Python identifiers<N># (for use as attributes of some object).<N>longopt_xlate = str.maketrans('-', '_')<N><N>
"""distutils.filelist<N><N>Provides the FileList class, used for poking about the filesystem<N>and building lists of files.<N>"""<N><N>import os<N>import re<N>import fnmatch<N>import functools<N><N>from distutils.util import convert_path<N>from distutils.errors import DistutilsTemplateError, DistutilsInternalError<N>from distutils import log<N><N>
"""distutils.file_util<N><N>Utility functions for operating on single files.<N>"""<N><N>import os<N>from distutils.errors import DistutilsFileError<N>from distutils import log<N><N># for generating verbose output in 'copy_file()'<N>_copy_action = { None:   'copying',<N>                 'hard': 'hard linking',<N>                 'sym':  'symbolically linking' }<N><N>
"""A simple log mechanism styled after PEP 282."""<N><N># The class here is styled after PEP 282 so that it could later be<N># replaced with a standard Python logging implementation.<N><N>import sys<N><N>DEBUG = 1<N>INFO = 2<N>WARN = 3<N>ERROR = 4<N>FATAL = 5<N><N>
<N>class Log:<N><N>    def __init__(self, threshold=WARN):<N>        self.threshold = threshold<N><N>    def _log(self, level, msg, args):<N>        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):<N>            raise ValueError('%s wrong log level' % str(level))<N><N>
"""distutils.msvc9compiler<N><N>Contains MSVCCompiler, an implementation of the abstract CCompiler class<N>for the Microsoft Visual Studio 2008.<N><N>The module is compatible with VS 2005 and VS 2008. You can find legacy support<N>for older versions of VS in distutils.msvccompiler.<N>"""<N><N>
# Written by Perry Stoll<N># hacked by Robin Becker and Thomas Heller to do a better job of<N>#   finding DevStudio (through the registry)<N># ported to VS2005 and VS 2008 by Christian Heimes<N><N>import os<N>import subprocess<N>import sys<N>import re<N><N>
from distutils.errors import DistutilsExecError, DistutilsPlatformError, \<N>                             CompileError, LibError, LinkError<N>from distutils.ccompiler import CCompiler, gen_lib_options<N>from distutils import log<N>from distutils.util import get_platform<N><N>
import winreg<N><N>RegOpenKeyEx = winreg.OpenKeyEx<N>RegEnumKey = winreg.EnumKey<N>RegEnumValue = winreg.EnumValue<N>RegError = winreg.error<N><N>HKEYS = (winreg.HKEY_USERS,<N>         winreg.HKEY_CURRENT_USER,<N>         winreg.HKEY_LOCAL_MACHINE,<N>         winreg.HKEY_CLASSES_ROOT)<N><N>
"""distutils.msvccompiler<N><N>Contains MSVCCompiler, an implementation of the abstract CCompiler class<N>for the Microsoft Visual Studio.<N>"""<N><N># Written by Perry Stoll<N># hacked by Robin Becker and Thomas Heller to do a better job of<N>#   finding DevStudio (through the registry)<N><N>
import sys, os<N>from distutils.errors import \<N>     DistutilsExecError, DistutilsPlatformError, \<N>     CompileError, LibError, LinkError<N>from distutils.ccompiler import \<N>     CCompiler, gen_lib_options<N>from distutils import log<N><N>_can_read_reg = False<N>try:<N>    import winreg<N><N>
    _can_read_reg = True<N>    hkey_mod = winreg<N><N>    RegOpenKeyEx = winreg.OpenKeyEx<N>    RegEnumKey = winreg.EnumKey<N>    RegEnumValue = winreg.EnumValue<N>    RegError = winreg.error<N><N>except ImportError:<N>    try:<N>        import win32api<N>        import win32con<N>        _can_read_reg = True<N>        hkey_mod = win32con<N><N>
        RegOpenKeyEx = win32api.RegOpenKeyEx<N>        RegEnumKey = win32api.RegEnumKey<N>        RegEnumValue = win32api.RegEnumValue<N>        RegError = win32api.error<N>    except ImportError:<N>        log.info("Warning: Can't read registry to find the "<N>                 "necessary compiler setting\n"<N>                 "Make sure that Python modules winreg, "<N>                 "win32api or win32con are installed.")<N>        pass<N><N>
import sys<N>import subprocess<N><N><N>def __optim_args_from_interpreter_flags():<N>    """Return a list of command-line arguments reproducing the current<N>    optimization settings in sys.flags."""<N>    args = []<N>    value = sys.flags.optimize<N>    if value > 0:<N>        args.append("-" + "O" * value)<N>    return args<N><N><N>_optim_args_from_interpreter_flags = getattr(<N>    subprocess,<N>    "_optim_args_from_interpreter_flags",<N>    __optim_args_from_interpreter_flags,<N>)<N>
def aix_platform(osname, version, release):<N>    try:<N>        import _aix_support<N>        return _aix_support.aix_platform()<N>    except ImportError:<N>        pass<N>    return "%s-%s.%s" % (osname, version, release)<N>
"""distutils.spawn<N><N>Provides the 'spawn()' function, a front-end to various platform-<N>specific functions for launching another program in a sub-process.<N>Also provides the 'find_executable()' to search the path for a given<N>executable name.<N>"""<N><N>
import sys<N>import os<N>import subprocess<N><N>from distutils.errors import DistutilsExecError<N>from distutils.debug import DEBUG<N>from distutils import log<N><N><N>def spawn(cmd, search_path=1, verbose=0, dry_run=0, env=None):<N>    """Run another program, specified as a command list 'cmd', in a new process.<N><N>
    'cmd' is just the argument list for the new process, ie.<N>    cmd[0] is the program to run and cmd[1:] are the rest of its arguments.<N>    There is no way to run a program with a name different from that of its<N>    executable.<N><N>    If 'search_path' is true (the default), the system's executable<N>    search path will be used to find the program; otherwise, cmd[0]<N>    must be the exact path to the executable.  If 'dry_run' is true,<N>    the command will not actually be run.<N><N>
    Raise DistutilsExecError if running the program fails in any way; just<N>    return on success.<N>    """<N>    # cmd is documented as a list, but just in case some code passes a tuple<N>    # in, protect our %-formatting code against horrible death<N>    cmd = list(cmd)<N><N>
    log.info(subprocess.list2cmdline(cmd))<N>    if dry_run:<N>        return<N><N>    if search_path:<N>        executable = find_executable(cmd[0])<N>        if executable is not None:<N>            cmd[0] = executable<N><N>    env = env if env is not None else dict(os.environ)<N><N>
    if sys.platform == 'darwin':<N>        from distutils.util import MACOSX_VERSION_VAR, get_macosx_target_ver<N>        macosx_target_ver = get_macosx_target_ver()<N>        if macosx_target_ver:<N>            env[MACOSX_VERSION_VAR] = macosx_target_ver<N><N>
    try:<N>        proc = subprocess.Popen(cmd, env=env)<N>        proc.wait()<N>        exitcode = proc.returncode<N>    except OSError as exc:<N>        if not DEBUG:<N>            cmd = cmd[0]<N>        raise DistutilsExecError(<N>            "command %r failed: %s" % (cmd, exc.args[-1])) from exc<N><N>
    if exitcode:<N>        if not DEBUG:<N>            cmd = cmd[0]<N>        raise DistutilsExecError(<N>              "command %r failed with exit code %s" % (cmd, exitcode))<N><N><N>def find_executable(executable, path=None):<N>    """Tries to find 'executable' in the directories listed in 'path'.<N><N>
    A string listing directories separated by 'os.pathsep'; defaults to<N>    os.environ['PATH'].  Returns the complete filename or None if not found.<N>    """<N>    _, ext = os.path.splitext(executable)<N>    if (sys.platform == 'win32') and (ext != '.exe'):<N>        executable = executable + '.exe'<N><N>
    if os.path.isfile(executable):<N>        return executable<N><N>    if path is None:<N>        path = os.environ.get('PATH', None)<N>        if path is None:<N>            try:<N>                path = os.confstr("CS_PATH")<N>            except (AttributeError, ValueError):<N>                # os.confstr() or CS_PATH is not available<N>                path = os.defpath<N>        # bpo-35755: Don't use os.defpath if the PATH environment variable is<N>        # set to an empty string<N><N>
    # PATH='' doesn't match, whereas PATH=':' looks in the current directory<N>    if not path:<N>        return None<N><N>    paths = path.split(os.pathsep)<N>    for p in paths:<N>        f = os.path.join(p, executable)<N>        if os.path.isfile(f):<N>            # the file exists, we have a shot at spawn working<N>            return f<N>    return None<N><N><N>
"""Provide access to Python's configuration information.  The specific<N>configuration variables available depend heavily on the platform and<N>configuration.  The values may be retrieved using<N>get_config_var(name), and the list of variables is available via<N>get_config_vars().keys().  Additional convenience functions are also<N>available.<N><N>
Written by:   Fred L. Drake, Jr.<N>Email:        <fdrake@acm.org><N>"""<N><N>import os<N>import re<N>import sys<N>import sysconfig<N><N>from .errors import DistutilsPlatformError<N>from . import py39compat<N><N>IS_PYPY = '__pypy__' in sys.builtin_module_names<N><N>
# These are needed in a couple of spots, so just compute them once.<N>PREFIX = os.path.normpath(sys.prefix)<N>EXEC_PREFIX = os.path.normpath(sys.exec_prefix)<N>BASE_PREFIX = os.path.normpath(sys.base_prefix)<N>BASE_EXEC_PREFIX = os.path.normpath(sys.base_exec_prefix)<N><N>
"""distutils.util<N><N>Miscellaneous utility functions -- anything that doesn't fit into<N>one of the other *util.py modules.<N>"""<N><N>import os<N>import re<N>import importlib.util<N>import string<N>import sys<N>import sysconfig<N>from distutils.errors import DistutilsPlatformError<N>from distutils.dep_util import newer<N>from distutils.spawn import spawn<N>from distutils import log<N>from distutils.errors import DistutilsByteCompileError<N>from .py35compat import _optim_args_from_interpreter_flags<N><N>
<N>def get_host_platform():<N>    """Return a string that identifies the current platform.  This is used mainly to<N>    distinguish platform-specific build directories and platform-specific built<N>    distributions.<N>    """<N><N>    # We initially exposed platforms as defined in Python 3.9<N>    # even with older Python versions when distutils was split out.<N>    # Now that we delegate to stdlib sysconfig we need to restore this<N>    # in case anyone has started to depend on it.<N><N>
#<N># distutils/version.py<N>#<N># Implements multiple version numbering conventions for the<N># Python Module Distribution Utilities.<N>#<N># $Id$<N>#<N><N>"""Provides classes to represent module version numbers (one class for<N>each style of version numbering).  There are currently two such classes<N>implemented: StrictVersion and LooseVersion.<N><N>
"""Module for parsing and testing package version predicate strings.<N>"""<N>import re<N>import distutils.version<N>import operator<N><N><N>re_validPackage = re.compile(r"(?i)^\s*([a-z_]\w*(?:\.[a-z_]\w*)*)(.*)",<N>    re.ASCII)<N># (package) (rest)<N><N>
import collections<N>import itertools<N><N><N># from jaraco.collections 3.5.1<N>class DictStack(list, collections.abc.Mapping):<N>    """<N>    A stack of dictionaries that behaves as a view on those dictionaries,<N>    giving preference to the last.<N><N>
import sys<N>import importlib<N><N><N>def bypass_compiler_fixup(cmd, args):<N>    return cmd<N><N><N>if sys.platform == 'darwin':<N>    compiler_fixup = importlib.import_module('_osx_support').compiler_fixup<N>else:<N>    compiler_fixup = bypass_compiler_fixup<N>
"""distutils._msvccompiler<N><N>Contains MSVCCompiler, an implementation of the abstract CCompiler class<N>for Microsoft Visual Studio 2015.<N><N>The module is compatible with VS 2015 and later. You can find legacy support<N>for older versions in distutils.msvc9compiler and distutils.msvccompiler.<N>"""<N><N>
# Written by Perry Stoll<N># hacked by Robin Becker and Thomas Heller to do a better job of<N>#   finding DevStudio (through the registry)<N># ported to VS 2005 and VS 2008 by Christian Heimes<N># ported to VS 2015 by Steve Dower<N><N>import os<N>import subprocess<N>import contextlib<N>import warnings<N>import unittest.mock<N>with contextlib.suppress(ImportError):<N>    import winreg<N><N>
from distutils.errors import DistutilsExecError, DistutilsPlatformError, \<N>                             CompileError, LibError, LinkError<N>from distutils.ccompiler import CCompiler, gen_lib_options<N>from distutils import log<N>from distutils.util import get_platform<N><N>
from itertools import count<N><N>def _find_vc2015():<N>    try:<N>        key = winreg.OpenKeyEx(<N>            winreg.HKEY_LOCAL_MACHINE,<N>            r"Software\Microsoft\VisualStudio\SxS\VC7",<N>            access=winreg.KEY_READ | winreg.KEY_WOW64_32KEY<N>        )<N>    except OSError:<N>        log.debug("Visual C++ is not registered")<N>        return None, None<N><N>
"""distutils<N><N>The main package for the Python Module Distribution Utilities.  Normally<N>used from a setup script as<N><N>   from distutils.core import setup<N><N>   setup (...)<N>"""<N><N>import sys<N>import importlib<N><N>__version__ = sys.version[:sys.version.index(' ')]<N><N>
<N>try:<N>    # Allow Debian and pkgsrc (only) to customize system<N>    # behavior. Ref pypa/distutils#2 and pypa/distutils#16.<N>    # This hook is deprecated and no other environments<N>    # should use it.<N>    importlib.import_module('_distutils_system_mod')<N>except ImportError:<N>    pass<N><N><N>
"""distutils.command.bdist<N><N>Implements the Distutils 'bdist' command (create a built [binary]<N>distribution)."""<N><N>import os<N>from distutils.core import Command<N>from distutils.errors import *<N>from distutils.util import get_platform<N><N>
<N>def show_formats():<N>    """Print list of available formats (arguments to "--format" option).<N>    """<N>    from distutils.fancy_getopt import FancyGetopt<N>    formats = []<N>    for format in bdist.format_commands:<N>        formats.append(("formats=" + format, None,<N>                        bdist.format_command[format][1]))<N>    pretty_printer = FancyGetopt(formats)<N>    pretty_printer.print_help("List of available distribution formats:")<N><N>
"""distutils.command.bdist_dumb<N><N>Implements the Distutils 'bdist_dumb' command (create a "dumb" built<N>distribution -- i.e., just an archive to be unpacked under $prefix or<N>$exec_prefix)."""<N><N>import os<N>from distutils.core import Command<N>from distutils.util import get_platform<N>from distutils.dir_util import remove_tree, ensure_relative<N>from distutils.errors import *<N>from distutils.sysconfig import get_python_version<N>from distutils import log<N><N>
"""distutils.command.bdist_rpm<N><N>Implements the Distutils 'bdist_rpm' command (create RPM source and binary<N>distributions)."""<N><N>import subprocess, sys, os<N>from distutils.core import Command<N>from distutils.debug import DEBUG<N>from distutils.file_util import write_file<N>from distutils.errors import *<N>from distutils.sysconfig import get_python_version<N>from distutils import log<N><N>
"""distutils.command.bdist_wininst<N><N>Implements the Distutils 'bdist_wininst' command: create a windows installer<N>exe-program."""<N><N>import os<N>import sys<N>import warnings<N>from distutils.core import Command<N>from distutils.util import get_platform<N>from distutils.dir_util import remove_tree<N>from distutils.errors import *<N>from distutils.sysconfig import get_python_version<N>from distutils import log<N><N>
"""distutils.command.build<N><N>Implements the Distutils 'build' command."""<N><N>import sys, os<N>from distutils.core import Command<N>from distutils.errors import DistutilsOptionError<N>from distutils.util import get_platform<N><N><N>def show_compilers():<N>    from distutils.ccompiler import show_compilers<N>    show_compilers()<N><N>
"""distutils.command.build_py<N><N>Implements the Distutils 'build_py' command."""<N><N>import os<N>import importlib.util<N>import sys<N>import glob<N><N>from distutils.core import Command<N>from distutils.errors import *<N>from distutils.util import convert_path<N>from distutils import log<N><N>
"""distutils.command.build_scripts<N><N>Implements the Distutils 'build_scripts' command."""<N><N>import os<N>import re<N>from stat import ST_MODE<N>from distutils import sysconfig<N>from distutils.core import Command<N>from distutils.dep_util import newer<N>from distutils.util import convert_path<N>from distutils import log<N>import tokenize<N><N>
shebang_pattern = re.compile('^#!.*python[0-9.]*([ \t].*)?$')<N>"""<N>Pattern matching a Python interpreter indicated in first line of a script.<N>"""<N><N># for Setuptools compatibility<N>first_line_re = shebang_pattern<N><N><N>class build_scripts(Command):<N><N>
    description = "\"build\" scripts (copy and fixup #! line)"<N><N>    user_options = [<N>        ('build-dir=', 'd', "directory to \"build\" (copy) to"),<N>        ('force', 'f', "forcibly build everything (ignore file timestamps"),<N>        ('executable=', 'e', "specify final destination interpreter path"),<N>        ]<N><N>
"""distutils.command.check<N><N>Implements the Distutils 'check' command.<N>"""<N>from email.utils import getaddresses<N><N>from distutils.core import Command<N>from distutils.errors import DistutilsSetupError<N><N>try:<N>    # docutils is installed<N>    from docutils.utils import Reporter<N>    from docutils.parsers.rst import Parser<N>    from docutils import frontend<N>    from docutils import nodes<N><N>
    class SilentReporter(Reporter):<N><N>        def __init__(self, source, report_level, halt_level, stream=None,<N>                     debug=0, encoding='ascii', error_handler='replace'):<N>            self.messages = []<N>            super().__init__(source, report_level, halt_level, stream,<N>                              debug, encoding, error_handler)<N><N>
        def system_message(self, level, message, *children, **kwargs):<N>            self.messages.append((level, message, children, kwargs))<N>            return nodes.system_message(message, level=level,<N>                                        type=self.levels[level],<N>                                        *children, **kwargs)<N><N>
"""distutils.command.clean<N><N>Implements the Distutils 'clean' command."""<N><N># contributed by Bastian Kleineidam <calvin@cs.uni-sb.de>, added 2000-03-18<N><N>import os<N>from distutils.core import Command<N>from distutils.dir_util import remove_tree<N>from distutils import log<N><N>
"""distutils.command.install_data<N><N>Implements the Distutils 'install_data' command, for installing<N>platform-independent data files."""<N><N># contributed by Bastian Kleineidam<N><N>import os<N>from distutils.core import Command<N>from distutils.util import change_root, convert_path<N><N>
class install_data(Command):<N><N>    description = "install data files"<N><N>    user_options = [<N>        ('install-dir=', 'd',<N>         "base directory for installing data files "<N>         "(default: installation base dir)"),<N>        ('root=', None,<N>         "install everything relative to this alternate root directory"),<N>        ('force', 'f', "force installation (overwrite existing files)"),<N>        ]<N><N>
    boolean_options = ['force']<N><N>    def initialize_options(self):<N>        self.install_dir = None<N>        self.outfiles = []<N>        self.root = None<N>        self.force = 0<N>        self.data_files = self.distribution.data_files<N>        self.warn_dir = 1<N><N>
    def finalize_options(self):<N>        self.set_undefined_options('install',<N>                                   ('install_data', 'install_dir'),<N>                                   ('root', 'root'),<N>                                   ('force', 'force'),<N>                                  )<N><N>
"""distutils.command.install_egg_info<N><N>Implements the Distutils 'install_egg_info' command, for installing<N>a package's PKG-INFO metadata."""<N><N><N>from distutils.cmd import Command<N>from distutils import log, dir_util<N>import os, sys, re<N><N>
class install_egg_info(Command):<N>    """Install an .egg-info file for the package"""<N><N>    description = "Install package's PKG-INFO metadata as an .egg-info file"<N>    user_options = [<N>        ('install-dir=', 'd', "directory to install to"),<N>    ]<N><N>
    def initialize_options(self):<N>        self.install_dir = None<N><N>    @property<N>    def basename(self):<N>        """<N>        Allow basename to be overridden by child class.<N>        Ref pypa/distutils#2.<N>        """<N>        return "%s-%s-py%d.%d.egg-info" % (<N>            to_filename(safe_name(self.distribution.get_name())),<N>            to_filename(safe_version(self.distribution.get_version())),<N>            *sys.version_info[:2]<N>        )<N><N>
"""distutils.command.install_headers<N><N>Implements the Distutils 'install_headers' command, to install C/C++ header<N>files to the Python include directory."""<N><N>from distutils.core import Command<N><N><N># XXX force is never used<N>class install_headers(Command):<N><N>
    description = "install C/C++ header files"<N><N>    user_options = [('install-dir=', 'd',<N>                     "directory to install header files to"),<N>                    ('force', 'f',<N>                     "force installation (overwrite existing files)"),<N>                   ]<N><N>
    boolean_options = ['force']<N><N>    def initialize_options(self):<N>        self.install_dir = None<N>        self.force = 0<N>        self.outfiles = []<N><N>    def finalize_options(self):<N>        self.set_undefined_options('install',<N>                                   ('install_headers', 'install_dir'),<N>                                   ('force', 'force'))<N><N>
<N>    def run(self):<N>        headers = self.distribution.headers<N>        if not headers:<N>            return<N><N>        self.mkpath(self.install_dir)<N>        for header in headers:<N>            (out, _) = self.copy_file(header, self.install_dir)<N>            self.outfiles.append(out)<N><N>
"""distutils.command.install_lib<N><N>Implements the Distutils 'install_lib' command<N>(install all Python modules)."""<N><N>import os<N>import importlib.util<N>import sys<N><N>from distutils.core import Command<N>from distutils.errors import DistutilsOptionError<N><N>
"""distutils.command.install_scripts<N><N>Implements the Distutils 'install_scripts' command, for installing<N>Python scripts."""<N><N># contributed by Bastian Kleineidam<N><N>import os<N>from distutils.core import Command<N>from distutils import log<N>from stat import ST_MODE<N><N>
<N>class install_scripts(Command):<N><N>    description = "install scripts (Python or otherwise)"<N><N>    user_options = [<N>        ('install-dir=', 'd', "directory to install scripts to"),<N>        ('build-dir=','b', "build directory (where to install from)"),<N>        ('force', 'f', "force installation (overwrite existing files)"),<N>        ('skip-build', None, "skip the build steps"),<N>    ]<N><N>
import sys<N><N><N>def _pythonlib_compat():<N>    """<N>    On Python 3.7 and earlier, distutils would include the Python<N>    library. See pypa/distutils#9.<N>    """<N>    from distutils import sysconfig<N>    if not sysconfig.get_config_var('Py_ENABLED_SHARED'):<N>        return<N><N>
    yield 'python{}.{}{}'.format(<N>        sys.hexversion >> 24,<N>        (sys.hexversion >> 16) & 0xff,<N>        sysconfig.get_config_var('ABIFLAGS'),<N>    )<N><N><N>def compose(f1, f2):<N>    return lambda *args, **kwargs: f1(f2(*args, **kwargs))<N><N>
"""distutils.command.register<N><N>Implements the Distutils 'register' command (register with the repository).<N>"""<N><N># created 2002/10/21, Richard Jones<N><N>import getpass<N>import io<N>import urllib.parse, urllib.request<N>from warnings import warn<N><N>
"""<N>distutils.command.upload<N><N>Implements the Distutils 'upload' subcommand (upload package to a package<N>index).<N>"""<N><N>import os<N>import io<N>import hashlib<N>from base64 import standard_b64encode<N>from urllib.request import urlopen, Request, HTTPError<N>from urllib.parse import urlparse<N>from distutils.errors import DistutilsError, DistutilsOptionError<N>from distutils.core import PyPIRCCommand<N>from distutils.spawn import spawn<N>from distutils import log<N><N>
<N># PyPI Warehouse supports MD5, SHA256, and Blake2 (blake2-256)<N># https://bugs.python.org/issue40698<N>_FILE_CONTENT_DIGESTS = {<N>    "md5_digest": getattr(hashlib, "md5", None),<N>    "sha256_digest": getattr(hashlib, "sha256", None),<N>    "blake2_256_digest": getattr(hashlib, "blake2b", None),<N>}<N><N>
<N>class upload(PyPIRCCommand):<N><N>    description = "upload binary package to PyPI"<N><N>    user_options = PyPIRCCommand.user_options + [<N>        ('sign', 's',<N>         'sign files to upload using gpg'),<N>        ('identity=', 'i', 'GPG identity used to sign files'),<N>        ]<N><N>
    boolean_options = PyPIRCCommand.boolean_options + ['sign']<N><N>    def initialize_options(self):<N>        PyPIRCCommand.initialize_options(self)<N>        self.username = ''<N>        self.password = ''<N>        self.show_response = 0<N>        self.sign = False<N>        self.identity = None<N><N>
"""<N>An OrderedSet is a custom MutableSet that remembers its order, so that every<N>entry has an index that can be looked up.<N><N>Based on a recipe originally posted to ActiveState Recipes by Raymond Hettiger,<N>and released under the MIT license.<N>"""<N>import itertools as it<N>from collections import deque<N><N>
try:<N>    # Python 3<N>    from collections.abc import MutableSet, Sequence<N>except ImportError:<N>    # Python 2.7<N>    from collections import MutableSet, Sequence<N><N>SLICE_ALL = slice(None)<N>__version__ = "3.1"<N><N><N>def is_iterable(obj):<N>    """<N>    Are we being asked to look up a list of things, instead of a single thing?<N>    We check for the `__iter__` attribute so that this can cover types that<N>    don't have to be known by this module, such as NumPy arrays.<N><N>
    Strings, however, should be considered as atomic values to look up, not<N>    iterables. The same goes for tuples, since they are immutable and therefore<N>    valid entries.<N><N>    We don't need to check for the Python 2 `unicode` type, because it doesn't<N>    have an `__iter__` attribute anyway.<N>    """<N>    return (<N>        hasattr(obj, "__iter__")<N>        and not isinstance(obj, str)<N>        and not isinstance(obj, tuple)<N>    )<N><N>
<N>class OrderedSet(MutableSet, Sequence):<N>    """<N>    An OrderedSet is a custom MutableSet that remembers its order, so that<N>    every entry has an index that can be looked up.<N><N>    Example:<N>        >>> OrderedSet([1, 1, 2, 3, 2])<N>        OrderedSet([1, 2, 3])<N>    """<N><N>
    def __init__(self, iterable=None):<N>        self.items = []<N>        self.map = {}<N>        if iterable is not None:<N>            self |= iterable<N><N>    def __len__(self):<N>        """<N>        Returns the number of unique elements in the ordered set<N><N>
        Example:<N>            >>> len(OrderedSet([]))<N>            0<N>            >>> len(OrderedSet([1, 2]))<N>            2<N>        """<N>        return len(self.items)<N><N>    def __getitem__(self, index):<N>        """<N>        Get the item at a given index.<N><N>
        If `index` is a slice, you will get back that slice of items, as a<N>        new OrderedSet.<N><N>        If `index` is a list or a similar iterable, you'll get a list of<N>        items corresponding to those indices. This is similar to NumPy's<N>        "fancy indexing". The result is not an OrderedSet because you may ask<N>        for duplicate indices, and the number of elements returned should be<N>        the number of elements asked for.<N><N>
import abc<N>import collections<N>import collections.abc<N>import operator<N>import sys<N>import typing<N><N># After PEP 560, internal typing API was substantially reworked.<N># This is especially important for Protocol class which uses internal APIs<N># quite extensively.<N>PEP_560 = sys.version_info[:3] >= (3, 7, 0)<N><N>
if PEP_560:<N>    GenericMeta = type<N>else:<N>    # 3.6<N>    from typing import GenericMeta, _type_vars  # noqa<N><N># The two functions below are copies of typing internal helpers.<N># They are needed by _ProtocolMeta<N><N><N>def _no_slots_copy(dct):<N>    dict_copy = dict(dct)<N>    if '__slots__' in dict_copy:<N>        for slot in dict_copy['__slots__']:<N>            dict_copy.pop(slot, None)<N>    return dict_copy<N><N>
<N>def _check_generic(cls, parameters):<N>    if not cls.__parameters__:<N>        raise TypeError(f"{cls} is not a generic class")<N>    alen = len(parameters)<N>    elen = len(cls.__parameters__)<N>    if alen != elen:<N>        raise TypeError(f"Too {'many' if alen > elen else 'few'} arguments for {cls};"<N>                        f" actual {alen}, expected {elen}")<N><N>
<N># Please keep __all__ alphabetized within each category.<N>__all__ = [<N>    # Super-special typing primitives.<N>    'ClassVar',<N>    'Concatenate',<N>    'Final',<N>    'ParamSpec',<N>    'Self',<N>    'Type',<N><N>    # ABCs (from collections.abc).<N>    'Awaitable',<N>    'AsyncIterator',<N>    'AsyncIterable',<N>    'Coroutine',<N>    'AsyncGenerator',<N>    'AsyncContextManager',<N>    'ChainMap',<N><N>
    # Concrete collection types.<N>    'ContextManager',<N>    'Counter',<N>    'Deque',<N>    'DefaultDict',<N>    'OrderedDict',<N>    'TypedDict',<N><N>    # Structural checks, a.k.a. protocols.<N>    'SupportsIndex',<N><N>    # One-off things.<N>    'Annotated',<N>    'final',<N>    'IntVar',<N>    'Literal',<N>    'NewType',<N>    'overload',<N>    'Protocol',<N>    'runtime',<N>    'runtime_checkable',<N>    'Text',<N>    'TypeAlias',<N>    'TypeGuard',<N>    'TYPE_CHECKING',<N>]<N><N>
if PEP_560:<N>    __all__.extend(["get_args", "get_origin", "get_type_hints"])<N><N># 3.6.2+<N>if hasattr(typing, 'NoReturn'):<N>    NoReturn = typing.NoReturn<N># 3.6.0-3.6.1<N>else:<N>    class _NoReturn(typing._FinalTypingBase, _root=True):<N>        """Special type indicating functions that never return.<N>        Example::<N><N>
          from typing import NoReturn<N><N>          def stop() -> NoReturn:<N>              raise Exception('no way')<N><N>        This type is invalid in other positions, e.g., ``List[NoReturn]``<N>        will fail in static type checkers.<N>        """<N>        __slots__ = ()<N><N>
        def __instancecheck__(self, obj):<N>            raise TypeError("NoReturn cannot be used with isinstance().")<N><N>        def __subclasscheck__(self, cls):<N>            raise TypeError("NoReturn cannot be used with issubclass().")<N><N>    NoReturn = _NoReturn(_root=True)<N><N>
# Some unconstrained type variables.  These are used by the container types.<N># (These are not for export.)<N>T = typing.TypeVar('T')  # Any type.<N>KT = typing.TypeVar('KT')  # Key type.<N>VT = typing.TypeVar('VT')  # Value type.<N>T_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.<N>T_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.<N><N>
ClassVar = typing.ClassVar<N><N># On older versions of typing there is an internal class named "Final".<N># 3.8+<N>if hasattr(typing, 'Final') and sys.version_info[:2] >= (3, 7):<N>    Final = typing.Final<N># 3.7<N>elif sys.version_info[:2] >= (3, 7):<N>    class _FinalForm(typing._SpecialForm, _root=True):<N><N>
        def __repr__(self):<N>            return 'typing_extensions.' + self._name<N><N>        def __getitem__(self, parameters):<N>            item = typing._type_check(parameters,<N>                                      f'{self._name} accepts only single type')<N>            return typing._GenericAlias(self, (item,))<N><N>
    Final = _FinalForm('Final',<N>                       doc="""A special typing construct to indicate that a name<N>                       cannot be re-assigned or overridden in a subclass.<N>                       For example:<N><N>                           MAX_SIZE: Final = 9000<N>                           MAX_SIZE += 1  # Error reported by type checker<N><N>
import io<N>import posixpath<N>import zipfile<N>import itertools<N>import contextlib<N>import sys<N>import pathlib<N><N>if sys.version_info < (3, 7):<N>    from collections import OrderedDict<N>else:<N>    OrderedDict = dict<N><N><N>__all__ = ['Path']<N><N>
<N>def _parents(path):<N>    """<N>    Given a path with elements separated by<N>    posixpath.sep, generate all parents of that path.<N><N>    >>> list(_parents('b/d'))<N>    ['b']<N>    >>> list(_parents('/b/d/'))<N>    ['/b']<N>    >>> list(_parents('b/d/f/'))<N>    ['b/d', 'b']<N>    >>> list(_parents('b'))<N>    []<N>    >>> list(_parents(''))<N>    []<N>    """<N>    return itertools.islice(_ancestry(path), 1, None)<N><N>
import collections<N><N><N># from jaraco.collections 3.3<N>class FreezableDefaultDict(collections.defaultdict):<N>    """<N>    Often it is desirable to prevent the mutation of<N>    a default dict after its initial construction, such<N>    as to prevent mutation during iteration.<N><N>
    >>> dd = FreezableDefaultDict(list)<N>    >>> dd[0].append('1')<N>    >>> dd.freeze()<N>    >>> dd[1]<N>    []<N>    >>> len(dd)<N>    1<N>    """<N><N>    def __missing__(self, key):<N>        return getattr(self, '_frozen', super().__missing__)(key)<N><N>
    def freeze(self):<N>        self._frozen = lambda key: self.default_factory()<N><N><N>class Pair(collections.namedtuple('Pair', 'name value')):<N>    @classmethod<N>    def parse(cls, text):<N>        return cls(*map(str.strip, text.split("=", 1)))<N><N><N>
import sys<N>import platform<N><N><N>__all__ = ['install', 'NullFinder', 'Protocol']<N><N><N>try:<N>    from typing import Protocol<N>except ImportError:  # pragma: no cover<N>    from ..typing_extensions import Protocol  # type: ignore<N><N><N>def install(cls):<N>    """<N>    Class decorator for installation on sys.meta_path.<N><N>
    Adds the backport DistributionFinder to sys.meta_path and<N>    attempts to disable the finder functionality of the stdlib<N>    DistributionFinder.<N>    """<N>    sys.meta_path.append(cls())<N>    disable_stdlib_finder()<N>    return cls<N><N><N>def disable_stdlib_finder():<N>    """<N>    Give the backport primacy for discovering path-based distributions<N>    by monkey-patching the stdlib O_O.<N><N>
    See #91 for more background for rationale on this sketchy<N>    behavior.<N>    """<N><N>    def matches(finder):<N>        return getattr(<N>            finder, '__module__', None<N>        ) == '_frozen_importlib_external' and hasattr(finder, 'find_distributions')<N><N>
    for finder in filter(matches, sys.meta_path):  # pragma: nocover<N>        del finder.find_distributions<N><N><N>class NullFinder:<N>    """<N>    A "Finder" (aka "MetaClassFinder") that never finds any modules,<N>    but may find distributions.<N>    """<N><N>
    @staticmethod<N>    def find_spec(*args, **kwargs):<N>        return None<N><N>    # In Python 2, the import system requires finders<N>    # to have a find_module() method, but this usage<N>    # is deprecated in Python 3 in favor of find_spec().<N>    # For the purposes of this finder (i.e. being present<N>    # on sys.meta_path but having no other import<N>    # system functionality), the two methods are identical.<N>    find_module = find_spec<N><N>
import types<N>import functools<N><N><N># from jaraco.functools 3.3<N>def method_cache(method, cache_wrapper=None):<N>    """<N>    Wrap lru_cache to support storing the cache data in the object instances.<N><N>    Abstracts the common paradigm where the method explicitly saves an<N>    underscore-prefixed protected property on first call and returns that<N>    subsequently.<N><N>
    >>> class MyClass:<N>    ...     calls = 0<N>    ...<N>    ...     @method_cache<N>    ...     def method(self, value):<N>    ...         self.calls += 1<N>    ...         return value<N><N>    >>> a = MyClass()<N>    >>> a.method(3)<N>    3<N>    >>> for x in range(75):<N>    ...     res = a.method(x)<N>    >>> a.calls<N>    75<N><N>
    Note that the apparent behavior will be exactly like that of lru_cache<N>    except that the cache is stored on each instance, so values in one<N>    instance will not flush values from another, and when an instance is<N>    deleted, so are the cached values for that instance.<N><N>
    >>> b = MyClass()<N>    >>> for x in range(35):<N>    ...     res = b.method(x)<N>    >>> b.calls<N>    35<N>    >>> a.method(0)<N>    0<N>    >>> a.calls<N>    75<N><N>    Note that if method had been decorated with ``functools.lru_cache()``,<N>    a.calls would have been 76 (due to the cached value of 0 having been<N>    flushed by the 'b' instance).<N><N>
    Clear the cache with ``.cache_clear()``<N><N>    >>> a.method.cache_clear()<N><N>    Same for a method that hasn't yet been called.<N><N>    >>> c = MyClass()<N>    >>> c.method.cache_clear()<N><N>    Another cache wrapper may be supplied:<N><N>    >>> cache = functools.lru_cache(maxsize=2)<N>    >>> MyClass.method2 = method_cache(lambda self: 3, cache_wrapper=cache)<N>    >>> a = MyClass()<N>    >>> a.method2()<N>    3<N><N>
    Caution - do not subsequently wrap the method with another decorator, such<N>    as ``@property``, which changes the semantics of the function.<N><N>    See also<N>    http://code.activestate.com/recipes/577452-a-memoize-decorator-for-instance-methods/<N>    for another implementation and additional justification.<N>    """<N>    cache_wrapper = cache_wrapper or functools.lru_cache()<N><N>
    def wrapper(self, *args, **kwargs):<N>        # it's the first call, replace the method with a cached, bound method<N>        bound_method = types.MethodType(method, self)<N>        cached_method = cache_wrapper(bound_method)<N>        setattr(self, method.__name__, cached_method)<N>        return cached_method(*args, **kwargs)<N><N>
    # Support cache clear even before cache has been created.<N>    wrapper.cache_clear = lambda: None<N><N>    return wrapper<N><N><N># From jaraco.functools 3.3<N>def pass_none(func):<N>    """<N>    Wrap func so it's not called if its first param is None<N><N>
    >>> print_text = pass_none(print)<N>    >>> print_text('text')<N>    text<N>    >>> print_text(None)<N>    """<N><N>    @functools.wraps(func)<N>    def wrapper(param, *args, **kwargs):<N>        if param is not None:<N>            return func(param, *args, **kwargs)<N><N>
from ._compat import Protocol<N>from typing import Any, Dict, Iterator, List, TypeVar, Union<N><N><N>_T = TypeVar("_T")<N><N><N>class PackageMetadata(Protocol):<N>    def __len__(self) -> int:<N>        ...  # pragma: no cover<N><N>    def __contains__(self, item: str) -> bool:<N>        ...  # pragma: no cover<N><N>
    def __getitem__(self, key: str) -> str:<N>        ...  # pragma: no cover<N><N>    def __iter__(self) -> Iterator[str]:<N>        ...  # pragma: no cover<N><N>    def get_all(self, name: str, failobj: _T = ...) -> Union[List[Any], _T]:<N>        """<N>        Return all values associated with a possibly multi-valued key.<N>        """<N><N>
    @property<N>    def json(self) -> Dict[str, Union[str, List[str]]]:<N>        """<N>        A JSON-compatible form of the metadata.<N>        """<N><N><N>class SimplePath(Protocol):<N>    """<N>    A minimal subset of pathlib.Path required by PathDistribution.<N>    """<N><N>
    def joinpath(self) -> 'SimplePath':<N>        ...  # pragma: no cover<N><N>    def __truediv__(self) -> 'SimplePath':<N>        ...  # pragma: no cover<N><N>    def parent(self) -> 'SimplePath':<N>        ...  # pragma: no cover<N><N>    def read_text(self) -> str:<N>        ...  # pragma: no cover<N><N><N>
import re<N><N>from ._functools import method_cache<N><N><N># from jaraco.text 3.5<N>class FoldedCase(str):<N>    """<N>    A case insensitive string class; behaves just like str<N>    except compares equal when the only variation is case.<N><N>    >>> s = FoldedCase('hello world')<N><N>
    >>> s == 'Hello World'<N>    True<N><N>    >>> 'Hello World' == s<N>    True<N><N>    >>> s != 'Hello World'<N>    False<N><N>    >>> s.index('O')<N>    4<N><N>    >>> s.split('O')<N>    ['hell', ' w', 'rld']<N><N>    >>> sorted(map(FoldedCase, ['GAMMA', 'alpha', 'Beta']))<N>    ['alpha', 'Beta', 'GAMMA']<N><N>
    Sequence membership is straightforward.<N><N>    >>> "Hello World" in [s]<N>    True<N>    >>> s in ["Hello World"]<N>    True<N><N>    You may test for set inclusion, but candidate and elements<N>    must both be folded.<N><N>    >>> FoldedCase("Hello World") in {s}<N>    True<N>    >>> s in {FoldedCase("Hello World")}<N>    True<N><N>
    String inclusion works as long as the FoldedCase object<N>    is on the right.<N><N>    >>> "hello" in FoldedCase("Hello World")<N>    True<N><N>    But not if the FoldedCase object is on the left:<N><N>    >>> FoldedCase('hello') in 'Hello World'<N>    False<N><N>
    In that case, use in_:<N><N>    >>> FoldedCase('hello').in_('Hello World')<N>    True<N><N>    >>> FoldedCase('hello') > FoldedCase('Hello')<N>    False<N>    """<N><N>    def __lt__(self, other):<N>        return self.lower() < other.lower()<N><N>
    def __gt__(self, other):<N>        return self.lower() > other.lower()<N><N>    def __eq__(self, other):<N>        return self.lower() == other.lower()<N><N>    def __ne__(self, other):<N>        return self.lower() != other.lower()<N><N>    def __hash__(self):<N>        return hash(self.lower())<N><N>
    def __contains__(self, other):<N>        return super().lower().__contains__(other.lower())<N><N>    def in_(self, other):<N>        "Does self appear in other?"<N>        return self in FoldedCase(other)<N><N>    # cache lower since it's likely to be called frequently.<N>    @method_cache<N>    def lower(self):<N>        return super().lower()<N><N>
import os<N>import re<N>import abc<N>import csv<N>import sys<N>from .. import zipp<N>import email<N>import pathlib<N>import operator<N>import textwrap<N>import warnings<N>import functools<N>import itertools<N>import posixpath<N>import collections<N><N>
from . import _adapters, _meta<N>from ._collections import FreezableDefaultDict, Pair<N>from ._compat import (<N>    NullFinder,<N>    install,<N>    pypy_partial,<N>)<N>from ._functools import method_cache, pass_none<N>from ._itertools import always_iterable, unique_everseen<N>from ._meta import PackageMetadata, SimplePath<N><N>
from contextlib import suppress<N>from importlib import import_module<N>from importlib.abc import MetaPathFinder<N>from itertools import starmap<N>from typing import List, Mapping, Optional, Union<N><N><N>__all__ = [<N>    'Distribution',<N>    'DistributionFinder',<N>    'PackageMetadata',<N>    'PackageNotFoundError',<N>    'distribution',<N>    'distributions',<N>    'entry_points',<N>    'files',<N>    'metadata',<N>    'packages_distributions',<N>    'requires',<N>    'version',<N>]<N><N>
<N>class PackageNotFoundError(ModuleNotFoundError):<N>    """The package was not found."""<N><N>    def __str__(self):<N>        return f"No package metadata was found for {self.name}"<N><N>    @property<N>    def name(self):<N>        (name,) = self.args<N>        return name<N><N>
<N>class Sectioned:<N>    """<N>    A simple entry point config parser for performance<N><N>    >>> for item in Sectioned.read(Sectioned._sample):<N>    ...     print(item)<N>    Pair(name='sec1', value='# comments ignored')<N>    Pair(name='sec1', value='a = 1')<N>    Pair(name='sec1', value='b = 2')<N>    Pair(name='sec2', value='a = 2')<N><N>
    >>> res = Sectioned.section_pairs(Sectioned._sample)<N>    >>> item = next(res)<N>    >>> item.name<N>    'sec1'<N>    >>> item.value<N>    Pair(name='a', value='1')<N>    >>> item = next(res)<N>    >>> item.value<N>    Pair(name='b', value='2')<N>    >>> item = next(res)<N>    >>> item.name<N>    'sec2'<N>    >>> item.value<N>    Pair(name='a', value='2')<N>    >>> list(res)<N>    []<N>    """<N><N>
    _sample = textwrap.dedent(<N>        """<N>        [sec1]<N>        # comments ignored<N>        a = 1<N>        b = 2<N><N>        [sec2]<N>        a = 2<N>        """<N>    ).lstrip()<N><N>    @classmethod<N>    def section_pairs(cls, text):<N>        return (<N>            section._replace(value=Pair.parse(section.value))<N>            for section in cls.read(text, filter_=cls.valid)<N>            if section.name is not None<N>        )<N><N>
    @staticmethod<N>    def read(text, filter_=None):<N>        lines = filter(filter_, map(str.strip, text.splitlines()))<N>        name = None<N>        for value in lines:<N>            section_match = value.startswith('[') and value.endswith(']')<N>            if section_match:<N>                name = value.strip('[]')<N>                continue<N>            yield Pair(name, value)<N><N>
    @staticmethod<N>    def valid(line):<N>        return line and not line.startswith('#')<N><N><N>class DeprecatedTuple:<N>    """<N>    Provide subscript item access for backward compatibility.<N><N>    >>> recwarn = getfixture('recwarn')<N>    >>> ep = EntryPoint(name='name', value='value', group='group')<N>    >>> ep[:]<N>    ('name', 'value', 'group')<N>    >>> ep[0]<N>    'name'<N>    >>> len(recwarn)<N>    1<N>    """<N><N>
    _warn = functools.partial(<N>        warnings.warn,<N>        "EntryPoint tuple interface is deprecated. Access members by name.",<N>        DeprecationWarning,<N>        stacklevel=pypy_partial(2),<N>    )<N><N>    def __getitem__(self, item):<N>        self._warn()<N>        return self._key()[item]<N><N>
<N>class EntryPoint(DeprecatedTuple):<N>    """An entry point as defined by Python packaging conventions.<N><N>    See `the packaging docs on entry points<N>    <https://packaging.python.org/specifications/entry-points/>`_<N>    for more information.<N>    """<N><N>
    pattern = re.compile(<N>        r'(?P<module>[\w.]+)\s*'<N>        r'(:\s*(?P<attr>[\w.]+)\s*)?'<N>        r'((?P<extras>\[.*\])\s*)?$'<N>    )<N>    """<N>    A regular expression describing the syntax for an entry point,<N>    which might look like:<N><N>
        - module<N>        - package.module<N>        - package.module:attribute<N>        - package.module:object.attribute<N>        - package.module:attr [extra1, extra2]<N><N>    Other combinations are possible as well.<N><N>    The expression is lenient about whitespace around the ':',<N>    following the attr, and following any extras.<N>    """<N><N>
import abc<N>from typing import BinaryIO, Iterable, Text<N><N>from ._compat import runtime_checkable, Protocol<N><N><N>class ResourceReader(metaclass=abc.ABCMeta):<N>    """Abstract base class for loaders to provide resource reading support."""<N><N>
import collections<N>import pathlib<N>import operator<N><N>from . import abc<N><N>from ._itertools import unique_everseen<N>from ._compat import ZipPath<N><N><N>def remove_duplicates(items):<N>    return iter(collections.OrderedDict.fromkeys(items))<N><N>
<N>class FileReader(abc.TraversableResources):<N>    def __init__(self, loader):<N>        self.path = pathlib.Path(loader.path).parent<N><N>    def resource_path(self, resource):<N>        """<N>        Return the file system path to prevent<N>        `resources.path()` from creating a temporary<N>        copy.<N>        """<N>        return str(self.path.joinpath(resource))<N><N>
    def files(self):<N>        return self.path<N><N><N>class ZipReader(abc.TraversableResources):<N>    def __init__(self, loader, module):<N>        _, _, name = module.rpartition('.')<N>        self.prefix = loader.prefix.replace('\\', '/') + name + '/'<N>        self.archive = loader.archive<N><N>
    def open_resource(self, resource):<N>        try:<N>            return super().open_resource(resource)<N>        except KeyError as exc:<N>            raise FileNotFoundError(exc.args[0])<N><N>    def is_resource(self, path):<N>        # workaround for `zipfile.Path.is_file` returning true<N>        # for non-existent paths.<N>        target = self.files().joinpath(path)<N>        return target.is_file() and target.exists()<N><N>
    def files(self):<N>        return ZipPath(self.archive, self.prefix)<N><N><N>class MultiplexedPath(abc.Traversable):<N>    """<N>    Given a series of Traversable objects, implement a merged<N>    version of the interface across all objects. Useful for<N>    namespace packages which may be multihomed at a single<N>    name.<N>    """<N><N>
    def __init__(self, *paths):<N>        self._paths = list(map(pathlib.Path, remove_duplicates(paths)))<N>        if not self._paths:<N>            message = 'MultiplexedPath must contain at least one path'<N>            raise FileNotFoundError(message)<N>        if not all(path.is_dir() for path in self._paths):<N>            raise NotADirectoryError('MultiplexedPath only supports directories')<N><N>
    def iterdir(self):<N>        files = (file for path in self._paths for file in path.iterdir())<N>        return unique_everseen(files, key=operator.attrgetter('name'))<N><N>    def read_bytes(self):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>
    def read_text(self, *args, **kwargs):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>    def is_dir(self):<N>        return True<N><N>    def is_file(self):<N>        return False<N><N>    def joinpath(self, child):<N>        # first try to find child in current paths<N>        for file in self.iterdir():<N>            if file.name == child:<N>                return file<N>        # if it does not exist, construct it with the first path<N>        return self._paths[0] / child<N><N>
    __truediv__ = joinpath<N><N>    def open(self, *args, **kwargs):<N>        raise FileNotFoundError(f'{self} is not a file')<N><N>    @property<N>    def name(self):<N>        return self._paths[0].name<N><N>    def __repr__(self):<N>        paths = ', '.join(f"'{path}'" for path in self._paths)<N>        return f'MultiplexedPath({paths})'<N><N>
<N>class NamespaceReader(abc.TraversableResources):<N>    def __init__(self, namespace_path):<N>        if 'NamespacePath' not in str(namespace_path):<N>            raise ValueError('Invalid path')<N>        self.path = MultiplexedPath(*list(namespace_path))<N><N>
    def resource_path(self, resource):<N>        """<N>        Return the file system path to prevent<N>        `resources.path()` from creating a temporary<N>        copy.<N>        """<N>        return str(self.path.joinpath(resource))<N><N>    def files(self):<N>        return self.path<N><N><N>
"""<N>Interface adapters for low-level readers.<N>"""<N><N>import abc<N>import io<N>import itertools<N>from typing import BinaryIO, List<N><N>from .abc import Traversable, TraversableResources<N><N><N>class SimpleReader(abc.ABC):<N>    """<N>    The minimum, low-level interface required from a resource<N>    provider.<N>    """<N><N>
    @abc.abstractproperty<N>    def package(self):<N>        # type: () -> str<N>        """<N>        The name of the package for which this reader loads resources.<N>        """<N><N>    @abc.abstractmethod<N>    def children(self):<N>        # type: () -> List['SimpleReader']<N>        """<N>        Obtain an iterable of SimpleReader for available<N>        child containers (e.g. directories).<N>        """<N><N>
    @abc.abstractmethod<N>    def resources(self):<N>        # type: () -> List[str]<N>        """<N>        Obtain available named resources for this virtual package.<N>        """<N><N>    @abc.abstractmethod<N>    def open_binary(self, resource):<N>        # type: (str) -> BinaryIO<N>        """<N>        Obtain a File-like for a named resource.<N>        """<N><N>
    @property<N>    def name(self):<N>        return self.package.split('.')[-1]<N><N><N>class ResourceHandle(Traversable):<N>    """<N>    Handle to a named resource in a ResourceReader.<N>    """<N><N>    def __init__(self, parent, name):<N>        # type: (ResourceContainer, str) -> None<N>        self.parent = parent<N>        self.name = name  # type: ignore<N><N>
    def is_file(self):<N>        return True<N><N>    def is_dir(self):<N>        return False<N><N>    def open(self, mode='r', *args, **kwargs):<N>        stream = self.parent.reader.open_binary(self.name)<N>        if 'b' not in mode:<N>            stream = io.TextIOWrapper(*args, **kwargs)<N>        return stream<N><N>
    def joinpath(self, name):<N>        raise RuntimeError("Cannot traverse into a resource")<N><N><N>class ResourceContainer(Traversable):<N>    """<N>    Traversable container for a package's resources via its reader.<N>    """<N><N>    def __init__(self, reader):<N>        # type: (SimpleReader) -> None<N>        self.reader = reader<N><N>
    def is_dir(self):<N>        return True<N><N>    def is_file(self):<N>        return False<N><N>    def iterdir(self):<N>        files = (ResourceHandle(self, name) for name in self.reader.resources)<N>        dirs = map(ResourceContainer, self.reader.children())<N>        return itertools.chain(files, dirs)<N><N>
from contextlib import suppress<N>from io import TextIOWrapper<N><N>from . import abc<N><N><N>class SpecLoaderAdapter:<N>    """<N>    Adapt a package spec to adapt the underlying loader.<N>    """<N><N>    def __init__(self, spec, adapter=lambda spec: spec.loader):<N>        self.spec = spec<N>        self.loader = adapter(spec)<N><N>
    def __getattr__(self, name):<N>        return getattr(self.spec, name)<N><N><N>class TraversableResourcesLoader:<N>    """<N>    Adapt a loader to provide TraversableResources.<N>    """<N><N>    def __init__(self, spec):<N>        self.spec = spec<N><N>
    def get_resource_reader(self, name):<N>        return CompatibilityFiles(self.spec)._native()<N><N><N>def _io_wrapper(file, mode='r', *args, **kwargs):<N>    if mode == 'r':<N>        return TextIOWrapper(file, *args, **kwargs)<N>    elif mode == 'rb':<N>        return file<N>    raise ValueError(<N>        "Invalid mode value '{}', only 'r' and 'rb' are supported".format(mode)<N>    )<N><N>
<N>class CompatibilityFiles:<N>    """<N>    Adapter for an existing or non-existent resource reader<N>    to provide a compatibility .files().<N>    """<N><N>    class SpecPath(abc.Traversable):<N>        """<N>        Path tied to a module spec.<N>        Can be read and exposes the resource reader children.<N>        """<N><N>
        def __init__(self, spec, reader):<N>            self._spec = spec<N>            self._reader = reader<N><N>        def iterdir(self):<N>            if not self._reader:<N>                return iter(())<N>            return iter(<N>                CompatibilityFiles.ChildPath(self._reader, path)<N>                for path in self._reader.contents()<N>            )<N><N>
        def is_file(self):<N>            return False<N><N>        is_dir = is_file<N><N>        def joinpath(self, other):<N>            if not self._reader:<N>                return CompatibilityFiles.OrphanPath(other)<N>            return CompatibilityFiles.ChildPath(self._reader, other)<N><N>
        @property<N>        def name(self):<N>            return self._spec.name<N><N>        def open(self, mode='r', *args, **kwargs):<N>            return _io_wrapper(self._reader.open_resource(None), mode, *args, **kwargs)<N><N>    class ChildPath(abc.Traversable):<N>        """<N>        Path tied to a resource reader child.<N>        Can be read but doesn't expose any meaningful children.<N>        """<N><N>
        def __init__(self, reader, name):<N>            self._reader = reader<N>            self._name = name<N><N>        def iterdir(self):<N>            return iter(())<N><N>        def is_file(self):<N>            return self._reader.is_resource(self.name)<N><N>
        def is_dir(self):<N>            return not self.is_file()<N><N>        def joinpath(self, other):<N>            return CompatibilityFiles.OrphanPath(self.name, other)<N><N>        @property<N>        def name(self):<N>            return self._name<N><N>
        def open(self, mode='r', *args, **kwargs):<N>            return _io_wrapper(<N>                self._reader.open_resource(self.name), mode, *args, **kwargs<N>            )<N><N>    class OrphanPath(abc.Traversable):<N>        """<N>        Orphan path, not tied to a module spec or resource reader.<N>        Can't be read and doesn't expose any meaningful children.<N>        """<N><N>
        def __init__(self, *path_parts):<N>            if len(path_parts) < 1:<N>                raise ValueError('Need at least one path part to construct a path')<N>            self._path = path_parts<N><N>        def iterdir(self):<N>            return iter(())<N><N>
        def is_file(self):<N>            return False<N><N>        is_dir = is_file<N><N>        def joinpath(self, other):<N>            return CompatibilityFiles.OrphanPath(*self._path, other)<N><N>        @property<N>        def name(self):<N>            return self._path[-1]<N><N>
        def open(self, mode='r', *args, **kwargs):<N>            raise FileNotFoundError("Can't open orphan path")<N><N>    def __init__(self, spec):<N>        self.spec = spec<N><N>    @property<N>    def _reader(self):<N>        with suppress(AttributeError):<N>            return self.spec.loader.get_resource_reader(self.spec.name)<N><N>
    def _native(self):<N>        """<N>        Return the native reader if it supports files().<N>        """<N>        reader = self._reader<N>        return reader if hasattr(reader, 'files') else self<N><N>    def __getattr__(self, attr):<N>        return getattr(self._reader, attr)<N><N>
    def files(self):<N>        return CompatibilityFiles.SpecPath(self.spec, self._reader)<N><N><N>def wrap_spec(package):<N>    """<N>    Construct a package spec with traversable compatibility<N>    on the spec/loader/reader.<N>    """<N>    return SpecLoaderAdapter(package.__spec__, TraversableResourcesLoader)<N><N><N>
import os<N>import pathlib<N>import tempfile<N>import functools<N>import contextlib<N>import types<N>import importlib<N><N>from typing import Union, Optional<N>from .abc import ResourceReader, Traversable<N><N>from ._compat import wrap_spec<N><N>Package = Union[types.ModuleType, str]<N><N>
# flake8: noqa<N><N>import abc<N>import sys<N>import pathlib<N>from contextlib import suppress<N><N>if sys.version_info >= (3, 10):<N>    from zipfile import Path as ZipPath  # type: ignore<N>else:<N>    from ..zipp import Path as ZipPath  # type: ignore<N><N>
<N>try:<N>    from typing import runtime_checkable  # type: ignore<N>except ImportError:<N><N>    def runtime_checkable(cls):  # type: ignore<N>        return cls<N><N><N>try:<N>    from typing import Protocol  # type: ignore<N>except ImportError:<N>    Protocol = abc.ABC  # type: ignore<N><N>
<N>class TraversableResourcesLoader:<N>    """<N>    Adapt loaders to provide TraversableResources and other<N>    compatibility.<N><N>    Used primarily for Python 3.9 and earlier where the native<N>    loaders do not yet implement TraversableResources.<N>    """<N><N>
    def __init__(self, spec):<N>        self.spec = spec<N><N>    @property<N>    def path(self):<N>        return self.spec.origin<N><N>    def get_resource_reader(self, name):<N>        from . import readers, _adapters<N><N>        def _zip_reader(spec):<N>            with suppress(AttributeError):<N>                return readers.ZipReader(spec.loader, spec.name)<N><N>
        def _namespace_reader(spec):<N>            with suppress(AttributeError, ValueError):<N>                return readers.NamespaceReader(spec.submodule_search_locations)<N><N>        def _available_reader(spec):<N>            with suppress(AttributeError):<N>                return spec.loader.get_resource_reader(spec.name)<N><N>
        def _native_reader(spec):<N>            reader = _available_reader(spec)<N>            return reader if hasattr(reader, 'files') else None<N><N>        def _file_reader(spec):<N>            try:<N>                path = pathlib.Path(self.path)<N>            except TypeError:<N>                return None<N>            if path.exists():<N>                return readers.FileReader(self)<N><N>
from itertools import filterfalse<N><N>from typing import (<N>    Callable,<N>    Iterable,<N>    Iterator,<N>    Optional,<N>    Set,<N>    TypeVar,<N>    Union,<N>)<N><N># Type and type variable definitions<N>_T = TypeVar('_T')<N>_U = TypeVar('_U')<N><N>
import functools<N>import os<N>import pathlib<N>import types<N>import warnings<N><N>from typing import Union, Iterable, ContextManager, BinaryIO, TextIO, Any<N><N>from . import _common<N><N>Package = Union[types.ModuleType, str]<N>Resource = str<N><N>
<N>def deprecated(func):<N>    @functools.wraps(func)<N>    def wrapper(*args, **kwargs):<N>        warnings.warn(<N>            f"{func.__name__} is deprecated. Use files() instead. "<N>            "Refer to https://importlib-resources.readthedocs.io"<N>            "/en/latest/using.html#migrating-from-legacy for migration advice.",<N>            DeprecationWarning,<N>            stacklevel=2,<N>        )<N>        return func(*args, **kwargs)<N><N>
    return wrapper<N><N><N>def normalize_path(path):<N>    # type: (Any) -> str<N>    """Normalize a path by ensuring it is a string.<N><N>    If the resulting string contains path separators, an exception is raised.<N>    """<N>    str_path = str(path)<N>    parent, file_name = os.path.split(str_path)<N>    if parent:<N>        raise ValueError(f'{path!r} must be only a file name')<N>    return file_name<N><N>
<N>@deprecated<N>def open_binary(package: Package, resource: Resource) -> BinaryIO:<N>    """Return a file-like object opened for binary reading of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).open('rb')<N><N><N>@deprecated<N>def read_binary(package: Package, resource: Resource) -> bytes:<N>    """Return the binary contents of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).read_bytes()<N><N>
<N>@deprecated<N>def open_text(<N>    package: Package,<N>    resource: Resource,<N>    encoding: str = 'utf-8',<N>    errors: str = 'strict',<N>) -> TextIO:<N>    """Return a file-like object opened for text reading of the resource."""<N>    return (_common.files(package) / normalize_path(resource)).open(<N>        'r', encoding=encoding, errors=errors<N>    )<N><N>
<N>@deprecated<N>def read_text(<N>    package: Package,<N>    resource: Resource,<N>    encoding: str = 'utf-8',<N>    errors: str = 'strict',<N>) -> str:<N>    """Return the decoded string of the resource.<N><N>    The decoding-related arguments have the same semantics as those of<N>    bytes.decode().<N>    """<N>    with open_text(package, resource, encoding, errors) as fp:<N>        return fp.read()<N><N>
<N>@deprecated<N>def contents(package: Package) -> Iterable[str]:<N>    """Return an iterable of entries in `package`.<N><N>    Note that not all entries are resources.  Specifically, directories are<N>    not considered resources.  Use `is_resource()` on each entry returned here<N>    to check if it is a resource or not.<N>    """<N>    return [path.name for path in _common.files(package).iterdir()]<N><N>
<N>@deprecated<N>def is_resource(package: Package, name: str) -> bool:<N>    """True if `name` is a resource inside `package`.<N><N>    Directories are *not* resources.<N>    """<N>    resource = normalize_path(name)<N>    return any(<N>        traversable.name == resource and traversable.is_file()<N>        for traversable in _common.files(package).iterdir()<N>    )<N><N>
"""Read resources contained within a package."""<N><N>from ._common import (<N>    as_file,<N>    files,<N>    Package,<N>)<N><N>from ._legacy import (<N>    contents,<N>    open_binary,<N>    read_binary,<N>    open_text,<N>    read_text,<N>    is_resource,<N>    path,<N>    Resource,<N>)<N><N>from .abc import ResourceReader<N><N><N>__all__ = [<N>    'Package',<N>    'Resource',<N>    'ResourceReader',<N>    'as_file',<N>    'contents',<N>    'files',<N>    'is_resource',<N>    'open_binary',<N>    'open_text',<N>    'path',<N>    'read_binary',<N>    'read_text',<N>]<N>
import os<N>import subprocess<N>import contextlib<N>import functools<N>import tempfile<N>import shutil<N>import operator<N><N><N>@contextlib.contextmanager<N>def pushd(dir):<N>    orig = os.getcwd()<N>    os.chdir(dir)<N>    try:<N>        yield dir<N>    finally:<N>        os.chdir(orig)<N><N>
import functools<N>import time<N>import inspect<N>import collections<N>import types<N>import itertools<N><N>import setuptools.extern.more_itertools<N><N>from typing import Callable, TypeVar<N><N><N>CallableT = TypeVar("CallableT", bound=Callable[..., object])<N><N>
<N>def compose(*funcs):<N>    """<N>    Compose any number of unary functions into a single unary function.<N><N>    >>> import textwrap<N>    >>> expected = str.strip(textwrap.dedent(compose.__doc__))<N>    >>> strip_and_dedent = compose(str.strip, textwrap.dedent)<N>    >>> strip_and_dedent(compose.__doc__) == expected<N>    True<N><N>
    Compose also allows the innermost function to take arbitrary arguments.<N><N>    >>> round_three = lambda x: round(x, ndigits=3)<N>    >>> f = compose(round_three, int.__truediv__)<N>    >>> [f(3*x, x+1) for x in range(1,10)]<N>    [1.5, 2.0, 2.25, 2.4, 2.5, 2.571, 2.625, 2.667, 2.7]<N>    """<N><N>
    def compose_two(f1, f2):<N>        return lambda *args, **kwargs: f1(f2(*args, **kwargs))<N><N>    return functools.reduce(compose_two, funcs)<N><N><N>def method_caller(method_name, *args, **kwargs):<N>    """<N>    Return a function that will call a named method on the<N>    target object with optional positional and keyword<N>    arguments.<N><N>
    >>> lower = method_caller('lower')<N>    >>> lower('MyString')<N>    'mystring'<N>    """<N><N>    def call_method(target):<N>        func = getattr(target, method_name)<N>        return func(*args, **kwargs)<N><N>    return call_method<N><N><N>def once(func):<N>    """<N>    Decorate func so it's only ever called the first time.<N><N>
    This decorator can ensure that an expensive or non-idempotent function<N>    will not be expensive on subsequent calls and is idempotent.<N><N>    >>> add_three = once(lambda a: a+3)<N>    >>> add_three(3)<N>    6<N>    >>> add_three(9)<N>    6<N>    >>> add_three('12')<N>    6<N><N>
    To reset the stored value, simply clear the property ``saved_result``.<N><N>    >>> del add_three.saved_result<N>    >>> add_three(9)<N>    12<N>    >>> add_three(8)<N>    12<N><N>    Or invoke 'reset()' on it.<N><N>    >>> add_three.reset()<N>    >>> add_three(-3)<N>    0<N>    >>> add_three(0)<N>    0<N>    """<N><N>
    @functools.wraps(func)<N>    def wrapper(*args, **kwargs):<N>        if not hasattr(wrapper, 'saved_result'):<N>            wrapper.saved_result = func(*args, **kwargs)<N>        return wrapper.saved_result<N><N>    wrapper.reset = lambda: vars(wrapper).__delitem__('saved_result')<N>    return wrapper<N><N>
<N>def method_cache(<N>    method: CallableT,<N>    cache_wrapper: Callable[<N>        [CallableT], CallableT<N>    ] = functools.lru_cache(),  # type: ignore[assignment]<N>) -> CallableT:<N>    """<N>    Wrap lru_cache to support storing the cache data in the object instances.<N><N>
    Abstracts the common paradigm where the method explicitly saves an<N>    underscore-prefixed protected property on first call and returns that<N>    subsequently.<N><N>    >>> class MyClass:<N>    ...     calls = 0<N>    ...<N>    ...     @method_cache<N>    ...     def method(self, value):<N>    ...         self.calls += 1<N>    ...         return value<N><N>
    >>> a = MyClass()<N>    >>> a.method(3)<N>    3<N>    >>> for x in range(75):<N>    ...     res = a.method(x)<N>    >>> a.calls<N>    75<N><N>    Note that the apparent behavior will be exactly like that of lru_cache<N>    except that the cache is stored on each instance, so values in one<N>    instance will not flush values from another, and when an instance is<N>    deleted, so are the cached values for that instance.<N><N>
    >>> b = MyClass()<N>    >>> for x in range(35):<N>    ...     res = b.method(x)<N>    >>> b.calls<N>    35<N>    >>> a.method(0)<N>    0<N>    >>> a.calls<N>    75<N><N>    Note that if method had been decorated with ``functools.lru_cache()``,<N>    a.calls would have been 76 (due to the cached value of 0 having been<N>    flushed by the 'b' instance).<N><N>
    Clear the cache with ``.cache_clear()``<N><N>    >>> a.method.cache_clear()<N><N>    Same for a method that hasn't yet been called.<N><N>    >>> c = MyClass()<N>    >>> c.method.cache_clear()<N><N>    Another cache wrapper may be supplied:<N><N>    >>> cache = functools.lru_cache(maxsize=2)<N>    >>> MyClass.method2 = method_cache(lambda self: 3, cache_wrapper=cache)<N>    >>> a = MyClass()<N>    >>> a.method2()<N>    3<N><N>
    Caution - do not subsequently wrap the method with another decorator, such<N>    as ``@property``, which changes the semantics of the function.<N><N>    See also<N>    http://code.activestate.com/recipes/577452-a-memoize-decorator-for-instance-methods/<N>    for another implementation and additional justification.<N>    """<N><N>
    def wrapper(self: object, *args: object, **kwargs: object) -> object:<N>        # it's the first call, replace the method with a cached, bound method<N>        bound_method: CallableT = types.MethodType(  # type: ignore[assignment]<N>            method, self<N>        )<N>        cached_method = cache_wrapper(bound_method)<N>        setattr(self, method.__name__, cached_method)<N>        return cached_method(*args, **kwargs)<N><N>
    # Support cache clear even before cache has been created.<N>    wrapper.cache_clear = lambda: None  # type: ignore[attr-defined]<N><N>    return (  # type: ignore[return-value]<N>        _special_method_cache(method, cache_wrapper) or wrapper<N>    )<N><N>
<N>def _special_method_cache(method, cache_wrapper):<N>    """<N>    Because Python treats special methods differently, it's not<N>    possible to use instance attributes to implement the cached<N>    methods.<N><N>    Instead, install the wrapper method under a different name<N>    and return a simple proxy to that wrapper.<N><N>
import re<N>import itertools<N>import textwrap<N>import functools<N><N>try:<N>    from importlib.resources import files  # type: ignore<N>except ImportError:  # pragma: nocover<N>    from setuptools.extern.importlib_resources import files  # type: ignore<N><N>
from setuptools.extern.jaraco.functools import compose, method_cache<N>from setuptools.extern.jaraco.context import ExceptionTrap<N><N><N>def substitution(old, new):<N>    """<N>    Return a function that will perform a substitution on a string<N>    """<N>    return lambda s: s.replace(old, new)<N><N>
"""Imported from the recipes section of the itertools documentation.<N><N>All functions taken from the recipes section of the itertools library docs<N>[1]_.<N>Some backward-compatible usability improvements have been made.<N><N>.. [1] http://docs.python.org/library/itertools.html#recipes<N><N>
"""<N>import warnings<N>from collections import deque<N>from itertools import (<N>    chain,<N>    combinations,<N>    count,<N>    cycle,<N>    groupby,<N>    islice,<N>    repeat,<N>    starmap,<N>    tee,<N>    zip_longest,<N>)<N>import operator<N>from random import randrange, sample, choice<N><N>
import contextlib<N>import sys<N><N><N>if sys.version_info >= (3, 10):<N>    import importlib.metadata as metadata<N>else:<N>    import setuptools.extern.importlib_metadata as metadata  # type: ignore # noqa: F401<N><N><N>def repair_extras(extras):<N>    """<N>    Repair extras that appear as match objects.<N><N>
    python/importlib_metadata#369 revealed a flaw in the EntryPoint<N>    implementation. This function wraps the extras to ensure<N>    they are proper strings even on older implementations.<N>    """<N>    with contextlib.suppress(AttributeError):<N>        return list(item.group(0) for item in extras)<N>    return extras<N><N><N>
import itertools<N>import functools<N>import contextlib<N><N>from setuptools.extern.packaging.requirements import Requirement<N>from setuptools.extern.packaging.version import Version<N>from setuptools.extern.more_itertools import always_iterable<N>from setuptools.extern.jaraco.context import suppress<N>from setuptools.extern.jaraco.functools import apply<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import operator<N>import os<N>import platform<N>import sys<N>from typing import Any, Callable, Dict, List, Optional, Tuple, Union<N><N>
from setuptools.extern.pyparsing import (  # noqa: N817<N>    Forward,<N>    Group,<N>    Literal as L,<N>    ParseException,<N>    ParseResults,<N>    QuotedString,<N>    ZeroOrMore,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .specifiers import InvalidSpecifier, Specifier<N><N>
__all__ = [<N>    "InvalidMarker",<N>    "UndefinedComparison",<N>    "UndefinedEnvironmentName",<N>    "Marker",<N>    "default_environment",<N>]<N><N>Operator = Callable[[str, str], bool]<N><N><N>class InvalidMarker(ValueError):<N>    """<N>    An invalid marker was found, users should refer to PEP 508.<N>    """<N><N>
<N>class UndefinedComparison(ValueError):<N>    """<N>    An invalid operation was attempted on a value that doesn't support it.<N>    """<N><N><N>class UndefinedEnvironmentName(ValueError):<N>    """<N>    A name was attempted to be used that does not exist inside of the<N>    environment.<N>    """<N><N>
<N>class Node:<N>    def __init__(self, value: Any) -> None:<N>        self.value = value<N><N>    def __str__(self) -> str:<N>        return str(self.value)<N><N>    def __repr__(self) -> str:<N>        return f"<{self.__class__.__name__}('{self}')>"<N><N>
    def serialize(self) -> str:<N>        raise NotImplementedError<N><N><N>class Variable(Node):<N>    def serialize(self) -> str:<N>        return str(self)<N><N><N>class Value(Node):<N>    def serialize(self) -> str:<N>        return f'"{self}"'<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>import string<N>import urllib.parse<N>from typing import List, Optional as TOptional, Set<N><N>
from setuptools.extern.pyparsing import (  # noqa<N>    Combine,<N>    Literal as L,<N>    Optional,<N>    ParseException,<N>    Regex,<N>    Word,<N>    ZeroOrMore,<N>    originalTextFor,<N>    stringEnd,<N>    stringStart,<N>)<N><N>from .markers import MARKER_EXPR, Marker<N>from .specifiers import LegacySpecifier, Specifier, SpecifierSet<N><N>
<N>class InvalidRequirement(ValueError):<N>    """<N>    An invalid requirement was found, users should refer to PEP 508.<N>    """<N><N><N>ALPHANUM = Word(string.ascii_letters + string.digits)<N><N>LBRACKET = L("[").suppress()<N>RBRACKET = L("]").suppress()<N>LPAREN = L("(").suppress()<N>RPAREN = L(")").suppress()<N>COMMA = L(",").suppress()<N>SEMICOLON = L(";").suppress()<N>AT = L("@").suppress()<N><N>
PUNCTUATION = Word("-_.")<N>IDENTIFIER_END = ALPHANUM | (ZeroOrMore(PUNCTUATION) + ALPHANUM)<N>IDENTIFIER = Combine(ALPHANUM + ZeroOrMore(IDENTIFIER_END))<N><N>NAME = IDENTIFIER("name")<N>EXTRA = IDENTIFIER<N><N>URI = Regex(r"[^ ]+")("url")<N>URL = AT + URI<N><N>
EXTRAS_LIST = EXTRA + ZeroOrMore(COMMA + EXTRA)<N>EXTRAS = (LBRACKET + Optional(EXTRAS_LIST) + RBRACKET)("extras")<N><N>VERSION_PEP440 = Regex(Specifier._regex_str, re.VERBOSE | re.IGNORECASE)<N>VERSION_LEGACY = Regex(LegacySpecifier._regex_str, re.VERBOSE | re.IGNORECASE)<N><N>
VERSION_ONE = VERSION_PEP440 ^ VERSION_LEGACY<N>VERSION_MANY = Combine(<N>    VERSION_ONE + ZeroOrMore(COMMA + VERSION_ONE), joinString=",", adjacent=False<N>)("_raw_spec")<N>_VERSION_SPEC = Optional((LPAREN + VERSION_MANY + RPAREN) | VERSION_MANY)<N>_VERSION_SPEC.setParseAction(lambda s, l, t: t._raw_spec or "")<N><N>
VERSION_SPEC = originalTextFor(_VERSION_SPEC)("specifier")<N>VERSION_SPEC.setParseAction(lambda s, l, t: t[1])<N><N>MARKER_EXPR = originalTextFor(MARKER_EXPR())("marker")<N>MARKER_EXPR.setParseAction(<N>    lambda s, l, t: Marker(s[t._original_start : t._original_end])<N>)<N>MARKER_SEPARATOR = SEMICOLON<N>MARKER = MARKER_SEPARATOR + MARKER_EXPR<N><N>
VERSION_AND_MARKER = VERSION_SPEC + Optional(MARKER)<N>URL_AND_MARKER = URL + Optional(MARKER)<N><N>NAMED_REQUIREMENT = NAME + Optional(EXTRAS) + (URL_AND_MARKER | VERSION_AND_MARKER)<N><N>REQUIREMENT = stringStart + NAMED_REQUIREMENT + stringEnd<N># setuptools.extern.pyparsing isn't thread safe during initialization, so we do it eagerly, see<N># issue #104<N>REQUIREMENT.parseString("x[]")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import abc<N>import functools<N>import itertools<N>import re<N>import warnings<N>from typing import (<N>    Callable,<N>    Dict,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Pattern,<N>    Set,<N>    Tuple,<N>    TypeVar,<N>    Union,<N>)<N><N>
from .utils import canonicalize_version<N>from .version import LegacyVersion, Version, parse<N><N>ParsedVersion = Union[Version, LegacyVersion]<N>UnparsedVersion = Union[Version, LegacyVersion, str]<N>VersionTypeVar = TypeVar("VersionTypeVar", bound=UnparsedVersion)<N>CallableOperator = Callable[[ParsedVersion, str], bool]<N><N>
<N>class InvalidSpecifier(ValueError):<N>    """<N>    An invalid specifier was found, users should refer to PEP 440.<N>    """<N><N><N>class BaseSpecifier(metaclass=abc.ABCMeta):<N>    @abc.abstractmethod<N>    def __str__(self) -> str:<N>        """<N>        Returns the str representation of this Specifier like object. This<N>        should be representative of the Specifier itself.<N>        """<N><N>
    @abc.abstractmethod<N>    def __hash__(self) -> int:<N>        """<N>        Returns a hash value for this Specifier like object.<N>        """<N><N>    @abc.abstractmethod<N>    def __eq__(self, other: object) -> bool:<N>        """<N>        Returns a boolean representing whether or not the two Specifier like<N>        objects are equal.<N>        """<N><N>
    @abc.abstractproperty<N>    def prereleases(self) -> Optional[bool]:<N>        """<N>        Returns whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>    @prereleases.setter<N>    def prereleases(self, value: bool) -> None:<N>        """<N>        Sets whether or not pre-releases as a whole are allowed by this<N>        specifier.<N>        """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import logging<N>import platform<N>import sys<N>import sysconfig<N>from importlib.machinery import EXTENSION_SUFFIXES<N>from typing import (<N>    Dict,<N>    FrozenSet,<N>    Iterable,<N>    Iterator,<N>    List,<N>    Optional,<N>    Sequence,<N>    Tuple,<N>    Union,<N>    cast,<N>)<N><N>
from . import _manylinux, _musllinux<N><N>logger = logging.getLogger(__name__)<N><N>PythonVersion = Sequence[int]<N>MacVersion = Tuple[int, int]<N><N>INTERPRETER_SHORT_NAMES: Dict[str, str] = {<N>    "python": "py",  # Generic.<N>    "cpython": "cp",<N>    "pypy": "pp",<N>    "ironpython": "ip",<N>    "jython": "jy",<N>}<N><N>
<N>_32_BIT_INTERPRETER = sys.maxsize <= 2 ** 32<N><N><N>class Tag:<N>    """<N>    A representation of the tag triple for a wheel.<N><N>    Instances are considered immutable and thus are hashable. Equality checking<N>    is also supported.<N>    """<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import re<N>from typing import FrozenSet, NewType, Tuple, Union, cast<N><N>
from .tags import Tag, parse_tag<N>from .version import InvalidVersion, Version<N><N>BuildTag = Union[Tuple[()], Tuple[int, str]]<N>NormalizedName = NewType("NormalizedName", str)<N><N><N>class InvalidWheelFilename(ValueError):<N>    """<N>    An invalid wheel filename was found, users should refer to PEP 427.<N>    """<N><N>
<N>class InvalidSdistFilename(ValueError):<N>    """<N>    An invalid sdist filename was found, users should refer to the packaging user guide.<N>    """<N><N><N>_canonicalize_regex = re.compile(r"[-_.]+")<N># PEP 427: The build number must start with a digit.<N>_build_tag_regex = re.compile(r"(\d+)(.*)")<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>import collections<N>import itertools<N>import re<N>import warnings<N>from typing import Callable, Iterator, List, Optional, SupportsInt, Tuple, Union<N><N>
"""PEP 656 support.<N><N>This module implements logic to detect if the currently running Python is<N>linked against musl, and what musl version is used.<N>"""<N><N>import contextlib<N>import functools<N>import operator<N>import os<N>import re<N>import struct<N>import subprocess<N>import sys<N>from typing import IO, Iterator, NamedTuple, Optional, Tuple<N><N>
<N>def _read_unpacked(f: IO[bytes], fmt: str) -> Tuple[int, ...]:<N>    return struct.unpack(fmt, f.read(struct.calcsize(fmt)))<N><N><N>def _parse_ld_musl_from_elf(f: IO[bytes]) -> Optional[str]:<N>    """Detect musl libc location by parsing the Python executable.<N><N>
    Based on: https://gist.github.com/lyssdod/f51579ae8d93c8657a5564aefc2ffbca<N>    ELF header: https://refspecs.linuxfoundation.org/elf/gabi4+/ch4.eheader.html<N>    """<N>    f.seek(0)<N>    try:<N>        ident = _read_unpacked(f, "16B")<N>    except struct.error:<N>        return None<N>    if ident[:4] != tuple(b"\x7fELF"):  # Invalid magic, not ELF.<N>        return None<N>    f.seek(struct.calcsize("HHI"), 1)  # Skip file type, machine, and version.<N><N>
    try:<N>        # e_fmt: Format for program header.<N>        # p_fmt: Format for section header.<N>        # p_idx: Indexes to find p_type, p_offset, and p_filesz.<N>        e_fmt, p_fmt, p_idx = {<N>            1: ("IIIIHHH", "IIIIIIII", (0, 1, 4)),  # 32-bit.<N>            2: ("QQQIHHH", "IIQQQQQQ", (0, 2, 5)),  # 64-bit.<N>        }[ident[4]]<N>    except KeyError:<N>        return None<N>    else:<N>        p_get = operator.itemgetter(*p_idx)<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N><N>class InfinityType:<N>    def __repr__(self) -> str:<N>        return "Infinity"<N><N>
    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return False<N><N>    def __le__(self, other: object) -> bool:<N>        return False<N><N>    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>
    def __gt__(self, other: object) -> bool:<N>        return True<N><N>    def __ge__(self, other: object) -> bool:<N>        return True<N><N>    def __neg__(self: object) -> "NegativeInfinityType":<N>        return NegativeInfinity<N><N><N>Infinity = InfinityType()<N><N>
<N>class NegativeInfinityType:<N>    def __repr__(self) -> str:<N>        return "-Infinity"<N><N>    def __hash__(self) -> int:<N>        return hash(repr(self))<N><N>    def __lt__(self, other: object) -> bool:<N>        return True<N><N>    def __le__(self, other: object) -> bool:<N>        return True<N><N>
    def __eq__(self, other: object) -> bool:<N>        return isinstance(other, self.__class__)<N><N>    def __gt__(self, other: object) -> bool:<N>        return False<N><N>    def __ge__(self, other: object) -> bool:<N>        return False<N><N>    def __neg__(self: object) -> InfinityType:<N>        return Infinity<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N><N>
__title__ = "packaging"<N>__summary__ = "Core utilities for Python packages"<N>__uri__ = "https://github.com/pypa/packaging"<N><N>__version__ = "21.3"<N><N>__author__ = "Donald Stufft and individual contributors"<N>__email__ = "donald@stufft.io"<N><N>
# This file is dual licensed under the terms of the Apache License, Version<N># 2.0, and the BSD License. See the LICENSE file in the root of this repository<N># for complete details.<N><N>from .__about__ import (<N>    __author__,<N>    __copyright__,<N>    __email__,<N>    __license__,<N>    __summary__,<N>    __title__,<N>    __uri__,<N>    __version__,<N>)<N><N>__all__ = [<N>    "__title__",<N>    "__summary__",<N>    "__uri__",<N>    "__version__",<N>    "__author__",<N>    "__email__",<N>    "__license__",<N>    "__copyright__",<N>]<N>
# SPDX-License-Identifier: MIT<N># SPDX-FileCopyrightText: 2021 Taneli Hukkinen<N># Licensed to PSF under a Contributor Agreement.<N><N>from __future__ import annotations<N><N>from collections.abc import Iterable<N>import string<N>from types import MappingProxyType<N>from typing import Any, BinaryIO, NamedTuple<N><N>
from ._re import (<N>    RE_DATETIME,<N>    RE_LOCALTIME,<N>    RE_NUMBER,<N>    match_to_datetime,<N>    match_to_localtime,<N>    match_to_number,<N>)<N>from ._types import Key, ParseFloat, Pos<N><N>ASCII_CTRL = frozenset(chr(i) for i in range(32)) | frozenset(chr(127))<N><N>
# Neither of these sets include quotation mark or backslash. They are<N># currently handled as separate cases in the parser functions.<N>ILLEGAL_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t")<N>ILLEGAL_MULTILINE_BASIC_STR_CHARS = ASCII_CTRL - frozenset("\t\n")<N><N>
ILLEGAL_LITERAL_STR_CHARS = ILLEGAL_BASIC_STR_CHARS<N>ILLEGAL_MULTILINE_LITERAL_STR_CHARS = ILLEGAL_MULTILINE_BASIC_STR_CHARS<N><N>ILLEGAL_COMMENT_CHARS = ILLEGAL_BASIC_STR_CHARS<N><N>TOML_WS = frozenset(" \t")<N>TOML_WS_AND_NEWLINE = TOML_WS | frozenset("\n")<N>BARE_KEY_CHARS = frozenset(string.ascii_letters + string.digits + "-_")<N>KEY_INITIAL_CHARS = BARE_KEY_CHARS | frozenset("\"'")<N>HEXDIGIT_CHARS = frozenset(string.hexdigits)<N><N>
BASIC_STR_ESCAPE_REPLACEMENTS = MappingProxyType(<N>    {<N>        "\\b": "\u0008",  # backspace<N>        "\\t": "\u0009",  # tab<N>        "\\n": "\u000A",  # linefeed<N>        "\\f": "\u000C",  # form feed<N>        "\\r": "\u000D",  # carriage return<N>        '\\"': "\u0022",  # quote<N>        "\\\\": "\u005C",  # backslash<N>    }<N>)<N><N>
<N>class TOMLDecodeError(ValueError):<N>    """An error raised if a document is not valid TOML."""<N><N><N>def load(__fp: BinaryIO, *, parse_float: ParseFloat = float) -> dict[str, Any]:<N>    """Parse TOML from a binary file object."""<N>    b = __fp.read()<N>    try:<N>        s = b.decode()<N>    except AttributeError:<N>        raise TypeError(<N>            "File must be opened in binary mode, e.g. use `open('foo.toml', 'rb')`"<N>        ) from None<N>    return loads(s, parse_float=parse_float)<N><N>
<N>def loads(__s: str, *, parse_float: ParseFloat = float) -> dict[str, Any]:  # noqa: C901<N>    """Parse TOML from a string."""<N><N>    # The spec allows converting "\r\n" to "\n", even in string<N>    # literals. Let's do so to simplify parsing.<N>    src = __s.replace("\r\n", "\n")<N>    pos = 0<N>    out = Output(NestedDict(), Flags())<N>    header: Key = ()<N>    parse_float = make_safe_parse_float(parse_float)<N><N>
# SPDX-License-Identifier: MIT<N># SPDX-FileCopyrightText: 2021 Taneli Hukkinen<N># Licensed to PSF under a Contributor Agreement.<N><N>from __future__ import annotations<N><N>from datetime import date, datetime, time, timedelta, timezone, tzinfo<N>from functools import lru_cache<N>import re<N>from typing import Any<N><N>
# SPDX-License-Identifier: MIT<N># SPDX-FileCopyrightText: 2021 Taneli Hukkinen<N># Licensed to PSF under a Contributor Agreement.<N><N>from typing import Any, Callable, Tuple<N><N># Type annotations<N>ParseFloat = Callable[[str], Any]<N>Key = Tuple[str, ...]<N>Pos = int<N>
# SPDX-License-Identifier: MIT<N># SPDX-FileCopyrightText: 2021 Taneli Hukkinen<N># Licensed to PSF under a Contributor Agreement.<N><N>__all__ = ("loads", "load", "TOMLDecodeError")<N>__version__ = "2.0.1"  # DO NOT EDIT THIS LINE MANUALLY. LET bump2version UTILITY DO IT<N><N>from ._parser import TOMLDecodeError, load, loads<N><N># Pretend this exception was created here.<N>TOMLDecodeError.__module__ = __name__<N>
from __future__ import absolute_import<N><N>import datetime<N>import logging<N>import os<N>import re<N>import socket<N>import warnings<N>from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>from .packages import six<N>from .packages.six.moves.http_client import HTTPConnection as _HTTPConnection<N>from .packages.six.moves.http_client import HTTPException  # noqa: F401<N>from .util.proxy import create_proxy_ssl_context<N><N>
try:  # Compiled with SSL?<N>    import ssl<N><N>    BaseSSLError = ssl.SSLError<N>except (ImportError, AttributeError):  # Platform-specific: No SSL.<N>    ssl = None<N><N>    class BaseSSLError(BaseException):<N>        pass<N><N><N>try:<N>    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.<N>    ConnectionError = ConnectionError<N>except NameError:<N>    # Python 2<N>    class ConnectionError(Exception):<N>        pass<N><N>
from __future__ import absolute_import<N><N>from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead<N><N># Base Exceptions<N><N><N>class HTTPError(Exception):<N>    """Base exception used by this module."""<N><N>    pass<N><N>
<N>class HTTPWarning(Warning):<N>    """Base warning used by this module."""<N><N>    pass<N><N><N>class PoolError(HTTPError):<N>    """Base exception for errors caused within a pool."""<N><N>    def __init__(self, pool, message):<N>        self.pool = pool<N>        HTTPError.__init__(self, "%s: %s" % (pool, message))<N><N>
    def __reduce__(self):<N>        # For pickling purposes.<N>        return self.__class__, (None, None)<N><N><N>class RequestError(PoolError):<N>    """Base exception for PoolErrors that have associated URLs."""<N><N>    def __init__(self, pool, url, message):<N>        self.url = url<N>        PoolError.__init__(self, pool, message)<N><N>
    def __reduce__(self):<N>        # For pickling purposes.<N>        return self.__class__, (None, self.url, None)<N><N><N>class SSLError(HTTPError):<N>    """Raised when SSL certificate fails in an HTTPS connection."""<N><N>    pass<N><N><N>class ProxyError(HTTPError):<N>    """Raised when the connection to a proxy fails."""<N><N>
    def __init__(self, message, error, *args):<N>        super(ProxyError, self).__init__(message, error, *args)<N>        self.original_error = error<N><N><N>class DecodeError(HTTPError):<N>    """Raised when automatic decoding based on Content-Type fails."""<N><N>
    pass<N><N><N>class ProtocolError(HTTPError):<N>    """Raised when something unexpected happens mid-request/response."""<N><N>    pass<N><N><N>#: Renamed to ProtocolError but aliased for backwards compatibility.<N>ConnectionError = ProtocolError<N><N>
<N># Leaf Exceptions<N><N><N>class MaxRetryError(RequestError):<N>    """Raised when the maximum number of retries is exceeded.<N><N>    :param pool: The connection pool<N>    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`<N>    :param string url: The requested Url<N>    :param exceptions.Exception reason: The underlying error<N><N>
    """<N><N>    def __init__(self, pool, url, reason=None):<N>        self.reason = reason<N><N>        message = "Max retries exceeded with url: %s (Caused by %r)" % (url, reason)<N><N>        RequestError.__init__(self, pool, url, message)<N><N><N>class HostChangedError(RequestError):<N>    """Raised when an existing pool gets a request for a foreign host."""<N><N>
    def __init__(self, pool, url, retries=3):<N>        message = "Tried to open a foreign host with url: %s" % url<N>        RequestError.__init__(self, pool, url, message)<N>        self.retries = retries<N><N><N>class TimeoutStateError(HTTPError):<N>    """Raised when passing an invalid state to a timeout"""<N><N>
    pass<N><N><N>class TimeoutError(HTTPError):<N>    """Raised when a socket timeout error occurs.<N><N>    Catching this error will catch both :exc:`ReadTimeoutErrors<N>    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.<N>    """<N><N>
    pass<N><N><N>class ReadTimeoutError(TimeoutError, RequestError):<N>    """Raised when a socket timeout occurs while receiving data from a server"""<N><N>    pass<N><N><N># This timeout error does not have a URL attached and needs to inherit from the<N># base HTTPError<N>class ConnectTimeoutError(TimeoutError):<N>    """Raised when a socket timeout occurs while connecting to a server"""<N><N>
    pass<N><N><N>class NewConnectionError(ConnectTimeoutError, PoolError):<N>    """Raised when we fail to establish a new connection. Usually ECONNREFUSED."""<N><N>    pass<N><N><N>class EmptyPoolError(PoolError):<N>    """Raised when a pool runs out of connections and no more are allowed."""<N><N>
    pass<N><N><N>class ClosedPoolError(PoolError):<N>    """Raised when a request enters a pool after the pool has been closed."""<N><N>    pass<N><N><N>class LocationValueError(ValueError, HTTPError):<N>    """Raised when there is something wrong with a given URL input."""<N><N>
    pass<N><N><N>class LocationParseError(LocationValueError):<N>    """Raised when get_host or similar fails to parse the URL input."""<N><N>    def __init__(self, location):<N>        message = "Failed to parse: %s" % location<N>        HTTPError.__init__(self, message)<N><N>
        self.location = location<N><N><N>class URLSchemeUnknown(LocationValueError):<N>    """Raised when a URL input has an unsupported scheme."""<N><N>    def __init__(self, scheme):<N>        message = "Not supported URL scheme %s" % scheme<N>        super(URLSchemeUnknown, self).__init__(message)<N><N>
        self.scheme = scheme<N><N><N>class ResponseError(HTTPError):<N>    """Used as a container for an error reason supplied in a MaxRetryError."""<N><N>    GENERIC_ERROR = "too many error responses"<N>    SPECIFIC_ERROR = "too many {status_code} error responses"<N><N>
<N>class SecurityWarning(HTTPWarning):<N>    """Warned when performing security reducing actions"""<N><N>    pass<N><N><N>class SubjectAltNameWarning(SecurityWarning):<N>    """Warned when connecting to a host with a certificate missing a SAN."""<N><N>
    pass<N><N><N>class InsecureRequestWarning(SecurityWarning):<N>    """Warned when making an unverified HTTPS request."""<N><N>    pass<N><N><N>class SystemTimeWarning(SecurityWarning):<N>    """Warned when system time is suspected to be wrong"""<N><N>
    pass<N><N><N>class InsecurePlatformWarning(SecurityWarning):<N>    """Warned when certain TLS/SSL configuration is not available on a platform."""<N><N>    pass<N><N><N>class SNIMissingWarning(HTTPWarning):<N>    """Warned when making a HTTPS request without SNI available."""<N><N>
    pass<N><N><N>class DependencyWarning(HTTPWarning):<N>    """<N>    Warned when an attempt is made to import a module with missing optional<N>    dependencies.<N>    """<N><N>    pass<N><N><N>class ResponseNotChunked(ProtocolError, ValueError):<N>    """Response needs to be chunked in order to read it as chunks."""<N><N>
    pass<N><N><N>class BodyNotHttplibCompatible(HTTPError):<N>    """<N>    Body should be :class:`http.client.HTTPResponse` like<N>    (have an fp attribute which returns raw chunks) for read_chunked().<N>    """<N><N>    pass<N><N><N>class IncompleteRead(HTTPError, httplib_IncompleteRead):<N>    """<N>    Response length doesn't match expected Content-Length<N><N>
    Subclass of :class:`http.client.IncompleteRead` to allow int value<N>    for ``partial`` to avoid creating large objects on streamed reads.<N>    """<N><N>    def __init__(self, partial, expected):<N>        super(IncompleteRead, self).__init__(partial, expected)<N><N>
    def __repr__(self):<N>        return "IncompleteRead(%i bytes read, %i more expected)" % (<N>            self.partial,<N>            self.expected,<N>        )<N><N><N>class InvalidChunkLength(HTTPError, httplib_IncompleteRead):<N>    """Invalid chunk length in a chunked response."""<N><N>
    def __init__(self, response, length):<N>        super(InvalidChunkLength, self).__init__(<N>            response.tell(), response.length_remaining<N>        )<N>        self.response = response<N>        self.length = length<N><N>    def __repr__(self):<N>        return "InvalidChunkLength(got length %r, %i bytes read)" % (<N>            self.length,<N>            self.partial,<N>        )<N><N>
<N>class InvalidHeader(HTTPError):<N>    """The header provided was somehow invalid."""<N><N>    pass<N><N><N>class ProxySchemeUnknown(AssertionError, URLSchemeUnknown):<N>    """ProxyManager does not support the supplied scheme"""<N><N>    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.<N><N>
from __future__ import absolute_import<N><N>import email.utils<N>import mimetypes<N>import re<N><N>from .packages import six<N><N><N>def guess_content_type(filename, default="application/octet-stream"):<N>    """<N>    Guess the "Content-Type" of a file.<N><N>
    :param filename:<N>        The filename to guess the "Content-Type" of using :mod:`mimetypes`.<N>    :param default:<N>        If no "Content-Type" can be guessed, default to `default`.<N>    """<N>    if filename:<N>        return mimetypes.guess_type(filename)[0] or default<N>    return default<N><N>
<N>def format_header_param_rfc2231(name, value):<N>    """<N>    Helper function to format and quote a single header parameter using the<N>    strategy defined in RFC 2231.<N><N>    Particularly useful for header parameters which might contain<N>    non-ASCII values, like file names. This follows<N>    `RFC 2388 Section 4.4 <https://tools.ietf.org/html/rfc2388#section-4.4>`_.<N><N>
    :param name:<N>        The name of the parameter, a string expected to be ASCII only.<N>    :param value:<N>        The value of the parameter, provided as ``bytes`` or `str``.<N>    :ret:<N>        An RFC-2231-formatted unicode string.<N>    """<N>    if isinstance(value, six.binary_type):<N>        value = value.decode("utf-8")<N><N>
    if not any(ch in value for ch in '"\\\r\n'):<N>        result = u'%s="%s"' % (name, value)<N>        try:<N>            result.encode("ascii")<N>        except (UnicodeEncodeError, UnicodeDecodeError):<N>            pass<N>        else:<N>            return result<N><N>
    if six.PY2:  # Python 2:<N>        value = value.encode("utf-8")<N><N>    # encode_rfc2231 accepts an encoded string and returns an ascii-encoded<N>    # string in Python 2 but accepts and returns unicode strings in Python 3<N>    value = email.utils.encode_rfc2231(value, "utf-8")<N>    value = "%s*=%s" % (name, value)<N><N>
    if six.PY2:  # Python 2:<N>        value = value.decode("utf-8")<N><N>    return value<N><N><N>_HTML5_REPLACEMENTS = {<N>    u"\u0022": u"%22",<N>    # Replace "\" with "\\".<N>    u"\u005C": u"\u005C\u005C",<N>}<N><N># All control characters from 0x00 to 0x1F *except* 0x1B.<N>_HTML5_REPLACEMENTS.update(<N>    {<N>        six.unichr(cc): u"%{:02X}".format(cc)<N>        for cc in range(0x00, 0x1F + 1)<N>        if cc not in (0x1B,)<N>    }<N>)<N><N>
<N>def _replace_multiple(value, needles_and_replacements):<N>    def replacer(match):<N>        return needles_and_replacements[match.group(0)]<N><N>    pattern = re.compile(<N>        r"|".join([re.escape(needle) for needle in needles_and_replacements.keys()])<N>    )<N><N>
    result = pattern.sub(replacer, value)<N><N>    return result<N><N><N>def format_header_param_html5(name, value):<N>    """<N>    Helper function to format and quote a single header parameter using the<N>    HTML5 strategy.<N><N>    Particularly useful for header parameters which might contain<N>    non-ASCII values, like file names. This follows the `HTML5 Working Draft<N>    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.<N><N>
    .. _HTML5 Working Draft Section 4.10.22.7:<N>        https://w3c.github.io/html/sec-forms.html#multipart-form-data<N><N>    :param name:<N>        The name of the parameter, a string expected to be ASCII only.<N>    :param value:<N>        The value of the parameter, provided as ``bytes`` or `str``.<N>    :ret:<N>        A unicode string, stripped of troublesome characters.<N>    """<N>    if isinstance(value, six.binary_type):<N>        value = value.decode("utf-8")<N><N>
    value = _replace_multiple(value, _HTML5_REPLACEMENTS)<N><N>    return u'%s="%s"' % (name, value)<N><N><N># For backwards-compatibility.<N>format_header_param = format_header_param_html5<N><N><N>class RequestField(object):<N>    """<N>    A data container for request body parameters.<N><N>
    :param name:<N>        The name of this request field. Must be unicode.<N>    :param data:<N>        The data/value body.<N>    :param filename:<N>        An optional filename of the request field. Must be unicode.<N>    :param headers:<N>        An optional dict-like object of headers to initially use for the field.<N>    :param header_formatter:<N>        An optional callable that is used to encode and format the headers. By<N>        default, this is :func:`format_header_param_html5`.<N>    """<N><N>
    def __init__(<N>        self,<N>        name,<N>        data,<N>        filename=None,<N>        headers=None,<N>        header_formatter=format_header_param_html5,<N>    ):<N>        self._name = name<N>        self._filename = filename<N>        self.data = data<N>        self.headers = {}<N>        if headers:<N>            self.headers = dict(headers)<N>        self.header_formatter = header_formatter<N><N>
    @classmethod<N>    def from_tuples(cls, fieldname, value, header_formatter=format_header_param_html5):<N>        """<N>        A :class:`~urllib3.fields.RequestField` factory from old-style tuple parameters.<N><N>        Supports constructing :class:`~urllib3.fields.RequestField` from<N>        parameter of key/value strings AND key/filetuple. A filetuple is a<N>        (filename, data, MIME type) tuple where the MIME type is optional.<N>        For example::<N><N>
            'foo': 'bar',<N>            'fakefile': ('foofile.txt', 'contents of foofile'),<N>            'realfile': ('barfile.txt', open('realfile').read()),<N>            'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'),<N>            'nonamefile': 'contents of nonamefile field',<N><N>
        Field names and filenames must be unicode.<N>        """<N>        if isinstance(value, tuple):<N>            if len(value) == 3:<N>                filename, data, content_type = value<N>            else:<N>                filename, data = value<N>                content_type = guess_content_type(filename)<N>        else:<N>            filename = None<N>            content_type = None<N>            data = value<N><N>
        request_param = cls(<N>            fieldname, data, filename=filename, header_formatter=header_formatter<N>        )<N>        request_param.make_multipart(content_type=content_type)<N><N>        return request_param<N><N>    def _render_part(self, name, value):<N>        """<N>        Overridable helper function to format a single header parameter. By<N>        default, this calls ``self.header_formatter``.<N><N>
        :param name:<N>            The name of the parameter, a string expected to be ASCII only.<N>        :param value:<N>            The value of the parameter, provided as a unicode string.<N>        """<N><N>        return self.header_formatter(name, value)<N><N>
from __future__ import absolute_import<N><N>import binascii<N>import codecs<N>import os<N>from io import BytesIO<N><N>from .fields import RequestField<N>from .packages import six<N>from .packages.six import b<N><N>writer = codecs.lookup("utf-8")[3]<N><N>
<N>def choose_boundary():<N>    """<N>    Our embarrassingly-simple replacement for mimetools.choose_boundary.<N>    """<N>    boundary = binascii.hexlify(os.urandom(16))<N>    if not six.PY2:<N>        boundary = boundary.decode("ascii")<N>    return boundary<N><N>
<N>def iter_field_objects(fields):<N>    """<N>    Iterate over fields.<N><N>    Supports list of (k, v) tuples and dicts, and lists of<N>    :class:`~urllib3.fields.RequestField`.<N><N>    """<N>    if isinstance(fields, dict):<N>        i = six.iteritems(fields)<N>    else:<N>        i = iter(fields)<N><N>
    for field in i:<N>        if isinstance(field, RequestField):<N>            yield field<N>        else:<N>            yield RequestField.from_tuples(*field)<N><N><N>def iter_fields(fields):<N>    """<N>    .. deprecated:: 1.6<N><N>    Iterate over fields.<N><N>
    The addition of :class:`~urllib3.fields.RequestField` makes this function<N>    obsolete. Instead, use :func:`iter_field_objects`, which returns<N>    :class:`~urllib3.fields.RequestField` objects.<N><N>    Supports list of (k, v) tuples and dicts.<N>    """<N>    if isinstance(fields, dict):<N>        return ((k, v) for k, v in six.iteritems(fields))<N><N>
    return ((k, v) for k, v in fields)<N><N><N>def encode_multipart_formdata(fields, boundary=None):<N>    """<N>    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.<N><N>    :param fields:<N>        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).<N><N>
    :param boundary:<N>        If not specified, then a random boundary will be generated using<N>        :func:`urllib3.filepost.choose_boundary`.<N>    """<N>    body = BytesIO()<N>    if boundary is None:<N>        boundary = choose_boundary()<N><N>
    for field in iter_field_objects(fields):<N>        body.write(b("--%s\r\n" % (boundary)))<N><N>        writer(body).write(field.render_headers())<N>        data = field.data<N><N>        if isinstance(data, int):<N>            data = str(data)  # Backwards compatibility<N><N>
        if isinstance(data, six.text_type):<N>            writer(body).write(data)<N>        else:<N>            body.write(data)<N><N>        body.write(b"\r\n")<N><N>    body.write(b("--%s--\r\n" % (boundary)))<N><N>    content_type = str("multipart/form-data; boundary=%s" % boundary)<N><N>
from __future__ import absolute_import<N><N>from .filepost import encode_multipart_formdata<N>from .packages.six.moves.urllib.parse import urlencode<N><N>__all__ = ["RequestMethods"]<N><N><N>class RequestMethods(object):<N>    """<N>    Convenience mixin for classes who implement a :meth:`urlopen` method, such<N>    as :class:`urllib3.HTTPConnectionPool` and<N>    :class:`urllib3.PoolManager`.<N><N>
    Provides behavior for making common types of HTTP request methods and<N>    decides which type of request field encoding to use.<N><N>    Specifically,<N><N>    :meth:`.request_encode_url` is for sending requests whose fields are<N>    encoded in the URL (such as GET, HEAD, DELETE).<N><N>
    :meth:`.request_encode_body` is for sending requests whose fields are<N>    encoded in the *body* of the request using multipart or www-form-urlencoded<N>    (such as for POST, PUT, PATCH).<N><N>    :meth:`.request` is for making any kind of request, it will look up the<N>    appropriate encoding format and use one of the above two methods to make<N>    the request.<N><N>
    Initializer parameters:<N><N>    :param headers:<N>        Headers to include with all requests, unless other headers are given<N>        explicitly.<N>    """<N><N>    _encode_url_methods = {"DELETE", "GET", "HEAD", "OPTIONS"}<N><N>    def __init__(self, headers=None):<N>        self.headers = headers or {}<N><N>
    def urlopen(<N>        self,<N>        method,<N>        url,<N>        body=None,<N>        headers=None,<N>        encode_multipart=True,<N>        multipart_boundary=None,<N>        **kw<N>    ):  # Abstract<N>        raise NotImplementedError(<N>            "Classes extending RequestMethods must implement "<N>            "their own ``urlopen`` method."<N>        )<N><N>
from __future__ import absolute_import<N><N>import io<N>import logging<N>import zlib<N>from contextlib import contextmanager<N>from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>try:<N>    try:<N>        import brotlicffi as brotli<N>    except ImportError:<N>        import brotli<N>except ImportError:<N>    brotli = None<N><N>
from ._collections import HTTPHeaderDict<N>from .connection import BaseSSLError, HTTPException<N>from .exceptions import (<N>    BodyNotHttplibCompatible,<N>    DecodeError,<N>    HTTPError,<N>    IncompleteRead,<N>    InvalidChunkLength,<N>    InvalidHeader,<N>    ProtocolError,<N>    ReadTimeoutError,<N>    ResponseNotChunked,<N>    SSLError,<N>)<N>from .packages import six<N>from .util.response import is_fp_closed, is_response_to_head<N><N>
log = logging.getLogger(__name__)<N><N><N>class DeflateDecoder(object):<N>    def __init__(self):<N>        self._first_try = True<N>        self._data = b""<N>        self._obj = zlib.decompressobj()<N><N>    def __getattr__(self, name):<N>        return getattr(self._obj, name)<N><N>
from __future__ import absolute_import<N><N>try:<N>    from collections.abc import Mapping, MutableMapping<N>except ImportError:<N>    from collections import Mapping, MutableMapping<N>try:<N>    from threading import RLock<N>except ImportError:  # Platform-specific: No threads available<N><N>
    class RLock:<N>        def __enter__(self):<N>            pass<N><N>        def __exit__(self, exc_type, exc_value, traceback):<N>            pass<N><N><N>from collections import OrderedDict<N><N>from .exceptions import InvalidHeader<N>from .packages import six<N>from .packages.six import iterkeys, itervalues<N><N>
__all__ = ["RecentlyUsedContainer", "HTTPHeaderDict"]<N><N><N>_Null = object()<N><N><N>class RecentlyUsedContainer(MutableMapping):<N>    """<N>    Provides a thread-safe dict-like container which maintains up to<N>    ``maxsize`` keys while throwing away the least-recently-used keys beyond<N>    ``maxsize``.<N><N>
    :param maxsize:<N>        Maximum number of recent elements to retain.<N><N>    :param dispose_func:<N>        Every time an item is evicted from the container,<N>        ``dispose_func(value)`` is called.  Callback which will get called<N>    """<N><N>
    ContainerCls = OrderedDict<N><N>    def __init__(self, maxsize=10, dispose_func=None):<N>        self._maxsize = maxsize<N>        self.dispose_func = dispose_func<N><N>        self._container = self.ContainerCls()<N>        self.lock = RLock()<N><N>
    def __getitem__(self, key):<N>        # Re-insert the item, moving it to the end of the eviction line.<N>        with self.lock:<N>            item = self._container.pop(key)<N>            self._container[key] = item<N>            return item<N><N>
    def __setitem__(self, key, value):<N>        evicted_value = _Null<N>        with self.lock:<N>            # Possibly evict the existing value of 'key'<N>            evicted_value = self._container.get(key, _Null)<N>            self._container[key] = value<N><N>
            # If we didn't evict an existing value, we might have to evict the<N>            # least recently used item from the beginning of the container.<N>            if len(self._container) > self._maxsize:<N>                _key, evicted_value = self._container.popitem(last=False)<N><N>
        if self.dispose_func and evicted_value is not _Null:<N>            self.dispose_func(evicted_value)<N><N>    def __delitem__(self, key):<N>        with self.lock:<N>            value = self._container.pop(key)<N><N>        if self.dispose_func:<N>            self.dispose_func(value)<N><N>
    def __len__(self):<N>        with self.lock:<N>            return len(self._container)<N><N>    def __iter__(self):<N>        raise NotImplementedError(<N>            "Iteration over this class is unlikely to be threadsafe."<N>        )<N><N>    def clear(self):<N>        with self.lock:<N>            # Copy pointers to all values, then wipe the mapping<N>            values = list(itervalues(self._container))<N>            self._container.clear()<N><N>
        if self.dispose_func:<N>            for value in values:<N>                self.dispose_func(value)<N><N>    def keys(self):<N>        with self.lock:<N>            return list(iterkeys(self._container))<N><N><N>class HTTPHeaderDict(MutableMapping):<N>    """<N>    :param headers:<N>        An iterable of field-value pairs. Must not contain multiple field names<N>        when compared case-insensitively.<N><N>
    :param kwargs:<N>        Additional field-value pairs to pass in to ``dict.update``.<N><N>    A ``dict`` like container for storing HTTP Headers.<N><N>    Field names are stored and compared case-insensitively in compliance with<N>    RFC 7230. Iteration provides the first case-sensitive key seen for each<N>    case-insensitive pair.<N><N>
    Using ``__setitem__`` syntax overwrites fields that compare equal<N>    case-insensitively in order to maintain ``dict``'s api. For fields that<N>    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``<N>    in a loop.<N><N>    If multiple fields that are equal case-insensitively are passed to the<N>    constructor or ``.update``, the behavior is undefined and some will be<N>    lost.<N><N>
    >>> headers = HTTPHeaderDict()<N>    >>> headers.add('Set-Cookie', 'foo=bar')<N>    >>> headers.add('set-cookie', 'baz=quxx')<N>    >>> headers['content-length'] = '7'<N>    >>> headers['SET-cookie']<N>    'foo=bar, baz=quxx'<N>    >>> headers['Content-Length']<N>    '7'<N>    """<N><N>
    def __init__(self, headers=None, **kwargs):<N>        super(HTTPHeaderDict, self).__init__()<N>        self._container = OrderedDict()<N>        if headers is not None:<N>            if isinstance(headers, HTTPHeaderDict):<N>                self._copy_from(headers)<N>            else:<N>                self.extend(headers)<N>        if kwargs:<N>            self.extend(kwargs)<N><N>
    def __setitem__(self, key, val):<N>        self._container[key.lower()] = [key, val]<N>        return self._container[key.lower()]<N><N>    def __getitem__(self, key):<N>        val = self._container[key.lower()]<N>        return ", ".join(val[1:])<N><N>
"""<N>Python HTTP library with thread-safe connection pooling, file post support, user friendly, and more<N>"""<N>from __future__ import absolute_import<N><N># Set default logging handler to avoid "No handler found" warnings.<N>import logging<N>import warnings<N>from logging import NullHandler<N><N>
from . import exceptions<N>from ._version import __version__<N>from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url<N>from .filepost import encode_multipart_formdata<N>from .poolmanager import PoolManager, ProxyManager, proxy_from_url<N>from .response import HTTPResponse<N>from .util.request import make_headers<N>from .util.retry import Retry<N>from .util.timeout import Timeout<N>from .util.url import get_host<N><N>
__author__ = "Andrey Petrov (andrey.petrov@shazow.net)"<N>__license__ = "MIT"<N>__version__ = __version__<N><N>__all__ = (<N>    "HTTPConnectionPool",<N>    "HTTPSConnectionPool",<N>    "PoolManager",<N>    "ProxyManager",<N>    "HTTPResponse",<N>    "Retry",<N>    "Timeout",<N>    "add_stderr_logger",<N>    "connection_from_url",<N>    "disable_warnings",<N>    "encode_multipart_formdata",<N>    "get_host",<N>    "make_headers",<N>    "proxy_from_url",<N>)<N><N>
"""<N>This module provides a pool manager that uses Google App Engine's<N>`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.<N><N>Example usage::<N><N>    from urllib3 import PoolManager<N>    from urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox<N><N>
    if is_appengine_sandbox():<N>        # AppEngineManager uses AppEngine's URLFetch API behind the scenes<N>        http = AppEngineManager()<N>    else:<N>        # PoolManager uses a socket-level API behind the scenes<N>        http = PoolManager()<N><N>
    r = http.request('GET', 'https://google.com/')<N><N>There are `limitations <https://cloud.google.com/appengine/docs/python/\<N>urlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be<N>the best choice for your application. There are three options for using<N>urllib3 on Google App Engine:<N><N>
"""<N>NTLM authenticating pool, contributed by erikcederstran<N><N>Issue #10, see: http://code.google.com/p/urllib3/issues/detail?id=10<N>"""<N>from __future__ import absolute_import<N><N>import warnings<N>from logging import getLogger<N><N>from ntlm import ntlm<N><N>
from .. import HTTPSConnectionPool<N>from ..packages.six.moves.http_client import HTTPSConnection<N><N>warnings.warn(<N>    "The 'urllib3.contrib.ntlmpool' module is deprecated and will be removed "<N>    "in urllib3 v2.0 release, urllib3 is not able to support it properly due "<N>    "to reasons listed in issue: https://github.com/urllib3/urllib3/issues/2282. "<N>    "If you are a user of this module please comment in the mentioned issue.",<N>    DeprecationWarning,<N>)<N><N>
"""<N>TLS with SNI_-support for Python 2. Follow these instructions if you would<N>like to verify TLS certificates in Python 2. Note, the default libraries do<N>*not* do certificate checking; you need to do additional work to validate<N>certificates yourself.<N><N>
This needs the following packages installed:<N><N>* `pyOpenSSL`_ (tested with 16.0.0)<N>* `cryptography`_ (minimum 1.3.4, from pyopenssl)<N>* `idna`_ (minimum 2.0, from cryptography)<N><N>However, pyopenssl depends on cryptography, which depends on idna, so while we<N>use all three directly here we end up having relatively few packages required.<N><N>
You can install them with the following command:<N><N>.. code-block:: bash<N><N>    $ python -m pip install pyopenssl cryptography idna<N><N>To activate certificate checking, call<N>:func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code<N>before you begin making HTTP requests. This can be done in a ``sitecustomize``<N>module, or at any other time before your application begins using ``urllib3``,<N>like this:<N><N>
.. code-block:: python<N><N>    try:<N>        import urllib3.contrib.pyopenssl<N>        urllib3.contrib.pyopenssl.inject_into_urllib3()<N>    except ImportError:<N>        pass<N><N>Now you can use :mod:`urllib3` as you normally would, and it will support SNI<N>when the required modules are installed.<N><N>
Activating this module also has the positive side effect of disabling SSL/TLS<N>compression in Python 2 (see `CRIME attack`_).<N><N>.. _sni: https://en.wikipedia.org/wiki/Server_Name_Indication<N>.. _crime attack: https://en.wikipedia.org/wiki/CRIME_(security_exploit)<N>.. _pyopenssl: https://www.pyopenssl.org<N>.. _cryptography: https://cryptography.io<N>.. _idna: https://github.com/kjd/idna<N>"""<N>from __future__ import absolute_import<N><N>
import OpenSSL.SSL<N>from cryptography import x509<N>from cryptography.hazmat.backends.openssl import backend as openssl_backend<N>from cryptography.hazmat.backends.openssl.x509 import _Certificate<N><N>try:<N>    from cryptography.x509 import UnsupportedExtension<N>except ImportError:<N>    # UnsupportedExtension is gone in cryptography >= 2.1.0<N>    class UnsupportedExtension(Exception):<N>        pass<N><N>
<N>from io import BytesIO<N>from socket import error as SocketError<N>from socket import timeout<N><N>try:  # Platform-specific: Python 2<N>    from socket import _fileobject<N>except ImportError:  # Platform-specific: Python 3<N>    _fileobject = None<N>    from ..packages.backports.makefile import backport_makefile<N><N>
import logging<N>import ssl<N>import sys<N><N>from .. import util<N>from ..packages import six<N>from ..util.ssl_ import PROTOCOL_TLS_CLIENT<N><N>__all__ = ["inject_into_urllib3", "extract_from_urllib3"]<N><N># SNI always works.<N>HAS_SNI = True<N><N>
# Map from urllib3 to PyOpenSSL compatible parameter-values.<N>_openssl_versions = {<N>    util.PROTOCOL_TLS: OpenSSL.SSL.SSLv23_METHOD,<N>    PROTOCOL_TLS_CLIENT: OpenSSL.SSL.SSLv23_METHOD,<N>    ssl.PROTOCOL_TLSv1: OpenSSL.SSL.TLSv1_METHOD,<N>}<N><N>
if hasattr(ssl, "PROTOCOL_SSLv3") and hasattr(OpenSSL.SSL, "SSLv3_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_SSLv3] = OpenSSL.SSL.SSLv3_METHOD<N><N>if hasattr(ssl, "PROTOCOL_TLSv1_1") and hasattr(OpenSSL.SSL, "TLSv1_1_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_TLSv1_1] = OpenSSL.SSL.TLSv1_1_METHOD<N><N>
if hasattr(ssl, "PROTOCOL_TLSv1_2") and hasattr(OpenSSL.SSL, "TLSv1_2_METHOD"):<N>    _openssl_versions[ssl.PROTOCOL_TLSv1_2] = OpenSSL.SSL.TLSv1_2_METHOD<N><N><N>_stdlib_to_openssl_verify = {<N>    ssl.CERT_NONE: OpenSSL.SSL.VERIFY_NONE,<N>    ssl.CERT_OPTIONAL: OpenSSL.SSL.VERIFY_PEER,<N>    ssl.CERT_REQUIRED: OpenSSL.SSL.VERIFY_PEER<N>    + OpenSSL.SSL.VERIFY_FAIL_IF_NO_PEER_CERT,<N>}<N>_openssl_to_stdlib_verify = dict((v, k) for k, v in _stdlib_to_openssl_verify.items())<N><N>
# OpenSSL will only write 16K at a time<N>SSL_WRITE_BLOCKSIZE = 16384<N><N>orig_util_HAS_SNI = util.HAS_SNI<N>orig_util_SSLContext = util.ssl_.SSLContext<N><N><N>log = logging.getLogger(__name__)<N><N><N>def inject_into_urllib3():<N>    "Monkey-patch urllib3 with PyOpenSSL-backed SSL-support."<N><N>
    _validate_dependencies_met()<N><N>    util.SSLContext = PyOpenSSLContext<N>    util.ssl_.SSLContext = PyOpenSSLContext<N>    util.HAS_SNI = HAS_SNI<N>    util.ssl_.HAS_SNI = HAS_SNI<N>    util.IS_PYOPENSSL = True<N>    util.ssl_.IS_PYOPENSSL = True<N><N>
<N>def extract_from_urllib3():<N>    "Undo monkey-patching by :func:`inject_into_urllib3`."<N><N>    util.SSLContext = orig_util_SSLContext<N>    util.ssl_.SSLContext = orig_util_SSLContext<N>    util.HAS_SNI = orig_util_HAS_SNI<N>    util.ssl_.HAS_SNI = orig_util_HAS_SNI<N>    util.IS_PYOPENSSL = False<N>    util.ssl_.IS_PYOPENSSL = False<N><N>
<N>def _validate_dependencies_met():<N>    """<N>    Verifies that PyOpenSSL's package-level dependencies have been met.<N>    Throws `ImportError` if they are not met.<N>    """<N>    # Method added in `cryptography==1.1`; not available in older versions<N>    from cryptography.x509.extensions import Extensions<N><N>
    if getattr(Extensions, "get_extension_for_class", None) is None:<N>        raise ImportError(<N>            "'cryptography' module missing required functionality.  "<N>            "Try upgrading to v1.3.4 or newer."<N>        )<N><N>    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509<N>    # attribute is only present on those versions.<N>    from OpenSSL.crypto import X509<N><N>
    x509 = X509()<N>    if getattr(x509, "_x509", None) is None:<N>        raise ImportError(<N>            "'pyOpenSSL' module missing required functionality. "<N>            "Try upgrading to v0.14 or newer."<N>        )<N><N><N>def _dnsname_to_stdlib(name):<N>    """<N>    Converts a dNSName SubjectAlternativeName field to the form used by the<N>    standard library on the given Python version.<N><N>
    Cryptography produces a dNSName as a unicode string that was idna-decoded<N>    from ASCII bytes. We need to idna-encode that string to get it back, and<N>    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib<N>    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).<N><N>
    If the name cannot be idna-encoded then we return None signalling that<N>    the name given should be skipped.<N>    """<N><N>    def idna_encode(name):<N>        """<N>        Borrowed wholesale from the Python Cryptography Project. It turns out<N>        that we can't just safely call `idna.encode`: it can explode for<N>        wildcard names. This avoids that problem.<N>        """<N>        import idna<N><N>
        try:<N>            for prefix in [u"*.", u"."]:<N>                if name.startswith(prefix):<N>                    name = name[len(prefix) :]<N>                    return prefix.encode("ascii") + idna.encode(name)<N>            return idna.encode(name)<N>        except idna.core.IDNAError:<N>            return None<N><N>
    # Don't send IPv6 addresses through the IDNA encoder.<N>    if ":" in name:<N>        return name<N><N>    name = idna_encode(name)<N>    if name is None:<N>        return None<N>    elif sys.version_info >= (3, 0):<N>        name = name.decode("utf-8")<N>    return name<N><N>
"""<N>SecureTranport support for urllib3 via ctypes.<N><N>This makes platform-native TLS available to urllib3 users on macOS without the<N>use of a compiler. This is an important feature because the Python Package<N>Index is moving to become a TLSv1.2-or-higher server, and the default OpenSSL<N>that ships with macOS is not capable of doing TLSv1.2. The only way to resolve<N>this is to give macOS users an alternative solution to the problem, and that<N>solution is to use SecureTransport.<N><N>
We use ctypes here because this solution must not require a compiler. That's<N>because pip is not allowed to require a compiler either.<N><N>This is not intended to be a seriously long-term solution to this problem.<N>The hope is that PEP 543 will eventually solve this issue for us, at which<N>point we can retire this contrib module. But in the short term, we need to<N>solve the impending tire fire that is Python on Mac without this kind of<N>contrib module. So...here we are.<N><N>
To use this module, simply import and inject it::<N><N>    import urllib3.contrib.securetransport<N>    urllib3.contrib.securetransport.inject_into_urllib3()<N><N>Happy TLSing!<N><N>This code is a bastardised version of the code found in Will Bond's oscrypto<N>library. An enormous debt is owed to him for blazing this trail for us. For<N>that reason, this code should be considered to be covered both by urllib3's<N>license and by oscrypto's:<N><N>
# -*- coding: utf-8 -*-<N>"""<N>This module contains provisional support for SOCKS proxies from within<N>urllib3. This module supports SOCKS4, SOCKS4A (an extension of SOCKS4), and<N>SOCKS5. To enable its functionality, either install PySocks or install this<N>module with the ``socks`` extra.<N><N>
The SOCKS implementation supports the full range of urllib3 features. It also<N>supports the following SOCKS features:<N><N>- SOCKS4A (``proxy_url='socks4a://...``)<N>- SOCKS4 (``proxy_url='socks4://...``)<N>- SOCKS5 with remote DNS (``proxy_url='socks5h://...``)<N>- SOCKS5 with local DNS (``proxy_url='socks5://...``)<N>- Usernames and passwords for the SOCKS proxy<N><N>
.. note::<N>   It is recommended to use ``socks5h://`` or ``socks4a://`` schemes in<N>   your ``proxy_url`` to ensure that DNS resolution is done from the remote<N>   server instead of client-side when connecting to a domain name.<N><N>SOCKS4 supports IPv4 and domain names with the SOCKS4A extension. SOCKS5<N>supports IPv4, IPv6, and domain names.<N><N>
When connecting to a SOCKS4 proxy the ``username`` portion of the ``proxy_url``<N>will be sent as the ``userid`` section of the SOCKS request:<N><N>.. code-block:: python<N><N>    proxy_url="socks4a://<userid>@proxy-host"<N><N>When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion<N>of the ``proxy_url`` will be sent as the username/password to authenticate<N>with the proxy:<N><N>
.. code-block:: python<N><N>    proxy_url="socks5h://<username>:<password>@proxy-host"<N><N>"""<N>from __future__ import absolute_import<N><N>try:<N>    import socks<N>except ImportError:<N>    import warnings<N><N>    from ..exceptions import DependencyWarning<N><N>
    warnings.warn(<N>        (<N>            "SOCKS support in urllib3 requires the installation of optional "<N>            "dependencies: specifically, PySocks.  For more information, see "<N>            "https://urllib3.readthedocs.io/en/1.26.x/contrib.html#socks-proxies"<N>        ),<N>        DependencyWarning,<N>    )<N>    raise<N><N>
from socket import error as SocketError<N>from socket import timeout as SocketTimeout<N><N>from ..connection import HTTPConnection, HTTPSConnection<N>from ..connectionpool import HTTPConnectionPool, HTTPSConnectionPool<N>from ..exceptions import ConnectTimeoutError, NewConnectionError<N>from ..poolmanager import PoolManager<N>from ..util.url import parse_url<N><N>
try:<N>    import ssl<N>except ImportError:<N>    ssl = None<N><N><N>class SOCKSConnection(HTTPConnection):<N>    """<N>    A plain-text HTTP connection that connects via a SOCKS proxy.<N>    """<N><N>    def __init__(self, *args, **kwargs):<N>        self._socks_options = kwargs.pop("_socks_options")<N>        super(SOCKSConnection, self).__init__(*args, **kwargs)<N><N>
    def _new_conn(self):<N>        """<N>        Establish a new connection via the SOCKS proxy.<N>        """<N>        extra_kw = {}<N>        if self.source_address:<N>            extra_kw["source_address"] = self.source_address<N><N>        if self.socket_options:<N>            extra_kw["socket_options"] = self.socket_options<N><N>
"""<N>This module provides means to detect the App Engine environment.<N>"""<N><N>import os<N><N><N>def is_appengine():<N>    return is_local_appengine() or is_prod_appengine()<N><N><N>def is_appengine_sandbox():<N>    """Reports if the app is running in the first generation sandbox.<N><N>
    The second generation runtimes are technically still in a sandbox, but it<N>    is much less restrictive, so generally you shouldn't need to check for it.<N>    see https://cloud.google.com/appengine/docs/standard/runtimes<N>    """<N>    return is_appengine() and os.environ["APPENGINE_RUNTIME"] == "python27"<N><N>
<N>def is_local_appengine():<N>    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(<N>        "SERVER_SOFTWARE", ""<N>    ).startswith("Development/")<N><N><N>def is_prod_appengine():<N>    return "APPENGINE_RUNTIME" in os.environ and os.environ.get(<N>        "SERVER_SOFTWARE", ""<N>    ).startswith("Google App Engine/")<N><N>
"""<N>This module uses ctypes to bind a whole bunch of functions and constants from<N>SecureTransport. The goal here is to provide the low-level API to<N>SecureTransport. These are essentially the C-level functions and constants, and<N>they're pretty gross to work with.<N><N>
This code is a bastardised version of the code found in Will Bond's oscrypto<N>library. An enormous debt is owed to him for blazing this trail for us. For<N>that reason, this code should be considered to be covered both by urllib3's<N>license and by oscrypto's:<N><N>
from __future__ import absolute_import<N><N>import socket<N><N>from ..contrib import _appengine_environ<N>from ..exceptions import LocationParseError<N>from ..packages import six<N>from .wait import NoWayToWaitForSocketError, wait_for_read<N><N><N>def is_connection_dropped(conn):  # Platform-specific<N>    """<N>    Returns True if the connection is dropped and should be closed.<N><N>
from .ssl_ import create_urllib3_context, resolve_cert_reqs, resolve_ssl_version<N><N><N>def connection_requires_http_tunnel(<N>    proxy_url=None, proxy_config=None, destination_scheme=None<N>):<N>    """<N>    Returns True if the connection requires an HTTP CONNECT through the proxy.<N><N>
    :param URL proxy_url:<N>        URL of the proxy.<N>    :param ProxyConfig proxy_config:<N>        Proxy configuration from poolmanager.py<N>    :param str destination_scheme:<N>        The scheme of the destination. (i.e https, http, etc)<N>    """<N>    # If we're not using a proxy, no way to use a tunnel.<N>    if proxy_url is None:<N>        return False<N><N>
    # HTTP destinations never require tunneling, we always forward.<N>    if destination_scheme == "http":<N>        return False<N><N>    # Support for forwarding with HTTPS proxies and HTTPS destinations.<N>    if (<N>        proxy_url.scheme == "https"<N>        and proxy_config<N>        and proxy_config.use_forwarding_for_https<N>    ):<N>        return False<N><N>
    # Otherwise always use a tunnel.<N>    return True<N><N><N>def create_proxy_ssl_context(<N>    ssl_version, cert_reqs, ca_certs=None, ca_cert_dir=None, ca_cert_data=None<N>):<N>    """<N>    Generates a default proxy ssl context if one hasn't been provided by the<N>    user.<N>    """<N>    ssl_context = create_urllib3_context(<N>        ssl_version=resolve_ssl_version(ssl_version),<N>        cert_reqs=resolve_cert_reqs(cert_reqs),<N>    )<N><N>
import collections<N><N>from ..packages import six<N>from ..packages.six.moves import queue<N><N>if six.PY2:<N>    # Queue is imported for side effects on MS Windows. See issue #229.<N>    import Queue as _unused_module_Queue  # noqa: F401<N><N><N>class LifoQueue(queue.Queue):<N>    def _init(self, _):<N>        self.queue = collections.deque()<N><N>    def _qsize(self, len=len):<N>        return len(self.queue)<N><N>    def _put(self, item):<N>        self.queue.append(item)<N><N>    def _get(self):<N>        return self.queue.pop()<N>
from __future__ import absolute_import<N><N>from base64 import b64encode<N><N>from ..exceptions import UnrewindableBodyError<N>from ..packages.six import b, integer_types<N><N># Pass as a value within ``headers`` to skip<N># emitting some HTTP headers that are added automatically.<N># The only headers that are supported are ``Accept-Encoding``,<N># ``Host``, and ``User-Agent``.<N>SKIP_HEADER = "@@@SKIP_HEADER@@@"<N>SKIPPABLE_HEADERS = frozenset(["accept-encoding", "host", "user-agent"])<N><N>
ACCEPT_ENCODING = "gzip,deflate"<N>try:<N>    try:<N>        import brotlicffi as _unused_module_brotli  # noqa: F401<N>    except ImportError:<N>        import brotli as _unused_module_brotli  # noqa: F401<N>except ImportError:<N>    pass<N>else:<N>    ACCEPT_ENCODING += ",br"<N><N>
_FAILEDTELL = object()<N><N><N>def make_headers(<N>    keep_alive=None,<N>    accept_encoding=None,<N>    user_agent=None,<N>    basic_auth=None,<N>    proxy_basic_auth=None,<N>    disable_cache=None,<N>):<N>    """<N>    Shortcuts for generating request headers.<N><N>
    :param keep_alive:<N>        If ``True``, adds 'connection: keep-alive' header.<N><N>    :param accept_encoding:<N>        Can be a boolean, list, or string.<N>        ``True`` translates to 'gzip,deflate'.<N>        List will get joined by comma.<N>        String will be used as provided.<N><N>
    :param user_agent:<N>        String representing the user-agent you want, such as<N>        "python-urllib3/0.6"<N><N>    :param basic_auth:<N>        Colon-separated username:password string for 'authorization: basic ...'<N>        auth header.<N><N>
    :param proxy_basic_auth:<N>        Colon-separated username:password string for 'proxy-authorization: basic ...'<N>        auth header.<N><N>    :param disable_cache:<N>        If ``True``, adds 'cache-control: no-cache' header.<N><N>    Example::<N><N>
from __future__ import absolute_import<N><N>from email.errors import MultipartInvariantViolationDefect, StartBoundaryNotFoundDefect<N><N>from ..exceptions import HeaderParsingError<N>from ..packages.six.moves import http_client as httplib<N><N><N>def is_fp_closed(obj):<N>    """<N>    Checks whether a given file-like object is closed.<N><N>
    :param obj:<N>        The file-like object to check.<N>    """<N><N>    try:<N>        # Check `isclosed()` first, in case Python3 doesn't set `closed`.<N>        # GH Issue #928<N>        return obj.isclosed()<N>    except AttributeError:<N>        pass<N><N>
    try:<N>        # Check via the official file-like-object way.<N>        return obj.closed<N>    except AttributeError:<N>        pass<N><N>    try:<N>        # Check if the object is a container for another file-like object that<N>        # gets released on exhaustion (e.g. HTTPResponse).<N>        return obj.fp is None<N>    except AttributeError:<N>        pass<N><N>
    raise ValueError("Unable to determine whether fp is closed.")<N><N><N>def assert_header_parsing(headers):<N>    """<N>    Asserts whether all headers have been successfully parsed.<N>    Extracts encountered errors from the result of parsing headers.<N><N>
    Only works on Python 3.<N><N>    :param http.client.HTTPMessage headers: Headers to verify.<N><N>    :raises urllib3.exceptions.HeaderParsingError:<N>        If parsing errors are found.<N>    """<N><N>    # This will fail silently if we pass in the wrong kind of parameter.<N>    # To make debugging easier add an explicit check.<N>    if not isinstance(headers, httplib.HTTPMessage):<N>        raise TypeError("expected httplib.Message, got {0}.".format(type(headers)))<N><N>
    defects = getattr(headers, "defects", None)<N>    get_payload = getattr(headers, "get_payload", None)<N><N>    unparsed_data = None<N>    if get_payload:<N>        # get_payload is actually email.message.Message.get_payload;<N>        # we're only interested in the result if it's not a multipart message<N>        if not headers.is_multipart():<N>            payload = get_payload()<N><N>
            if isinstance(payload, (bytes, str)):<N>                unparsed_data = payload<N>    if defects:<N>        # httplib is assuming a response body is available<N>        # when parsing headers even when httplib only sends<N>        # header data to parse_headers() This results in<N>        # defects on multipart responses in particular.<N>        # See: https://github.com/urllib3/urllib3/issues/800<N><N>
from __future__ import absolute_import<N><N>import email<N>import logging<N>import re<N>import time<N>import warnings<N>from collections import namedtuple<N>from itertools import takewhile<N><N>from ..exceptions import (<N>    ConnectTimeoutError,<N>    InvalidHeader,<N>    MaxRetryError,<N>    ProtocolError,<N>    ProxyError,<N>    ReadTimeoutError,<N>    ResponseError,<N>)<N>from ..packages import six<N><N>
log = logging.getLogger(__name__)<N><N><N># Data structure for representing the metadata of requests that result in a retry.<N>RequestHistory = namedtuple(<N>    "RequestHistory", ["method", "url", "error", "status", "redirect_location"]<N>)<N><N><N># TODO: In v2 we can remove this sentinel and metaclass with deprecated options.<N>_Default = object()<N><N>
<N>class _RetryMeta(type):<N>    @property<N>    def DEFAULT_METHOD_WHITELIST(cls):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_ALLOWED_METHODS<N><N>
    @DEFAULT_METHOD_WHITELIST.setter<N>    def DEFAULT_METHOD_WHITELIST(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_METHOD_WHITELIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_ALLOWED_METHODS' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_ALLOWED_METHODS = value<N><N>
    @property<N>    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT<N><N>
    @DEFAULT_REDIRECT_HEADERS_BLACKLIST.setter<N>    def DEFAULT_REDIRECT_HEADERS_BLACKLIST(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.DEFAULT_REDIRECT_HEADERS_BLACKLIST' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_REMOVE_HEADERS_ON_REDIRECT' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_REMOVE_HEADERS_ON_REDIRECT = value<N><N>
    @property<N>    def BACKOFF_MAX(cls):<N>        warnings.warn(<N>            "Using 'Retry.BACKOFF_MAX' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",<N>            DeprecationWarning,<N>        )<N>        return cls.DEFAULT_BACKOFF_MAX<N><N>
    @BACKOFF_MAX.setter<N>    def BACKOFF_MAX(cls, value):<N>        warnings.warn(<N>            "Using 'Retry.BACKOFF_MAX' is deprecated and "<N>            "will be removed in v2.0. Use 'Retry.DEFAULT_BACKOFF_MAX' instead",<N>            DeprecationWarning,<N>        )<N>        cls.DEFAULT_BACKOFF_MAX = value<N><N>
<N>@six.add_metaclass(_RetryMeta)<N>class Retry(object):<N>    """Retry configuration.<N><N>    Each retry attempt will create a new Retry object with updated values, so<N>    they can be safely reused.<N><N>    Retries can be defined as a default for a pool::<N><N>
        retries = Retry(connect=5, read=2, redirect=5)<N>        http = PoolManager(retries=retries)<N>        response = http.request('GET', 'http://example.com/')<N><N>    Or per-request (which overrides the default for the pool)::<N><N>        response = http.request('GET', 'http://example.com/', retries=Retry(10))<N><N>
    Retries can be disabled by passing ``False``::<N><N>        response = http.request('GET', 'http://example.com/', retries=False)<N><N>    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless<N>    retries are disabled, in which case the causing exception will be raised.<N><N>
    :param int total:<N>        Total number of retries to allow. Takes precedence over other counts.<N><N>        Set to ``None`` to remove this constraint and fall back on other<N>        counts.<N><N>        Set to ``0`` to fail on the first retry.<N><N>
        Set to ``False`` to disable and imply ``raise_on_redirect=False``.<N><N>    :param int connect:<N>        How many connection-related errors to retry on.<N><N>        These are errors raised before the request is sent to the remote server,<N>        which we assume has not triggered the server to process the request.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int read:<N>        How many times to retry on read errors.<N><N>        These errors are raised after the request was sent to the server, so the<N>        request may have side-effects.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int redirect:<N>        How many redirects to perform. Limit this to avoid infinite redirect<N>        loops.<N><N>        A redirect is a HTTP response with a status code 301, 302, 303, 307 or<N>        308.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>        Set to ``False`` to disable and imply ``raise_on_redirect=False``.<N><N>    :param int status:<N>        How many times to retry on bad status codes.<N><N>        These are retries made on responses, where status code matches<N>        ``status_forcelist``.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>    :param int other:<N>        How many times to retry on other errors.<N><N>        Other errors are errors that are not connect, read, redirect or status errors.<N>        These errors might be raised after the request was sent to the server, so the<N>        request might have side-effects.<N><N>
        Set to ``0`` to fail on the first retry of this type.<N><N>        If ``total`` is not set, it's a good idea to set this to 0 to account<N>        for unexpected edge cases and avoid infinite retry loops.<N><N>    :param iterable allowed_methods:<N>        Set of uppercased HTTP method verbs that we should retry on.<N><N>
        By default, we only retry on methods which are considered to be<N>        idempotent (multiple requests with the same parameters end with the<N>        same state). See :attr:`Retry.DEFAULT_ALLOWED_METHODS`.<N><N>        Set to a ``False`` value to retry on any verb.<N><N>
        .. warning::<N><N>            Previously this parameter was named ``method_whitelist``, that<N>            usage is deprecated in v1.26.0 and will be removed in v2.0.<N><N>    :param iterable status_forcelist:<N>        A set of integer HTTP status codes that we should force a retry on.<N>        A retry is initiated if the request method is in ``allowed_methods``<N>        and the response status code is in ``status_forcelist``.<N><N>
        By default, this is disabled with ``None``.<N><N>    :param float backoff_factor:<N>        A backoff factor to apply between attempts after the second try<N>        (most errors are resolved immediately by a second try without a<N>        delay). urllib3 will sleep for::<N><N>
            {backoff factor} * (2 ** ({number of total retries} - 1))<N><N>        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep<N>        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer<N>        than :attr:`Retry.DEFAULT_BACKOFF_MAX`.<N><N>
        By default, backoff is disabled (set to 0).<N><N>    :param bool raise_on_redirect: Whether, if the number of redirects is<N>        exhausted, to raise a MaxRetryError, or to return a response with a<N>        response code in the 3xx range.<N><N>
    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:<N>        whether we should raise an exception, or return a response,<N>        if status falls in ``status_forcelist`` range and retries have<N>        been exhausted.<N><N>    :param tuple history: The history of the request encountered during<N>        each call to :meth:`~Retry.increment`. The list is in the order<N>        the requests occurred. Each list item is of class :class:`RequestHistory`.<N><N>
    :param bool respect_retry_after_header:<N>        Whether to respect Retry-After header on status codes defined as<N>        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.<N><N>    :param iterable remove_headers_on_redirect:<N>        Sequence of headers to remove from the request when a response<N>        indicating a redirect is returned before firing off the redirected<N>        request.<N>    """<N><N>
    #: Default methods to be used for ``allowed_methods``<N>    DEFAULT_ALLOWED_METHODS = frozenset(<N>        ["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]<N>    )<N><N>    #: Default status codes to be used for ``status_forcelist``<N>    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])<N><N>
import io<N>import socket<N>import ssl<N><N>from ..exceptions import ProxySchemeUnsupported<N>from ..packages import six<N><N>SSL_BLOCKSIZE = 16384<N><N><N>class SSLTransport:<N>    """<N>    The SSLTransport wraps an existing socket and establishes an SSL connection.<N><N>
    Contrary to Python's implementation of SSLSocket, it allows you to chain<N>    multiple TLS connections together. It's particularly useful if you need to<N>    implement TLS within TLS.<N><N>    The class supports most of the socket API operations.<N>    """<N><N>
    @staticmethod<N>    def _validate_ssl_context_for_tls_in_tls(ssl_context):<N>        """<N>        Raises a ProxySchemeUnsupported if the provided ssl_context can't be used<N>        for TLS in TLS.<N><N>        The only requirement is that the ssl_context provides the 'wrap_bio'<N>        methods.<N>        """<N><N>
        if not hasattr(ssl_context, "wrap_bio"):<N>            if six.PY2:<N>                raise ProxySchemeUnsupported(<N>                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "<N>                    "supported on Python 2"<N>                )<N>            else:<N>                raise ProxySchemeUnsupported(<N>                    "TLS in TLS requires SSLContext.wrap_bio() which isn't "<N>                    "available on non-native SSLContext"<N>                )<N><N>
    def __init__(<N>        self, socket, ssl_context, server_hostname=None, suppress_ragged_eofs=True<N>    ):<N>        """<N>        Create an SSLTransport around socket using the provided ssl_context.<N>        """<N>        self.incoming = ssl.MemoryBIO()<N>        self.outgoing = ssl.MemoryBIO()<N><N>
        self.suppress_ragged_eofs = suppress_ragged_eofs<N>        self.socket = socket<N><N>        self.sslobj = ssl_context.wrap_bio(<N>            self.incoming, self.outgoing, server_hostname=server_hostname<N>        )<N><N>        # Perform initial handshake.<N>        self._ssl_io_loop(self.sslobj.do_handshake)<N><N>
    def __enter__(self):<N>        return self<N><N>    def __exit__(self, *_):<N>        self.close()<N><N>    def fileno(self):<N>        return self.socket.fileno()<N><N>    def read(self, len=1024, buffer=None):<N>        return self._wrap_ssl_read(len, buffer)<N><N>
from __future__ import absolute_import<N><N>import hmac<N>import os<N>import sys<N>import warnings<N>from binascii import hexlify, unhexlify<N>from hashlib import md5, sha1, sha256<N><N>from ..exceptions import (<N>    InsecurePlatformWarning,<N>    ProxySchemeUnsupported,<N>    SNIMissingWarning,<N>    SSLError,<N>)<N>from ..packages import six<N>from .url import BRACELESS_IPV6_ADDRZ_RE, IPV4_RE<N><N>
SSLContext = None<N>SSLTransport = None<N>HAS_SNI = False<N>IS_PYOPENSSL = False<N>IS_SECURETRANSPORT = False<N>ALPN_PROTOCOLS = ["http/1.1"]<N><N># Maps the length of a digest to a possible hash function producing this digest<N>HASHFUNC_MAP = {32: md5, 40: sha1, 64: sha256}<N><N>
<N>def _const_compare_digest_backport(a, b):<N>    """<N>    Compare two digests of equal length in constant time.<N><N>    The digests must be of type str/bytes.<N>    Returns True if the digests match, and False otherwise.<N>    """<N>    result = abs(len(a) - len(b))<N>    for left, right in zip(bytearray(a), bytearray(b)):<N>        result |= left ^ right<N>    return result == 0<N><N>
<N>_const_compare_digest = getattr(hmac, "compare_digest", _const_compare_digest_backport)<N><N>try:  # Test for SSL features<N>    import ssl<N>    from ssl import CERT_REQUIRED, wrap_socket<N>except ImportError:<N>    pass<N><N>try:<N>    from ssl import HAS_SNI  # Has SNI?<N>except ImportError:<N>    pass<N><N>
try:<N>    from .ssltransport import SSLTransport<N>except ImportError:<N>    pass<N><N><N>try:  # Platform-specific: Python 3.6<N>    from ssl import PROTOCOL_TLS<N><N>    PROTOCOL_SSLv23 = PROTOCOL_TLS<N>except ImportError:<N>    try:<N>        from ssl import PROTOCOL_SSLv23 as PROTOCOL_TLS<N><N>
        PROTOCOL_SSLv23 = PROTOCOL_TLS<N>    except ImportError:<N>        PROTOCOL_SSLv23 = PROTOCOL_TLS = 2<N><N>try:<N>    from ssl import PROTOCOL_TLS_CLIENT<N>except ImportError:<N>    PROTOCOL_TLS_CLIENT = PROTOCOL_TLS<N><N><N>try:<N>    from ssl import OP_NO_COMPRESSION, OP_NO_SSLv2, OP_NO_SSLv3<N>except ImportError:<N>    OP_NO_SSLv2, OP_NO_SSLv3 = 0x1000000, 0x2000000<N>    OP_NO_COMPRESSION = 0x20000<N><N>
from __future__ import absolute_import<N><N>import time<N><N># The default socket timeout, used by httplib to indicate that no timeout was<N># specified by the user<N>from socket import _GLOBAL_DEFAULT_TIMEOUT<N><N>from ..exceptions import TimeoutStateError<N><N>
# A sentinel value to indicate that no timeout was specified by the user in<N># urllib3<N>_Default = object()<N><N><N># Use time.monotonic if available.<N>current_time = getattr(time, "monotonic", time.time)<N><N><N>class Timeout(object):<N>    """Timeout configuration.<N><N>
    Timeouts can be defined as a default for a pool:<N><N>    .. code-block:: python<N><N>       timeout = Timeout(connect=2.0, read=7.0)<N>       http = PoolManager(timeout=timeout)<N>       response = http.request('GET', 'http://example.com/')<N><N>
    Or per-request (which overrides the default for the pool):<N><N>    .. code-block:: python<N><N>       response = http.request('GET', 'http://example.com/', timeout=Timeout(10))<N><N>    Timeouts can be disabled by setting all the parameters to ``None``:<N><N>
from __future__ import absolute_import<N><N>import re<N>from collections import namedtuple<N><N>from ..exceptions import LocationParseError<N>from ..packages import six<N><N>url_attrs = ["scheme", "auth", "host", "port", "path", "query", "fragment"]<N><N>
import errno<N>import select<N>import sys<N>from functools import partial<N><N>try:<N>    from time import monotonic<N>except ImportError:<N>    from time import time as monotonic<N><N>__all__ = ["NoWayToWaitForSocketError", "wait_for_read", "wait_for_write"]<N><N>
#<N># (C) Copyright 2017 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>import warnings<N>from win32ctypes.pywin32.pywintypes import *<N><N>warnings.warn(<N>    "Please use 'from win32ctypes.pywin32 import pywintypes'",<N>    DeprecationWarning)<N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>import warnings<N>from win32ctypes.pywin32.win32api import *<N><N>warnings.warn(<N>    "Please use 'from win32ctypes.pywin32 import win32api'",<N>    DeprecationWarning)<N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>import warnings<N>from win32ctypes.pywin32.win32cred import *<N><N>warnings.warn(<N>    "Please use 'from win32ctypes.pywin32 import win32cred'",<N>    DeprecationWarning)<N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from .version import __version__  # noqa<N>
import sys<N><N>if sys.version_info[0] >= 3:<N>    PY3 = True<N>    PY2 = False<N>else:<N>    PY3 = False<N>    PY2 = True<N><N>if PY3:<N>    def is_bytes(b):<N>        return isinstance(b, bytes)<N><N>    def is_text(s):<N>        return isinstance(s, str)<N><N>
    def is_integer(i):<N>        return isinstance(i, int)<N><N>    text_type = str<N>else:<N>    def is_text(s):<N>        return isinstance(s, unicode)<N><N>    def is_bytes(b):<N>        return isinstance(b, (bytes, str))<N><N>    def is_integer(i):<N>        return isinstance(i, (int, long))<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N><N>ERROR_NOT_FOUND = 0x490<N>
#<N># (C) Copyright 2014-2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>import sys<N>import importlib<N><N>
from . import _winerrors  # noqa<N><N>try:<N>    import cffi<N>except ImportError:<N>    _backend = 'ctypes'<N>else:<N>    del cffi<N>    _backend = 'cffi'<N>try:<N>    from importlib.abc import MetaPathFinder, Loader<N>except ImportError:<N>    MetaPathFinder = object<N>    Loader = object<N><N>
# Setup module redirection based on the backend<N><N>class BackendLoader(Loader):<N><N>    def __init__(self, redirect_module):<N>        self.redirect_module = redirect_module<N><N>    def load_module(self, fullname):<N>        module = importlib.import_module(self.redirect_module)<N>        sys.modules[fullname] = module<N>        return module<N><N>
    # NOTE: Defined here to make python 3.3.x happy<N>    def module_repr(self, module):<N>        # The exception will cause ModuleType.__repr__ to ignore this method.<N>        raise NotImplementedError<N><N><N>class BackendFinder(MetaPathFinder):<N><N>
#<N># (C) Copyright 2015 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from weakref import WeakKeyDictionary<N><N>
from win32ctypes.core.compat import is_text<N>from ._util import ffi, check_zero, dlls<N>from ._nl_support import _GetACP<N>from ._common import _PyBytes_FromStringAndSize<N><N><N>ffi.cdef("""<N><N>typedef struct _FILETIME {<N>  DWORD dwLowDateTime;<N>  DWORD dwHighDateTime;<N>} FILETIME, *PFILETIME;<N><N>
#<N># (C) Copyright 2015 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from weakref import WeakKeyDictionary<N><N>
from ._util import ffi<N><N>_keep_alive = WeakKeyDictionary()<N><N><N>def _PyBytes_FromStringAndSize(pointer, size):<N>    buffer = ffi.buffer(pointer, size)<N>    return buffer[:]<N><N><N>def byreference(x):<N>    return ffi.new(ffi.getctype(ffi.typeof(x), '*'), x)<N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from win32ctypes.core.compat import text_type<N>from ._util import ffi, check_null, check_false, dlls, HMODULE, PVOID<N><N>
<N>ffi.cdef("""<N><N>HMODULE WINAPI LoadLibraryExW(LPCTSTR lpFileName, HANDLE hFile, DWORD dwFlags);<N>BOOL WINAPI FreeLibrary(HMODULE hModule);<N><N>""")<N><N><N>def _LoadLibraryEx(lpFilename, hFile, dwFlags):<N>    result = check_null(<N>        dlls.kernel32.LoadLibraryExW(<N>            text_type(lpFilename), ffi.NULL, dwFlags),<N>        function_name='LoadLibraryEx')<N>    return HMODULE(result)<N><N>
#<N># (C) Copyright 2015-18 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ._util import ffi, dlls<N><N>ffi.cdef("""<N><N>UINT WINAPI GetACP(void);<N><N>""")<N><N><N>def _GetACP():<N>    return dlls.kernel32.GetACP()<N>
#<N># (C) Copyright 2015 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N><N>from win32ctypes.core.compat import text_type<N>from ._util import (<N>    ffi, check_null, check_zero, check_false, HMODULE,<N>    PVOID, RESOURCE, resource, dlls)<N><N>
<N>ffi.cdef("""<N><N>typedef int WINBOOL;<N>typedef WINBOOL (__stdcall *ENUMRESTYPEPROC) (HANDLE, LPTSTR, LONG_PTR);<N>typedef WINBOOL (__stdcall *ENUMRESNAMEPROC) (HANDLE, LPCTSTR, LPTSTR, LONG_PTR);<N>typedef WINBOOL (__stdcall *ENUMRESLANGPROC) (HANDLE, LPCTSTR, LPCTSTR, WORD, LONG_PTR);<N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ._util import ffi, dlls<N><N>
# TODO: retrieve this value using ffi<N>MAX_PATH = 260<N>MAX_PATH_BUF = u'wchar_t[{0}]'.format(MAX_PATH)<N><N>ffi.cdef("""<N><N>BOOL WINAPI Beep(DWORD dwFreq, DWORD dwDuration);<N>UINT WINAPI GetWindowsDirectoryW(LPTSTR lpBuffer, UINT uSize);<N>UINT WINAPI GetSystemDirectoryW(LPTSTR lpBuffer, UINT uSize);<N><N>
""")<N><N><N>def _GetWindowsDirectory():<N>    buffer = ffi.new(MAX_PATH_BUF)<N>    l = dlls.kernel32.GetWindowsDirectoryW(buffer, MAX_PATH)<N>    return ffi.unpack(buffer, l)<N><N><N>def _GetSystemDirectory():<N>    buffer = ffi.new(MAX_PATH_BUF)<N>    l = dlls.kernel32.GetSystemDirectoryW(buffer, MAX_PATH)<N>    return ffi.unpack(buffer, l)<N><N><N>
#<N># (C) Copyright 2015-18 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ._util import ffi, dlls<N><N>ffi.cdef("""<N><N>DWORD WINAPI GetTickCount(void);<N><N>""")<N><N><N>def _GetTickCount():<N>    return dlls.kernel32.GetTickCount()<N>
#<N># (C) Copyright 2015 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N><N>""" Utility functions to help with cffi wrapping.<N>"""<N>from __future__ import absolute_import<N><N>
from win32ctypes.core.compat import is_bytes, is_integer, text_type<N>from cffi import FFI<N><N>ffi = FFI()<N>ffi.set_unicode(True)<N><N><N>def HMODULE(cdata):<N>    return int(ffi.cast("intptr_t", cdata))<N><N><N>def PVOID(x):<N>    return ffi.cast("void *", x)<N><N>
<N>def IS_INTRESOURCE(x):<N>    """ Check if x is an index into the id list.<N><N>    """<N>    return int(ffi.cast("uintptr_t", x)) >> 16 == 0<N><N><N>def RESOURCE(resource):<N>    """ Convert a resource into a compatible input for cffi.<N><N>    """<N>    if is_integer(resource):<N>        resource = ffi.cast('wchar_t *', resource)<N>    elif is_bytes(resource):<N>        resource = text_type(resource)<N>    return resource<N><N>
<N>def resource(lpRESOURCEID):<N>    """ Convert the windows RESOURCE into a python friendly object.<N>    """<N>    if IS_INTRESOURCE(lpRESOURCEID):<N>        resource = int(ffi.cast("uintptr_t", lpRESOURCEID))<N>    else:<N>        resource = ffi.string(lpRESOURCEID)<N>    return resource<N><N>
<N>class ErrorWhen(object):<N>    """ Callable factory for raising errors when calling cffi functions.<N><N>    """<N><N>    def __init__(self, check, raise_on_zero=True):<N>        """ Constructor<N><N>        Parameters<N>        ----------<N>        check :<N>            The return value that designates that an error has taken place.<N><N>
        raise_on_zero : bool<N>            When set any error will be raised. When false the winerror<N>            is checked and only non-zero win errors are raised. Currently<N>            this parameters is used to workaround issues with the win32<N>            implementation in ``wine``.<N><N>
        """<N>        self._check = check<N>        self._raise_on_zero = raise_on_zero<N><N>    def __call__(self, value, function_name=''):<N>        if value == self._check:<N>            self._raise_error(function_name)<N>        else:<N>            return value<N><N>
    def _raise_error(self, function_name=''):<N>        code, message = ffi.getwinerror()<N>        exception = WindowsError()<N>        exception.errno = ffi.errno<N>        exception.winerror = code<N>        exception.strerror = message<N>        exception.function = function_name<N>        raise exception<N><N>
<N>check_null = ErrorWhen(ffi.NULL)<N>check_zero = ErrorWhen(0)<N>check_false = ErrorWhen(False)<N><N><N>class Libraries(object):<N><N>    def __getattr__(self, name):<N>        library = ffi.dlopen('{}.dll'.format(name))<N>        self.__dict__[name] = library<N>        return library<N><N>
#<N># (C) Copyright 2014-18 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>import ctypes<N>from ctypes import POINTER, Structure, c_void_p, c_wchar_p, c_char_p, cast<N>from ctypes.wintypes import (<N>    BOOL, DWORD, FILETIME, LPCWSTR)<N><N>
from win32ctypes.core.compat import is_text<N>from ._common import LPBYTE, _PyBytes_FromStringAndSize<N>from ._util import function_factory, check_zero_factory, dlls<N>from ._nl_support import _GetACP<N><N><N>SUPPORTED_CREDKEYS = set((<N>    u'Type', u'TargetName', u'Persist',<N>    u'UserName', u'Comment', u'CredentialBlob'))<N><N>
<N>class CREDENTIAL(Structure):<N>    _fields_ = [<N>        ("Flags", DWORD),<N>        ("Type", DWORD),<N>        ("TargetName", c_wchar_p),<N>        ("Comment", c_wchar_p),<N>        ("LastWritten", FILETIME),<N>        ("CredentialBlobSize", DWORD),<N>        ("CredentialBlob", LPBYTE),<N>        ("Persist", DWORD),<N>        ("_DO_NOT_USE_AttributeCount", DWORD),<N>        ("__DO_NOT_USE_Attribute", c_void_p),<N>        ("TargetAlias", c_wchar_p),<N>        ("UserName", c_wchar_p)]<N><N>
    @classmethod<N>    def fromdict(cls, credential, flags=0):<N>        unsupported = set(credential.keys()) - SUPPORTED_CREDKEYS<N>        if len(unsupported):<N>            raise ValueError("Unsupported keys: {0}".format(unsupported))<N>        if flags != 0:<N>            raise ValueError("flag != 0 not yet supported")<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>import ctypes<N>import sys<N>from ctypes import (<N>    pythonapi, POINTER, c_void_p, py_object, c_char_p, c_int, c_long, c_int64,<N>    c_longlong)<N>from ctypes import cast  # noqa imported here for convenience<N>from ctypes.wintypes import BYTE<N><N>
from win32ctypes.core.compat import PY3<N>from ._util import function_factory<N><N>PPy_UNICODE = c_void_p<N>LPBYTE = POINTER(BYTE)<N>is_64bits = sys.maxsize > 2**32<N>Py_ssize_t = c_int64 if is_64bits else c_int<N><N>if ctypes.sizeof(c_long) == ctypes.sizeof(c_void_p):<N>    LONG_PTR = c_long<N>elif ctypes.sizeof(c_longlong) == ctypes.sizeof(c_void_p):<N>    LONG_PTR = c_longlong<N><N>
if PY3:<N>    _PyBytes_FromStringAndSize = function_factory(<N>        pythonapi.PyBytes_FromStringAndSize,<N>        [c_char_p, Py_ssize_t],<N>        return_type=py_object)<N>else:<N>    _PyBytes_FromStringAndSize = function_factory(<N>        pythonapi.PyString_FromStringAndSize,<N>        [c_char_p, Py_ssize_t],<N>        return_type=py_object)<N><N>
<N>def IS_INTRESOURCE(x):<N>    return x >> 16 == 0<N><N><N>byreference = ctypes.byref<N><N><N>def dereference(x):<N>    return x.contents<N><N><N>class Libraries(object):<N><N>    def __getattr__(self, name):<N>        library = ctypes.WinDLL(name)<N>        self.__dict__[name] = library<N>        return library<N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ctypes.wintypes import BOOL, DWORD, HANDLE, HMODULE, LPCWSTR<N><N>
from ._util import check_null, check_false, function_factory, dlls<N><N>_LoadLibraryEx = function_factory(<N>    dlls.kernel32.LoadLibraryExW,<N>    [LPCWSTR, HANDLE, DWORD],<N>    HMODULE, check_null)<N><N>_FreeLibrary = function_factory(<N>    dlls.kernel32.FreeLibrary,<N>    [HMODULE],<N>    BOOL,<N>    check_false)<N><N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ctypes.wintypes import UINT<N><N>from ._util import function_factory, dlls<N><N>_GetACP = function_factory(dlls.kernel32.GetACP, None, UINT)<N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>import ctypes<N>from ctypes.wintypes import (<N>    BOOL, DWORD, HANDLE, HMODULE, LPCWSTR, WORD, HRSRC,<N>    HGLOBAL, LPVOID)<N><N>
from ._common import LONG_PTR, IS_INTRESOURCE<N>from ._util import check_null, check_zero, check_false, function_factory, dlls<N><N>_ENUMRESTYPEPROC = ctypes.WINFUNCTYPE(BOOL, HMODULE, LPVOID, LONG_PTR)<N>_ENUMRESNAMEPROC = ctypes.WINFUNCTYPE(BOOL, HMODULE, LPVOID, LPVOID, LONG_PTR)<N>_ENUMRESLANGPROC = ctypes.WINFUNCTYPE(<N>    BOOL, HMODULE, LPVOID, LPVOID, WORD, LONG_PTR)<N><N>
<N>def ENUMRESTYPEPROC(callback):<N>    def wrapped(handle, type_, param):<N>        if IS_INTRESOURCE(type_):<N>            type_ = int(type_)<N>        else:<N>            type_ = ctypes.cast(type_, LPCWSTR).value<N>        return callback(handle, type_, param)<N><N>
    return _ENUMRESTYPEPROC(wrapped)<N><N><N>def ENUMRESNAMEPROC(callback):<N>    def wrapped(handle, type_, name, param):<N>        if IS_INTRESOURCE(type_):<N>            type_ = int(type_)<N>        else:<N>            type_ = ctypes.cast(type_, LPCWSTR).value<N>        if IS_INTRESOURCE(name):<N>            name = int(name)<N>        else:<N>            name = ctypes.cast(name, LPCWSTR).value<N>        return callback(handle, type_, name, param)<N><N>
    return _ENUMRESNAMEPROC(wrapped)<N><N><N>def ENUMRESLANGPROC(callback):<N>    def wrapped(handle, type_, name, language, param):<N>        if IS_INTRESOURCE(type_):<N>            type_ = int(type_)<N>        else:<N>            type_ = ctypes.cast(type_, LPCWSTR).value<N>        if IS_INTRESOURCE(name):<N>            name = int(name)<N>        else:<N>            name = ctypes.cast(name, LPCWSTR).value<N>        return callback(handle, type_, name, language, param)<N><N>
    return _ENUMRESLANGPROC(wrapped)<N><N><N>def _UpdateResource(hUpdate, lpType, lpName, wLanguage, lpData, cbData):<N>    lp_type = LPCWSTR(lpType)<N>    lp_name = LPCWSTR(lpName)<N>    _BaseUpdateResource(hUpdate, lp_type, lp_name, wLanguage, lpData, cbData)<N><N>
<N>def _EnumResourceNames(hModule, lpszType, lpEnumFunc, lParam):<N>    resource_type = LPCWSTR(lpszType)<N>    _BaseEnumResourceNames(hModule, resource_type, lpEnumFunc, lParam)<N><N><N>def _EnumResourceLanguages(hModule, lpType, lpName, lpEnumFunc, lParam):<N>    resource_type = LPCWSTR(lpType)<N>    resource_name = LPCWSTR(lpName)<N>    _BaseEnumResourceLanguages(<N>        hModule, resource_type, resource_name, lpEnumFunc, lParam)<N><N>
<N>def _FindResourceEx(hModule, lpType, lpName, wLanguage):<N>    resource_type = LPCWSTR(lpType)<N>    resource_name = LPCWSTR(lpName)<N>    return _BaseFindResourceEx(<N>        hModule, resource_type, resource_name, wLanguage)<N><N><N>_EnumResourceTypes = function_factory(<N>    dlls.kernel32.EnumResourceTypesW,<N>    [HMODULE, _ENUMRESTYPEPROC, LONG_PTR],<N>    BOOL,<N>    check_false)<N><N>
_LoadResource = function_factory(<N>    dlls.kernel32.LoadResource,<N>    [HMODULE, HRSRC],<N>    HGLOBAL,<N>    check_null)<N><N>_LockResource = function_factory(<N>    dlls.kernel32.LockResource,<N>    [HGLOBAL],<N>    LPVOID,<N>    check_null)<N><N>
_SizeofResource = function_factory(<N>    dlls.kernel32.SizeofResource,<N>    [HMODULE, HRSRC],<N>    DWORD,<N>    check_zero)<N><N>_BeginUpdateResource = function_factory(<N>    dlls.kernel32.BeginUpdateResourceW,<N>    [LPCWSTR, BOOL],<N>    HANDLE,<N>    check_null)<N><N>
_EndUpdateResource = function_factory(<N>    dlls.kernel32.EndUpdateResourceW,<N>    [HANDLE, BOOL],<N>    BOOL,<N>    check_false)<N><N>_BaseEnumResourceNames = function_factory(<N>    dlls.kernel32.EnumResourceNamesW,<N>    [HMODULE, LPCWSTR, _ENUMRESNAMEPROC, LONG_PTR],<N>    BOOL,<N>    check_false)<N><N>
_BaseEnumResourceLanguages = function_factory(<N>    dlls.kernel32.EnumResourceLanguagesW,<N>    [HMODULE, LPCWSTR, LPCWSTR, _ENUMRESLANGPROC, LONG_PTR],<N>    BOOL,<N>    check_false)<N><N>_BaseFindResourceEx = function_factory(<N>    dlls.kernel32.FindResourceExW,<N>    [HMODULE, LPCWSTR, LPCWSTR, WORD],<N>    HRSRC,<N>    check_null)<N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>import ctypes<N>from ctypes.wintypes import LPCWSTR, UINT, LPWSTR, MAX_PATH<N><N>
from ._util import check_zero, function_factory, dlls<N><N><N>def _GetWindowsDirectory():<N>    buffer = ctypes.create_unicode_buffer(MAX_PATH)<N>    _BaseGetWindowsDirectory(buffer, MAX_PATH)<N>    return ctypes.cast(buffer, LPCWSTR).value<N><N><N>def _GetSystemDirectory():<N>    buffer = ctypes.create_unicode_buffer(MAX_PATH)<N>    _BaseGetSystemDirectory(buffer, MAX_PATH)<N>    return ctypes.cast(buffer, LPCWSTR).value<N><N>
<N>_BaseGetWindowsDirectory = function_factory(<N>    dlls.kernel32.GetWindowsDirectoryW,<N>    [LPWSTR, UINT],<N>    UINT,<N>    check_zero)<N><N>_BaseGetSystemDirectory = function_factory(<N>    dlls.kernel32.GetSystemDirectoryW,<N>    [LPWSTR, UINT],<N>    UINT,<N>    check_zero)<N><N><N>
#<N># (C) Copyright 2018 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from ctypes.wintypes import DWORD<N><N>from ._util import function_factory, dlls<N><N><N>_GetTickCount = function_factory(<N>    dlls.kernel32.GetTickCount,<N>    None, DWORD)<N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N><N>""" Utility functions to help with ctypes wrapping.<N>"""<N>from __future__ import absolute_import<N><N>
from ctypes import GetLastError, FormatError, WinDLL<N><N><N>def function_factory(<N>        function, argument_types=None,<N>        return_type=None, error_checking=None):<N>    if argument_types is not None:<N>        function.argtypes = argument_types<N>    function.restype = return_type<N>    if error_checking is not None:<N>        function.errcheck = error_checking<N>    return function<N><N>
<N>def make_error(function, function_name=None):<N>    code = GetLastError()<N>    description = FormatError(code).strip()<N>    if function_name is None:<N>        function_name = function.__name__<N>    exception = WindowsError()<N>    exception.winerror = code<N>    exception.function = function_name<N>    exception.strerror = description<N>    return exception<N><N>
<N>def check_null_factory(function_name=None):<N>    def check_null(result, function, arguments, *args):<N>        if result is None:<N>            raise make_error(function, function_name)<N>        return result<N>    return check_null<N><N><N>check_null = check_null_factory()<N><N>
<N>def check_zero_factory(function_name=None):<N>    def check_zero(result, function, arguments, *args):<N>        if result == 0:<N>            raise make_error(function, function_name)<N>        return result<N>    return check_zero<N><N><N>check_zero = check_zero_factory()<N><N>
<N>def check_false_factory(function_name=None):<N>    def check_false(result, function, arguments, *args):<N>        if not bool(result):<N>            raise make_error(function, function_name)<N>        else:<N>            return True<N>    return check_false<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>""" A module which supports common Windows types. """<N><N>from __future__ import absolute_import<N>import contextlib<N><N>
<N>class error(Exception):<N>    def __init__(self, *args, **kw):<N>        nargs = len(args)<N>        if nargs > 0:<N>            self.winerror = args[0]<N>        else:<N>            self.winerror = None<N>        if nargs > 1:<N>            self.funcname = args[1]<N>        else:<N>            self.funcname = None<N>        if nargs > 2:<N>            self.strerror = args[2]<N>        else:<N>            self.strerror = None<N>        Exception.__init__(self, *args, **kw)<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>""" A module, encapsulating the Windows Win32 API. """<N><N>from __future__ import absolute_import<N><N>
from win32ctypes.core import (<N>    _common, _dll, _resource, _system_information, _backend, _time)<N>from win32ctypes.pywin32.pywintypes import pywin32error as _pywin32error<N><N>LOAD_LIBRARY_AS_DATAFILE = 0x2<N>LANG_NEUTRAL = 0x00<N><N><N>def LoadLibraryEx(fileName, handle, flags):<N>    """ Loads the specified DLL, and returns the handle.<N><N>
    Parameters<N>    ----------<N>    fileName : unicode<N>        The filename of the module to load.<N><N>    handle : int<N>        Reserved, always zero.<N><N>    flags : int<N>        The action to be taken when loading the module.<N><N>    Returns<N>    -------<N>    handle : hModule<N>        The handle of the loaded module<N><N>
    """<N>    if not handle == 0:<N>        raise ValueError("handle != 0 not supported")<N>    with _pywin32error():<N>        return _dll._LoadLibraryEx(fileName, 0, flags)<N><N><N>def EnumResourceTypes(hModule):<N>    """ Enumerates resource types within a module.<N><N>
    Parameters<N>    ----------<N>    hModule : handle<N>        The handle to the module.<N><N>    Returns<N>    -------<N>    resource_types : list<N>       The list of resource types in the module.<N><N>    """<N>    resource_types = []<N><N>    def callback(hModule, type_, param):<N>        resource_types.append(type_)<N>        return True<N><N>
    with _pywin32error():<N>        _resource._EnumResourceTypes(<N>            hModule, _resource.ENUMRESTYPEPROC(callback), 0)<N>    return resource_types<N><N><N>def EnumResourceNames(hModule, resType):<N>    """ Enumerates all the resources of the specified type within a module.<N><N>
    Parameters<N>    ----------<N>    hModule : handle<N>        The handle to the module.<N>    resType : str : int<N>        The type or id of resource to enumerate.<N><N>    Returns<N>    -------<N>    resource_names : list<N>       The list of resource names (unicode strings) of the specific<N>       resource type in the module.<N><N>
    """<N>    resource_names = []<N><N>    def callback(hModule, type_, type_name, param):<N>        resource_names.append(type_name)<N>        return True<N><N>    with _pywin32error():<N>        _resource._EnumResourceNames(<N>            hModule, resType, _resource.ENUMRESNAMEPROC(callback), 0)<N>    return resource_names<N><N>
<N>def EnumResourceLanguages(hModule, lpType, lpName):<N>    """ List languages of a resource module.<N><N>    Parameters<N>    ----------<N>    hModule : handle<N>        Handle to the resource module.<N><N>    lpType : str : int<N>        The type or id of resource to enumerate.<N><N>
    lpName : str : int<N>        The type or id of resource to enumerate.<N><N>    Returns<N>    -------<N>    resource_languages : list<N>        List of the resource language ids.<N><N>    """<N>    resource_languages = []<N><N>    def callback(hModule, type_name, res_name, language_id, param):<N>        resource_languages.append(language_id)<N>        return True<N><N>
    with _pywin32error():<N>        _resource._EnumResourceLanguages(<N>            hModule, lpType, lpName, _resource.ENUMRESLANGPROC(callback), 0)<N>    return resource_languages<N><N><N>def LoadResource(hModule, type, name, language=LANG_NEUTRAL):<N>    """ Find and Load a resource component.<N><N>
    Parameters<N>    ----------<N>    handle : hModule<N>        The handle of the module containing the resource.<N>        Use None for current process executable.<N><N>    type : str : int<N>        The type of resource to load.<N><N>    name : str : int<N>        The name or Id of the resource to load.<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>""" Interface to credentials management functions. """<N><N>from __future__ import absolute_import<N><N>
from win32ctypes.core import _authentication, _common, _backend<N>from win32ctypes.pywin32.pywintypes import pywin32error as _pywin32error<N><N>CRED_TYPE_GENERIC = 0x1<N>CRED_PERSIST_SESSION = 0x1<N>CRED_PERSIST_LOCAL_MACHINE = 0x2<N>CRED_PERSIST_ENTERPRISE = 0x3<N>CRED_PRESERVE_CREDENTIAL_BLOB = 0<N><N>
<N>def CredWrite(Credential, Flags=CRED_PRESERVE_CREDENTIAL_BLOB):<N>    """ Creates or updates a stored credential.<N><N>    Parameters<N>    ----------<N>    Credential : dict<N>        A dictionary corresponding to the PyWin32 ``PyCREDENTIAL``<N>        structure.<N>    Flags : int<N>        Always pass ``CRED_PRESERVE_CREDENTIAL_BLOB`` (i.e. 0).<N><N>
    """<N>    c_creds = _authentication.CREDENTIAL.fromdict(Credential, Flags)<N>    c_pcreds = _authentication.PCREDENTIAL(c_creds)<N>    with _pywin32error():<N>        _authentication._CredWrite(c_pcreds, 0)<N><N><N>def CredRead(TargetName, Type, Flags=0):<N>    """ Retrieves a stored credential.<N><N>
    Parameters<N>    ----------<N>    TargetName : unicode<N>        The target name to fetch from the keyring.<N>    Type : int<N>        One of the CRED_TYPE_* constants.<N>    Flags : int<N>        Reserved, always use 0.<N><N>    Returns<N>    -------<N>    credentials : dict<N>        ``None`` if the target name was not found or A dictionary<N>        corresponding to the PyWin32 ``PyCREDENTIAL`` structure.<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N><N>from win32ctypes.pywin32 import pywintypes<N>from win32ctypes.pywin32 import win32api<N>from win32ctypes.pywin32 import win32cred<N><N>__all__ = ['win32api', 'win32cred', 'pywintypes']<N>
import sys<N><N>__all__ = ['TestCase']<N><N>if sys.version_info[:2] == (2, 6):<N>    import contextlib<N>    from unittest import TestCase as BaseTestCase<N><N>    class SkipException(Exception):<N>        pass<N><N>    class ExceptionContext(object):<N><N>
        def __init__(self):<N>            self.exception = None<N><N>    class TestCase(BaseTestCase):<N><N>        def assertIs(self, a, b):<N>            self.assertTrue(a is b)<N><N>        def assertIsNot(self, a, b):<N>            self.assertTrue(a is not b)<N><N>
        def assertIsNone(self, a):<N>            self.assertTrue(a is None)<N><N>        def assertIsNotNone(self, a):<N>            self.assertTrue(a is not None)<N><N>        def assertIn(self, a, b):<N>            self.assertTrue(a in b)<N><N>        def assertNotIn(self, a, b):<N>            self.assertTrue(a not in b)<N><N>
        def assertIsInstance(self, a, b):<N>            self.assertTrue(isinstance(a, b))<N><N>        def assertNotIsInstance(self, a, b):<N>            self.assertTrue(not isinstance(a, b))<N><N>        def assertSequenceEqual(self, a, b, msg=None, seq_type=None):<N>            return self.assertEqual(tuple(a), tuple(b), msg=msg)<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>import os<N>import sys<N>import unittest<N>import contextlib<N>import tempfile<N>import shutil<N><N>
import win32api<N><N>from win32ctypes import pywin32<N>from win32ctypes.pywin32.pywintypes import error<N>from win32ctypes.tests import compat<N><N><N>skip_on_wine = 'SKIP_WINE_KNOWN_FAILURES' in os.environ<N><N><N>class TestWin32API(compat.TestCase):<N><N>
    # the pywin32ctypes implementation<N>    module = pywin32.win32api<N><N>    def setUp(self):<N>        self.tempdir = tempfile.mkdtemp()<N>        shutil.copy(sys.executable, self.tempdir)<N><N>    def tearDown(self):<N>        shutil.rmtree(self.tempdir)<N><N>
    @contextlib.contextmanager<N>    def load_library(self, module, library=sys.executable, flags=0x2):<N>        handle = module.LoadLibraryEx(library, 0, flags)<N>        try:<N>            yield handle<N>        finally:<N>            module.FreeLibrary(handle)<N><N>
    @contextlib.contextmanager<N>    def resource_update(self, module, library=sys.executable):<N>        handle = module.BeginUpdateResource(library, False)<N>        try:<N>            yield handle<N>        finally:<N>            module.EndUpdateResource(handle, False)<N><N>
    def test_load_library_ex(self):<N>        with self.load_library(win32api) as expected:<N>            with self.load_library(self.module) as handle:<N>                self.assertEqual(handle, expected)<N><N>        with self.assertRaises(error):<N>            self.module.LoadLibraryEx(u'ttt.dll', 0, 0x2)<N><N>
    def test_free_library(self):<N>        with self.load_library(win32api) as handle:<N>            self.assertTrue(win32api.FreeLibrary(handle) is None)<N>            self.assertNotEqual(self.module.FreeLibrary(handle), 0)<N><N>        with self.assertRaises(error):<N>            self.module.FreeLibrary(-3)<N><N>
    def test_enum_resource_types(self):<N>        with self.load_library(win32api, u'shell32.dll') as handle:<N>            expected = win32api.EnumResourceTypes(handle)<N><N>        with self.load_library(pywin32.win32api, u'shell32.dll') as handle:<N>            resource_types = self.module.EnumResourceTypes(handle)<N><N>
#<N># (C) Copyright 2014 Enthought, Inc., Austin, TX<N># All right reserved.<N>#<N># This file is open source software distributed according to the terms in<N># LICENSE.txt<N>#<N>from __future__ import absolute_import<N>import os<N>import sys<N>import unittest<N><N>
import win32cred<N><N>from win32ctypes.core._winerrors import ERROR_NOT_FOUND<N>from win32ctypes.pywin32.pywintypes import error<N>from win32ctypes.pywin32.win32cred import (<N>    CredDelete, CredRead, CredWrite,<N>    CRED_PERSIST_ENTERPRISE, CRED_TYPE_GENERIC)<N>from win32ctypes.tests import compat<N><N>
# find the pywin32 version<N>version_file = os.path.join(<N>    os.path.dirname(os.path.dirname(win32cred.__file__)), 'pywin32.version.txt')<N>if os.path.exists(version_file):<N>    with open(version_file) as handle:<N>        pywin32_build = handle.read().strip()<N>else:<N>    pywin32_build = None<N><N>
<N>class TestCred(compat.TestCase):<N><N>    @unittest.skipIf(<N>        pywin32_build == "223" and sys.version_info[:2] == (3,7),<N>        "pywin32 version 223 bug with CredRead (mhammond/pywin32#1232)")<N>    def test_write_to_pywin32(self):<N>        username = u"john"<N>        password = u"doefsajfsakfj"<N>        comment = u"Created by MiniPyWin32Cred test suite"<N><N>
        target = "{0}@{1}".format(username, password)<N><N>        credentials = {"Type": CRED_TYPE_GENERIC,<N>                       "TargetName": target,<N>                       "UserName": username,<N>                       "CredentialBlob": password,<N>                       "Comment": comment,<N>                       "Persist": CRED_PERSIST_ENTERPRISE}<N><N>
        CredWrite(credentials)<N><N>        res = win32cred.CredRead(<N>            TargetName=target, Type=CRED_TYPE_GENERIC)<N><N>        self.assertEqual(res["Type"], CRED_TYPE_GENERIC)<N>        self.assertEqual(res["UserName"], username)<N>        self.assertEqual(res["TargetName"], target)<N>        self.assertEqual(res["Comment"], comment)<N>        self.assertEqual(<N>            res["CredentialBlob"].decode('utf-16'), password)<N><N>
<N>__all__ = ['Composer', 'ComposerError']<N><N>from .error import MarkedYAMLError<N>from .events import *<N>from .nodes import *<N><N>class ComposerError(MarkedYAMLError):<N>    pass<N><N>class Composer:<N><N>    def __init__(self):<N>        self.anchors = {}<N><N>
    def check_node(self):<N>        # Drop the STREAM-START event.<N>        if self.check_event(StreamStartEvent):<N>            self.get_event()<N><N>        # If there are more documents available?<N>        return not self.check_event(StreamEndEvent)<N><N>
    def get_node(self):<N>        # Get the root node of the next document.<N>        if not self.check_event(StreamEndEvent):<N>            return self.compose_document()<N><N>    def get_single_node(self):<N>        # Drop the STREAM-START event.<N>        self.get_event()<N><N>
<N>__all__ = [<N>    'BaseConstructor',<N>    'SafeConstructor',<N>    'FullConstructor',<N>    'UnsafeConstructor',<N>    'Constructor',<N>    'ConstructorError'<N>]<N><N>from .error import *<N>from .nodes import *<N><N>import collections.abc, datetime, base64, binascii, re, sys, types<N><N>
class ConstructorError(MarkedYAMLError):<N>    pass<N><N>class BaseConstructor:<N><N>    yaml_constructors = {}<N>    yaml_multi_constructors = {}<N><N>    def __init__(self):<N>        self.constructed_objects = {}<N>        self.recursive_objects = {}<N>        self.state_generators = []<N>        self.deep_construct = False<N><N>
<N>__all__ = [<N>    'CBaseLoader', 'CSafeLoader', 'CFullLoader', 'CUnsafeLoader', 'CLoader',<N>    'CBaseDumper', 'CSafeDumper', 'CDumper'<N>]<N><N>from yaml._yaml import CParser, CEmitter<N><N>from .constructor import *<N><N>from .serializer import *<N>from .representer import *<N><N>
from .resolver import *<N><N>class CBaseLoader(CParser, BaseConstructor, BaseResolver):<N><N>    def __init__(self, stream):<N>        CParser.__init__(self, stream)<N>        BaseConstructor.__init__(self)<N>        BaseResolver.__init__(self)<N><N>
class CSafeLoader(CParser, SafeConstructor, Resolver):<N><N>    def __init__(self, stream):<N>        CParser.__init__(self, stream)<N>        SafeConstructor.__init__(self)<N>        Resolver.__init__(self)<N><N>class CFullLoader(CParser, FullConstructor, Resolver):<N><N>
    def __init__(self, stream):<N>        CParser.__init__(self, stream)<N>        FullConstructor.__init__(self)<N>        Resolver.__init__(self)<N><N>class CUnsafeLoader(CParser, UnsafeConstructor, Resolver):<N><N>    def __init__(self, stream):<N>        CParser.__init__(self, stream)<N>        UnsafeConstructor.__init__(self)<N>        Resolver.__init__(self)<N><N>
class CLoader(CParser, Constructor, Resolver):<N><N>    def __init__(self, stream):<N>        CParser.__init__(self, stream)<N>        Constructor.__init__(self)<N>        Resolver.__init__(self)<N><N>class CBaseDumper(CEmitter, BaseRepresenter, BaseResolver):<N><N>
<N># Emitter expects events obeying the following grammar:<N># stream ::= STREAM-START document* STREAM-END<N># document ::= DOCUMENT-START node DOCUMENT-END<N># node ::= SCALAR | sequence | mapping<N># sequence ::= SEQUENCE-START node* SEQUENCE-END<N># mapping ::= MAPPING-START (node node)* MAPPING-END<N><N>
<N>__all__ = ['Mark', 'YAMLError', 'MarkedYAMLError']<N><N>class Mark:<N><N>    def __init__(self, name, index, line, column, buffer, pointer):<N>        self.name = name<N>        self.index = index<N>        self.line = line<N>        self.column = column<N>        self.buffer = buffer<N>        self.pointer = pointer<N><N>
<N># Abstract classes.<N><N>class Event(object):<N>    def __init__(self, start_mark=None, end_mark=None):<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N>    def __repr__(self):<N>        attributes = [key for key in ['anchor', 'tag', 'implicit', 'value']<N>                if hasattr(self, key)]<N>        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))<N>                for key in attributes])<N>        return '%s(%s)' % (self.__class__.__name__, arguments)<N><N>
<N>__all__ = ['BaseLoader', 'FullLoader', 'SafeLoader', 'Loader', 'UnsafeLoader']<N><N>from .reader import *<N>from .scanner import *<N>from .parser import *<N>from .composer import *<N>from .constructor import *<N>from .resolver import *<N><N>class BaseLoader(Reader, Scanner, Parser, Composer, BaseConstructor, BaseResolver):<N><N>
    def __init__(self, stream):<N>        Reader.__init__(self, stream)<N>        Scanner.__init__(self)<N>        Parser.__init__(self)<N>        Composer.__init__(self)<N>        BaseConstructor.__init__(self)<N>        BaseResolver.__init__(self)<N><N>
class FullLoader(Reader, Scanner, Parser, Composer, FullConstructor, Resolver):<N><N>    def __init__(self, stream):<N>        Reader.__init__(self, stream)<N>        Scanner.__init__(self)<N>        Parser.__init__(self)<N>        Composer.__init__(self)<N>        FullConstructor.__init__(self)<N>        Resolver.__init__(self)<N><N>
class SafeLoader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):<N><N>    def __init__(self, stream):<N>        Reader.__init__(self, stream)<N>        Scanner.__init__(self)<N>        Parser.__init__(self)<N>        Composer.__init__(self)<N>        SafeConstructor.__init__(self)<N>        Resolver.__init__(self)<N><N>
class Loader(Reader, Scanner, Parser, Composer, Constructor, Resolver):<N><N>    def __init__(self, stream):<N>        Reader.__init__(self, stream)<N>        Scanner.__init__(self)<N>        Parser.__init__(self)<N>        Composer.__init__(self)<N>        Constructor.__init__(self)<N>        Resolver.__init__(self)<N><N>
# UnsafeLoader is the same as Loader (which is and was always unsafe on<N># untrusted input). Use of either Loader or UnsafeLoader should be rare, since<N># FullLoad should be able to load almost all YAML safely. Loader is left intact<N># to ensure backwards compatibility.<N>class UnsafeLoader(Reader, Scanner, Parser, Composer, Constructor, Resolver):<N><N>
    def __init__(self, stream):<N>        Reader.__init__(self, stream)<N>        Scanner.__init__(self)<N>        Parser.__init__(self)<N>        Composer.__init__(self)<N>        Constructor.__init__(self)<N>        Resolver.__init__(self)<N><N><N>
<N>__all__ = ['BaseRepresenter', 'SafeRepresenter', 'Representer',<N>    'RepresenterError']<N><N>from .error import *<N>from .nodes import *<N><N>import datetime, copyreg, types, base64, collections<N><N>class RepresenterError(YAMLError):<N>    pass<N><N>
class BaseRepresenter:<N><N>    yaml_representers = {}<N>    yaml_multi_representers = {}<N><N>    def __init__(self, default_style=None, default_flow_style=False, sort_keys=True):<N>        self.default_style = default_style<N>        self.sort_keys = sort_keys<N>        self.default_flow_style = default_flow_style<N>        self.represented_objects = {}<N>        self.object_keeper = []<N>        self.alias_key = None<N><N>
<N>__all__ = ['BaseResolver', 'Resolver']<N><N>from .error import *<N>from .nodes import *<N><N>import re<N><N>class ResolverError(YAMLError):<N>    pass<N><N>class BaseResolver:<N><N>    DEFAULT_SCALAR_TAG = 'tag:yaml.org,2002:str'<N>    DEFAULT_SEQUENCE_TAG = 'tag:yaml.org,2002:seq'<N>    DEFAULT_MAPPING_TAG = 'tag:yaml.org,2002:map'<N><N>
<N># Scanner produces tokens of the following types:<N># STREAM-START<N># STREAM-END<N># DIRECTIVE(name, value)<N># DOCUMENT-START<N># DOCUMENT-END<N># BLOCK-SEQUENCE-START<N># BLOCK-MAPPING-START<N># BLOCK-END<N># FLOW-SEQUENCE-START<N># FLOW-MAPPING-START<N># FLOW-SEQUENCE-END<N># FLOW-MAPPING-END<N># BLOCK-ENTRY<N># FLOW-ENTRY<N># KEY<N># VALUE<N># ALIAS(value)<N># ANCHOR(value)<N># TAG(value)<N># SCALAR(value, plain, style)<N>#<N># Read comments in the Scanner code for more details.<N>#<N><N>
__all__ = ['Scanner', 'ScannerError']<N><N>from .error import MarkedYAMLError<N>from .tokens import *<N><N>class ScannerError(MarkedYAMLError):<N>    pass<N><N>class SimpleKey:<N>    # See below simple keys treatment.<N><N>    def __init__(self, token_number, required, index, line, column, mark):<N>        self.token_number = token_number<N>        self.required = required<N>        self.index = index<N>        self.line = line<N>        self.column = column<N>        self.mark = mark<N><N>
<N>class Token(object):<N>    def __init__(self, start_mark, end_mark):<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N>    def __repr__(self):<N>        attributes = [key for key in self.__dict__<N>                if not key.endswith('_mark')]<N>        attributes.sort()<N>        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))<N>                for key in attributes])<N>        return '%s(%s)' % (self.__class__.__name__, arguments)<N><N>
#class BOMToken(Token):<N>#    id = '<byte order mark>'<N><N>class DirectiveToken(Token):<N>    id = '<directive>'<N>    def __init__(self, name, value, start_mark, end_mark):<N>        self.name = name<N>        self.value = value<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N><N>
class DocumentStartToken(Token):<N>    id = '<document start>'<N><N>class DocumentEndToken(Token):<N>    id = '<document end>'<N><N>class StreamStartToken(Token):<N>    id = '<stream start>'<N>    def __init__(self, start_mark=None, end_mark=None,<N>            encoding=None):<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N>        self.encoding = encoding<N><N>
class StreamEndToken(Token):<N>    id = '<stream end>'<N><N>class BlockSequenceStartToken(Token):<N>    id = '<block sequence start>'<N><N>class BlockMappingStartToken(Token):<N>    id = '<block mapping start>'<N><N>class BlockEndToken(Token):<N>    id = '<block end>'<N><N>
class FlowSequenceStartToken(Token):<N>    id = '['<N><N>class FlowMappingStartToken(Token):<N>    id = '{'<N><N>class FlowSequenceEndToken(Token):<N>    id = ']'<N><N>class FlowMappingEndToken(Token):<N>    id = '}'<N><N>class KeyToken(Token):<N>    id = '?'<N><N>
class ValueToken(Token):<N>    id = ':'<N><N>class BlockEntryToken(Token):<N>    id = '-'<N><N>class FlowEntryToken(Token):<N>    id = ','<N><N>class AliasToken(Token):<N>    id = '<alias>'<N>    def __init__(self, value, start_mark, end_mark):<N>        self.value = value<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N><N>
class AnchorToken(Token):<N>    id = '<anchor>'<N>    def __init__(self, value, start_mark, end_mark):<N>        self.value = value<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N><N>class TagToken(Token):<N>    id = '<tag>'<N>    def __init__(self, value, start_mark, end_mark):<N>        self.value = value<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N><N>
class ScalarToken(Token):<N>    id = '<scalar>'<N>    def __init__(self, value, plain, start_mark, end_mark, style=None):<N>        self.value = value<N>        self.plain = plain<N>        self.start_mark = start_mark<N>        self.end_mark = end_mark<N>        self.style = style<N><N>
<N>from .error import *<N><N>from .tokens import *<N>from .events import *<N>from .nodes import *<N><N>from .loader import *<N>from .dumper import *<N><N>__version__ = '6.0'<N>try:<N>    from .cyaml import *<N>    __with_libyaml__ = True<N>except ImportError:<N>    __with_libyaml__ = False<N><N>
import io<N><N>#------------------------------------------------------------------------------<N># XXX "Warnings control" is now deprecated. Leaving in the API function to not<N># break code that uses it.<N>#------------------------------------------------------------------------------<N>def warnings(settings=None):<N>    if settings is None:<N>        return {}<N><N>
#------------------------------------------------------------------------------<N>def scan(stream, Loader=Loader):<N>    """<N>    Scan a YAML stream and produce scanning tokens.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        while loader.check_token():<N>            yield loader.get_token()<N>    finally:<N>        loader.dispose()<N><N>
def parse(stream, Loader=Loader):<N>    """<N>    Parse a YAML stream and produce parsing events.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        while loader.check_event():<N>            yield loader.get_event()<N>    finally:<N>        loader.dispose()<N><N>
def compose(stream, Loader=Loader):<N>    """<N>    Parse the first YAML document in a stream<N>    and produce the corresponding representation tree.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        return loader.get_single_node()<N>    finally:<N>        loader.dispose()<N><N>
def compose_all(stream, Loader=Loader):<N>    """<N>    Parse all YAML documents in a stream<N>    and produce corresponding representation trees.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        while loader.check_node():<N>            yield loader.get_node()<N>    finally:<N>        loader.dispose()<N><N>
def load(stream, Loader):<N>    """<N>    Parse the first YAML document in a stream<N>    and produce the corresponding Python object.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        return loader.get_single_data()<N>    finally:<N>        loader.dispose()<N><N>
def load_all(stream, Loader):<N>    """<N>    Parse all YAML documents in a stream<N>    and produce corresponding Python objects.<N>    """<N>    loader = Loader(stream)<N>    try:<N>        while loader.check_data():<N>            yield loader.get_data()<N>    finally:<N>        loader.dispose()<N><N>
def full_load(stream):<N>    """<N>    Parse the first YAML document in a stream<N>    and produce the corresponding Python object.<N><N>    Resolve all tags except those known to be<N>    unsafe on untrusted input.<N>    """<N>    return load(stream, FullLoader)<N><N>
def full_load_all(stream):<N>    """<N>    Parse all YAML documents in a stream<N>    and produce corresponding Python objects.<N><N>    Resolve all tags except those known to be<N>    unsafe on untrusted input.<N>    """<N>    return load_all(stream, FullLoader)<N><N>
def safe_load(stream):<N>    """<N>    Parse the first YAML document in a stream<N>    and produce the corresponding Python object.<N><N>    Resolve only basic YAML tags. This is known<N>    to be safe for untrusted input.<N>    """<N>    return load(stream, SafeLoader)<N><N>
def safe_load_all(stream):<N>    """<N>    Parse all YAML documents in a stream<N>    and produce corresponding Python objects.<N><N>    Resolve only basic YAML tags. This is known<N>    to be safe for untrusted input.<N>    """<N>    return load_all(stream, SafeLoader)<N><N>
def unsafe_load(stream):<N>    """<N>    Parse the first YAML document in a stream<N>    and produce the corresponding Python object.<N><N>    Resolve all tags, even those known to be<N>    unsafe on untrusted input.<N>    """<N>    return load(stream, UnsafeLoader)<N><N>
def unsafe_load_all(stream):<N>    """<N>    Parse all YAML documents in a stream<N>    and produce corresponding Python objects.<N><N>    Resolve all tags, even those known to be<N>    unsafe on untrusted input.<N>    """<N>    return load_all(stream, UnsafeLoader)<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2005-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N># Installing `osgeo` Conda packages requires to set `GDAL_DATA`<N><N>is_win = sys.platform.startswith('win')<N>if is_win:<N><N>    gdal_data = os.path.join(sys._MEIPASS, 'data', 'gdal')<N>    if not os.path.exists(gdal_data):<N><N>
        gdal_data = os.path.join(sys._MEIPASS, 'Library', 'share', 'gdal')<N>        # last attempt, check if one of the required file is in the generic folder Library/data<N>        if not os.path.exists(os.path.join(gdal_data, 'gcs.csv')):<N>            gdal_data = os.path.join(sys._MEIPASS, 'Library', 'data')<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2021, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import pygraphviz<N><N># Override pygraphviz.AGraph._which method to search for graphviz executables inside sys._MEIPASS<N>if hasattr(pygraphviz.AGraph, '_which'):<N>    def _pygraphviz_override_which(self, name):<N>        import os<N>        import sys<N>        import platform<N><N>
        program_name = name<N>        if platform.system() == "Windows":<N>            program_name += ".exe"<N><N>        program_path = os.path.join(sys._MEIPASS, program_name)<N>        if not os.path.isfile(program_path):<N>            raise ValueError(f"Prog {name} not found in the PyInstaller-frozen application bundle!")<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2015-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
import os<N>import sys<N><N># Installing `pyproj` Conda packages requires to set `PROJ_LIB`<N><N>is_win = sys.platform.startswith('win')<N>if is_win:<N><N>    proj_data = os.path.join(sys._MEIPASS, 'Library', 'share', 'proj')<N><N>else:<N>    proj_data = os.path.join(sys._MEIPASS, 'share', 'proj')<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2005-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
<N># 'traitlets' uses module 'inspect' from default Python library to inspect<N># source code of modules. However, frozen app does not contain source code<N># of Python modules.<N>#<N># hook-IPython depends on module 'traitlets'.<N><N>import traitlets.traitlets<N><N>
#-----------------------------------------------------------------------------<N># Copyright (c) 2013-2020, PyInstaller Development Team.<N>#<N># Licensed under the Apache License, Version 2.0 (the "License");<N># you may not use this file except in compliance with the License.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N>#-----------------------------------------------------------------------------<N><N>
<N>import ctypes<N>import glob<N>import os<N>import sys<N># Pyusb changed these libusb module names in commit 2082e7.<N>try:<N>    import usb.backend.libusb10 as libusb10<N>except:<N>    import usb.backend.libusb1 as libusb10<N>try:<N>    import usb.backend.libusb01 as libusb01<N>except:<N>    import usb.backend.libusb0 as libusb01 <N>import usb.backend.openusb as openusb<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N>import os<N>DIR = os.path.dirname(__file__)<N><N>
"""<N>This sub-package includes runtime hooks for pyinstaller.<N>"""<N><N><N>def get_hook_dirs():<N>    dirs = []<N>    # For every directory and sub directory (including cwd)<N>    for path, _, _ in os.walk(DIR):<N>        # Add the norm'd path to dirs<N>        dirs.append(os.path.normpath(path))<N>    <N>    return dirs<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>from PyInstaller.utils.hooks import collect_data_files<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>AnyIO contains a number of back-ends as dynamically imported modules.<N>This hook was tested against AnyIO v1.4.0.<N>"""<N><N>from PyInstaller.utils.hooks import collect_submodules<N><N>hiddenimports = collect_submodules('anyio._backends')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Import hook for appdirs.<N><N>On Windows, appdirs tries 2 different methods to get well-known directories<N>from the system: First with win32com, then with ctypes. Excluding win32com here<N>avoids including all the win32com related DLLs in programs that don't include<N>them otherwise.<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>APScheduler uses entry points to dynamically load executors, job<N>stores and triggers.<N>This hook was tested against APScheduler 3.6.3.<N>"""<N><N>from PyInstaller.utils.hooks import (collect_submodules, copy_metadata,<N>                                     is_module_satisfies)<N><N>
if is_module_satisfies("apscheduler < 4"):<N>    if is_module_satisfies("pyinstaller >= 4.4"):<N>        datas = copy_metadata('APScheduler', recursive=True)<N>    else:<N>        datas = copy_metadata('APScheduler')<N><N>    hiddenimports = collect_submodules('apscheduler')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ["_cffi_backend"]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_submodules, \<N>    copy_metadata, is_module_satisfies<N><N># Astropy includes a number of non-Python files that need to be present<N># at runtime, so we include these explicitly here.<N>datas = collect_data_files('astropy')<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>from PyInstaller.utils.hooks import collect_submodules<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>"""<N>Avro is a serialization and RPC framework.<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for Bacon (https://github.com/aholkner/bacon)<N># Bacon requires its native DLLs to be copied alongside frozen executable.<N><N>import os<N>import ctypes<N><N>from PyInstaller.compat import is_win, is_darwin<N>from PyInstaller.utils.hooks import get_package_paths<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N># hook for https://github.com/hbldh/bleak<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_dynamic_libs<N>from PyInstaller.compat import is_win<N><N>if is_win:<N>    datas = collect_data_files('bleak', subdir=r'backends\dotnet')<N>    binaries = collect_dynamic_libs('bleak')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>from PyInstaller.utils.hooks import collect_data_files<N><N># core/_templates/*<N># server/static/**/*<N># subcommands/*.py<N># bokeh/_sri.json<N><N>datas = collect_data_files('bokeh.core') + \<N>        collect_data_files('bokeh.server') + \<N>        collect_data_files('bokeh.command.subcommands', include_py_files=True) + \<N>        collect_data_files('bokeh')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>import ctypes.util<N>import os<N><N>
from PyInstaller.depend.utils import _resolveCtypesImports<N>from PyInstaller.utils.hooks import collect_data_files, logger<N><N>datas = collect_data_files("cairocffi")<N><N>binaries = []<N><N># NOTE: Update this if cairocffi requires more libraries<N>libs = ["cairo-2", "cairo", "libcairo-2"]<N><N>
try:<N>    lib_basenames = []<N>    for lib in libs:<N>        libname = ctypes.util.find_library(lib)<N>        if libname is not None:<N>            lib_basenames += [os.path.basename(libname)]<N><N>    if lib_basenames:<N>        resolved_libs = _resolveCtypesImports(lib_basenames)<N>        for resolved_lib in resolved_libs:<N>            binaries.append((resolved_lib[1], '.'))<N>except Exception as e:<N>    logger.warning("Error while trying to find system-installed Cairo library: %s", e)<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>import ctypes.util<N>import os<N><N>
from PyInstaller.depend.utils import _resolveCtypesImports<N>from PyInstaller.utils.hooks import collect_data_files, logger<N><N>datas = collect_data_files("cairosvg")<N><N>binaries = []<N><N># NOTE: Update this if cairosvg requires more libraries<N>libs = ["cairo-2", "cairo", "libcairo-2"]<N><N>
try:<N>    lib_basenames = []<N>    for lib in libs:<N>        libname = ctypes.util.find_library(lib)<N>        if libname is not None:<N>            lib_basenames += [os.path.basename(libname)]<N><N>    if lib_basenames:<N>        resolved_libs = _resolveCtypesImports(lib_basenames)<N>        for resolved_lib in resolved_libs:<N>            binaries.append((resolved_lib[1], '.'))<N>except Exception as e:<N>    logger.warning("Error while trying to find system-installed Cairo library: %s", e)<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Certifi is a carefully curated collection of Root Certificates for<N># validating the trustworthiness of SSL certificates while verifying<N># the identity of TLS hosts.<N><N># It has been extracted from the Requests project.<N><N>from PyInstaller.utils.hooks import collect_data_files<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>pythonnet requires both clr.pyd and Python.Runtime.dll, <N>but the latter isn't found by PyInstaller.<N>"""<N><N><N>import ctypes.util<N>from PyInstaller.log import logger<N><N>try:<N>    from importlib.metadata import files<N>except ImportError:<N>    from importlib_metadata import files<N><N>
datas = []<N><N>filepaths = [f for f in files('pythonnet') if 'Python.Runtime.dll' in str(f)]<N>if len(filepaths) == 1:<N>    pyruntime_path = filepaths[0]<N>    datas = [(pyruntime_path.locate(), pyruntime_path.parent.as_posix())]<N>elif len(filepaths) > 1:<N>    logger.warning('More than one Python.Runtime.dll found in site packages! Cannot resolve.')<N><N>
if len(datas) == 0:<N>    # Fallback to legacy way of finding Python.Runtime dependency<N>    library = ctypes.util.find_library('Python.Runtime')<N>    if library:<N>        datas = [(library, '.')]<N>        logger.warning('Legacy method of finding Python.Runtime.dll was used!')<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>Hook for PyCryptodome library: https://pypi.python.org/pypi/pycryptodome<N><N>PyCryptodome is an almost drop-in replacement for the now unmaintained<N>PyCrypto library. The two are mutually exclusive as they live under<N>the same package ("Crypto").<N><N>
PyCryptodome distributes dynamic libraries and builds them as if they were<N>Python C extensions (even though they are not extensions - as they can't be<N>imported by Python). It might sound a bit weird, but this decision is rooted<N>in PyPy and its partial and slow support for C extensions. However, this also<N>invalidates several of the existing methods used by PyInstaller to decide the<N>right files to pull in.<N><N>
Even though this hook is meant to help with PyCryptodome only, it will be<N>triggered also when PyCrypto is installed, so it must be tested with both.<N><N>Tested with PyCryptodome 3.5.1, PyCrypto 2.6.1, Python 2.7 & 3.6, Fedora & Windows<N>"""<N><N>
import os<N>import glob<N><N>from PyInstaller.compat import EXTENSION_SUFFIXES<N>from PyInstaller.utils.hooks import get_module_file_attribute<N><N># Include the modules as binaries in a subfolder named like the package.<N># Cryptodome's loader expects to find them inside the package directory for<N># the main module. We cannot use hiddenimports because that would add the<N># modules outside the package.<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>Hook for Cryptodome module: https://pypi.python.org/pypi/pycryptodomex<N><N>Tested with Cryptodomex 3.4.2, Python 2.7 & 3.5, Windows<N>"""<N><N>import os<N>import glob<N><N>from PyInstaller.compat import EXTENSION_SUFFIXES<N>from PyInstaller.utils.hooks import get_module_file_attribute<N><N>
# Include the modules as binaries in a subfolder named like the package.<N># Cryptodome's loader expects to find them inside the package directory for<N># the main module. We cannot use hiddenimports because that would add the<N># modules outside the package.<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for cryptography module from the Python Cryptography Authority.<N>"""<N><N>import os.path<N>import glob<N><N>from PyInstaller.compat import EXTENSION_SUFFIXES<N>from PyInstaller.utils.hooks import collect_submodules, get_module_file_attribute<N>from PyInstaller.utils.hooks import copy_metadata<N><N>
# get the package data so we can load the backends<N>datas = copy_metadata('cryptography')<N><N># Add the backends as hidden imports<N>hiddenimports = collect_submodules('cryptography.hazmat.backends')<N><N># Add the OpenSSL FFI binding modules as hidden imports<N>hiddenimports += collect_submodules('cryptography.hazmat.bindings.openssl') + ['_cffi_backend']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['decimal']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># Hook for the diStorm3 module: https://pypi.python.org/pypi/distorm3<N># Tested with distorm3 3.3.0, Python 2.7, Windows<N><N>from PyInstaller.utils.hooks import collect_dynamic_libs<N><N># distorm3 dynamic library should be in the path with other dynamic libraries.<N>binaries = collect_dynamic_libs('distorm3', destdir='.')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>from PyInstaller.utils.hooks import collect_submodules, collect_data_files<N><N>hiddenimports = (collect_submodules('docutils.languages') +<N>                 collect_submodules('docutils.writers') +<N>                 collect_submodules('docutils.parsers.rst.languages') +<N>                 collect_submodules('docutils.parsers.rst.directives'))<N>datas = collect_data_files('docutils')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later.<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Import hook for PyEnchant.<N><N>Tested with PyEnchant 1.6.6.<N>"""<N><N>import os<N><N>from PyInstaller.compat import is_darwin<N>from PyInstaller.utils.hooks import exec_statement, collect_data_files, \<N>    collect_dynamic_libs, get_installer<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>enzyme:<N>https://github.com/Diaoul/enzyme<N>"""<N><N>import os<N>from PyInstaller.utils.hooks import get_package_paths<N><N># get path of enzyme<N>ep = get_package_paths('enzyme')<N><N># add the data<N>data = os.path.join(ep[1], 'parsers', 'ebml', 'specs', 'matroska.xml')<N>datas = [(data, "enzyme/parsers/ebml/specs")]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_submodules, collect_data_files<N><N>hiddenimports = collect_submodules('faker.providers')<N>datas = (<N>    collect_data_files('text_unidecode') +  # noqa: W504<N>    collect_data_files('faker.providers', include_py_files=True)<N>)<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import eval_statement, collect_submodules<N><N>hiddenimports = collect_submodules("ffpyplayer")<N>binaries = []<N># ffpyplayer has an internal variable tells us where the libraries it was using<N>for bin in eval_statement("import ffpyplayer; print(ffpyplayer.dep_bins)"):<N>    binaries += [(bin, '.')]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for FMPy, a library to simulate Functional Mockup Units (FMUs)<N>https://github.com/CATIA-Systems/FMPy<N><N>Adds the data files that are required at runtime:<N><N>- XSD schema files<N>- dynamic libraries for the CVode solver<N>- source and header files for the compilation of c-code FMUs<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ["sql_mar"]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>from PyInstaller.utils.hooks import copy_metadata<N>datas = copy_metadata('gcloud')<N>
# ------------------------------------------------------------------<N># Copyright (c) 2005-2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Client library URL: https://googleapis.dev/python/cloudkms/latest/<N># Import Example for client library:<N># https://cloud.google.com/kms/docs/reference/libraries#client-libraries-install-python<N><N>from PyInstaller.utils.hooks import copy_metadata<N>datas = copy_metadata('google-cloud-kms')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import copy_metadata<N>from PyInstaller.utils.hooks import collect_data_files<N><N># googleapiclient.model queries the library version via<N># pkg_resources.get_distribution("google-api-python-client").version,<N># so we need to collect that package's metadata<N>datas = copy_metadata('google_api_python_client')<N>datas += collect_data_files('googleapiclient.discovery', excludes=['*.txt', '**/__pycache__'])<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># GStreamer contains a lot of plugins. We need to collect them and bundle<N># them wih the exe file.<N># We also need to resolve binary dependencies of these GStreamer plugins.<N><N><N>import glob<N>import os<N>from PyInstaller.compat import is_win<N>from PyInstaller.utils.hooks import exec_statement<N><N>
<N>hiddenimports = ['gmodule', 'gobject']<N><N>statement = """<N>import os<N>import gst<N>reg = gst.registry_get_default()<N>plug = reg.find_plugin('coreelements')<N>path = plug.get_filename()<N>print(os.path.dirname(path))<N>"""<N><N>plugin_path = exec_statement(statement)<N><N>
if is_win:<N>    # TODO Verify that on Windows gst plugins really end with .dll.<N>    pattern = os.path.join(plugin_path, '*.dll')<N>else:<N>    # Even on OSX plugins end with '.so'.<N>    pattern = os.path.join(plugin_path, '*.so')<N><N>binaries = [<N>    (os.path.join('gst_plugins', os.path.basename(f)), f)<N>    # 'f' contains the absolute path<N>    for f in glob.glob(pattern)]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>hiddenimports = ['gtkglext', 'gdkgl', 'gdkglext', 'gdk', 'gtk.gdk', 'gtk.gtkgl',<N>                 'gtk.gtkgl._gtkgl', 'gtkgl', 'pangocairo', 'pango', 'atk',<N>                 'gobject', 'gtk.glade', 'cairo', 'gio',<N>                 'gtk.keysyms']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>This modest package contains various common humanization utilities, like turning a number into a fuzzy human<N>readable duration ("3 minutes ago") or into a human readable size or throughput.<N><N>https://pypi.org/project/humanize<N><N>This hook was tested against humanize 3.5.0.<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for imageio: http://imageio.github.io/<N><N>from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N>datas = collect_data_files('imageio', subdir="resources")<N><N># imageio plugins are imported lazily since ImageIO version 2.11.0.<N># They are very light-weight, so we can safely include all of them.<N>hiddenimports = collect_submodules('imageio.plugins')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# add hooks for iminuit: https://github.com/scikit-hep/iminuit<N><N># iminuit imports subpackages through a cython module which aren't<N># found by default<N><N>from PyInstaller.utils.hooks import collect_submodules<N><N>hiddenimports = []<N><N># the iminuit package contains tests which aren't needed when distributing<N>for mod in collect_submodules('iminuit'):<N>    if not mod.startswith('iminuit.tests'):<N>        hiddenimports.append(mod)<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># Tested with IPython 4.0.0.<N><N>from PyInstaller.compat import is_win, is_darwin<N>from PyInstaller.utils.hooks import collect_data_files<N><N># Ignore 'matplotlib'. IPython contains support for matplotlib.<N># Ignore GUI libraries. IPython supports integration with GUI frameworks.<N># Assume that it will be imported by any other module when the user really<N># uses it.<N>excludedimports = ['gtk', 'matplotlib', 'PyQt4', 'PyQt5', 'PySide']<N><N>
# IPython uses 'tkinter' for clipboard access on Linux/Unix. Exclude it on Windows and OS X.<N>if is_win or is_darwin:<N>    excludedimports.append('tkinter')<N><N>datas = collect_data_files('IPython')<N><N># IPython imports extensions by changing to the extensions directory and using<N># importlib.import_module, so we need to copy over the extensions as if they<N># were data files.<N>datas += collect_data_files('IPython.extensions', include_py_files=True)<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['jinja2.ext']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = [<N>    'jinxed.terminfo.ansicon', 'jinxed.terminfo.vtwin10'<N>]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# This is needed to bundle draft3.json and draft4.json files that come<N># with jsonschema module<N><N>from PyInstaller.utils.hooks import collect_data_files, copy_metadata<N>datas = collect_data_files('jsonschema')<N>datas += copy_metadata('jsonschema')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># kinterbasdb<N>hiddenimports = ['k_exceptions', 'services', 'typeconv_naked',<N>                 'typeconv_backcompat', 'typeconv_23plus',<N>                 'typeconv_datetime_stdlib', 'typeconv_datetime_mx',<N>                 'typeconv_datetime_naked', 'typeconv_fixed_fixedpoint',<N>                 'typeconv_fixed_stdlib',  'typeconv_text_unicode',<N>                 'typeconv_util_isinstance', '_kinterbasdb', '_kiservices']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files<N># bundle xml DB files, skip other files (like DLL files on Windows)<N>datas = list(filter(lambda p: p[0].endswith('.xml'), collect_data_files('lensfunpy')))<N>hiddenimports = ['numpy', 'enum']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['lxml._elementpath', 'gzip', 'contextlib']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['lxml.etree']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N># hook for https://github.com/python-lz4/python-lz4<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>from PyInstaller.compat import is_py38<N><N><N># The MariaDB uses a .pyd file and uses import within its __init__.py<N># The decimal import seems to be hidden in Python version 3.7 and older<N>if not is_py38:<N>    hiddenimports = ['decimal']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import (<N>    collect_submodules,<N>    copy_metadata,<N>    is_module_satisfies,<N>)<N><N>hiddenimports = collect_submodules('markdown.extensions')<N><N># Markdown 3.3 introduced markdown.htmlparser submodule with hidden<N># dependency on html.parser<N>if is_module_satisfies("markdown >= 3.3"):<N>    hiddenimports += ['html.parser']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import copy_metadata, collect_data_files<N><N># MetPy requires metadata, because it queries its version via<N># pkg_resources.get_distribution(__package__).version or, in newer<N># versions, importlib.metadata.version(__package__)<N>datas = copy_metadata('metpy')<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2022 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>from PyInstaller.utils.hooks import collect_data_files<N>from PyInstaller.compat import is_win, base_prefix<N><N>import os, sys<N><N><N># mpl_toolkits.basemap (tested with v.1.0.7) is shipped with auxiliary data,<N># usually stored in mpl_toolkits\basemap\data and used to plot maps<N>datas = collect_data_files('mpl_toolkits.basemap', subdir='data')<N><N>
# check if the data has been effectively found<N>if len(datas) == 0:<N>    <N>    # - conda-specific<N><N>    if is_win:<N>        tgt_basemap_data = os.path.join('Library', 'share', 'basemap')<N>        src_basemap_data = os.path.join(base_prefix, 'Library', 'share', 'basemap')<N><N>
    else:  # both linux and darwin<N>        tgt_basemap_data = os.path.join('share', 'basemap')<N>        src_basemap_data = os.path.join(base_prefix, 'share', 'basemap')<N><N>    if os.path.exists(src_basemap_data):<N>        datas.append((src_basemap_data, tgt_basemap_data))<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># Tested with PyNaCl 0.3.0 on Mac OS X.<N><N><N>import os.path<N>import glob<N><N>from PyInstaller.compat import EXTENSION_SUFFIXES<N>from PyInstaller.utils.hooks import collect_data_files, get_module_file_attribute<N><N><N>datas = collect_data_files('nacl')<N><N>
# Include the cffi extensions as binaries in a subfolder named like the package.<N>binaries = []<N>nacl_dir = os.path.dirname(get_module_file_attribute('nacl'))<N>for ext in EXTENSION_SUFFIXES:<N>    ffimods = glob.glob(os.path.join(nacl_dir, '_lib', '*_cffi_*%s*' % ext))<N>    dest_dir = os.path.join('nacl', '_lib')<N>    for f in ffimods:<N>        binaries.append((f, dest_dir))<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, copy_metadata<N><N>datas = collect_data_files('nbconvert')<N><N># nbconvert uses entrypoints to read nbconvert.exporters from metadata file entry_points.txt.<N>datas += copy_metadata('nbconvert')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for ncclient. ncclient is a Python library that facilitates client-side<N>scripting and application development around the NETCONF protocol.<N>https://pypi.python.org/pypi/ncclient<N><N>This hook was tested with ncclient 0.4.3.<N>"""<N>from PyInstaller.utils.hooks import collect_submodules<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># hook for nltk<N>import nltk<N>import os<N>from PyInstaller.utils.hooks import collect_data_files<N><N># add datas for nltk<N>datas = collect_data_files('nltk', False)<N><N># loop through the data directories and add them<N>for p in nltk.data.path:<N>    if os.path.exists(p):<N>        datas.append((p, "nltk_data"))<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import os<N>from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N>from jupyter_core.paths import jupyter_config_path, jupyter_path<N><N># collect modules for handlers<N>hiddenimports = collect_submodules('notebook', filter=lambda name: name.endswith('.handles'))<N>hiddenimports.append('notebook.services.shutdown')<N><N>
datas = collect_data_files('notebook')<N><N># Collect share and etc folder for pre-installed extensions<N>datas += [(path, 'share/jupyter')<N>          for path in jupyter_path() if os.path.exists(path)]<N>datas += [(path, 'etc/jupyter')<N>          for path in jupyter_config_path() if os.path.exists(path)]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
"""<N>Office365-REST-Python-Client contains xml templates that are needed by some methods<N>This hook ensures that all of the data used by the package is bundled<N>"""<N><N>from PyInstaller.utils.hooks import collect_data_files<N><N>datas = collect_data_files("office365")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for PyOpenGL 3.x versions from 3.0.0b6 up. Previous versions have a<N>plugin system based on pkg_resources which is problematic to handle correctly<N>under pyinstaller; 2.x versions used to run fine without hooks, so this one<N>shouldn't hurt.<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>OpenGL_accelerate contais modules written in cython. This module<N>should speed up some functions from OpenGL module. The following<N>hiddenimports are not resolved by PyInstaller because OpenGL_accelerate<N>is compiled to native Python modules.<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2022 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N>datas = collect_data_files("panel")<N><N># Some models are lazy-loaded on runtime, so we need to collect them<N>hiddenimports = collect_submodules("panel.models")<N><N><N>
# -----------------------------------------------------------------------------<N># Copyright (c) 2013-2018, PyInstaller Development Team.<N>#<N># Distributed under the terms of the GNU General Public License with exception<N># for distributing bootloader.<N>#<N># The full license is in the file COPYING.txt, distributed with this software.<N># -----------------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Handlers are imported by a lazy-load proxy, based on a<N># name-to-package mapping. Collect all handlers to ease packaging.<N># If you want to reduce the size of your application, used<N># `--exclude-module` to remove unused ones.<N>hiddenimports = [<N>    "passlib.handlers",<N>    "passlib.handlers.digests",<N>    "configparser",<N>]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ['patsy.builtins']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N><N># Pendulum checks for locale modules via os.path.exists before import.<N># If the include_py_files option is turned off, this check fails, pendulum<N># will raise a ValueError.<N>datas = collect_data_files("pendulum.locales", include_py_files=True)<N>hiddenimports = collect_submodules("pendulum.locales")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for the pinyin package: https://pypi.python.org/pypi/pinyin<N># Tested with pinyin 0.4.0 and Python 3.6.2, on Windows 10 x64.<N><N>from PyInstaller.utils.hooks import collect_data_files<N><N># pinyin relies on 'Mandarin.dat' and 'cedict.txt.gz'<N># for character and word translation.<N>datas = collect_data_files('pinyin')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.compat import is_darwin, is_win<N><N>modules = ["platformdirs"]<N><N># platfromdirs contains dynamically loaded per-platform submodules.<N>if is_darwin:<N>    modules.append("platformdirs.macos")<N>elif is_win:<N>    modules.append("platformdirs.windows")<N>else:<N>    # default to unix for all other platforms<N>    # this includes unix, cygwin, and msys2<N>    modules.append("platformdirs.unix")<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files<N>from PyInstaller.utils.hooks import collect_submodules<N><N>datas = collect_data_files('plotly', includes=['package_data/**/*.*'])<N>hiddenimports = collect_submodules('plotly.validators') + ['pandas', 'cmath']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['mx.DateTime']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2022 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for https://pypi.org/project/pyarrow/<N><N>from PyInstaller.utils.hooks import collect_data_files, collect_dynamic_libs<N><N>hiddenimports = [<N>    "pyarrow._parquet",<N>    "pyarrow.lib",<N>    "pyarrow.compat",<N>]<N><N>datas = collect_data_files('pyarrow')<N>binaries = collect_dynamic_libs('pyarrow')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, copy_metadata<N><N># pycountry requires the ISO databases for country data.<N># Tested v1.15 on Linux/Ubuntu.<N># https://pypi.python.org/pypi/pycountry<N>datas = copy_metadata('pycountry') + collect_data_files('pycountry')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# pycparser needs two modules -- lextab.py and yacctab.py -- which it<N># generates at runtime if they cannot be imported.<N>#<N># Those modules are written to the current working directory for which<N># the running process may not have write permissions, leading to a runtime<N># exception.<N>#<N># This hook tells pyinstaller about those hidden imports, avoiding the<N># possibility of such runtime failures.<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import glob<N>import os<N>import shutil<N><N>from PyInstaller.compat import is_win, is_darwin<N>from PyInstaller.depend.bindepend import findLibrary<N><N>binaries = []<N>datas = []<N><N># List of binaries agraph.py may invoke.<N>progs = [<N>    "neato",<N>    "dot",<N>    "twopi",<N>    "circo",<N>    "fdp",<N>    "nop",<N>    "acyclic",<N>    "gvpr",<N>    "gvcolor",<N>    "ccomps",<N>    "sccmap",<N>    "tred",<N>    "sfdp",<N>    "unflatten",<N>]<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.compat import is_win, is_darwin<N>from PyInstaller.utils.hooks import collect_dynamic_libs, logger<N><N># Collect bundled mediainfo shared library (available in Windows and macOS wheels on PyPI).<N>binaries = collect_dynamic_libs("pymediainfo")<N><N>
# On linux, no wheels are available, and pymediainfo uses system shared library.<N>if not binaries and not (is_win or is_darwin):<N>    def _find_system_mediainfo_library():<N>        import os<N>        import ctypes.util<N>        from PyInstaller.depend.utils import _resolveCtypesImports<N><N>
        libname = ctypes.util.find_library("mediainfo")<N>        if libname is not None:<N>            resolved_binary = _resolveCtypesImports([os.path.basename(libname)])<N>            if resolved_binary:<N>                return resolved_binary[0][1]<N><N>
    try:<N>        mediainfo_lib = _find_system_mediainfo_library()<N>    except Exception as e:<N>        logger.warning("Error while trying to find system-installed MediaInfo library: %s", e)<N>        mediainfo_lib = None<N><N>    if mediainfo_lib:<N>        # Put the library into pymediainfo sub-directory, to keep layout consistent with that of wheels.<N>        binaries += [(mediainfo_lib, 'pymediainfo')]<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020-2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import is_module_satisfies<N><N>hiddenimports = ["decimal"]<N># In newer versions of pymssql,  the _mssql was under pymssql<N>if is_module_satisfies("pymssql > 2.1.5"):<N>    hiddenimports += ["pymssql._mssql", "uuid"]<N>else:<N>    hiddenimports += ["_mssql"]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>from PyInstaller.utils.hooks import get_pyextension_imports<N><N><N># It's hard to detect imports of binary Python module without importing it.<N># Let's try importing that module in a subprocess.<N># TODO function get_pyextension_imports() is experimental and we need<N>#      to evaluate its usage here and its suitability for other hooks.<N>hiddenimports = get_pyextension_imports('pyodbc')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import os<N>import sys<N>from PyInstaller.utils.hooks import collect_data_files, is_module_satisfies<N>from PyInstaller.compat import is_win<N><N><N>hiddenimports = [<N>    "pyproj.datadir"<N>]<N><N># Versions prior to 2.3.0 also require pyproj._datadir<N>if not is_module_satisfies("pyproj >= 2.3.0"):<N>    hiddenimports += ["pyproj._datadir"]<N><N>
# Starting with version 3.0.0, pyproj._compat is needed<N>if is_module_satisfies("pyproj >= 3.0.0"):<N>    hiddenimports += ["pyproj._compat"]<N>    # Linux and macOS also require distutils.<N>    if not is_win:<N>        hiddenimports += ["distutils.util"]<N><N>
# Data collection<N>datas = collect_data_files('pyproj')<N><N>if hasattr(sys, 'real_prefix'):  # check if in a virtual environment<N>    root_path = sys.real_prefix<N>else:<N>    root_path = sys.prefix<N><N># - conda-specific<N>if is_win:<N>    tgt_proj_data = os.path.join('Library', 'share', 'proj')<N>    src_proj_data = os.path.join(root_path, 'Library', 'share', 'proj')<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2022 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# The bundled paexec.exe file needs to be collected (as data file; on any platform)<N># because it is deployed to the remote side during execution.<N><N>from PyInstaller.utils.hooks import collect_data_files<N><N>datas = collect_data_files('pypsexec')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>pywin32 module supports frozen mode. In frozen mode it is looking<N>in sys.path for file pythoncomXX.dll. Include the pythoncomXX.dll<N>as a data file. The path to this dll is contained in __file__<N>attribute.<N>"""<N><N>import os.path<N>from PyInstaller.utils.hooks import get_pywin32_module_file_attribute<N><N>
_pth = get_pywin32_module_file_attribute('pythoncom')<N><N># Binaries that should be included with the module 'pythoncom'.<N>binaries = [<N>    (<N>        # Absolute path on hard disk.<N>        _pth,<N>        # Relative directory path in the ./dist/app_name/ directory.<N>        '.',<N>    )<N>]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>pyttsx imports drivers module based on specific platform.<N>Found at http://mrmekon.tumblr.com/post/5272210442/pyinstaller-and-pyttsx<N>"""<N><N><N>hiddenimports = [<N>    'drivers',<N>    'drivers.dummy',<N>    'drivers.espeak',<N>    'drivers.nsss',<N>    'drivers.sapi5',<N>]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# pyttsx3 conditionally imports drivers module based on specific platform.<N># https://github.com/nateshmbhat/pyttsx3/blob/5a19376a94fdef6bfaef8795539e755b1f363fbf/pyttsx3/driver.py#L40-L50<N><N>import sys<N><N>hiddenimports = ["pyttsx3.drivers", "pyttsx3.drivers.dummy"]<N><N>
# Take directly from the link above.<N>if sys.platform == 'darwin':<N>    driverName = 'nsss'<N>elif sys.platform == 'win32':<N>    driverName = 'sapi5'<N>else:<N>    driverName = 'espeak'<N># import driver module<N>name = 'pyttsx3.drivers.%s' % driverName<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>from PyInstaller.utils.hooks import collect_dynamic_libs<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>pywin32 module supports frozen mode. In frozen mode it is looking<N>in sys.path for file pywintypesXX.dll. Include the pywintypesXX.dll<N>as a data file. The path to this dll is contained in __file__<N>attribute.<N>"""<N><N>from PyInstaller.utils.hooks import get_pywin32_module_file_attribute<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for https://github.com/PyWavelets/pywt<N><N>hiddenimports = ['pywt._extensions._cwt']<N><N># NOTE: There is another project `https://github.com/Knapstad/pywt installing<N># a packagre `pywt`, too. This name clash is not much of a problem, even if<N># this hook is picked up for the other package, since PyInstaller will simply<N># skip any module added by this hook but acutally missing. If the other project<N># requires a hook, too, simply add it to this file.<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['raven.events', 'raven.processors']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['redmine.resources']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N>hiddenimports = ['warnings']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N><N># Needed for ReportLab 3<N>hiddenimports = [<N>    'reportlab.rl_settings',<N>]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_submodules<N><N># Tested on Windows 7 x64 with Python 2.7.6 x32 using ReportLab 3.0<N># This has been observed to *not* work on ReportLab 2.7<N>hiddenimports = collect_submodules('reportlab.pdfbase',<N>                  lambda name: name.startswith('reportlab.pdfbase._fontdata_'))<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.compat import is_pure_conda<N>from PyInstaller.utils.hooks import collect_dynamic_libs, conda<N><N>binaries = collect_dynamic_libs('rtree', destdir='rtree/lib')<N>if not binaries and is_pure_conda:<N>    binaries = conda.collect_dynamic_libs('libspatialindex', dest='rtree/lib', dependencies=False)<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import os<N>from ctypes.util import find_library<N><N>from PyInstaller.utils.hooks import get_package_paths<N>from PyInstaller.utils.hooks import is_module_satisfies<N>from PyInstaller import compat<N><N># Necessary when using the vectorized subpackage<N>hiddenimports = ['shapely.prepared']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files<N><N># Shotgun is using "six" to import these and<N># PyInstaller does not seem to catch them correctly.<N>hiddenimports = ["xmlrpc", "xmlrpc.client"]<N><N># Collect the following files:<N>#   /shotgun_api3/lib/httplib2/python2/cacerts.txt<N>#   /shotgun_api3/lib/httplib2/python3/cacerts.txt<N>#   /shotgun_api3/lib/certifi/cacert.pem<N>datas = collect_data_files("shotgun_api3")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import is_module_satisfies<N><N># The following missing module prevents import of skimage.feature<N># with skimage 0.18.x.<N>if is_module_satisfies("scikit_image >= 0.18.0"):<N>    hiddenimports = ['skimage.filters.rank.core_cy_3d', ]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># This hook was tested with scikit-image (skimage) 0.14.1:<N># https://scikit-image.org<N><N>from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N>datas = collect_data_files("skimage.io._plugins")<N>hiddenimports = collect_submodules('skimage.io._plugins')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>from PyInstaller.utils.hooks import collect_data_files<N><N>
# Hook tested with scikit-image (skimage) 0.9.3 on Mac OS 10.9 and Windows 7<N># 64-bit<N>hiddenimports = ['skimage.draw.draw',<N>                 'skimage._shared.geometry',<N>                 'skimage._shared.transform',<N>                 'skimage.filters.rank.core_cy']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import is_module_satisfies<N><N># sklearn.linear_model in scikit-learn 0.24.x has a hidden import of<N># sklearn.utils._weight_vector<N>if is_module_satisfies("scikit_learn >= 0.24"):<N>    hiddenimports = ['sklearn.utils._weight_vector', ]<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import is_module_satisfies<N><N>hiddenimports = []<N><N>if is_module_satisfies("scikit_learn >= 0.22"):<N>    # 0.22 and later<N>    hiddenimports += [<N>        'sklearn.neighbors._typedefs',<N>        'sklearn.neighbors._quad_tree',<N>    ]<N>else:<N>    # 0.21<N>    hiddenimports += [<N>        'sklearn.neighbors.typedefs',<N>        'sklearn.neighbors.quad_tree',<N>    ]<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ['sklearn.tree._utils', ]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ['sklearn.utils._cython_blas', ]<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>pysoundfile:<N>https://github.com/bastibe/SoundFile<N>"""<N><N>import os<N><N>from PyInstaller.compat import is_win, is_darwin<N>from PyInstaller.utils.hooks import get_package_paths<N><N># get path of soundfile<N>sfp = get_package_paths('soundfile')<N><N>
# add binaries packaged by soundfile on OSX and Windows<N># an external dependency (libsndfile) is used on GNU/Linux<N>path = None<N>if is_win:<N>    path = os.path.join(sfp[0], '_soundfile_data')<N>elif is_darwin:<N>    path = os.path.join(sfp[0], '_soundfile_data', 'libsndfile.dylib')<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
"""<N>Spacy contains hidden imports and data files which are needed to import it<N>"""<N><N>from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N>datas = collect_data_files("spacy")<N>hiddenimports = collect_submodules("spacy")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# Hook for speech_recognition: https://pypi.python.org/pypi/SpeechRecognition/<N># Tested on Windows 8.1 x64 with SpeechRecognition 1.5<N><N>from PyInstaller.utils.hooks import collect_data_files<N><N>datas = collect_data_files("speech_recognition")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_submodules, copy_metadata<N><N>hiddenimports = collect_submodules("sunpy", filter=lambda x: "tests" not in x.split("."))<N>datas = collect_data_files("sunpy", excludes=['**/tests/', '**/test/'])<N>datas += collect_data_files("drms")<N>datas += copy_metadata("sunpy")<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import is_module_satisfies, \<N>    collect_submodules, collect_data_files<N><N>tf_pre_1_15_0 = is_module_satisfies("tensorflow < 1.15.0")<N>tf_post_1_15_0 = is_module_satisfies("tensorflow >= 1.15.0")<N>tf_pre_2_0_0 = is_module_satisfies("tensorflow < 2.0.0")<N>tf_pre_2_2_0 = is_module_satisfies("tensorflow < 2.2.0")<N><N>
<N># Exclude from data collection:<N>#  - development headers in include subdirectory<N>#  - XLA AOT runtime sources<N>#  - libtensorflow_framework shared library (to avoid duplication)<N>#  - import library (.lib) files (Windows-only)<N>data_excludes = [<N>    "include",<N>    "xla_aot_runtime_src",<N>    "libtensorflow_framework.*",<N>    "**/*.lib",<N>]<N><N>
# Under tensorflow 2.3.0 (the most recent version at the time of writing),<N># _pywrap_tensorflow_internal extension module ends up duplicated; once<N># as an extension, and once as a shared library. In addition to increasing<N># program size, this also causes problems on macOS, so we try to prevent<N># the extension module "variant" from being picked up.<N>#<N># See pyinstaller/pyinstaller-hooks-contrib#49 for details.<N>excluded_submodules = ['tensorflow.python._pywrap_tensorflow_internal']<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the Apache License 2.0<N>#<N># The full license is available in LICENSE.APL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: Apache-2.0<N># ------------------------------------------------------------------<N><N>
"""<N>Thinc contains data files and hidden imports. This hook was created to make spacy work correctly.<N>"""<N>from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N>datas = collect_data_files("thinc")<N>hiddenimports = collect_submodules("thinc")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for tinycss2. tinycss2 is a low-level CSS parser and generator.<N>https://github.com/Kozea/tinycss2<N>"""<N>from PyInstaller.utils.hooks import collect_data_files<N><N><N># Hook no longer required for tinycss2 >= 1.0.0<N>def hook(hook_api):<N>    hook_api.add_datas(collect_data_files(hook_api.__name__))<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>"""<N>Hook for use with the ttkthemes package<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>"""<N>Hook for use with the ttkwidgets package<N><N>
ttkwidgets provides a set of cross-platform widgets for Tkinter/ttk,<N>some of which depend on image files in order to function properly.<N><N>These images files are all provided in the `ttkwidgets/assets` folder,<N>which has to be copied by PyInstaller.<N><N>
This hook has been tested on Ubuntu 18.04 (Python 3.6.8 venv) and<N>Windows 7 (Python 3.5.4 system-wide).<N><N>>>> import tkinter as tk<N>>>> from ttkwidgets import CheckboxTreeview<N>>>><N>>>> window = tk.Tk()<N>>>> tree = CheckboxTreeview(window)<N>>>> tree.insert("", tk.END, "test", text="Hello World!")<N>>>> tree.insert("test", tk.END, "test2", text="Hello World again!")<N>>>> tree.insert("test", tk.END, "test3", text="Hello World again again!")<N>>>> tree.pack()<N>>>> window.mainloop()<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
from PyInstaller.utils.hooks import collect_data_files, collect_submodules<N><N># Collect timezone data files<N>datas = collect_data_files("tzdata")<N><N># Collect submodules; each data subdirectory is in fact a package<N># (e.g., zoneinfo.Europe), so we need its __init__.py for data files<N># (e.g., zoneinfo/Europe/Ljubljana) to be discoverable via<N># importlib.resources<N>hiddenimports = collect_submodules("tzdata")<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
"""<N>Pyinstaller hook for u1db module<N><N>This hook was tested with:<N>- u1db 0.1.4 : https://launchpad.net/u1db<N>- Python 2.7.10<N>- Linux Debian GNU/Linux unstable (sid)<N><N>Test script used for testing:<N><N>    import u1db<N>    db = u1db.open("mydb1.u1db", create=True)<N>    doc = db.create_doc({"key": "value"}, doc_id="testdoc")<N>    print doc.content<N>    print doc.doc_id<N>"""<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N># Hook for the unidecode package: https://pypi.python.org/pypi/unidecode<N># Tested with Unidecode 0.4.21 and Python 3.6.2, on Windows 10 x64.<N><N>from PyInstaller.utils.hooks import collect_submodules<N><N># Unidecode dynamically imports modules with relevant character mappings.<N># Non-ASCII characters are ignored if the mapping files are not found.<N>hiddenimports = collect_submodules('unidecode')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import ctypes.util<N>import os<N><N>from PyInstaller.depend.utils import _resolveCtypesImports<N>from PyInstaller.compat import is_cygwin, getenv<N>from PyInstaller.utils.hooks import logger<N><N><N># Include glob for library lookup in run-time hook.<N>hiddenimports = ['glob']<N><N>
# https://github.com/walac/pyusb/blob/master/docs/faq.rst<N># https://github.com/walac/pyusb/blob/master/docs/tutorial.rst<N><N>binaries = []<N><N><N># Running usb.core.find() in this script crashes Ubuntu 14.04LTS,<N># let users circumvent pyusb discovery with an environment variable.<N>skip_pyusb_discovery = \<N>    bool(getenv('PYINSTALLER_USB_HOOK_SKIP_PYUSB_DISCOVERY'))<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>import os<N>if os.name == 'posix':<N>    hiddenimports = ['libvtkCommonPython','libvtkFilteringPython','libvtkIOPython','libvtkImagingPython','libvtkGraphicsPython','libvtkRenderingPython','libvtkHybridPython','libvtkParallelPython','libvtkPatentedPython']<N>else:<N>    hiddenimports = ['vtkCommonPython','vtkFilteringPython','vtkIOPython','vtkImagingPython','vtkGraphicsPython','vtkRenderingPython','vtkHybridPython','vtkParallelPython','vtkPatentedPython']<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>from PyInstaller.utils.hooks import copy_metadata<N><N>datas = copy_metadata("web3")<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>from PyInstaller.utils.hooks import collect_data_files<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# hook for https://github.com/r0x0r/pywebview<N><N>from PyInstaller.utils.hooks import collect_data_files, collect_dynamic_libs<N>from PyInstaller.compat import is_win<N><N>if is_win:<N>    datas = collect_data_files('webview', subdir='lib')<N>    binaries = collect_dynamic_libs('webview')<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ['wx._xml', 'wx']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N># Hook for https://github.com/Shoobx/xmldiff<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>"""<N>Hook for PyZMQ. Cython based Python bindings for messaging library ZeroMQ.<N>http://www.zeromq.org/<N>"""<N>import os<N>import glob<N>from PyInstaller.utils.hooks import collect_submodules<N>from PyInstaller.utils.hooks import is_module_satisfies, get_module_file_attribute<N>from PyInstaller.compat import is_win<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2021 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>hiddenimports = ['uuid']<N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>import os<N>DIR = os.path.dirname(__file__)<N><N>
"""<N>All sub folders in this folder - "stdhooks" - are considered hook directories.<N><N>All sub folders MUST define an `__init__.py` file.<N>We recommend that it contains the copyright header, and nothing else.<N>"""<N><N><N>def get_hook_dirs():<N>    <N>    dirs = []<N>    # For every directory and sub directory (including cwd)<N>    for path, _, _ in os.walk(DIR):<N>        # Add the norm'd path to dirs<N>        dirs.append(os.path.normpath(path))<N>    <N>    return dirs<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N>import os<N>DIR = os.path.dirname(__file__)<N><N>
"""<N>This directory and every sub directory contains tests<N>"""<N><N><N>def get_test_dirs():<N>    <N>    dirs = []<N>    # For every directory and sub directory (including cwd)<N>    for path, _, _ in os.walk(DIR):<N>        # Add the norm'd path to dirs<N>        dirs.append(os.path.normpath(path))<N>    <N>    return dirs<N><N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
import os<N><N># Force CPU<N>os.environ['CUDA_VISIBLE_DEVICES'] = '-1'<N><N># Display only warnings and errors<N>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'<N><N># Begin test - import tensorflow after environment variables are set<N>import tensorflow as tf  # noqa: E402<N><N>
# Input data: batch of four 28x28x3 images<N>input_shape = (4, 28, 28, 3)<N>x = tf.random.normal(input_shape)<N><N># Convolution with 3x3 kernel, two output filters<N>y = tf.keras.layers.Conv2D(<N>    2,<N>    (3, 3),<N>    activation='relu',<N>    input_shape=input_shape[1:]<N>)(x)<N><N>
# ------------------------------------------------------------------<N># Copyright (c) 2020 PyInstaller Development Team.<N>#<N># This file is distributed under the terms of the GNU General Public<N># License (version 2.0 or later).<N>#<N># The full license is available in LICENSE.GPL.txt, distributed with<N># this software.<N>#<N># SPDX-License-Identifier: GPL-2.0-or-later<N># ------------------------------------------------------------------<N><N>
<N>import os<N><N># Force CPU<N>os.environ['CUDA_VISIBLE_DEVICES'] = '-1'<N><N># Display only warnings and errors<N>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'<N><N># Begin test - import tensorflow after environment variables are set<N>import tensorflow as tf  # noqa: E402<N><N>
# Load and normalize the dataset<N>mnist = tf.keras.datasets.mnist<N><N>(x_train, y_train), (x_test, y_test) = mnist.load_data()<N>x_train, x_test = x_train / 255.0, x_test / 255.0<N><N># Define model...<N>model = tf.keras.models.Sequential([<N>    tf.keras.layers.Flatten(input_shape=(28, 28)),<N>    tf.keras.layers.Dense(128, activation='relu'),<N>    tf.keras.layers.Dropout(0.2),<N>    tf.keras.layers.Dense(10)<N>])<N><N>
# ... and loss function<N>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<N><N># Train<N>model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])<N>model.fit(x_train, y_train, epochs=1, verbose=1)<N><N># Evaluate<N>results = model.evaluate(x_test,  y_test, verbose=1)<N><N>
# This is a stub package designed to roughly emulate the _yaml<N># extension module, which previously existed as a standalone module<N># and has been moved into the `yaml` package namespace.<N># It does not perfectly mimic its old counterpart, but should get<N># close enough for anyone who's relying on it even when they shouldn't.<N>import yaml<N><N>
# in some circumstances, the yaml module we imoprted may be from a different version, so we need<N># to tread carefully when poking at it here (it may not have the attributes we expect)<N>if not getattr(yaml, '__with_libyaml__', False):<N>    from sys import version_info<N><N>
#!d:\users\administrator\pycharmprojects\dingdonghelper\venv\scripts\python.exe<N># EASY-INSTALL-ENTRY-SCRIPT: 'future==0.18.2','console_scripts','futurize'<N>import re<N>import sys<N><N># for compatibility with easy_install; see #2198<N>__requires__ = 'future==0.18.2'<N><N>
#!d:\users\administrator\pycharmprojects\dingdonghelper\venv\scripts\python.exe<N># EASY-INSTALL-ENTRY-SCRIPT: 'future==0.18.2','console_scripts','pasteurize'<N>import re<N>import sys<N><N># for compatibility with easy_install; see #2198<N>__requires__ = 'future==0.18.2'<N><N>
import torch<N>import torch.nn as nn<N><N>class CNN(nn.Module):<N>    def __init__(self, train_shape, category):<N>        super(CNN, self).__init__()<N>        self.layer = nn.Sequential(<N>            nn.Conv2d(1, 64, (9, 1), (2, 1), (4, 0)),<N>            nn.BatchNorm2d(64),<N>            nn.ReLU(),<N><N>
import numpy as np<N>import models_and_tools as t<N><N>file_dict = {<N>    0: 'down',<N>    1: 'sit',<N>    2: 'stand',<N>    3: 'up',<N>    4: 'walk'<N>}<N><N>acc_x_train, acc_x_val, acc_x_test = t.get_accdata_and_save_label(file_dict, 128, 6, 0, save_label=True)<N>acc_y_train, acc_y_val, acc_y_test = t.get_accdata_and_save_label(file_dict, 128, 6, 1)<N>acc_z_train, acc_z_val, acc_z_test = t.get_accdata_and_save_label(file_dict, 128, 6, 2)<N><N>
X_train = t.unit_axis(acc_x_train, acc_y_train, acc_z_train)<N>X_val = t.unit_axis(acc_x_val, acc_y_val, acc_z_val)<N>X_test = t.unit_axis(acc_x_test, acc_y_test, acc_z_test)<N><N>np.save('npy/train/X_train.npy', X_train)<N>np.save('npy/val/X_val.npy', X_val)<N>np.save('npy/test/X_test.npy', X_test)<N><N>
import torch<N>import pandas as pd<N>import models_and_tools as t<N>import numpy as np<N><N>res = t.Resnet().cuda().eval()<N>cnn = t.CNN().cuda().eval()<N>rnn = t.RNN().cuda().eval()<N><N>net_dict = {cnn: 'cnn_model_cnn.pth.tar',<N>            rnn: 'cnn_model_rnn.pth.tar',<N>            res: 'cnn_model_res.pth.tar'}<N><N>
file_dict = {<N>    0: 'down',<N>    1: 'sit',<N>    2: 'stand',<N>    3: 'up',<N>    4: 'walk'<N>}<N>def get_m_and_v(number, axis):<N>    temp = t.column_to_row('txt/'+file_dict[number]+'.txt')<N>    mean = np.mean(temp[axis])<N>    var = np.var(temp[axis])<N>    return mean, var<N><N>
import numpy as np<N>import pandas as pd<N>import torch.nn as nn<N><N>class CNN(nn.Module):<N>    def __init__(self):<N>        super(CNN, self).__init__()<N>        self.conv = nn.Sequential(<N>            nn.Conv1d(3, 16, 3, 1, 1),<N>            nn.BatchNorm1d(16),<N>            nn.ReLU(),<N>            nn.MaxPool1d(2),<N><N>
            nn.Conv1d(16, 32, 3, 1, 1),<N>            nn.BatchNorm1d(32),<N>            nn.ReLU(),<N>            nn.MaxPool1d(2),<N><N>            nn.Conv1d(32, 64, 3, 1, 1),<N>            nn.BatchNorm1d(64),<N>            nn.ReLU(),<N>            nn.MaxPool1d(2),<N><N>
            nn.Conv1d(64, 128, 3, 1, 1),<N>            nn.BatchNorm1d(128),<N>            nn.ReLU(),<N>            nn.MaxPool1d(2)<N>        )<N>        self.flc = nn.Linear(1024, 5)<N>    def forward(self, x):<N>        x = self.conv(x)<N>        x = x.reshape(x.size(0), -1)<N>        # self.flc = nn.Linear(x.size(1), 5)<N>        x = self.flc(x)<N>        return x<N><N>
class RNN(nn.Module):<N>    def __init__(self):<N>        super(RNN, self).__init__()<N>        self.rnn = nn.LSTM(<N>            input_size=128,<N>            hidden_size=512,<N>            num_layers=1,<N>            batch_first=True<N>        )<N>        self.flc = nn.Linear(512, 5)<N>    def forward(self, x):<N>        x, h = self.rnn(x, None)<N>        x = self.flc(x[:, -1, :])<N>        return x<N><N>
import numpy as np<N>a = np.load('acc_x.npy')<N><N>b = np.load('X_test.npy')<N>c = np.load('Y_test.npy')<N>print(a.shape, b.shape, c.shape, c)
import numpy as np<N>import torch<N>import torch.nn as nn<N>a = np.load('acc_x.npy')<N><N>b = np.load('X_train.npy')<N>c = np.load('Y_train.npy')<N>print(a.shape, b.shape, c.shape)<N><N>a = torch.tensor([1, 2, 3])<N>print(a.size())<N>b = nn.functional.one_hot(a, 5).float()<N>print(b)<N>
import numpy as np<N>a = np.load('acc_x.npy')<N><N>b = np.load('X_val.npy')<N>c = np.load('Y_val.npy')<N>print(a.shape, b.shape, c.shape)
import numpy as np<N>import pandas as pd<N><N>'''<N>WINDOW_SIZE=128 # int<N>OVERLAP_RATE=0.5 # float in [0，1）<N>SPLIT_RATE=-- # tuple or list  <N>'''<N><N>def UCI(dataset_dir='UCI_HAR_Dataset'):<N>    dataset = dataset_dir<N><N>    signal_class = [<N>        'body_acc_x_',<N>        'body_acc_y_',<N>        'body_acc_z_',<N>        'body_gyro_x_',<N>        'body_gyro_x_',<N>        'body_gyro_x_',<N>        'total_acc_x_',<N>        'total_acc_y_',<N>        'total_acc_z_',<N>    ]<N><N>
    def xload(X_path):<N>        x = []<N>        for each in X_path:<N>            with open(each, 'r') as f:<N>                x.append(np.array([eachline.replace('  ', ' ').strip().split(' ') for eachline in f], dtype=np.float32))<N>        x = np.transpose(x, (1, 2, 0))<N>        return x<N><N>
import matplotlib.pyplot as plt<N>import numpy as np<N><N>def unit(file):<N>    with open(file, 'r') as x:<N>        temp = []<N>        for eachline in x:<N>            if len(temp) == 10:<N>                break<N>            temp.append(eachline.replace('  ', ' ').strip().split(' '))<N>        result = []<N>        for eachline in temp:<N>            for each in eachline:<N>                result.append(each)<N>        return np.array(result)<N><N>
import os<N><N>from setuptools import setup<N>from src.version import __version__<N><N><N>def getdes():<N>    des = ""<N>    if os.path.isfile(os.path.join(os.getcwd(), "README.md")):<N>        with open(os.path.join(os.getcwd(), "README.md")) as fi:<N>            des = fi.read()<N>    return des<N><N>
#!/usr/bin/python<N># coding:utf-8<N>'''Usage: <N>    1. ezmntor [pid]<N>    2. ezmntor [processName]<N>    3. ezmntor [your command line]<N>'''<N><N>import sys<N>import os<N>import subprocess<N><N>from .src import *<N>from .utils import *<N>from .version import __version__<N><N>
# coding:utf-8<N><N>import os<N>import time<N>import psutil<N>import datetime<N>import numpy as np<N>import pandas as pd<N>import matplotlib.pyplot as plt<N><N>from threading import Thread<N>from collections import OrderedDict<N><N>from .utils import *<N><N>
<N>class ezmonitor(object):<N><N>    cpu_tick = 0.3<N><N>    def __init__(self, ivs=None, pid=os.getpid(), logfile=None, verbose=False):<N>        if ivs:<N>            self.cpu_tick = ivs<N>        self.pid = pid<N>        self.logfile = logfile<N>        self.verbose = verbose<N><N>
import os<N>import sys<N>import math<N>import time<N>import signal<N>import logging<N><N>from threading import Thread<N><N><N>class ParseSingal(Thread):<N>    def __init__(self):<N>        super(ParseSingal, self).__init__()<N>        signal.signal(signal.SIGINT, self.signal_handler)<N>        signal.signal(signal.SIGTERM, self.signal_handler)<N><N>
    def run(self):<N>        time.sleep(1)<N><N>    def signal_handler(self, signum, frame):<N>        os._exit(signum)<N>        # sys.exit(signum)<N><N><N>def pgrep(name):<N>    pids = []<N>    if name:<N>        with os.popen("pgrep %s" % name) as fi:<N>            pids = fi.read().split()<N>    return pids<N><N>
<N>import networkx as nx<N>import scipy<N>import numpy as np<N>from scipy.sparse import diags<N>from sklearn.metrics import roc_auc_score<N>from sklearn.metrics import average_precision_score<N>from munkres import Munkres<N>from sklearn import metrics<N>class clustering_metrics():<N>    "from https://github.com/Ruiqi-Hu/ARGA"<N><N>
    def __init__(self, true_label, predict_label):<N>        self.true_label = true_label<N>        self.pred_label = predict_label<N><N>    def clusteringAcc(self):<N>        # best mapping between true_label and predict label<N>        l1 = list(set(self.true_label))<N>        numclass1 = len(l1)<N><N>
        l2 = list(set(self.pred_label))<N>        numclass2 = len(l2)<N>        if numclass1 != numclass2:<N>            print('Class Not equal, Error!!!!')<N>            return 0<N><N>        cost = np.zeros((numclass1, numclass2), dtype=int)<N>        for i, c1 in enumerate(l1):<N>            mps = [i1 for i1, e1 in enumerate(self.true_label) if e1 == c1]<N>            for j, c2 in enumerate(l2):<N>                mps_d = [i1 for i1 in mps if self.pred_label[i1] == c2]<N><N>
                cost[i][j] = len(mps_d)<N><N>        # match two clustering results by Munkres algorithm<N>        m = Munkres()<N>        cost = cost.__neg__().tolist()<N><N>        indexes = m.compute(cost)<N><N>        # get the match results<N>        new_predict = np.zeros(len(self.pred_label))<N>        for i, c in enumerate(l1):<N>            # correponding label in l2:<N>            c2 = l2[indexes[i][1]]<N><N>
import itertools<N>from typing import Tuple, List<N>import hashlib<N>import random<N>import os<N>import gc<N>from scipy.sparse import diags<N>import numpy as np<N>import torch<N>import torch.nn.functional as F<N>from sklearn.metrics import roc_auc_score<N>from termcolor import cprint<N><N>
import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>class LogReg(nn.Module):<N>    def __init__(self, ft_in, nb_classes):<N>        super(LogReg, self).__init__()<N>        self.fc = nn.Linear(ft_in, nb_classes)<N>        for m in self.modules():<N>            self.weights_init(m)<N><N>
    def weights_init(self, m):<N>        if isinstance(m, nn.Linear):<N>            torch.nn.init.xavier_uniform_(m.weight.data)<N>            if m.bias is not None:<N>                m.bias.data.fill_(0.0)<N><N>    def forward(self, seq):<N>        ret = self.fc(seq)<N>        return ret<N><N>
import numpy as np<N>import os<N><N><N>def load_dataset(data_dir,data_length,scale):<N><N>    #gloabal numbers<N><N>    txt_files = os.listdir(data_dir)<N>    numbers = len(txt_files)<N>    data_list = []<N>    if str(data_dir[-1]) == 'n':<N>        txt_files.sort(key=lambda x: int(x[7:-6]))<N>        for txt in txt_files:<N>            data_tmp = np.loadtxt(data_dir + '/' + txt)<N>            data_list.append(data_tmp)<N><N>
        data_list = np.array(data_list).reshape(numbers, data_length//scale)<N>    else:<N>        txt_files.sort(key= lambda x : int(x[7:-4]))<N>        for txt in txt_files:<N>            data_tmp = np.loadtxt(data_dir + '/' + txt)<N>            data_list.append(data_tmp)<N><N>
import argparse<N>import os<N>import numpy as np<N>import torch.utils.data<N>from torch import nn<N>from tqdm import tqdm<N>from model import FDCN,FDRN<N>from multiprocessing import cpu_count<N>from torch.utils.data import Dataset, DataLoader, TensorDataset<N>from torch.autograd import Variable<N>import torch.optim as optim<N>import data as d<N>import logging<N>import time<N><N>
import argparse<N>import os<N>import numpy as np<N>import torch.utils.data<N>from torch import nn<N>from tqdm import tqdm<N>from model import FDCN, FDRN<N>from multiprocessing import cpu_count<N>from torch.utils.data import Dataset, DataLoader, TensorDataset<N>from torch.autograd import Variable<N>import torch.optim as optim<N>import data as d<N>import logging<N>import time<N><N>
import os<N>import os.path as osp<N>import shutil<N>import sys<N>import warnings<N>from setuptools import find_packages, setup<N><N><N>def readme():<N>    """Load README.md."""<N>    with open('README.md', encoding='utf-8') as f:<N>        content = f.read()<N>    return content<N><N>
<N>def get_version():<N>    """Get version of mmrotate."""<N>    version_file = 'mmrotate/version.py'<N>    with open(version_file, 'r') as f:<N>        exec(compile(f.read(), version_file, 'exec'))<N>    return locals()['__version__']<N><N><N>def parse_requirements(fname='requirements.txt', with_version=True):<N>    """Parse the package dependencies listed in a requirements file but strips<N>    specific versioning information.<N><N>
    Args:<N>        fname (str): path to requirements file<N>        with_version (bool, default=False): if True include version specs<N><N>    Returns:<N>        List[str]: list of requirements items<N><N>    CommandLine:<N>        python -c "import setup; print(setup.parse_requirements())"<N>    """<N>    import sys<N>    from os.path import exists<N>    import re<N>    require_fpath = fname<N><N>
_base_ = ['../rotated_reppoints/rotated_reppoints_r50_fpn_1x_dota_le135.py']<N><N>model = dict(<N>    bbox_head=dict(use_reassign=True),<N>    train_cfg=dict(<N>        refine=dict(assigner=dict(pos_iou_thr=0.1, neg_iou_thr=0.1))))<N>
_base_ = ['../rotated_reppoints/rotated_reppoints_r50_fpn_1x_dota_oc.py']<N><N>model = dict(<N>    bbox_head=dict(use_reassign=True),<N>    train_cfg=dict(<N>        refine=dict(assigner=dict(pos_iou_thr=0.1, neg_iou_thr=0.1))))<N>
_base_ = ['./cfa_r50_fpn_1x_dota_oc.py']<N><N># evaluation<N>evaluation = dict(interval=40, metric='mAP')<N># learning policy<N>lr_config = dict(<N>    policy='step',<N>    warmup='linear',<N>    warmup_iters=500,<N>    warmup_ratio=1.0 / 3,<N>    step=[24, 32, 38])<N>runner = dict(type='EpochBasedRunner', max_epochs=40)<N>checkpoint_config = dict(interval=10)<N>
_base_ = ['../rotated_retinanet/rotated_retinanet_hbb_r50_fpn_1x_dota_oc.py']<N><N>model = dict(<N>    bbox_head=dict(<N>        type='RotatedRetinaHeadDistribution',<N>        reg_max=8,<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(type='GDLoss', loss_type='gwd', loss_weight=5.0)))<N><N>
_base_ = [<N>    '../rotated_retinanet/rotated_retinanet_obb_r50_fpn_1x_dota_le135.py'<N>]<N><N>model = dict(<N>    bbox_head=dict(<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(type='GDLoss', loss_type='gwd', loss_weight=5.0)))<N>
_base_ = ['../rotated_retinanet/rotated_retinanet_obb_r50_fpn_1x_dota_le90.py']<N><N>model = dict(<N>    bbox_head=dict(<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(type='GDLoss', loss_type='gwd', loss_weight=5.0)))<N>
_base_ = ['../rotated_reppoints/rotated_reppoints_r50_fpn_1x_dota_oc.py']<N><N>angle_version = 'le135'<N><N>model = dict(<N>    bbox_head=dict(<N>        version=angle_version,<N>        type='KLDRepPointsHead',<N>        loss_bbox_init=dict(type='KLDRepPointsLoss'),<N>        loss_bbox_refine=dict(type='KLDRepPointsLoss')),<N>    train_cfg=dict(<N>        refine=dict(<N>            assigner=dict(_delete_=True, type='ATSSKldAssigner', topk=9))))<N><N>
_base_ = ['./r3det_kfiou_ln_swin_tiny_adamw_fpn_1x_dota_ms_rr_oc.py']<N><N>evaluation = dict(interval=24, metric='mAP')<N>runner = dict(type='EpochBasedRunner', max_epochs=24)<N>lr_config = dict(<N>    policy='step',<N>    warmup='linear',<N>    warmup_iters=500,<N>    warmup_ratio=1.0 / 3,<N>    step=[18, 22])<N>
_base_ = ['../rotated_retinanet/rotated_retinanet_hbb_r50_fpn_1x_dota_oc.py']<N><N>angle_version = 'oc'<N>model = dict(<N>    bbox_head=dict(<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(<N>            _delete_=True,<N>            type='GDLoss_v1',<N>            loss_type='kld',<N>            fun='log1p',<N>            tau=1,<N>            loss_weight=1.0)))<N><N>
_base_ = [<N>    '../rotated_retinanet/rotated_retinanet_obb_r50_fpn_1x_dota_le135.py'<N>]<N><N>model = dict(<N>    bbox_head=dict(<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(<N>            _delete_=True,<N>            type='GDLoss_v1',<N>            loss_type='kld',<N>            fun='log1p',<N>            tau=1,<N>            loss_weight=1.0)))<N>
_base_ = ['../rotated_retinanet/rotated_retinanet_obb_r50_fpn_1x_dota_le90.py']<N><N>model = dict(<N>    bbox_head=dict(<N>        reg_decoded_bbox=True,<N>        loss_bbox=dict(<N>            _delete_=True,<N>            type='GDLoss_v1',<N>            loss_type='kld',<N>            fun='log1p',<N>            tau=1,<N>            loss_weight=1.0)))<N>
_base_ = ['./roi_trans_r50_fpn_1x_dota_le90.py']<N><N>data_root = '../datasets/split_ms_dotav1/'<N>data = dict(<N>    train=dict(<N>        ann_file=data_root + 'trainval/annfiles/',<N>        img_prefix=data_root + 'trainval/images/'),<N>    val=dict(<N>        ann_file=data_root + 'trainval/annfiles/',<N>        img_prefix=data_root + 'trainval/images/'),<N>    test=dict(<N>        ann_file=data_root + 'test/images/',<N>        img_prefix=data_root + 'test/images/'))<N>
_base_ = ['../rotated_reppoints/rotated_reppoints_r50_fpn_1x_dota_oc.py']<N><N>model = dict(<N>    bbox_head=dict(<N>        type='SAMRepPointsHead',<N>        loss_bbox_init=dict(type='BCConvexGIoULoss', loss_weight=0.375)),<N><N>    # training and testing settings<N>    train_cfg=dict(<N>        refine=dict(<N>            _delete_=True,<N>            assigner=dict(type='SASAssigner', topk=9),<N>            allowed_border=-1,<N>            pos_weight=-1,<N>            debug=False)))<N>
# yapf:disable<N>log_config = dict(<N>    interval=50,<N>    hooks=[<N>        dict(type='TextLoggerHook'),<N>        # dict(type='TensorboardLoggerHook')<N>    ])<N># yapf:enable<N><N>dist_params = dict(backend='nccl')<N>log_level = 'INFO'<N>load_from = None<N>resume_from = None<N>workflow = [('train', 1)]<N>
# evaluation<N>evaluation = dict(interval=12, metric='mAP')<N># optimizer<N>optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)<N>optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))<N># learning policy<N>lr_config = dict(<N>    policy='step',<N>    warmup='linear',<N>    warmup_iters=500,<N>    warmup_ratio=1.0 / 3,<N>    step=[8, 11])<N>runner = dict(type='EpochBasedRunner', max_epochs=12)<N>checkpoint_config = dict(interval=12)<N>
# evaluation<N>evaluation = dict(interval=36, metric='mAP')<N># optimizer<N>optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)<N>optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))<N># learning policy<N>lr_config = dict(<N>    policy='step',<N>    warmup='linear',<N>    warmup_iters=500,<N>    warmup_ratio=1.0 / 3,<N>    step=[24, 33])<N>runner = dict(type='EpochBasedRunner', max_epochs=36)<N>checkpoint_config = dict(interval=12)<N>
# evaluation<N>evaluation = dict(interval=40, metric='mAP')<N># optimizer<N>optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)<N>optimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))<N># learning policy<N>lr_config = dict(<N>    policy='step',<N>    warmup='linear',<N>    warmup_iters=500,<N>    warmup_ratio=1.0 / 3,<N>    step=[24, 32, 38])<N>runner = dict(type='EpochBasedRunner', max_epochs=40)<N>checkpoint_config = dict(interval=10)<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>"""Inference on single image.<N><N>Example:<N>python demo/image_demo.py \<N>    demo/demo.jpg \<N>        configs/oriented_rcnn/oriented_rcnn_r50_fpn_1x_dota_v3.py \<N>        work_dirs/oriented_rcnn_r50_fpn_1x_dota_v3/epoch_12.pth \<N>        demo/vis.jpg<N>"""  # nowq<N><N>
# Configuration file for the Sphinx documentation builder.<N>#<N># This file only contains a selection of the most common options. For a full<N># list see the documentation:<N># https://www.sphinx-doc.org/en/master/usage/configuration.html<N><N># -- Path setup --------------------------------------------------------------<N><N>
# If extensions (or modules to document with autodoc) are in another directory,<N># add these directories to sys.path here. If the directory is relative to the<N># documentation root, use os.path.abspath to make it absolute, like shown here.<N>#<N>import os<N>import sys<N><N>
import pytorch_sphinx_theme<N><N>sys.path.insert(0, os.path.abspath('../..'))<N><N># -- Project information -----------------------------------------------------<N><N>project = 'mmrotate'<N>copyright = '2018-2022, OpenMMLab'<N>author = 'MMRotate Author'<N>version_file = '../../mmrotate/version.py'<N><N>
#!/usr/bin/env python<N>import functools as func<N>import glob<N>import os.path as osp<N>import re<N><N>import numpy as np<N><N>url_prefix = 'https://github.com/open-mmlab/mmrotate/blob/master/'<N><N>files = sorted(glob.glob('../configs/*/README.md'))<N><N>
stats = []<N>titles = []<N>num_ckpts = 0<N><N>for f in files:<N>    url = osp.dirname(f.replace('../', url_prefix))<N><N>    with open(f, 'r') as content_file:<N>        content = content_file.read()<N><N>    title = content.split('\n')[0].replace('# ', '').strip()<N>    ckpts = set(x.lower().strip()<N>                for x in re.findall(r'\[model\]\((https?.*)\)', content))<N><N>
    if len(ckpts) == 0:<N>        continue<N><N>    _papertype = [x for x in re.findall(r'\[([A-Z]+)\]', content)]<N>    assert len(_papertype) > 0<N>    papertype = _papertype[0]<N><N>    paper = set([(papertype, title)])<N><N>    titles.append(title)<N>    num_ckpts += len(ckpts)<N><N>
    statsmsg = f"""<N>\t* [{papertype}] [{title}]({url}) ({len(ckpts)} ckpts)<N>"""<N>    stats.append((paper, ckpts, statsmsg))<N><N>allpapers = func.reduce(lambda a, b: a.union(b), [p for p, _, _ in stats])<N>msglist = '\n'.join(x for _, _, x in stats)<N><N>
papertypes, papercounts = np.unique([t for t, _ in allpapers],<N>                                    return_counts=True)<N>countstr = '\n'.join(<N>    [f'   - {t}: {c}' for t, c in zip(papertypes, papercounts)])<N><N>modelzoo = f"""<N># Model Zoo Statistics<N><N>
# Configuration file for the Sphinx documentation builder.<N>#<N># This file only contains a selection of the most common options. For a full<N># list see the documentation:<N># https://www.sphinx-doc.org/en/master/usage/configuration.html<N><N># -- Path setup --------------------------------------------------------------<N><N>
# If extensions (or modules to document with autodoc) are in another directory,<N># add these directories to sys.path here. If the directory is relative to the<N># documentation root, use os.path.abspath to make it absolute, like shown here.<N>#<N>import os<N>import sys<N><N>
import pytorch_sphinx_theme<N><N>sys.path.insert(0, os.path.abspath('../..'))<N><N># -- Project information -----------------------------------------------------<N><N>project = 'mmrotate'<N>copyright = '2018-2022, OpenMMLab'<N>author = 'MMRotate Author'<N>version_file = '../../mmrotate/version.py'<N><N>
#!/usr/bin/env python<N>import functools as func<N>import glob<N>import os.path as osp<N>import re<N><N>import numpy as np<N><N>url_prefix = 'https://github.com/open-mmlab/mmrotate/blob/master/'<N><N>files = sorted(glob.glob('../configs/*/README.md'))<N><N>
stats = []<N>titles = []<N>num_ckpts = 0<N><N>for f in files:<N>    url = osp.dirname(f.replace('../', url_prefix))<N><N>    with open(f, 'r') as content_file:<N>        content = content_file.read()<N><N>    title = content.split('\n')[0].replace('# ', '').strip()<N>    ckpts = set(x.lower().strip()<N>                for x in re.findall(r'\[model\]\((https?.*)\)', content))<N><N>
    if len(ckpts) == 0:<N>        continue<N><N>    _papertype = [x for x in re.findall(r'\[([A-Z]+)\]', content)]<N>    assert len(_papertype) > 0<N>    papertype = _papertype[0]<N><N>    paper = set([(papertype, title)])<N><N>    titles.append(title)<N>    num_ckpts += len(ckpts)<N><N>
    statsmsg = f"""<N>\t* [{papertype}] [{title}]({url}) ({len(ckpts)} ckpts)<N>"""<N>    stats.append((paper, ckpts, statsmsg))<N><N>allpapers = func.reduce(lambda a, b: a.union(b), [p for p, _, _ in stats])<N>msglist = '\n'.join(x for _, _, x in stats)<N><N>
papertypes, papercounts = np.unique([t for t, _ in allpapers],<N>                                    return_counts=True)<N>countstr = '\n'.join(<N>    [f'   - {t}: {c}' for t, c in zip(papertypes, papercounts)])<N><N>modelzoo = f"""<N># Model Zoo Statistics<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N><N>from .core import *  # noqa: F401, F403<N>from .datasets import *  # noqa: F401, F403<N>from .models import *  # noqa: F401, F403<N>from .version import __version__, short_version<N><N>
<N>def digit_version(version_str):<N>    """Digit version."""<N>    digit_version = []<N>    for x in version_str.split('.'):<N>        if x.isdigit():<N>            digit_version.append(int(x))<N>        elif x.find('rc') != -1:<N>            patch_version = x.split('rc')<N>            digit_version.append(int(patch_version[0]) - 1)<N>            digit_version.append(int(patch_version[1]))<N>    return digit_version<N><N>
<N>mmcv_minimum_version = '1.4.5'<N>mmcv_maximum_version = '1.5.0'<N>mmcv_version = digit_version(mmcv.__version__)<N><N>assert (mmcv_version >= digit_version(mmcv_minimum_version)<N>        and mmcv_version <= digit_version(mmcv_maximum_version)), \<N>    f'MMCV=={mmcv.__version__} is used but incompatible. ' \<N>    f'Please install mmcv>={mmcv_minimum_version}, <={mmcv_maximum_version}.'<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .train import train_detector<N><N>__all__ = ['train_detector']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .anchor import *  # noqa: F401, F403<N>from .bbox import *  # noqa: F401, F403<N>from .post_processing import *  # noqa: F401, F403<N>from .visualization import *  # noqa: F401, F403<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.utils import to_2tuple<N>from mmdet.core.anchor import AnchorGenerator<N><N>from .builder import ROTATED_ANCHOR_GENERATORS<N><N><N>@ROTATED_ANCHOR_GENERATORS.register_module()<N>class RotatedAnchorGenerator(AnchorGenerator):<N>    """Standard rotate anchor generator for 2D anchor-based detectors."""<N><N>
    def single_level_grid_priors(self,<N>                                 featmap_size,<N>                                 level_idx,<N>                                 dtype=torch.float32,<N>                                 device='cuda'):<N>        """Generate grid anchors of a single level.<N><N>
        Note:<N>            This function is usually called by method ``self.grid_priors``.<N><N>        Args:<N>            featmap_size (tuple[int]): Size of the feature maps.<N>            level_idx (int): The index of corresponding feature map level.<N>            dtype (obj:`torch.dtype`): Date type of points.Defaults to<N>            ``torch.float32``.<N>            device (str, optional): The device the tensor will be put on.<N>            Defaults to 'cuda'.<N><N>
        Returns:<N>            torch.Tensor: Anchors in the overall feature maps.<N>        """<N>        anchors = super(RotatedAnchorGenerator, self).single_level_grid_priors(<N>            featmap_size, level_idx, dtype=dtype, device=device)<N><N>
        num_anchors = anchors.size(0)<N>        xy = (anchors[:, 2:] + anchors[:, :2]) / 2<N>        wh = anchors[:, 2:] - anchors[:, :2]<N>        theta = xy.new_zeros((num_anchors, 1))<N>        anchors = torch.cat([xy, wh, theta], axis=1)<N><N>        return anchors<N><N>
<N>@ROTATED_ANCHOR_GENERATORS.register_module()<N>class PseudoAnchorGenerator(AnchorGenerator):<N>    """Non-Standard pseudo anchor generator that is used to generate valid<N>    flags only!"""<N><N>    def __init__(self, strides):<N>        self.strides = [to_2tuple(stride) for stride in strides]<N><N>
    @property<N>    def num_base_anchors(self):<N>        """list[int]: total number of base anchors in a feature grid"""<N>        return [1 for _ in self.strides]<N><N>    def single_level_grid_anchors(self, featmap_sizes, device='cuda'):<N>        """Calling its grid_anchors() method will raise NotImplementedError!"""<N>        raise NotImplementedError<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmcv.utils import build_from_cfg<N>from mmdet.core.anchor.builder import ANCHOR_GENERATORS<N><N>ROTATED_ANCHOR_GENERATORS = ANCHOR_GENERATORS<N><N><N>def build_prior_generator(cfg, default_args=None):<N>    return build_from_cfg(cfg, ROTATED_ANCHOR_GENERATORS, default_args)<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>def rotated_anchor_inside_flags(flat_anchors,<N>                                valid_flags,<N>                                img_shape,<N>                                allowed_border=0):<N>    """Check whether the rotated anchors are inside the border.<N><N>
    Args:<N>        flat_anchors (torch.Tensor): Flatten anchors, shape (n, 5).<N>        valid_flags (torch.Tensor): An existing valid flags of anchors.<N>        img_shape (tuple(int)): Shape of current image.<N>        allowed_border (int, optional): The border to allow the valid anchor.<N>            Defaults to 0.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .anchor_generator import PseudoAnchorGenerator, RotatedAnchorGenerator<N>from .builder import ROTATED_ANCHOR_GENERATORS, build_prior_generator<N>from .utils import rotated_anchor_inside_flags<N><N>__all__ = [<N>    'RotatedAnchorGenerator', 'rotated_anchor_inside_flags',<N>    'PseudoAnchorGenerator', 'ROTATED_ANCHOR_GENERATORS',<N>    'build_prior_generator'<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmcv.utils import build_from_cfg<N>from mmdet.core.bbox.builder import BBOX_ASSIGNERS, BBOX_CODERS, BBOX_SAMPLERS<N><N>ROTATED_BBOX_ASSIGNERS = BBOX_ASSIGNERS<N>ROTATED_BBOX_SAMPLERS = BBOX_SAMPLERS<N>ROTATED_BBOX_CODERS = BBOX_CODERS<N><N>
<N>def build_assigner(cfg, **default_args):<N>    """Builder of box assigner."""<N>    return build_from_cfg(cfg, ROTATED_BBOX_ASSIGNERS, default_args)<N><N><N>def build_sampler(cfg, **default_args):<N>    """Builder of box sampler."""<N>    return build_from_cfg(cfg, ROTATED_BBOX_SAMPLERS, default_args)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import math<N><N>import cv2<N>import numpy as np<N>import torch<N><N><N>def bbox_flip(bboxes, img_shape, direction='horizontal'):<N>    """Flip bboxes horizontally or vertically.<N><N>    Args:<N>        bboxes (Tensor): Shape (..., 5*k)<N>        img_shape (tuple): Image shape.<N>        direction (str): Flip direction, options are "horizontal", "vertical",<N>            "diagonal". Default: "horizontal"<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.ops import points_in_polygons<N>from mmdet.core.bbox.assigners.assign_result import AssignResult<N>from mmdet.core.bbox.assigners.base_assigner import BaseAssigner<N><N>from mmrotate.core.bbox.utils import GaussianMixture<N>from ..builder import ROTATED_BBOX_ASSIGNERS<N>from ..transforms import gt2gaussian<N><N>
<N>@ROTATED_BBOX_ASSIGNERS.register_module()<N>class ATSSKldAssigner(BaseAssigner):<N>    """Assign a corresponding gt bbox or background to each bbox.<N><N>    Each proposals will be assigned with `0` or a positive integer<N>    indicating the ground truth index.<N><N>
    - 0: negative sample, no assigned gt<N>    - positive integer: positive sample, index (1-based) of assigned gt<N><N>    Args:<N>        topk (float): Number of bbox selected in each level.<N>        use_reassign (bool, optional): If true, it is used to reassign samples.<N>    """<N><N>
    def __init__(self, topk, use_reassign=False):<N>        self.topk = topk<N>        self.use_reassign = use_reassign<N><N>    def assign(self,<N>               bboxes,<N>               num_level_bboxes,<N>               gt_bboxes,<N>               gt_bboxes_ignore=None,<N>               gt_labels=None):<N>        """Assign gt to bboxes.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmdet.core.bbox.assigners.assign_result import AssignResult<N>from mmdet.core.bbox.assigners.base_assigner import BaseAssigner<N><N>from ..builder import ROTATED_BBOX_ASSIGNERS<N><N>
<N>@ROTATED_BBOX_ASSIGNERS.register_module()<N>class ConvexAssigner(BaseAssigner):<N>    """Assign a corresponding gt bbox or background to each bbox. Each<N>    proposals will be assigned with `0` or a positive integer indicating the<N>    ground truth index.<N><N>
    - 0: negative sample, no assigned gt<N>    - positive integer: positive sample, index (1-based) of assigned gt<N>    Args:<N>        scale (float): IoU threshold for positive bboxes.<N>        pos_num (float): find the nearest pos_num points to gt center in this<N>        level.<N>    """<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.ops import convex_iou<N>from mmdet.core.bbox.assigners.assign_result import AssignResult<N>from mmdet.core.bbox.assigners.base_assigner import BaseAssigner<N><N>from ..builder import ROTATED_BBOX_ASSIGNERS<N><N>
<N>@ROTATED_BBOX_ASSIGNERS.register_module()<N>class MaxConvexIoUAssigner(BaseAssigner):<N>    """Assign a corresponding gt bbox or background to each bbox. Each<N>    proposals will be assigned with `-1`, or a semi-positive integer indicating<N>    the ground truth index.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.ops import convex_iou, points_in_polygons<N>from mmdet.core.bbox.assigners.assign_result import AssignResult<N>from mmdet.core.bbox.assigners.base_assigner import BaseAssigner<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .atss_kld_assigner import ATSSKldAssigner<N>from .convex_assigner import ConvexAssigner<N>from .max_convex_iou_assigner import MaxConvexIoUAssigner<N>from .sas_assigner import SASAssigner<N><N>__all__ = [<N>    'ConvexAssigner', 'MaxConvexIoUAssigner', 'SASAssigner', 'ATSSKldAssigner'<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N># Modified from jbwang1997: https://github.com/jbwang1997/OBBDetection<N>import mmcv<N>import numpy as np<N>import torch<N>from mmdet.core.bbox.coder.base_bbox_coder import BaseBBoxCoder<N><N>from ..builder import ROTATED_BBOX_CODERS<N>from ..transforms import obb2poly, obb2xyxy, poly2obb<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>import numpy as np<N>import torch<N>from mmdet.core.bbox.coder.base_bbox_coder import BaseBBoxCoder<N><N>from ..builder import ROTATED_BBOX_CODERS<N>from ..transforms import norm_angle<N><N>
<N>@ROTATED_BBOX_CODERS.register_module()<N>class DeltaXYWHAHBBoxCoder(BaseBBoxCoder):<N>    """Delta XYWHA HBBox coder.<N><N>    this coder encodes bbox (x1, y1, x2, y2) into delta (dx, dy, dw, dh, da)<N>        and decodes delta (dx, dy, dw, dh, da) back to original bbox<N>        (cx, cy, w, h, a).<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>import numpy as np<N>import torch<N>from mmdet.core.bbox.coder.base_bbox_coder import BaseBBoxCoder<N><N>from ..builder import ROTATED_BBOX_CODERS<N>from ..transforms import norm_angle<N><N>
<N>@ROTATED_BBOX_CODERS.register_module()<N>class DeltaXYWHAOBBoxCoder(BaseBBoxCoder):<N>    """Delta XYWHA OBBox coder. This coder is used for rotated objects<N>    detection (for example on task1 of DOTA dataset). this coder encodes bbox<N>    (xc, yc, w, h, a) into delta (dx, dy, dw, dh, da) and decodes delta (dx,<N>    dy, dw, dh, da) back to original bbox (xc, yc, w, h, a).<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N># Modified from jbwang1997: https://github.com/jbwang1997/OBBDetection<N>import torch<N>from mmdet.core.bbox.coder.base_bbox_coder import BaseBBoxCoder<N><N>from ..builder import ROTATED_BBOX_CODERS<N>from ..transforms import obb2poly, poly2obb<N><N>
<N>@ROTATED_BBOX_CODERS.register_module()<N>class GVFixCoder(BaseBBoxCoder):<N>    """Gliding vertex fix coder.<N><N>    this coder encodes bbox (cx, cy, w, h, a) into delta (dt, dr, dd, dl) and<N>    decodes delta (dt, dr, dd, dl) back to original bbox (cx, cy, w, h, a).<N>    Args:<N>        angle_range (str, optional): Angle representations. Defaults to 'oc'.<N>    """<N><N>
    def __init__(self, angle_range='oc', **kwargs):<N>        self.version = angle_range<N><N>        super(GVFixCoder, self).__init__(**kwargs)<N><N>    def encode(self, rbboxes):<N>        """Get box regression transformation deltas.<N><N>        Args:<N>            rbboxes (torch.Tensor): Source boxes, e.g., object proposals.<N>        Returns:<N>            torch.Tensor: Box transformation deltas<N>        """<N>        assert rbboxes.size(1) == 5<N><N>
        polys = obb2poly(rbboxes, self.version)<N><N>        max_x, max_x_idx = polys[:, ::2].max(1)<N>        min_x, min_x_idx = polys[:, ::2].min(1)<N>        max_y, max_y_idx = polys[:, 1::2].max(1)<N>        min_y, min_y_idx = polys[:, 1::2].min(1)<N>        hbboxes = torch.stack([min_x, min_y, max_x, max_y], dim=1)<N><N>
        polys = polys.view(-1, 4, 2)<N>        num_polys = polys.size(0)<N>        polys_ordered = torch.zeros_like(polys)<N>        polys_ordered[:, 0] = polys[range(num_polys), min_y_idx]<N>        polys_ordered[:, 1] = polys[range(num_polys), max_x_idx]<N>        polys_ordered[:, 2] = polys[range(num_polys), max_y_idx]<N>        polys_ordered[:, 3] = polys[range(num_polys), min_x_idx]<N><N>
        t_x = polys_ordered[:, 0, 0]<N>        r_y = polys_ordered[:, 1, 1]<N>        d_x = polys_ordered[:, 2, 0]<N>        l_y = polys_ordered[:, 3, 1]<N><N>        dt = (t_x - hbboxes[:, 0]) / (hbboxes[:, 2] - hbboxes[:, 0])<N>        dr = (r_y - hbboxes[:, 1]) / (hbboxes[:, 3] - hbboxes[:, 1])<N>        dd = (hbboxes[:, 2] - d_x) / (hbboxes[:, 2] - hbboxes[:, 0])<N>        dl = (hbboxes[:, 3] - l_y) / (hbboxes[:, 3] - hbboxes[:, 1])<N><N>
        h_mask = (polys_ordered[:, 0, 1] - polys_ordered[:, 1, 1] == 0) | \<N>                 (polys_ordered[:, 1, 0] - polys_ordered[:, 2, 0] == 0)<N>        fix_deltas = torch.stack([dt, dr, dd, dl], dim=1)<N>        fix_deltas[h_mask, :] = 1<N>        return fix_deltas<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .delta_midpointoffset_rbbox_coder import MidpointOffsetCoder<N>from .delta_xywha_hbbox_coder import DeltaXYWHAHBBoxCoder<N>from .delta_xywha_rbbox_coder import DeltaXYWHAOBBoxCoder<N>from .gliding_vertex_coder import GVFixCoder, GVRatioCoder<N><N>__all__ = [<N>    'DeltaXYWHAOBBoxCoder', 'DeltaXYWHAHBBoxCoder', 'MidpointOffsetCoder',<N>    'GVFixCoder', 'GVRatioCoder'<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmcv.utils import build_from_cfg<N>from mmdet.core.bbox.iou_calculators.builder import IOU_CALCULATORS<N><N>ROTATED_IOU_CALCULATORS = IOU_CALCULATORS<N><N><N>def build_iou_calculator(cfg, default_args=None):<N>    """Builder of IoU calculator."""<N>    return build_from_cfg(cfg, ROTATED_IOU_CALCULATORS, default_args)<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmcv.ops import box_iou_rotated<N><N>from .builder import ROTATED_IOU_CALCULATORS<N><N><N>@ROTATED_IOU_CALCULATORS.register_module()<N>class RBboxOverlaps2D(object):<N>    """2D Overlaps (e.g. IoUs, GIoUs) Calculator."""<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .rotate_iou2d_calculator import RBboxOverlaps2D, rbbox_overlaps<N><N>__all__ = ['RBboxOverlaps2D', 'rbbox_overlaps']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmdet.core.bbox.samplers.base_sampler import BaseSampler<N>from mmdet.core.bbox.samplers.sampling_result import SamplingResult<N><N>from ..builder import ROTATED_BBOX_SAMPLERS<N><N>
<N>@ROTATED_BBOX_SAMPLERS.register_module()<N>class RRandomSampler(BaseSampler):<N>    """Random sampler.<N><N>    Args:<N>        num (int): Number of samples<N>        pos_fraction (float): Fraction of positive samples<N>        neg_pos_up (int, optional): Upper bound number of negative and<N>            positive samples. Defaults to -1.<N>        add_gt_as_proposals (bool, optional): Whether to add ground truth<N>            boxes as proposals. Defaults to True.<N>    """<N><N>
    def __init__(self,<N>                 num,<N>                 pos_fraction,<N>                 neg_pos_ub=-1,<N>                 add_gt_as_proposals=True,<N>                 **kwargs):<N>        from mmdet.core.bbox import demodata<N>        super(RRandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,<N>                                             add_gt_as_proposals)<N>        self.rng = demodata.ensure_rng(kwargs.get('rng', None))<N><N>
    def random_choice(self, gallery, num):<N>        """Random select some elements from the gallery.<N><N>        If `gallery` is a Tensor, the returned indices will be a Tensor;<N>        If `gallery` is a ndarray or list, the returned indices will be a<N>        ndarray.<N><N>
        Args:<N>            gallery (Tensor | ndarray | list): indices pool.<N>            num (int): expected sample num.<N><N>        Returns:<N>            Tensor or ndarray: sampled indices.<N>        """<N>        assert len(gallery) >= num<N><N>
        is_tensor = isinstance(gallery, torch.Tensor)<N>        if not is_tensor:<N>            gallery = torch.tensor(<N>                gallery, dtype=torch.long, device=torch.cuda.current_device())<N>        perm = torch.randperm(gallery.numel(), device=gallery.device)[:num]<N>        rand_inds = gallery[perm]<N>        if not is_tensor:<N>            rand_inds = rand_inds.cpu().numpy()<N>        return rand_inds<N><N>
    def _sample_pos(self, assign_result, num_expected, **kwargs):<N>        """Randomly sample some positive samples."""<N>        pos_inds = torch.nonzero(assign_result.gt_inds > 0, as_tuple=False)<N>        if pos_inds.numel() != 0:<N>            pos_inds = pos_inds.squeeze(1)<N>        if pos_inds.numel() <= num_expected:<N>            return pos_inds<N>        else:<N>            return self.random_choice(pos_inds, num_expected)<N><N>
    def _sample_neg(self, assign_result, num_expected, **kwargs):<N>        """Randomly sample some negative samples."""<N>        neg_inds = torch.nonzero(assign_result.gt_inds == 0, as_tuple=False)<N>        if neg_inds.numel() != 0:<N>            neg_inds = neg_inds.squeeze(1)<N>        if len(neg_inds) <= num_expected:<N>            return neg_inds<N>        else:<N>            return self.random_choice(neg_inds, num_expected)<N><N>
    def sample(self,<N>               assign_result,<N>               bboxes,<N>               gt_bboxes,<N>               gt_labels=None,<N>               **kwargs):<N>        """Sample positive and negative bboxes.<N><N>        This is a simple implementation of bbox sampling given candidates,<N>        assigning results and ground truth bboxes.<N><N>
        Args:<N>            assign_result (:obj:`AssignResult`): Bbox assigning results.<N>            bboxes (torch.Tensor): Boxes to be sampled from.<N>            gt_bboxes (torch.Tensor): Ground truth bboxes.<N>            gt_labels (Tensor, optional): Class labels of ground truth bboxes.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .rotate_random_sampler import RRandomSampler<N><N>__all__ = ['RRandomSampler']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from math import pi<N><N>import numpy as np<N>import torch<N><N><N>class GaussianMixture():<N>    """Initializes the Gaussian mixture model and brings all tensors into their<N>    required shape.<N><N>
    Args:<N>        n_components (int): number of components.<N>        n_features (int, optional): number of features.<N>        mu_init (torch.Tensor, optional): (T, k, d)<N>        var_init (torch.Tensor, optional): (T, k, d) or (T, k, d, d)<N>        eps (float, optional): Defaults to 1e-6.<N>        requires_grad (bool, optional): Defaults to False.<N>    """<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .gmm import GaussianMixture<N><N>__all__ = ['GaussianMixture']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.ops import nms_rotated<N><N><N>def multiclass_nms_rotated(multi_bboxes,<N>                           multi_scores,<N>                           score_thr,<N>                           nms,<N>                           max_num=-1,<N>                           score_factors=None,<N>                           return_inds=False):<N>    """NMS for multi-class bboxes.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .bbox_nms_rotated import (aug_multiclass_nms_rotated,<N>                               multiclass_nms_rotated)<N><N>__all__ = ['multiclass_nms_rotated', 'aug_multiclass_nms_rotated']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import math<N><N>import cv2<N>import numpy as np<N>from mmcv.image import imread, imwrite<N>from mmcv.visualization.color import color_val<N><N><N>def imshow(img, win_name='', wait_time=0):<N>    """Show an image.<N><N>
    Args:<N>        img (str or ndarray): The image to be displayed.<N>        win_name (str): The window name.<N>        wait_time (int): Value of waitKey param.<N>    """<N>    cv2.imshow(win_name, imread(img))<N>    if wait_time == 0:  # prevent from hanging if windows was closed<N>        while True:<N>            ret = cv2.waitKey(1)<N><N>
            closed = cv2.getWindowProperty(win_name, cv2.WND_PROP_VISIBLE) < 1<N>            # if user closed window or if some key pressed<N>            if closed or ret != -1:<N>                break<N>    else:<N>        ret = cv2.waitKey(wait_time)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .image import imshow_det_rbboxes<N><N>__all__ = ['imshow_det_rbboxes']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import platform<N><N>from mmcv.utils import build_from_cfg<N>from mmdet.datasets import DATASETS, PIPELINES<N>from mmdet.datasets.builder import _concat_dataset<N><N>ROTATED_DATASETS = DATASETS<N>ROTATED_PIPELINES = PIPELINES<N><N>
if platform.system() != 'Windows':<N>    # https://github.com/pytorch/pytorch/issues/973<N>    import resource<N>    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)<N>    base_soft_limit = rlimit[0]<N>    hard_limit = rlimit[1]<N>    soft_limit = min(max(4096, base_soft_limit), hard_limit)<N>    resource.setrlimit(resource.RLIMIT_NOFILE, (soft_limit, hard_limit))<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import glob<N>import os<N>import os.path as osp<N>import re<N>import tempfile<N>import time<N>import zipfile<N>from collections import defaultdict<N>from functools import partial<N>from multiprocessing import Pool<N><N>
import mmcv<N>import numpy as np<N>import torch<N>from mmcv.ops import box_iou_rotated, nms_rotated<N>from mmcv.utils import print_log<N>from mmdet.core.evaluation import average_precision<N>from mmdet.datasets.custom import CustomDataset<N>from terminaltables import AsciiTable<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .builder import ROTATED_DATASETS<N>from .dota import DOTADataset<N><N><N>@ROTATED_DATASETS.register_module()<N>class SARDataset(DOTADataset):<N>    """SAR ship dataset for detection (Support RSSDD and HRSID)."""<N>    CLASSES = ('ship', )<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .builder import build_dataset  # noqa: F401, F403<N>from .dota import DOTADataset  # noqa: F401, F403<N>from .pipelines import *  # noqa: F401, F403<N>from .sar import SARDataset  # noqa: F401, F403<N><N>__all__ = ['SARDataset', 'DOTADataset', 'build_dataset']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import cv2<N>import numpy as np<N>from mmdet.datasets.pipelines.transforms import RandomFlip, Resize<N><N>from mmrotate.core import norm_angle, obb2poly_np, poly2obb_np<N>from ..builder import ROTATED_PIPELINES<N><N>
<N>@ROTATED_PIPELINES.register_module()<N>class RResize(Resize):<N>    """Resize images & rotated bbox Inherit Resize pipeline class to handle<N>    rotated bboxes.<N><N>    Args:<N>        img_scale (tuple or list[tuple]): Images scales for resizing.<N>        multiscale_mode (str): Either "range" or "value".<N>        ratio_range (tuple[float]): (min_ratio, max_ratio).<N>    """<N><N>
    def __init__(self,<N>                 img_scale=None,<N>                 multiscale_mode='range',<N>                 ratio_range=None):<N>        super(RResize, self).__init__(<N>            img_scale=img_scale,<N>            multiscale_mode=multiscale_mode,<N>            ratio_range=ratio_range,<N>            keep_ratio=True)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .transforms import PolyRandomRotate, RRandomFlip, RResize<N><N>__all__ = ['RResize', 'RRandomFlip', 'PolyRandomRotate']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import warnings<N><N>from mmdet.models.builder import MODELS<N><N>ROTATED_BACKBONES = MODELS<N>ROTATED_LOSSES = MODELS<N>ROTATED_DETECTORS = MODELS<N>ROTATED_ROI_EXTRACTORS = MODELS<N>ROTATED_HEADS = MODELS<N>ROTATED_NECKS = MODELS<N>ROTATED_SHARED_HEADS = MODELS<N><N>
<N>def build_backbone(cfg):<N>    """Build backbone."""<N>    return ROTATED_BACKBONES.build(cfg)<N><N><N>def build_neck(cfg):<N>    """Build neck."""<N>    return ROTATED_NECKS.build(cfg)<N><N><N>def build_roi_extractor(cfg):<N>    """Build roi extractor."""<N>    return ROTATED_ROI_EXTRACTORS.build(cfg)<N><N>
<N>def build_shared_head(cfg):<N>    """Build shared head."""<N>    return ROTATED_SHARED_HEADS.build(cfg)<N><N><N>def build_head(cfg):<N>    """Build head."""<N>    return ROTATED_HEADS.build(cfg)<N><N><N>def build_loss(cfg):<N>    """Build loss."""<N>    return ROTATED_LOSSES.build(cfg)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N># Modified from csuhan: https://github.com/csuhan/ReDet<N>import warnings<N><N>import e2cnn.nn as enn<N>import torch.nn as nn<N>import torch.utils.checkpoint as cp<N>from mmcv.runner import BaseModule<N>from torch.nn.modules.batchnorm import _BatchNorm<N><N>
from ..builder import ROTATED_BACKBONES<N>from ..utils import (build_enn_divide_feature, build_enn_norm_layer,<N>                     build_enn_trivial_feature, ennAvgPool, ennConv,<N>                     ennMaxPool, ennReLU, ennTrivialConv)<N><N><N>class BasicBlock(enn.EquivariantModule):<N>    """BasicBlock for ReResNet.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .re_resnet import ReResNet<N><N>__all__ = ['ReResNet']<N>
# Copyright (c) SJTU. All rights reserved.<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule, bias_init_with_prob, normal_init<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N>from ..utils import ORConv2d, RotationInvariantPooling<N>from .kfiou_rotate_retina_head import KFIoURRetinaHead<N><N>
# Copyright (c) SJTU. All rights reserved.<N>from ..builder import ROTATED_HEADS<N>from .rotated_retina_head import RotatedRetinaHead<N><N><N>@ROTATED_HEADS.register_module()<N>class KFIoURRetinaHead(RotatedRetinaHead):<N>    """Rotational Anchor-based refine head.<N><N>
# Copyright (c) SJTU. All rights reserved.<N>import torch<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N>from .kfiou_rotate_retina_head import KFIoURRetinaHead<N><N><N>@ROTATED_HEADS.register_module()<N>class KFIoURRetinaRefineHead(KFIoURRetinaHead):<N>    """Rotational Anchor-based refine head.<N><N>
import torch<N>import torch.nn.functional as F<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmrotate.core import (build_assigner, build_bbox_coder, build_prior_generator, <N>                        build_sampler, obb2hbb, rotated_anchor_inside_flags)<N>from mmdet.models.builder import HEADS<N><N>
from .rotated_retina_head_distribution import RotatedRetinaHeadDistribution<N>from mmdet.core.bbox.iou_calculators import build_iou_calculator<N>from ..builder import ROTATED_HEADS, build_loss<N><N>@ROTATED_HEADS.register_module()<N>class LDRotatedRetinaHead(RotatedRetinaHeadDistribution):<N>    r"""An anchor-based head used in `RetinaNet<N>    <https://arxiv.org/pdf/1708.02002.pdf>`_.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule, bias_init_with_prob, normal_init<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N>from ..utils import ORConv2d, RotationInvariantPooling<N>from .rotated_retina_head import RotatedRetinaHead<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import copy<N><N>import torch<N>import torch.nn as nn<N>from mmcv.ops import batched_nms<N>from mmdet.core import unmap<N><N>from mmrotate.core import obb2xyxy, rotated_anchor_inside_flags<N>from ..builder import ROTATED_HEADS<N>from .rotated_rpn_head import RotatedRPNHead<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from inspect import signature<N><N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import normal_init<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmdet.models.dense_heads.base_dense_head import BaseDenseHead<N><N>
from mmrotate.core import (aug_multiclass_nms_rotated, bbox_mapping_back,<N>                           build_assigner, build_bbox_coder,<N>                           build_prior_generator, build_sampler,<N>                           multiclass_nms_rotated, obb2hbb,<N>                           rotated_anchor_inside_flags)<N>from ..builder import ROTATED_HEADS, build_loss<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from inspect import signature<N><N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import normal_init<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmdet.models.dense_heads.base_dense_head import BaseDenseHead<N><N>
from mmrotate.core import (aug_multiclass_nms_rotated, bbox_mapping_back,<N>                           build_assigner, build_bbox_coder,<N>                           build_prior_generator, build_sampler,<N>                           multiclass_nms_rotated, obb2hbb,<N>                           rotated_anchor_inside_flags)<N>from ..builder import ROTATED_HEADS, build_loss<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import numpy as np<N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule<N>from mmcv.ops import DeformConv2d, min_area_polygons<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmdet.core.anchor.point_generator import MlvlPointGenerator<N>from mmdet.core.utils import select_single_mlvl<N>from mmdet.models.dense_heads.base_dense_head import BaseDenseHead<N><N>
from mmrotate.core import (build_assigner, build_sampler,<N>                           multiclass_nms_rotated, obb2poly, poly2obb)<N>from ..builder import ROTATED_HEADS, build_loss<N>from .utils import convex_overlaps, levels_to_images<N><N><N>@ROTATED_HEADS.register_module()<N>class RotatedRepPointsHead(BaseDenseHead):<N>    """Rotated RepPoints head.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule, bias_init_with_prob, normal_init<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N>from .rotated_anchor_head import RotatedAnchorHead<N><N>
<N>@ROTATED_HEADS.register_module()<N>class RotatedRetinaHead(RotatedAnchorHead):<N>    r"""An anchor-based head used in `RotatedRetinaNet<N>    <https://arxiv.org/pdf/1708.02002.pdf>`_.<N><N>    The head contains two subnetworks. The first classifies anchor boxes and<N>    the second regresses deltas for the anchors.<N><N>
    Example:<N>        >>> import torch<N>        >>> self = RetinaHead(11, 7)<N>        >>> x = torch.rand(1, 7, 32, 32)<N>        >>> cls_score, bbox_pred = self.forward_single(x)<N>        >>> # Each anchor predicts a score for each class except background<N>        >>> cls_per_anchor = cls_score.shape[1] / self.num_anchors<N>        >>> box_per_anchor = bbox_pred.shape[1] / self.num_anchors<N>        >>> assert cls_per_anchor == (self.num_classes)<N>        >>> assert box_per_anchor == 4<N>    """<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from mmcv.cnn import ConvModule, bias_init_with_prob, normal_init<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.runner import force_fp32<N><N>from ..builder import ROTATED_HEADS<N>from . import RotatedRetinaHead<N><N><N>@ROTATED_HEADS.register_module()<N>class RotatedRetinaRefineHead(RotatedRetinaHead):<N>    """Rotational Anchor-based refine head.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import copy<N><N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from mmcv.ops import batched_nms<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmdet.models.dense_heads.anchor_head import AnchorHead<N><N>
from mmrotate.core import obb2xyxy, rotated_anchor_inside_flags<N>from ..builder import ROTATED_HEADS<N><N><N>@ROTATED_HEADS.register_module()<N>class RotatedRPNHead(AnchorHead):<N>    """Rotated RPN head for rotated bboxes.<N><N>    Args:<N>        in_channels (int): Number of channels in the input feature map.<N>        init_cfg (dict or list[dict], optional): Initialization config dict.<N>    """  # noqa: W605<N><N>
    def __init__(self,<N>                 in_channels,<N>                 init_cfg=dict(type='Normal', layer='Conv2d', std=0.01),<N>                 version='oc',<N>                 **kwargs):<N>        self.version = version<N>        super(RotatedRPNHead, self).__init__(<N>            1, in_channels, init_cfg=init_cfg, **kwargs)<N><N>
    def _init_layers(self):<N>        """Initialize layers of the head."""<N>        self.rpn_conv = nn.Conv2d(<N>            self.in_channels, self.feat_channels, 3, padding=1)<N>        self.rpn_cls = nn.Conv2d(self.feat_channels,<N>                                 self.num_anchors * self.cls_out_channels, 1)<N>        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)<N><N>
    def forward_single(self, x):<N>        """Forward feature map of a single scale level."""<N>        x = self.rpn_conv(x)<N>        x = F.relu(x, inplace=True)<N>        rpn_cls_score = self.rpn_cls(x)<N>        rpn_bbox_pred = self.rpn_reg(x)<N>        return rpn_cls_score, rpn_bbox_pred<N><N>
    def _get_targets_single(self,<N>                            flat_anchors,<N>                            valid_flags,<N>                            gt_bboxes,<N>                            gt_bboxes_ignore,<N>                            gt_labels,<N>                            img_meta,<N>                            label_channels=1,<N>                            unmap_outputs=True):<N>        """Compute regression and classification targets for anchors in a<N>        single image.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import numpy as np<N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule<N>from mmcv.ops import DeformConv2d, min_area_polygons<N>from mmcv.runner import force_fp32<N>from mmdet.core import images_to_levels, multi_apply, unmap<N>from mmdet.core.anchor.point_generator import MlvlPointGenerator<N>from mmdet.core.utils import select_single_mlvl<N>from mmdet.models.dense_heads.base_dense_head import BaseDenseHead<N><N>
from mmrotate.core import (build_assigner, build_sampler,<N>                           multiclass_nms_rotated, obb2poly, poly2obb)<N>from ..builder import ROTATED_HEADS, build_loss<N>from .utils import get_num_level_anchors_inside, points_center_pts<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>from mmcv.ops import convex_iou<N><N><N>def points_center_pts(RPoints, y_first=True):<N>    '''<N>    RPoints:[:, 18]  the  lists of Pointsets (9 points)<N>    center_pts: the mean_center coordination of Pointsets<N>    '''<N>    RPoints = RPoints.reshape(-1, 9, 2)<N><N>
    if y_first:<N>        pts_dy = RPoints[:, :, 0::2]<N>        pts_dx = RPoints[:, :, 1::2]<N>    else:<N>        pts_dx = RPoints[:, :, 0::2]<N>        pts_dy = RPoints[:, :, 1::2]<N>    pts_dy_mean = pts_dy.mean(dim=1, keepdim=True).reshape(-1, 1)<N>    pts_dx_mean = pts_dx.mean(dim=1, keepdim=True).reshape(-1, 1)<N>    center_pts = torch.cat([pts_dx_mean, pts_dy_mean], dim=1).reshape(-1, 2)<N>    return center_pts<N><N>
<N>def convex_overlaps(gt_bboxes, points):<N>    """Compute overlaps between polygons and points.<N><N>    Args:<N>        gt_rbboxes (torch.Tensor): Groundtruth polygons, shape (k, 8).<N>        points (torch.Tensor): Points to be assigned, shape(n, 18).<N>    Returns:<N>        overlaps (torch.Tensor): Overlaps between k gt_bboxes and n bboxes,<N>            shape(k, n).<N>    """<N>    overlaps = convex_iou(points, gt_bboxes)<N>    overlaps = overlaps.transpose(1, 0)<N>    return overlaps<N><N>
<N>def levels_to_images(mlvl_tensor, flatten=False):<N>    """Concat multi-level feature maps by image.<N><N>    [feature_level0, feature_level1...] -> [feature_image0, feature_image1...]<N>    Convert the shape of each element in mlvl_tensor from (N, C, H, W) to<N>    (N, H*W , C), then split the element to N elements with shape (H*W, C), and<N>    concat elements in same image of all level along first dimension.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>import numpy as np<N>from mmdet.models import BaseDetector<N><N>from mmrotate.core import imshow_det_rbboxes<N>from ..builder import ROTATED_DETECTORS<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedBaseDetector(BaseDetector):<N>    """Base class for rotated detectors."""<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .two_stage import RotatedTwoStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class GlidingVertex(RotatedTwoStageDetector):<N>    """Implementation of `Gliding Vertex on the Horizontal Bounding Box for<N>    Multi-Oriented Object Detection <https://arxiv.org/pdf/1911.09358.pdf>`_"""<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .two_stage import RotatedTwoStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class OrientedRCNN(RotatedTwoStageDetector):<N>    """Implementation of `Oriented R-CNN for Object Detection.<N><N>
# Copyright (c) SJTU. All rights reserved.<N>import torch.nn as nn<N><N>from mmrotate.core import rbbox2result<N>from ..builder import ROTATED_DETECTORS, build_backbone, build_head, build_neck<N>from .base import RotatedBaseDetector<N>from .utils import FeatureRefineModule<N><N>
<N>@ROTATED_DETECTORS.register_module()<N>class R3Det(RotatedBaseDetector):<N>    """Rotated Refinement RetinaNet."""<N><N>    def __init__(self,<N>                 num_refine_stages,<N>                 backbone,<N>                 neck=None,<N>                 bbox_head=None,<N>                 frm_cfgs=None,<N>                 refine_heads=None,<N>                 train_cfg=None,<N>                 test_cfg=None,<N>                 pretrained=None):<N>        super(R3Det, self).__init__()<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .two_stage import RotatedTwoStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class ReDet(RotatedTwoStageDetector):<N>    """Implementation of `ReDet: A Rotation-equivariant Detector for Aerial<N>    Object Detection.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .two_stage import RotatedTwoStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RoITransformer(RotatedTwoStageDetector):<N>    """Implementation of `Learning RoI Transformer for Oriented Object<N>    Detection in Aerial Images.<N><N>
    <https://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_<N>    Learning_RoI_Transformer_for_Oriented_Object_Detection_in_<N>    Aerial_Images_CVPR_2019_paper.pdf#:~:text=The%20core%20idea<N>    %20of%20RoI%20Transformer%20is%20to,embed-%20ded%20into%20<N>    detectors%20for%20oriented%20object%20detection.>`_<N>    """<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>from numpy.lib.twodim_base import tri<N>import torch<N>from mmcv.runner import load_checkpoint<N><N>from .. import build_detector<N>from ..builder import ROTATED_DETECTORS<N>from .single_stage import RotatedSingleStageDetector<N>from mmrotate.core import rbbox2result<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .single_stage import RotatedSingleStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedRepPoints(RotatedSingleStageDetector):<N>    """Implementation of RepPoints."""<N><N>
    def __init__(self,<N>                 backbone,<N>                 neck,<N>                 bbox_head,<N>                 train_cfg=None,<N>                 test_cfg=None,<N>                 pretrained=None):<N>        super(RotatedRepPoints, self).__init__(backbone, neck, bbox_head,<N>                                               train_cfg, test_cfg, pretrained)<N>        self.bbox_head.init_weights()<N><N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .single_stage import RotatedSingleStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedRetinaNet(RotatedSingleStageDetector):<N>    """Implementation of Rotated `RetinaNet.<N><N>
    <https://arxiv.org/abs/1708.02002>`_<N>    """<N><N>    def __init__(self,<N>                 backbone,<N>                 neck,<N>                 bbox_head,<N>                 train_cfg=None,<N>                 test_cfg=None,<N>                 pretrained=None):<N>        super(RotatedRetinaNet, self).__init__(backbone, neck, bbox_head,<N>                                               train_cfg, test_cfg, pretrained)<N>        self.bbox_head.init_weights()<N><N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .single_stage import RotatedSingleStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedRetinaNetDistribution(RotatedSingleStageDetector):<N>    """Implementation of Rotated `RetinaNet.<N><N>
    <https://arxiv.org/abs/1708.02002>`_<N>    """<N><N>    def __init__(self,<N>                 backbone,<N>                 neck,<N>                 bbox_head,<N>                 train_cfg=None,<N>                 test_cfg=None,<N>                 pretrained=None):<N>        super(RotatedRetinaNetDistribution, self).__init__(backbone, neck, bbox_head,<N>                                               train_cfg, test_cfg, pretrained)<N>        self.bbox_head.init_weights()<N><N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from ..builder import ROTATED_DETECTORS<N>from .two_stage import RotatedTwoStageDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedFasterRCNN(RotatedTwoStageDetector):<N>    """Implementation of Rotated `Faster R-CNN.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmrotate.core import rbbox2result<N>from ..builder import ROTATED_DETECTORS, build_backbone, build_head, build_neck<N>from .base import RotatedBaseDetector<N>from .utils import AlignConvModule<N><N>
<N>@ROTATED_DETECTORS.register_module()<N>class S2ANet(RotatedBaseDetector):<N>    """Rotated Refinement RetinaNet."""<N><N>    def __init__(self,<N>                 backbone,<N>                 neck=None,<N>                 fam_head=None,<N>                 align_cfgs=None,<N>                 odm_head=None,<N>                 train_cfg=None,<N>                 test_cfg=None,<N>                 pretrained=None):<N>        super(S2ANet, self).__init__()<N><N>
        backbone.pretrained = pretrained<N>        self.backbone = build_backbone(backbone)<N>        if neck is not None:<N>            self.neck = build_neck(neck)<N>        if train_cfg is not None:<N>            fam_head.update(train_cfg=train_cfg['fam_cfg'])<N>        fam_head.update(test_cfg=test_cfg)<N>        self.fam_head = build_head(fam_head)<N>        self.fam_head.init_weights()<N><N>
        self.align_conv_type = align_cfgs['type']<N>        self.align_conv_size = align_cfgs['kernel_size']<N>        self.feat_channels = align_cfgs['channels']<N>        self.featmap_strides = align_cfgs['featmap_strides']<N><N>        if self.align_conv_type == 'AlignConv':<N>            self.align_conv = AlignConvModule(self.feat_channels,<N>                                              self.featmap_strides,<N>                                              self.align_conv_size)<N><N>
        if train_cfg is not None:<N>            odm_head.update(train_cfg=train_cfg['odm_cfg'])<N>        odm_head.update(test_cfg=test_cfg)<N>        self.odm_head = build_head(odm_head)<N>        self.odm_head.init_weights()<N><N>        self.train_cfg = train_cfg<N>        self.test_cfg = test_cfg<N><N>
    def extract_feat(self, img):<N>        """Directly extract features from the backbone+neck."""<N>        x = self.backbone(img)<N>        if self.with_neck:<N>            x = self.neck(x)<N>        return x<N><N>    def forward_dummy(self, img):<N>        """Used for computing network flops.<N><N>
        See `mmedetection/tools/get_flops.py`<N>        """<N>        x = self.extract_feat(img)<N>        outs = self.fam_head(x)<N>        rois = self.fam_head.refine_bboxes(*outs)<N>        # rois: list(indexed by images) of list(indexed by levels)<N>        align_feat = self.align_conv(x, rois)<N>        outs = self.odm_head(align_feat)<N><N>
        return outs<N><N>    def forward_train(self,<N>                      img,<N>                      img_metas,<N>                      gt_bboxes,<N>                      gt_labels,<N>                      gt_bboxes_ignore=None):<N>        """Forward function of S2ANet."""<N>        losses = dict()<N>        x = self.extract_feat(img)<N><N>
        outs = self.fam_head(x)<N><N>        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)<N>        loss_base = self.fam_head.loss(<N>            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)<N>        for name, value in loss_base.items():<N>            losses[f'fam.{name}'] = value<N><N>
        rois = self.fam_head.refine_bboxes(*outs)<N>        # rois: list(indexed by images) of list(indexed by levels)<N>        align_feat = self.align_conv(x, rois)<N>        outs = self.odm_head(align_feat)<N>        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas)<N>        loss_refine = self.odm_head.loss(<N>            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore, rois=rois)<N>        for name, value in loss_refine.items():<N>            losses[f'odm.{name}'] = value<N><N>
        return losses<N><N>    def simple_test(self, img, img_meta, rescale=False):<N>        """Test function without test time augmentation.<N><N>        Args:<N>            imgs (list[torch.Tensor]): List of multiple images<N>            img_metas (list[dict]): List of image information.<N>            rescale (bool, optional): Whether to rescale the results.<N>                Defaults to False.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import warnings<N><N>from mmrotate.core import rbbox2result<N>from ..builder import ROTATED_DETECTORS, build_backbone, build_head, build_neck<N>from .base import RotatedBaseDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedSingleStageDetector(RotatedBaseDetector):<N>    """Base class for rotated single-stage detectors.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import warnings<N><N>import torch<N><N>from ..builder import ROTATED_DETECTORS, build_backbone, build_head, build_neck<N>from .base import RotatedBaseDetector<N><N><N>@ROTATED_DETECTORS.register_module()<N>class RotatedTwoStageDetector(RotatedBaseDetector):<N>    """Base class for rotated two-stage detectors.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import normal_init<N>from mmcv.ops import DeformConv2d, rotated_feature_align<N><N><N>class AlignConv(nn.Module):<N>    """Implementation of `Align Deep Features for Oriented Object Detection.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>from mmcv.ops import convex_giou<N>from torch.autograd import Function<N>from torch.autograd.function import once_differentiable<N><N>from ..builder import ROTATED_LOSSES<N><N>
<N>class ConvexGIoULossFuction(Function):<N>    """The function of Convex GIoU loss."""<N><N>    @staticmethod<N>    def forward(ctx,<N>                pred,<N>                target,<N>                weight=None,<N>                reduction=None,<N>                avg_factor=None,<N>                loss_weight=1.0):<N>        """Forward function.<N><N>
# Copyright (c) SJTU. All rights reserved.<N>from copy import deepcopy<N><N>import torch<N>from mmdet.models.losses.utils import weighted_loss<N>from torch import nn<N><N>from ..builder import ROTATED_LOSSES<N><N><N>def xy_wh_r_2_xy_sigma(xywhr):<N>    """Convert oriented bounding box to 2-D Gaussian distribution.<N><N>
# Copyright (c) SJTU. All rights reserved.<N>from copy import deepcopy<N><N>import torch<N>from torch import nn<N><N>from ..builder import ROTATED_LOSSES<N><N><N>def xy_wh_r_2_xy_sigma(xywhr):<N>    """Convert oriented bounding box to 2-D Gaussian distribution.<N><N>
from re import S<N>import mmcv<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N><N>from ..builder import ROTATED_LOSSES<N>from mmdet.models.losses.utils import weighted_loss<N><N>@mmcv.jit(derivate=True, coderize=True)<N>@weighted_loss<N>def im_loss(x, soft_target):<N>    # print(x.shape, soft_target.shape)<N>    # print(F.mse_loss(x, soft_target))<N>    return F.mse_loss(x, soft_target)<N><N>
@ROTATED_LOSSES.register_module()<N>class IMLoss(nn.Module):<N><N>    def __init__(self, reduction='mean', loss_weight=1.0):<N>        super().__init__()<N>        self.reduction = reduction<N>        self.loss_weight = loss_weight<N><N>    def forward(self,<N>                x,<N>                soft_target,<N>                weight=None,<N>                avg_factor=None,<N>                reduction_override=None):<N>        assert reduction_override in (None, 'none', 'mean', 'sum')<N><N>
# Copyright (c) SJTU. All rights reserved.<N>import torch<N>from mmdet.models.losses.utils import weighted_loss<N>from torch import nn<N><N>from ..builder import ROTATED_LOSSES<N><N><N>def xy_wh_r_2_xy_sigma(xywhr):<N>    """Convert oriented bounding box to 2-D Gaussian distribution.<N><N>
# Copyright (c) SJTU. All rights reserved.<N>import torch<N>import torch.nn as nn<N>from mmdet.models.losses.utils import weighted_loss<N><N>from mmrotate.core import GaussianMixture, gt2gaussian<N>from ..builder import ROTATED_LOSSES<N><N><N>def kld_single2single(g1, g2):<N>    """Compute Kullback-Leibler Divergence.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .convex_giou_loss import BCConvexGIoULoss, ConvexGIoULoss<N>from .gaussian_dist_loss import GDLoss<N>from .gaussian_dist_loss_v1 import GDLoss_v1<N>from .kf_iou_loss import KFLoss<N>from .kld_reppoints_loss import KLDRepPointsLoss<N>from .kd_loss import IMLoss<N><N>__all__ = [<N>    'GDLoss', 'GDLoss_v1', 'KFLoss', 'ConvexGIoULoss', 'BCConvexGIoULoss',<N>    'KLDRepPointsLoss', 'IMLoss'<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N># Modified from csuhan: https://github.com/csuhan/ReDet<N>import warnings<N><N>import e2cnn.nn as enn<N>import torch.nn as nn<N>from mmcv.runner import BaseModule, auto_fp16<N><N>from ..builder import ROTATED_NECKS<N>from ..utils import (build_enn_feature, build_enn_norm_layer, ennConv,<N>                     ennInterpolate, ennMaxPool, ennReLU)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmdet.core import bbox2roi<N><N>from ..builder import ROTATED_HEADS<N>from .rotate_standard_roi_head import RotatedStandardRoIHead<N><N><N>@ROTATED_HEADS.register_module()<N>class GVRatioRoIHead(RotatedStandardRoIHead):<N>    """Gliding vertex roi head including one bbox head."""<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N><N>from mmrotate.core import rbbox2roi<N>from ..builder import ROTATED_HEADS<N>from .rotate_standard_roi_head import RotatedStandardRoIHead<N><N><N>@ROTATED_HEADS.register_module()<N>class OrientedStandardRoIHead(RotatedStandardRoIHead):<N>    """Oriented RCNN roi head including one bbox head."""<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from abc import ABCMeta<N><N>import torch<N>from mmcv.runner import BaseModule, ModuleList<N>from mmdet.core import bbox2roi<N><N>from mmrotate.core import (build_assigner, build_sampler, obb2xyxy,<N>                           rbbox2result, rbbox2roi)<N>from ..builder import ROTATED_HEADS, build_head, build_roi_extractor<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from abc import ABCMeta<N><N>import torch<N>from mmcv.runner import BaseModule<N>from mmdet.core import bbox2roi<N><N>from mmrotate.core import build_assigner, build_sampler, obb2xyxy, rbbox2result<N>from ..builder import (ROTATED_HEADS, build_head, build_roi_extractor,<N>                       build_shared_head)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .bbox_heads import (RotatedBBoxHead, RotatedConvFCBBoxHead,<N>                         RotatedShared2FCBBoxHead)<N>from .gv_ratio_roi_head import GVRatioRoIHead<N>from .oriented_standard_roi_head import OrientedStandardRoIHead<N>from .roi_extractors import RotatedSingleRoIExtractor<N>from .roi_trans_roi_head import RoITransRoIHead<N>from .rotate_standard_roi_head import RotatedStandardRoIHead<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>from mmcv.cnn import ConvModule<N>from mmcv.runner import force_fp32<N>from mmdet.models.losses import accuracy<N>from mmdet.models.utils import build_linear_layer<N><N>
from ...builder import ROTATED_HEADS<N>from .rotated_bbox_head import RotatedBBoxHead<N><N><N>@ROTATED_HEADS.register_module()<N>class RotatedConvFCBBoxHead(RotatedBBoxHead):<N>    r"""More general bbox head, with shared conv and fc layers and two optional<N>    separated branches.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from mmcv.runner import BaseModule, auto_fp16, force_fp32<N>from mmcv.utils import to_2tuple<N>from mmdet.core import multi_apply<N>from mmdet.models.losses import accuracy<N>from mmdet.models.utils import build_linear_layer<N><N>
from mmrotate.core import (build_bbox_coder, hbb2obb, multiclass_nms_rotated,<N>                           obb2xyxy)<N>from ...builder import ROTATED_HEADS, build_loss<N><N><N>@ROTATED_HEADS.register_module()<N>class GVBBoxHead(BaseModule):<N>    """Gliding Vertex's RoI bbox head.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>import torch.nn.functional as F<N>from mmcv.runner import BaseModule, auto_fp16, force_fp32<N>from mmcv.utils import to_2tuple<N>from mmdet.core import multi_apply<N>from mmdet.models.losses import accuracy<N>from mmdet.models.utils import build_linear_layer<N><N>
from mmrotate.core import build_bbox_coder, multiclass_nms_rotated<N>from ...builder import ROTATED_HEADS, build_loss<N><N><N>@ROTATED_HEADS.register_module()<N>class RotatedBBoxHead(BaseModule):<N>    """Simplest RoI head, with only two fc layers for classification and<N>    regression respectively.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .convfc_rbbox_head import (RotatedConvFCBBoxHead,<N>                                RotatedKFIoUShared2FCBBoxHead,<N>                                RotatedShared2FCBBoxHead)<N>from .gv_bbox_head import GVBBoxHead<N>from .rotated_bbox_head import RotatedBBoxHead<N><N>__all__ = [<N>    'RotatedBBoxHead', 'RotatedConvFCBBoxHead', 'RotatedShared2FCBBoxHead',<N>    'GVBBoxHead', 'RotatedKFIoUShared2FCBBoxHead'<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import torch<N>import torch.nn as nn<N>from mmcv import ops<N>from mmcv.runner import force_fp32<N>from mmcv.utils import to_2tuple<N>from mmdet.models.roi_heads.roi_extractors.base_roi_extractor import \<N>    BaseRoIExtractor<N><N>
from ...builder import ROTATED_ROI_EXTRACTORS<N><N><N>@ROTATED_ROI_EXTRACTORS.register_module()<N>class RotatedSingleRoIExtractor(BaseRoIExtractor):<N>    """Extract RoI features from a single level feature map.<N><N>    If there are multiple input feature levels, each RoI is mapped to a level<N>    according to its scale. The mapping rule is proposed in<N>    `FPN <https://arxiv.org/abs/1612.03144>`_.<N><N>
    Args:<N>        roi_layer (dict): Specify RoI layer type and arguments.<N>        out_channels (int): Output channels of RoI layers.<N>        featmap_strides (List[int]): Strides of input feature maps.<N>        finest_scale (int): Scale threshold of mapping to level 0. Default: 56.<N>        init_cfg (dict or list[dict], optional): Initialization config dict.<N>            Default: None<N>    """<N><N>
    def __init__(self,<N>                 roi_layer,<N>                 out_channels,<N>                 featmap_strides,<N>                 finest_scale=56,<N>                 init_cfg=None):<N>        super(RotatedSingleRoIExtractor,<N>              self).__init__(roi_layer, out_channels, featmap_strides,<N>                             init_cfg)<N>        self.finest_scale = finest_scale<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .rotate_single_level_roi_extractor import RotatedSingleRoIExtractor<N><N>__all__ = ['RotatedSingleRoIExtractor']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import e2cnn.nn as enn<N>from e2cnn import gspaces<N><N>N = 8<N>gspace = gspaces.Rot2dOnR2(N=N)<N><N><N>def build_enn_divide_feature(planes):<N>    """build a enn regular feature map with the specified number of channels<N>    divided by N."""<N>    assert gspace.fibergroup.order() > 0<N>    N = gspace.fibergroup.order()<N>    planes = planes / N<N>    planes = int(planes)<N>    return enn.FieldType(gspace, [gspace.regular_repr] * planes)<N><N>
<N>def build_enn_feature(planes):<N>    """build a enn regular feature map with the specified number of<N>    channels."""<N>    return enn.FieldType(gspace, planes * [gspace.regular_repr])<N><N><N>def build_enn_trivial_feature(planes):<N>    """build a enn trivial feature map with the specified number of<N>    channels."""<N>    return enn.FieldType(gspace, planes * [gspace.trivial_repr])<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from __future__ import absolute_import<N>import math<N><N>import torch<N>import torch.nn.functional as F<N>from mmcv.ops import active_rotated_filter<N>from mmcv.utils import to_2tuple<N>from torch.nn.modules import Conv2d<N>from torch.nn.parameter import Parameter<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from torch import nn<N><N><N>class RotationInvariantPooling(nn.Module):<N>    """Rotating invariant pooling module."""<N><N>    def __init__(self, nInputPlane, nOrientation=8):<N>        super(RotationInvariantPooling, self).__init__()<N>        self.nInputPlane = nInputPlane<N>        self.nOrientation = nOrientation<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .enn import (build_enn_divide_feature, build_enn_feature,<N>                  build_enn_norm_layer, build_enn_trivial_feature, ennAvgPool,<N>                  ennConv, ennInterpolate, ennMaxPool, ennReLU, ennTrivialConv)<N>from .orconv import ORConv2d<N>from .ripool import RotationInvariantPooling<N><N>
__all__ = [<N>    'ORConv2d', 'RotationInvariantPooling', 'ennConv', 'ennReLU', 'ennAvgPool',<N>    'ennMaxPool', 'ennInterpolate', 'build_enn_divide_feature',<N>    'build_enn_feature', 'build_enn_norm_layer', 'build_enn_trivial_feature',<N>    'ennTrivialConv'<N>]<N><N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from mmcv.utils import collect_env as collect_basic_env<N>from mmcv.utils import get_git_hash<N><N>import mmrotate<N><N><N>def collect_env():<N>    """Collect environment information."""<N>    env_info = collect_basic_env()<N>    env_info['MMRotate'] = (<N>        mmrotate.__version__ + '+' + get_git_hash(digits=7))<N>    return env_info<N><N><N>if __name__ == '__main__':<N>    for name, val in collect_env().items():<N>        print(f'{name}: {val}')<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .collect_env import collect_env<N>from .logger import get_root_logger<N>from .misc import find_latest_checkpoint<N><N>__all__ = [<N>    'get_root_logger',<N>    'collect_env',<N>    'find_latest_checkpoint',<N>]<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import os.path as osp<N>import shutil<N>import tempfile<N><N>import numpy as np<N>import pytest<N>import torch<N>from mmdet.datasets import build_dataset<N><N>from mmrotate.datasets.dota import DOTADataset<N><N>
<N>def _create_dummy_results():<N>    """Create dummy results."""<N>    boxes = [<N>        np.array([[4.3150e+02, 7.0600e+02, 6.7686e+01, 2.1990e+01, 2.9842e-02],<N>                  [5.6351e+02, 5.3575e+02, 1.0018e+02, 1.8971e+01, 5.5499e-02],<N>                  [5.7450e+02, 5.8450e+02, 9.5567e+01, 2.1094e+01,<N>                   8.4012e-02]])<N>    ]<N>    return [boxes]<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import copy<N><N>import numpy as np<N>from mmcv.utils import build_from_cfg<N>from mmdet.datasets.builder import PIPELINES<N><N>from .utils import _check_fields<N><N><N>def check_result_same(results, pipeline_results):<N>    """Check whether the `pipeline_results` is the same with the predefined<N>    `results`.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import numpy as np<N><N><N>def _check_fields(results, pipeline_results, keys):<N>    """Check data in fields from two results are same."""<N>    for key in keys:<N>        assert np.equal(results[key], pipeline_results[key]).all()<N>        assert results[key].dtype == pipeline_results[key].dtype<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>from .utils import _check_fields<N><N>__all__ = ['_check_fields']<N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import pytest<N>import torch<N><N>from mmrotate.models.backbones import ReResNet<N><N><N>def test_reresnet_backbone():<N>    """Test reresnet backbone."""<N>    with pytest.raises(KeyError):<N>        # ResNet depth should be in [18, 34, 50, 101, 152]<N>        ReResNet(20)<N><N>
    with pytest.raises(AssertionError):<N>        # In ResNet: 1 <= num_stages <= 4<N>        ReResNet(50, num_stages=0)<N><N>    with pytest.raises(AssertionError):<N>        # In ResNet: 1 <= num_stages <= 4<N>        ReResNet(50, num_stages=5)<N><N>
    with pytest.raises(AssertionError):<N>        # len(strides) == len(dilations) == num_stages<N>        ReResNet(50, strides=(1, ), dilations=(1, 1), num_stages=3)<N><N>    with pytest.raises(TypeError):<N>        # pretrained must be a string path<N>        model = ReResNet(50, pretrained=0)<N><N>
    with pytest.raises(AssertionError):<N>        # Style must be in ['pytorch', 'caffe']<N>        ReResNet(50, style='tensorflow')<N><N>    # test reresnet50<N>    model = ReResNet(<N>        depth=50,<N>        num_stages=4,<N>        out_indices=(0, 1, 2, 3),<N>        frozen_stages=1,<N>        style='pytorch')<N>    model.train()<N><N>
    imgs = torch.randn(1, 3, 32, 32)<N>    feat = model(imgs)<N>    assert len(feat) == 4<N>    assert feat[0].shape == torch.Size([1, 256, 8, 8])<N>    assert feat[1].shape == torch.Size([1, 512, 4, 4])<N>    assert feat[2].shape == torch.Size([1, 1024, 2, 2])<N>    assert feat[3].shape == torch.Size([1, 2048, 1, 1])<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>import pytest<N>import torch<N><N>from mmrotate.models.dense_heads import RotatedAnchorHead<N><N><N>@pytest.mark.skipif(<N>    not torch.cuda.is_available(), reason='requires CUDA support')<N>def test_anchor_head_loss():<N>    """Tests rotate anchor head loss when truth is empty and non-empty."""<N>    s = 256<N>    img_metas = [{<N>        'img_shape': (s, s, 3),<N>        'scale_factor': 1,<N>        'pad_shape': (s, s, 3)<N>    }]<N><N>
    cfg = mmcv.Config(<N>        dict(<N>            assigner=dict(<N>                type='MaxIoUAssigner',<N>                pos_iou_thr=0.5,<N>                neg_iou_thr=0.4,<N>                min_pos_iou=0,<N>                ignore_iof_thr=-1,<N>                iou_calculator=dict(type='RBboxOverlaps2D')),<N>            allowed_border=-1,<N>            pos_weight=-1,<N>            debug=False))<N>    self = RotatedAnchorHead(num_classes=4, in_channels=1, train_cfg=cfg)<N><N>
    # Anchor head expects a multiple levels of features per image<N>    feat = [<N>        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))<N>        for i in range(len(self.anchor_generator.strides))<N>    ]<N>    cls_scores, bbox_preds = self.forward(feat)<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import mmcv<N>import pytest<N>import torch<N><N>from mmrotate.models.dense_heads import SAMRepPointsHead<N><N><N>@pytest.mark.skipif(<N>    not torch.cuda.is_available(), reason='requires CUDA support')<N>@pytest.mark.parametrize('reassign', [True, False])<N>def test_sam_head_loss(reassign):<N>    """Tests sam head loss when truth is empty and non-empty.<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import tempfile<N><N>from mmrotate.utils import find_latest_checkpoint<N><N><N>def test_find_latest_checkpoint():<N>    """Test find latest checkpoint."""<N>    with tempfile.TemporaryDirectory() as tmpdir:<N>        path = tmpdir<N>        latest = find_latest_checkpoint(path)<N>        # There are no checkpoints in the path.<N>        assert latest is None<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>"""<N>CommandLine:<N>    pytest tests/test_utils/test_ranchor.py<N>    xdoctest tests/test_utils/test_ranchor.py zero<N>"""<N>import torch<N><N><N>def test_standard_prior_generator():<N>    """Test standard prior generator."""<N>    from mmrotate.core.anchor import build_prior_generator<N>    anchor_generator_cfg = dict(<N>        type='RotatedAnchorGenerator',<N>        scales=[8],<N>        ratios=[0.5, 1.0, 2.0],<N>        strides=[4, 8])<N><N>
    anchor_generator = build_prior_generator(anchor_generator_cfg)<N>    assert anchor_generator.num_base_priors == \<N>           anchor_generator.num_base_anchors<N>    assert anchor_generator.num_base_priors == [3, 3]<N>    assert anchor_generator is not None<N><N>
<N>def test_strides():<N>    """Test strides."""<N>    from mmrotate.core import RotatedAnchorGenerator<N>    # Square strides<N>    self = RotatedAnchorGenerator([10], [1.], [1.], [10])<N>    anchors = self.grid_priors([(2, 2)], device='cpu')<N><N>    expected_anchors = torch.tensor([[0., 0., 10., 10., 0.],<N>                                     [10., 0., 10., 10., 0.],<N>                                     [0., 10., 10., 10., 0.],<N>                                     [10., 10., 10., 10., 0.]])<N><N>
    assert torch.equal(anchors[0], expected_anchors)<N><N>    # Different strides in x and y direction<N>    self = RotatedAnchorGenerator([(10, 20)], [1.], [1.], [10])<N>    anchors = self.grid_priors([(2, 2)], device='cpu')<N><N>    expected_anchors = torch.tensor([[0., 0., 10., 10., 0.],<N>                                     [10., 0., 10., 10., 0.],<N>                                     [0., 20., 10., 10., 0.],<N>                                     [10., 20., 10., 10., 0.]])<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import argparse<N>import copy<N>import os<N>import os.path as osp<N>import time<N>import warnings<N><N>import mmcv<N>import torch<N>from mmcv import Config, DictAction<N>from mmcv.runner import get_dist_info, init_dist<N>from mmcv.utils import get_git_hash<N>from mmdet import __version__<N>from mmdet.apis import init_random_seed, set_random_seed<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import argparse<N>import json<N>from collections import defaultdict<N><N>import matplotlib.pyplot as plt<N>import numpy as np<N><N>try:<N>    import seaborn as sns<N>except ImportError:<N>    sns = None<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N># Written by jbwang1997<N># Reference: https://github.com/jbwang1997/BboxToolkit<N><N>import argparse<N>import codecs<N>import datetime<N>import itertools<N>import json<N>import logging<N>import os<N>import os.path as osp<N>import time<N>from functools import partial, reduce<N>from math import ceil<N>from multiprocessing import Manager, Pool<N><N>
import cv2<N>import numpy as np<N>from PIL import Image<N><N>Image.MAX_IMAGE_PIXELS = None<N><N>try:<N>    import shapely.geometry as shgeo<N>except ImportError:<N>    shgeo = None<N><N><N>def add_parser(parser):<N>    """Add arguments."""<N>    parser.add_argument(<N>        '--base_json',<N>        type=str,<N>        default=None,<N>        help='json config file for split images')<N>    parser.add_argument(<N>        '--nproc', type=int, default=10, help='the procession number')<N><N>
# Copyright (c) OpenMMLab. All rights reserved.<N>import argparse<N>import subprocess<N><N>import torch<N><N><N>def parse_args():<N>    """Parse parameters."""<N>    parser = argparse.ArgumentParser(<N>        description='Process a checkpoint to be published')<N>    parser.add_argument('in_file', help='input checkpoint filename')<N>    parser.add_argument('out_file', help='output checkpoint filename')<N>    args = parser.parse_args()<N>    return args<N><N>
